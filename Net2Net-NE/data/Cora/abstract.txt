several computer algorithms for discovering patterns in groups of protein sequences are in use that are based on fitting the parameters of a statistical model to a group of related sequences . these include hidden markov model ( hmm ) algorithms for multiple sequence alignment , and the meme and gibbs sampler algorithms for discovering motifs . these algorithms are sometimes prone to producing models that are incorrect because two or more patterns have been combined . the statistical model produced in this situation is a convex combination ( weighted average ) of two or more different models . this paper presents a solution to the problem of convex combinations in the form of a heuristic based on using extremely low variance dirichlet mixture priors as part of the statistical model . this heuristic , which we call the megaprior heuristic , increases the strength ( i . e . , decreases the variance ) of the prior in proportion to the size of the sequence dataset . this causes each column in the final model to strongly resemble the mean of a single component of the prior , regardless of the size of the dataset . we describe the cause of the convex combination problem , analyze it mathematically , motivate and describe the implementation of the megaprior heuristic , and show how it can effectively eliminate the problem of convex combinations in protein sequence pattern discovery .
in cellular telephone systems , an important problem is to dynamically allocate the communication resource ( channels ) so as to maximize service in a stochastic caller environment . this problem is naturally formulated as a dynamic programming problem and we use a reinforcement learning ( rl ) method to find dynamic channel allocation policies that are better than previous heuristic solutions . the policies obtained perform well for a broad variety of call traffic patterns . we present results on a large cellular system in cellular communication systems , an important problem is to allocate the communication resource ( bandwidth ) so as to maximize the service provided to a set of mobile callers whose demand for service changes stochastically . a given geographical area is divided into mutually disjoint cells , and each cell serves the calls that are within its boundaries ( see figure #NUM# a ) . the total system bandwidth is divided into channels , with each channel centered around a frequency . each channel can be used simultaneously at different cells , provided these cells are sufficiently separated spatially , so that there is no interference between them . the minimum separation distance between simultaneous reuse of the same channel is called the channel reuse constraint . when a call requests service in a given cell either a free channel ( one that does not violate the channel reuse constraint ) may be assigned to the call , or else the call is blocked from the system ; this will happen if no free channel can be found . also , when a mobile caller crosses from one cell to another , the call is " handed off " to the cell of entry ; that is , a new free channel is provided to the call at the new cell . if no such channel is available , the call must be dropped / disconnected from the system . one objective of a channel allocation policy is to allocate the available channels to calls so that the number of blocked calls is minimized . an additional objective is to minimize the number of calls that are dropped when they are handed off to a busy cell . these two objectives must be weighted appropriately to reflect their relative importance , since dropping existing calls is generally more undesirable than blocking new calls . with approximately #NUM# #NUM# states .
in this paper , we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains . we begin by introducing the theory of markov decision processes ( mdps ) and partially observable mdps ( pomdps ) . we then outline a novel algorithm for solving pomdps off line and show how , in some cases , a finite - memory controller can be extracted from the solution to a pomdp . we conclude with a discussion of how our approach relates to previous work , the complexity of finding exact solutions to pomdps , and of some possibilities for finding approximate solutions . consider the problem of a robot navigating in a large office building . the robot can move from hallway intersection to intersection and can make local observations of its world . its actions are not completely reliable , however . sometimes , when it intends to move , it stays where it is or goes too far ; sometimes , when it intends to turn , it overshoots . it has similar problems with observation . sometimes a corridor looks like a corner ; sometimes a t - junction looks like an l - junction . how can such an error - plagued robot navigate , even given a map of the corridors ? in general , the robot will have to remember something about its history of actions and observations and use this information , together with its knowledge of the underlying dynamics of the world ( the map and other information ) , to maintain an estimate of its location . many engineering applications follow this approach , using methods like the kalman filter [ #NUM# ] to maintain a running estimate of the robot ' s spatial uncertainty , expressed as an ellipsoid or normal distribution in cartesian space . this approach will not do for our robot , though . its uncertainty may be discrete : it might be almost certain that it is in the north - east corner of either the fourth or the seventh floors , though it admits a chance that it is on the fifth floor , as well . then , given an uncertain estimate of its location , the robot has to decide what actions to take . in some cases , it might be sufficient to ignore its uncertainty and take actions that would be appropriate for the most likely location . in other cases , it might be better for
graphical models enhance the representational power of probability models through qualitative characterization of their properties . this also leads to greater efficiency in terms of the computational algorithms that empower such representations . the increasing complexity of these models , however , quickly renders exact probabilistic calculations infeasible . we propose a principled framework for approximating graphical models based on variational methods . we develop variational techniques from the perspective that unifies and expands their applicability to graphical models . these methods allow the ( recursive ) computation of upper and lower bounds on the quantities of interest . such bounds yield considerably more information than mere approximations and provide an inherent error metric for tailoring the approximations individually to the cases considered . these desirable properties , concomitant to the variational methods , are unlikely to arise as a result of other deterministic or stochastic approximations .
real - time decision algorithms are a class of incremental resource - bounded [ horvitz , #NUM# ] or anytime [ dean , #NUM# ] algorithms for evaluating influence diagrams . we present a test domain for real - time decision algorithms , and the results of experiments with several real - time decision algorithms in this domain . the results demonstrate high performance for two algorithms , a decision - evaluation variant of incremental probabilisitic inference [ dambrosio , #NUM# ] and a variant of an algorithm suggested by goldszmidt , [ goldszmidt , #NUM# ] , pk - reduced . we discuss the implications of these experimental results and explore the broader applicability of these algorithms .
speedup learning seeks to improve the computational efficiency of problem solving with experience . in this paper , we develop a formal framework for learning efficient problem solving from random problems and their solutions . we apply this framework to two different representations of learned knowledge , namely control rules and macro - operators , and prove theorems that identify sufficient conditions for learning in each representation . our proofs are constructive in that they are accompanied with learning algorithms . our framework captures both empirical and explanation - based speedup learning in a unified fashion . we illustrate our framework with implementations in two domains : symbolic integration and eight puzzle . this work integrates many strands of experimental and theoretical work in machine learning , including empirical learning of control rules , macro - operator learning ,
in a previous paper , we showed how finite automata could be used to define objective functions for assessing the quality of an alignment of two ( or more ) sequences . in this paper , we show some results of using such cost functions . we also show how to extend hischberg ' s linear space algorithm [ hir #NUM# ] to this setting , thus generalizing a result of myers and miller .
we present an off - line variant of the mistake - bound model of learning . just like in the well studied on - line model , a learner in the offline model has to learn an unknown concept from a sequence of elements of the instance space on which he makes " guess and test " trials . in both models , the aim of the learner is to make as few mistakes as possible . the difference between the models is that , while in the on - line model only the set of possible elements is known , in the off - line model the sequence of elements ( i . e . , the identity of the elements as well as the order in which they are to be presented ) is known to the learner in advance . we give a combinatorial characterization of the number of mistakes in the off - line model . we apply this characterization to solve several natural questions that arise for the new model . first , we compare the mistake bounds of an off - line learner to those of a learner learning the same concept classes in the on - line scenario . we show that the number of mistakes in the on - line learning is at most a log n factor more than the off - line learning , where n is the length of the sequence . in addition , we show that if there is an off - line algorithm that does not make more than a constant number of mistakes for each sequence then there is an online algorithm that also does not make more than a constant number of mistakes . the second issue we address is the effect of the ordering of the elements on the number of mistakes of an off - line learner . it turns out that there are sequences on which an off - line learner can guarantee at most one mistake , yet a permutation of the same sequence forces him to err on many elements . we prove , however , that the gap , between the off - line mistake bounds on permutations of the same sequence of n - many elements , cannot be larger than a multiplicative factor of log n , and we present examples that obtain such a gap .
wahba , wang , gu , klein and klein ( #NUM# ) introduced smoothing spline analysis of variance ( ss anova ) method for data from exponential families . based on rkpack , which fits ss anova models to gaussian data , we introduce grkpack : a collection of fortran subroutines for binary , binomial , poisson and gamma data . we also show how to calculate bayesian confidence intervals for ss anova estimates .
this paper presents an evolutionary approach and an incremental approach to find learning rules of several supervised learning tasks . in evolutionary approach potential solutions are represented as variable length mathematical ( lisp s - ) expressions . thus , it is similar to genetic programming ( gp ) but it employs only a fixed set of non - problem specific functions to solve a variety of problems . the model is tested on three monks ' and parity problems . the results indicate the usefulness of the encoding schema in discovering learning rules for simple supervised learning problems . however , hard learning problems require special attention in terms of their need for larger size codings of the potential solutions and their ability of generalisa - tion over the testing set . in order to find better solutions to these issues , a hill climbing strategy with an incremental coding of potential solutions is used in discovering learning rules for the same problems . it is found that with this strategy larger solutions can easily be coded for with less computational effort . although a better performance is achieved in training for the hard learning problems , the ability of the generalisation over the testing cases is observed to be poor .
the overfit problem in inductive learning and the utility problem in speedup learning both describe a common behavior of machine learning methods : the eventual degradation of performance due to increasing amounts of learned knowledge . plotting the performance of the changing knowledge during execution of a learning method ( the performance response ) reveals similar curves for several methods . the performance response generally indicates an increase to a single peak followed by a more gradual decrease in performance . the similarity in performance responses suggests a model relating performance to the amount of learned knowledge . this paper provides empirical evidence for the existence of a general model by plotting the performance responses of several learning programs . formal models of the performance response are also discussed . these models can be used to control the amount of learning and avoid degradation of performance .
hidden markov models ( hmms ) are applied to the problems of statistical modeling , database searching and multiple sequence alignment of protein families and protein domains . these methods are demonstrated on the globin family , the protein kinase catalytic domain , and the ef - hand calcium binding motif . in each case the parameters of an hmm are estimated from a training set of unaligned sequences . after the hmm is built , it is used to obtain a multiple alignment of all the training sequences . it is also used to search the swiss - prot #NUM# database for other sequences that are members of the given protein family , or contain the given domain . the hmm produces multiple alignments of good quality that agree closely with the alignments produced by programs that incorporate three - dimensional structural information . when employed in discrimination tests ( by examining how closely the sequences in a database fit the globin , kinase and ef - hand hmms ) , the hmm is able to distinguish members of these families from non - members with a high degree of accuracy . both the hmm and pro - filesearch ( a technique used to search for relationships between a protein sequence and multiply aligned sequences ) perform better in these tests than prosite ( a dictionary of sites and patterns in proteins ) . the hmm appears to have a slight advantage
this paper explores the effect of initial weight selection on feed - forward networks learning simple functions with the back - propagation technique . we first demonstrate , through the use of monte carlo techniques , that the magnitude of the initial condition vector ( in weight space ) is a very significant parameter in convergence time variability . in order to further understand this result , additional deterministic experiments were performed . the results of these experiments demonstrate the extreme sensitivity of back propagation to initial weight configuration .
some forms of memory rely temporarily on a system of brain structures located in the medial temporal lobe that includes the hippocampus . the recall of recent events is one task that relies crucially on the proper functioning of this system . as the event becomes less recent , the medial temporal lobe becomes less critical to the recall of the event , and the recollection appears to rely more upon the neocortex . it has been proposed that a process called consolidation is responsible for transfer of memory from the medial temporal lobe to the neocortex . we examine a network model proposed by p . alvarez and l . squire designed to incorporate some of the known features of consolidation , and propose several possible experiments intended to help evaluate the performance of this model under more realistic conditions . finally , we implement an extended version of the model that can accommodate varying assumptions about the number of areas and connections within the brain and memory capacity , and examine the performance of our model on alvarez and squire ' s original task .
the map from eye to brain in vertebrates is topographic , i . e . neighbouring points in the eye map to neighbouring points in the brain . in addition , when two eyes innervate the same target structure , the two sets of fibres segregate to form ocular dominance stripes . experimental evidence from the frog and goldfish suggests that these two phenomena may be subserved by the same mechanisms . we present a computational model that addresses the formation of both topography and ocular dominance . the model is based on a form of competitive learning with subtractive enforcement of a weight normalization rule . inputs to the model are distributed patterns of activity presented simultaneously in both eyes . an important aspect of this model is that ocular dominance segregation can occur when the two eyes are positively correlated , whereas previous models have tended to assume zero or negative correlations between the eyes . this allows investigation of the dependence of the pattern of stripes on the degree of correlation between the eyes : we find that increasing correlation leads to narrower stripes . experiments are suggested to test this prediction .
we examine methods to estimate the average and variance of test error rates over a set of classifiers . we begin with the process of drawing a classifier at random for each example . given validation data , the average test error rate can be estimated as if validating a single classifier . given the test example inputs , the variance can be computed exactly . next , we consider the process of drawing a classifier at random and using it on all examples . once again , the expected test error rate can be validated as if validating a single classifier . however , the variance must be estimated by validating all classifers , which yields loose or uncertain bounds .
we consider formal models of learning from noisy data . specifically , we focus on learning in the probability approximately correct model as defined by valiant . two of the most widely studied models of noise in this setting have been classification noise and malicious errors . however , a more realistic model combining the two types of noise has not been formalized . we define a learning environment based on a natural combination of these two noise models . we first show that hypothesis testing is possible in this model . we next describe a simple technique for learning in this model , and then describe a more powerful technique based on statistical query learning . we show that the noise tolerance of this improved technique is roughly optimal with respect to the desired learning accuracy and that it provides a smooth tradeoff between the tolerable amounts of the two types of noise . finally , we show that statistical query simulation yields learning algorithms for other combinations of noise models , thus demonstrating that statistical query specification truly an important goal of research in machine learning is to determine which tasks can be automated , and for those which can , to determine their information and computation requirements . one way to answer these questions is through the development and investigation of formal models of machine learning which capture the task of learning under plausible assumptions . in this work , we consider the formal model of learning from examples called " probably approximately correct " ( pac ) learning as defined by valiant [ val #NUM# ] . in this setting , a learner attempts to approximate an unknown target concept simply by viewing positive and negative examples of the concept . an adversary chooses , from some specified function class , a hidden f #NUM# ; #NUM# g - valued target function defined over some specified domain of examples and chooses a probability distribution over this domain . the goal of the learner is to output in both polynomial time and with high probability , an hypothesis which is " close " to the target function with respect to the distribution of examples . the learner gains information about the target function and distribution by interacting with an example oracle . at each request by the learner , this oracle draws an example randomly according to the hidden distribution , labels it according to the hidden target function , and returns the labelled example to the learner . a class of functions f is said to be pac learnable if captures the generic fault tolerance of a learning algorithm .
we present a decision tree based approach to function approximation in reinforcement learning . we compare our approach with table lookup and a neural network function approximator on three problems : the well known mountain car and pole balance problems as well as a simulated automobile race car . we find that the decision tree can provide better learning performance than the neural network function approximation and can solve large problems that are infeasible using table lookup .
an approach to develop new game playing strategies based on artificial evolution of neural networks is presented . evolution was directed to discover strategies in othello against a random - moving opponent and later against an ff - fi search program . the networks discovered first a standard positional strategy , and subsequently a mobility strategy , an advanced strategy rarely seen outside of tournaments . the latter discovery demonstrates how evolutionary neural networks can develop novel solutions by turning an initial disadvantage into an advantage in a changed environment .
we derive general bounds on the complexity of learning in the statistical query model and in the pac model with classification noise . we do so by considering the problem of boosting the accuracy of weak learning algorithms which fall within the statistical query model . this new model was introduced by kearns [ #NUM# ] to provide a general framework for efficient pac learning in the presence of classification noise . we first show a general scheme for boosting the accuracy of weak sq learning algorithms , proving that weak sq learning is equivalent to strong sq learning . the boosting is efficient and is used to show our main result of the first general upper bounds on the complexity of strong sq learning . specifically , we derive simultaneous upper bounds with respect to * on the number of queries , o ( log #NUM# #NUM# * ) , the vapnik - chervonenkis dimension of the query space , o ( log #NUM# * ) , and the inverse of the minimum tolerance , o ( #NUM# * log #NUM# * ) . in addition , we show that these general upper bounds are nearly optimal by describing a class of learning problems for which we simultaneously lower bound the number of queries by ( log #NUM# * ) we further apply our boosting results in the sq model to learning in the pac model with classification noise . since nearly all pac learning algorithms can be cast in the sq model , we can apply our boosting techniques to convert these pac algorithms into highly efficient sq algorithms . by simulating these efficient sq algorithms in the pac model with classification noise , we show that nearly all pac algorithms can be converted into highly efficient pac algorithms which tolerate classification noise . we give an upper bound on the sample complexity of these noise - tolerant pac algorithms which is nearly optimal with respect to the noise rate . we also give upper bounds on space complexity and hypothesis size and show that these two measures are in fact independent of the noise rate . we note that the running times of these noise - tolerant pac algorithms are efficient . this sequence of simulations also demonstrates that it is possible to boost the accuracy of nearly all pac algorithms even in the presence of noise . this provides a partial answer to an open problem of schapire [ #NUM# ] and the first theoretical evidence for an empirical result of drucker , schapire and simard [ #NUM# ] .
the tremendous current effort to propose neurally inspired methods of computation forces closer scrutiny of real world application potential of these models . this paper categorizes applications into classes and particularly discusses features of applications which make them efficiently amenable to neural network methods . computational machines do deterministic mappings of inputs to outputs and many computational mechanisms have been proposed for problem solutions . neural network features include parallel execution , adaptive learning , generalization , and fault tolerance . often , much effort is given to a model and applications which can already be implemented in a much more efficient way with an alternate technology . neural networks are potentially powerful devices for many classes of applications , but not all . however , it is proposed that the class of applications for which neural networks are efficient is both large and commonly occurring in nature . comparison of supervised , unsupervised , and generalizing systems is also included .
recurrent neural networks have become popular models for system identification and time series prediction . narx ( nonlinear autoregressive models with exogenous inputs ) neural network models are a popular subclass of recurrent networks and have been used in many applications . though embedded memory can be found in all recurrent network models , it is particularly prominent in narx models . we show that using intelligent memory order selection through pruning and good initial heuristics significantly improves the generalization and predictive performance of these nonlinear systems on problems as diverse as grammatical inference and time series prediction .
this paper presents an incremental concept learning approach to identiflcation of concepts with high overall accuracy . the main idea is to address the concept overlap as a central problem when learning multiple descriptions . many traditional inductive algorithms , as those from the disjunctive version space family considered here , face this problem . the approach focuses on combinations of confldent , possibly overlapping , concepts with an original stochastic complexity formula . the focusing is e - cient because it is organized as a simulated annealing - based beam search . the experiments show that the approach is especially suitable for developing incremental learning algorithms with the following advantages : flrst , it generates highly accurate concepts ; second , it overcomes to a certain degree the sensitivity to the order of examples ; and third , it handles noisy examples .
case - based reasoning ( cbr ) has a great deal to offer in supporting creative design , particularly processes that rely heavily on previous design experience , such as framing the problem and evaluating design alternatives . however , most existing cbr systems are not living up to their potential . they tend to adapt and reuse old solutions in routine ways , producing robust but uninspired results . little research effort has been directed towards the kinds of situation assessment , evaluation , and assimilation processes that facilitate the exploration of ideas and the elaboration and redefinition of problems that are crucial to creative design . also , their typically rigid control structures do not facilitate the kinds of strategic control and opportunism inherent in creative reasoning . in this paper , we describe the types of behavior we would like case - based design systems to support , based on a study of designers working on a mechanical engineering problem . we show how the standard cbr framework should be extended and we describe an architecture we are developing to experiment with these ideas . #NUM#
in this paper we present a framework for building probabilistic automata parameterized by context - dependent probabilities . gibbs distributions are used to model state transitions and output generation , and parameter estimation is carried out using an em algorithm where the m - step uses a generalized iterative scaling procedure . we discuss relations with certain classes of stochastic feedforward neural networks , a geometric interpretation for parameter estimation , and a simple example of a statistical language model constructed using this methodology .
one of the characteristics of design is that designers rely extensively on past experience in order to create new designs . because of this , memory - based techniques from artificial intelligence , which help store , organise , retrieve , and reuse experiential knowledge held in memory , are good candidates for aiding designers . another characteristic of design is the phenomenon of exploration in the early stages of design configuration . a designer begins with an ill - structured , partially defined , problem specification , and through a process of exploration gradually refines and modifies it as his / her understanding of the problem improves . in this paper we describe demex , an interactive computer - aided design system that employs memory - based techniques to help its users explore the design problems they pose to the system , in order to acquire a better understanding of the requirements of the problems . demex has been applied in the domain of structural design of buildings .
up - propagation is an algorithm for inverting and learning neural network generative models . sensory input is processed by inverting a model that generates patterns from hidden variables using top - down connections . the inversion process is iterative , utilizing a negative feedback loop that depends on an error signal propagated by bottom - up connections . the error signal is also used to learn the generative model from examples . the algorithm is benchmarked against principal component analysis in in his doctrine of unconscious inference , helmholtz argued that perceptions are formed by the interaction of bottom - up sensory data with top - down expectations . according to one interpretation of this doctrine , perception is a procedure of sequential hypothesis testing . we propose a new algorithm , called up - propagation , that realizes this interpretation in layered neural networks . it uses top - down connections to generate hypotheses , and bottom - up connections to revise them . it is important to understand the difference between up - propagation and its ancestor , the backpropagation algorithm [ #NUM# ] . backpropagation is a learning algorithm for recognition models . as shown in figure #NUM# a , bottom - up connections recognize patterns , while top - down connections propagate an error signal that is used to learn the recognition model . in contrast , up - propagation is an algorithm for inverting and learning generative models , as shown in figure #NUM# b . top - down connections generate patterns from a set of hidden variables . sensory input is processed by inverting the generative model , recovering hidden variables that could have generated the sensory data . this operation is called either pattern recognition or pattern analysis , depending on the meaning of the hidden variables . inversion of the generative model is done iteratively , through a negative feedback loop driven by an error signal from the bottom - up connections . the error signal is also used for learning the connections experiments on images of handwritten digits .
this paper demonstrates the exploitation of certain vision processing techniques to index into a case base of surfaces . the surfaces are the result of reinforcement learning and represent the optimum choice of actions to achieve some goal from anywhere in the state space . this paper shows how strong features that occur in the interaction of the system with its environment can be detected early in the learning process . such features allow the system to identify when an identical , or very similar , task has been solved previously and to retrieve the relevant surface . this results in an orders of magnitude increase in learning rate .
combining different machine learning algorithms in the same system can produce benefits above and beyond what either method could achieve alone . this paper demonstrates that genetic algorithms can be used in conjunction with lazy learning to solve examples of a difficult class of delayed reinforcement learning problems better than either method alone . this class , the class of differential games , includes numerous important control problems that arise in robotics , planning , game playing , and other areas , and solutions for differential games suggest solution strategies for the general class of planning and control problems . we conducted a series of experiments applying three learning approaches | lazy q - learning , k - nearest neighbor ( k - nn ) , and a genetic algorithm | to a particular differential game called a pursuit game . our experiments demonstrate that k - nn had great difficulty solving the problem , while a lazy version of q - learning performed moderately well and the genetic algorithm performed even better . these results motivated the next step in the experiments , where we hypothesized k - nn was having difficulty because it did not have good examples - a common source of difficulty for lazy learning . therefore , we used the genetic algorithm as a bootstrapping method for k - nn to create a system to provide these examples . our experiments demonstrate that the resulting joint system learned to solve the pursuit games with a high degree of accuracy - outperforming either method alone - and with relatively small memory requirements .
we first describe a hierarchical , generative model that can be viewed as a non - linear generalisation of factor analysis and can be implemented in a neural network . the model performs perceptual inference in a probabilistically consistent manner by using top - down , bottom - up and lateral connections . these connections can be learned using simple rules that require only locally available information . we then show how to incorporate lateral connections into the generative model . the model extracts a sparse , distributed , hierarchical representation of depth from simplified random - dot stereograms and the localised disparity detectors in the first hidden layer form a topographic map . when presented with image patches from natural scenes , the model develops topo graphically organised local feature detectors .
in most applications of neuro - evolution , each individual in the population represents a complete neural network . recent work on the sane system , however , has demonstrated that evolving individual neurons often produces a more efficient genetic search . this paper demonstrates that while sane can solve easy tasks very quickly , it often stalls in larger problems . a hierarchical approach to neuro - evolution is presented that overcomes sane ' s difficulties by integrating both a neuron - level exploratory search and a network - level exploitive search . in a robot arm manipulation task , the hierarchical approach outperforms both a neuron - based search and a network - based search .
reinforcement learning addresses the problem of learning to select actions in order to maximize one ' s performance in unknown environments . to scale reinforcement learning to complex real - world tasks , such as typically studied in ai , one must ultimately be able to discover the structure in the world , in order to abstract away the myriad of details and to operate in more tractable problem spaces . this paper presents the skills algorithm . skills discovers skills , which are partially defined action policies that arise in the context of multiple , related tasks . skills collapse whole action sequences into single operators . they are learned by minimizing the compactness of action policies , using a description length argument on their representation . empirical results in simple grid navigation tasks illustrate the successful discovery of structure in reinforcement learning .
we consider a generalization of the mistake - bound model ( for learning f #NUM# ; #NUM# g - valued functions ) in which the learner must satisfy a general constraint on the number m + of incorrect #NUM# predictions and the number m of incorrect #NUM# predictions . we describe a general - purpose optimal algorithm for our formulation of this problem . we describe several applications of our general results , involving situations in which the learner wishes to satisfy linear inequalities in m + and m .
a critical issue for users of markov chain monte carlo ( mcmc ) methods in applications is how to determine when it is safe to stop sampling and use the samples to estimate characteristics of the distribution of interest . research into methods of computing theoretical convergence bounds holds promise for the future but currently has yielded relatively little that is of practical use in applied work . consequently , most mcmc users address the convergence problem by applying diagnostic tools to the output produced by running their samplers . after giving a brief overview of the area , we provide an expository review of thirteen convergence diagnostics , describing the theoretical basis and practical implementation of each . we then compare their performance in two simple models and conclude that all the methods can fail to detect the sorts of convergence failure they were designed to identify . we thus recommend a combination of strategies aimed at evaluating and accelerating mcmc sampler convergence , including applying diagnostic procedures to a small number of parallel chains , monitoring autocorrelations and cross - correlations , and modifying parameterizations or sampling algorithms appropriately . we emphasize , however , that it is not possible to say with certainty that a finite sample from an mcmc algorithm is representative of an underlying stationary distribution . mary kathryn cowles is assistant professor of biostatistics , harvard school of public health , boston , ma #NUM# . bradley p . carlin is associate professor , division of biostatistics , school of public health , university of minnesota , minneapolis , mn #NUM# . much of the work was done while the first author was a graduate student in the divison of biostatistics at the university of minnesota and then assistant professor , biostatistics section , department of preventive and societal medicine , university of nebraska medical center , omaha , ne #NUM# . the work of both authors was supported in part by national institute of allergy and infectious diseases first award #NUM# - r #NUM# - ai #NUM# . the authors thank the developers of the diagnostics studied here for sharing their insights , experiences , and software , and drs . thomas louis and luke tierney for helpful discussions and suggestions which greatly improved the manuscript .
although the detection of invariant structure in a given set of input patterns is vital to many recognition tasks , connectionist learning rules tend to focus on directions of high variance ( principal components ) . the prediction paradigm is often used to reconcile this dichotomy ; here we suggest a more direct approach to invariant learning based on an anti - hebbian learning rule . an unsupervised two - layer network implementing this method in a competitive setting learns to extract coherent depth information from random - dot stereograms .
instance - based learning methods explicitly remember all the data that they receive . they usually have no training phase , and only at prediction time do they perform computation . then , they take a query , search the database for similar datapoints and build an on - line local model ( such as a local average or local regression ) with which to predict an output value . in this paper we review the advantages of instance based methods for autonomous systems , but we also note the ensuing cost : hopelessly slow computation as the database grows large . we present and evaluate a new way of structuring a database and a new algorithm for accessing it that maintains the advantages of instance - based learning . earlier attempts to combat the cost of instance - based learning have sacrificed the explicit retention of all data , or been applicable only to instance - based predictions based on a small number of near neighbors or have had to re - introduce an explicit training phase in the form of an interpolative data structure . our approach builds a multiresolution data structure to summarize the database of experiences at all resolutions of interest simultaneously . this permits us to query the database with the same exibility as a conventional linear search , but at greatly reduced computational cost .
discrete bayesian models have been used to model uncertainty for mobile - robot navigation , but the question of how actions should be chosen remains largely unexplored . this paper presents the optimal solution to the problem , formulated as a partially observable markov decision process . since solving for the optimal control policy is intractable , in general , it goes on to explore a variety of heuristic control strategies . the control strategies are compared experimentally , both in simulation and in runs on a robot .
this nrl ncarai technical note ( aic - #NUM# - #NUM# ) describes work with salzberg ' s ( #NUM# ) nge . i recently implemented this algorithm and have run a few case studies . the purpose of this note is to publicize this implementation and note a curious result while using it . this implementation of nge is available at under my www address
simulated annealing | moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions | has traditionally been used as an inexact method of handling isolated modes in markov chain samplers . here , it is shown how one can use the markov chain transitions for such an annealing sequence to define an importance sampler . the markov chain aspect allows this method to perform acceptably even for high - dimensional problems , where finding good importance sampling distributions would otherwise be very difficult , while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases . this annealed importance sampling procedure resembles the second half of the previously - studied tempered transitions , and can be seen as a generalization of a recently - proposed variant of sequential importance sampling . it is also related to thermodynamic integration methods for estimating ratios of normalizing constants . annealed importance sampling is most attractive when isolated modes are present , or when estimates of normalizing constants are required , but it may also be more generally useful , since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in markov chain samplers .
we describe an ongoing project to develop an adaptive training system ( ats ) that dynamically models a students learning processes and can provide specialized tutoring adapted to a students knowledge state and learning style . the student modeling component of the ats , ml - modeler , uses machine learning ( ml ) techniques to emulate the students novice - to - expert transition . ml - modeler infers which learning methods the student has used to reach the current knowledge state by comparing the students solution trace to an expert solution and generating plausible hypotheses about what misconceptions and errors the student has made . a case - based approach is used to generate hypotheses through incorrectly applying analogy , overgeneralization , and overspecialization . the student and expert models use a network - based representation that includes abstract concepts and relationships as well as strategies for problem solving . fuzzy methods are used to represent the uncertainty in the student model . this paper describes the design of the ats and ml - modeler , and gives a detailed example of how the system would model and tutor the student in a typical session . the domain we use for this example is high - school level chemistry .
metacognition addresses the issues of knowledge about cognition and regulating cognition . we argue that the regulation process should be improved with growing experience . therefore mental models are needed which facilitate the re - use of previous regulation processes . we will satisfy this requirement by describing a case - based approach to introspection planning which utilises previous experience obtained during reasoning at the meta - level and at the object level . the introspection plans used in this approach support various metacognitive tasks which are identified by the generation of self - questions . as an example of introspection planning , the metacognitive behaviour of our system , iulian , is described .
graphical markov models use graphs , either undirected , directed , or mixed , to represent possible dependences among statistical variables . applications of undirected graphs ( udgs ) include models for spatial dependence and image analysis , while acyclic directed graphs ( adgs ) , which are especially convenient for statistical analysis , arise in such fields as genetics and psychometrics and as models for expert systems and bayesian belief networks . lauritzen , wer - muth , and frydenberg ( lwf ) introduced a markov property for chain graphs , which are mixed graphs that can be used to represent simultaneously both causal and associative dependencies and which include both udgs and adgs as special cases . in this paper an alternative markov property ( amp ) for chain graphs is introduced , which in some ways is a more direct extension of the adg markov property than is the lwf property for chain graph .
the fault hierarchy representation is widely used in expert systems for the diagnosis of complex mechanical devices . on the assumption that an appropriate bias for a knowledge representation language is also an appropriate bias for learning in this domain , we have developed a theory revision method that operates directly on a fault hierarchy . this task presents several challenges : a typical training instance is missing most feature values , and the pattern of missing features is significant , rather than merely an effect of noise . moreover , the accuracy of a candidate theory is measured by considering both the sequence of tests required to arrive at a diagnosis and its agreement with the diagnostic endpoints provided by an expert . this paper first describes the algorithm for theory revision of fault hierarchies that was designed to address these challenges , then discusses its application in knowledge base maintenance and reports on experiments that use to revise a fielded diagnostic system .
machine learning of game strategies has often depended on competitive methods that continually develop new strategies capable of defeating previous ones . we use a very inclusive definition of game and consider a framework within which a competitive algorithm makes repeated use of a strategy learning component that can learn strategies which defeat a given set of opponents . we describe game learning in terms of sets h and x of first and second player strategies , and connect the model with more familiar models of concept learning . we show the importance of the ideas of teaching set [ #NUM# ] and specification number [ #NUM# ] k in this new context . the performance of several competitive algorithms is investigated , using both worst - case and randomized strategy learning algorithms . our central result ( theorem #NUM# ) is a competitive algorithm that solves games in a total number of strategies polynomial in lg ( jhj ) , lg ( jx j ) , and k . its use is demonstrated , including an application in concept learning with a new kind of counterexample oracle . we conclude with a complexity analysis of game learning , and list a number of new questions arising from this work .
most of the work which attempts to give bounds on the generalization error of the hypothesis generated by a learning algorithm is based on methods from the theory of uniform convergence . these bounds are a - priori bounds that hold for any distribution of examples and are calculated before any data is observed . in this paper we propose a different approach for bounding the generalization error after the data has been observed . a self - bounding learning algorithm is an algorithm which , in addition to the hypothesis that it outputs , outputs a reliable upper bound on the generalization error of this hypothesis . we first explore the idea in the statistical query learning framework of kearns [ #NUM# ] . after that we give an explicit self bounding algorithm for learning algorithms that are based on local search .
in this paper we propose a new framework for studying markov decision processes ( mdps ) , based on ideas from statistical mechanics . the goal of learning in mdps is to find a policy that yields the maximum expected return over time . in choosing policies , agents must therefore weigh the prospects of short - term versus long - term gains . we study a simple mdp in which the agent must constantly decide between exploratory jumps and local reward mining in state space . the number of policies to choose from grows exponentially with the size of the state space , n . we view the expected returns as defining an energy landscape over policy space . methods from statistical mechanics are used to analyze this landscape in the thermodynamic limit n ! #NUM# . we calculate the overall distribution of expected returns , as well as the distribution of returns for policies at a fixed hamming distance from the optimal one . we briefly discuss the problem of learning optimal policies from empirical estimates of the expected return . as a first step , we relate our findings for the entropy to the limit of high - temperature learning . numerical simulations support the theoretical results .
this paper shows that neural networks which use continuous activation functions have vc dimension at least as large as the square of the number of weights w . this result settles a long - standing open question , namely whether the well - known o ( w log w ) bound , known for hard - threshold nets , also held for more general sigmoidal nets . implications for the number of samples needed for valid gen eralization are discussed .
novel on - line learning algorithms with self adaptive learning rates ( parameters ) for blind separation of signals are proposed . the main motivation for development of new learning rules is to improve convergence speed and to reduce cross - talking , especially for non - stationary signals . furthermore , we have discovered that under some conditions the proposed neural network models with associated learning algorithms exhibit a random switch of attention , i . e . they have ability of chaotic or random switching or cross - over of output signals in such way that a specified separated signal may appear at various outputs at different time windows . validity , performance and dynamic properties of the proposed learning algorithms are investigated by computer simulation experiments .
i present a modular network architecture and a learning algorithm based on incremental dynamic programming that allows a single learning agent to learn to solve multiple markovian decision tasks ( mdts ) with significant transfer of learning across the tasks . i consider a class of mdts , called composite tasks , formed by temporally concatenating a number of simpler , elemental mdts . the architecture is trained on a set of composite and elemental mdts . the temporal structure of a composite task is assumed to be unknown and the architecture learns to produce a temporal decomposition . it is shown that under certain conditions the solution of a composite mdt can be constructed by computationally inexpensive modifications of the solutions of its constituent elemental mdts .
scientists and engineers face recurring problems of constructing , testing and modifying numerical simulation programs . the process of coding and revising such simulators is extremely time - consuming , because they are almost always written in conventional programming languages . scientists and engineers can therefore benefit from software that facilitates construction of programs for simulating physical systems . our research adapts the methodology of deductive program synthesis to the problem of constructing numerical simulation codes . we have focused on simulators that can be represented as second order functional programs composed of numerical integration and root extraction routines . we have developed a system that uses first order horn logic to synthesize numerical simulators built from these components . our approach is based on two ideas : first , we axiomatize only the relationship between integration and differentiation . we neither attempt nor require a complete axiomatization of mathematical analysis . second , our system uses a representation in which functions are reified as objects . function objects are encoded as lambda expressions . our knowledge base includes an axiomatization of term equality in the lambda calculus . it also includes axioms defining the semantics of numerical integration and root extraction routines . we use depth bounded sld resolution to construct proofs and synthesize programs . our system has successfully constructed numerical simulators for computational design of jet engine nozzles and sailing yachts , among others . our results demonstrate that deductive synthesis techniques can be used to construct numerical simulation programs for realistic applications .
bayesian networks provide a language for qualitatively representing the conditional independence properties of a distribution . this allows a natural and compact representation of the distribution , eases knowledge acquisition , and supports effective inference algorithms . it is well - known , however , that there are certain independencies that we cannot capture qualitatively within the bayesian network structure : independencies that hold only in certain contexts , i . e . , given a specific assignment of values to certain variables . in this paper , we propose a formal notion of context - specific independence ( csi ) , based on regularities in the conditional probability tables ( cpts ) at a node . we present a technique , analogous to ( and based on ) d - separation , for determining when such independence holds in a given network . we then focus on a particular qualitative representation schemetree - structured cpts for capturing csi . we suggest ways in which this representation can be used to support effective inference algorithms . in particular , we present a structural decomposition of the resulting network which can improve the performance of clustering algorithms , and an alternative algorithm based on cutset conditioning .
the eligibility trace is one of the basic mechanisms used in reinforcement learning to handle delayed reward . in this paper we introduce a new kind of eligibility trace , the replacing trace , analyze it theoretically , and show that it results in faster , more reliable learning than the conventional trace . both kinds of trace assign credit to prior events according to how recently they occurred , but only the conventional trace gives greater credit to repeated events . our analysis is for conventional and replace - trace versions of the o * ine td ( #NUM# ) algorithm applied to undiscounted absorbing markov chains . first , we show that these methods converge under repeated presentations of the training set to the same predictions as two well known monte carlo methods . we then analyze the relative efficiency of the two monte carlo methods . we show that the method corresponding to conventional td is biased , whereas the method corresponding to replace - trace td is unbiased . in addition , we show that the method corresponding to replacing traces is closely related to the maximum likelihood solution for these tasks , and that its mean squared error is always lower in the long run . computational results confirm these analyses and show that they are applicable more generally . in particular , we show that replacing traces significantly improve performance and reduce parameter sensitivity on the " mountain - car " task , a full reinforcement - learning problem with a continuous state space , when using a feature - based function approximator .
reading has been studied for decades by both artificial intelligent researchers and by cognitive psychologists . yet there still does not exist a theory which accurately describes the process nor a computer system which performs it . to overcome the shortfalls of previous systems , we have developed a functional theory of the reading comprehension process . the theory not only enables us to implement a model of the reading task , the isaac ( integrated story analysis and comprehension ) system ; it also allows us to analyze past systems and their problems more completely and in a common framework . we also use this paper to present our general research philosophy , the current level of the implementation of our ideas , and our plans for future research .
we study the problem of combining updates | a special instance of theory change | and counterfactual conditionals in propositional knowledgebases . intuitively , an update means that the world described by the knowledgebase has changed . this is opposed to revisions | another instance of theory change | where our knowledge about a static world changes . a counterfactual implication is a statement of the form ` if a were the case , then b would also be the case ' , where the negation of a may be derivable from our current knowledge . we present a decidable logic , called vcu #NUM# , that has both update and counterfactual implication as connectives in the object language . our update operator is a generalization of operators previously proposed and studied in the literature . we show that our operator satisfies certain postulates set forth for any reasonable update . the logic vcu #NUM# is an extension of d . k . lewis ' logic vcu for counterfactual conditionals . the semantics of vcu #NUM# is that of a multimodal propositional calculus , and is based on possible worlds . the infamous ramsey rule becomes a derivation rule in our sound and complete axiomatization . we then show that gardenfors ' triviality theorem , about the impossibility to combine theory change and counterfactual conditionals via the ramsey rule , does not hold in our logic . it is thus seen that the triviality theorem applies only to revision operators , not to updates .
many machine learning algorithms aim at finding " simple " rules to explain training data . the expectation is : the " simpler " the rules , the better the generalization on test data ( ! occam ' s razor ) . most practical implementations , however , use measures for " simplicity " that lack the power , universality and elegance of those based on kolmogorov complexity and solomonoff ' s algorithmic probability . likewise , most previous approaches ( especially those of the " bayesian " kind ) suffer from the problem of choosing appropriate priors . this paper addresses both issues . it first reviews some basic concepts of algorithmic complexity theory relevant to machine learning , and how the solomonoff - levin distribution ( or universal prior ) deals with the prior problem . the universal prior leads to a probabilistic method for finding " algorithmically simple " problem solutions with high generalization capability . the method is based on levin complexity ( a time - bounded generalization of kolmogorov complexity ) and inspired by levin ' s optimal universal search algorithm . with a given problem , solution candidates are computed by efficient " self - sizing " programs that influence their own runtime and storage size . the probabilistic search algorithm finds the " good " programs ( the ones quickly computing algorithmically probable solutions fitting the training data ) . simulations focus on the task of discovering " algorithmically simple " neural networks with low kolmogorov complexity and high generalization capability . it is demonstrated that the method , at least with certain toy problems where it is computationally feasible , can lead to generalization results unmatchable by previous neural net algorithms . much remains do be done , however , to make large scale applications and " incremental learning " feasible .
one of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large , and often is observed to decrease even after the training error reaches zero . in this paper , we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule , where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label . we show that techniques used in the analysis of vapnik ' s support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error . we also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples . finally , we compare our explanation to those based on the bias - variance decomposition .
real - world learning tasks may involve high - dimensional data sets with arbitrary patterns of missing data . in this paper we present a framework based on maximum likelihood density estimation for learning from such data sets . we use mixture models for the density estimates and make two distinct appeals to the expectation - maximization ( em ) principle ( dempster et al . , #NUM# ) in deriving a learning algorithm | em is used both for the estimation of mixture components and for coping with missing data . the resulting algorithm is applicable to a wide range of supervised as well as unsupervised learning problems . results from a classification benchmark | the iris data set | are presented .
the hierarchical feature map system recognizes an input story as an instance of a particular script by classifying it at three levels scripts , tracks and role bindings . the recognition taxonomy , i . e . the breakdown of each script into the tracks and roles , is extracted automatically and independently for each script from examples of script instantiations in an unsupervised self - organizing process . the process resembles human learning in that the differentiation of the most frequently encountered scripts become gradually the most detailed . the resulting structure is a hierachical pyramid of feature maps . the hierarchy visualizes the taxonomy and the maps lay out the topology of each level . the number of input lines and the self - organization time are considerably reduced compared to the ordinary single - level feature mapping . the system can recognize incomplete stories and recover the missing events . the taxonomy also serves as memory organization for script - based episodic memory . the maps assign a unique memory location for each script instantiation . the most salient parts of the input data are separated and most resources are concentrated on representing them accurately .
it is shown how ` static ' neural approaches to adaptive target detection can be replaced by a more efficient and more sequential alternative . the latter is inspired by the observation that biological systems employ sequential eye - movements for pattern recognition . a system is described which builds an adaptive model of the time - varying inputs of an artificial fovea controlled by an adaptive neural controller . the controller uses the adaptive model for learning the sequential generation of fovea trajectories causing the fovea to move to a target in a visual scene . the system also learns to track moving targets . no teacher provides the desired activations of ` eye - muscles ' at various times . the only goal information is the shape of the target . since the task is a ` reward - only - at - goal ' task , it involves a complex temporal credit assignment problem . some implications for adaptive attentive systems in general are discussed .
we present a tree - structured architecture for supervised learning . the statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models ( glim ' s ) . learning is treated as a maximum likelihood problem ; in particular , we present an expectation - maximization ( em ) algorithm for adjusting the parameters of the architecture . we also develop an on - line learning algorithm in which the parameters are updated incrementally . comparative simulation results are presented in the robot dynamics domain . * we want to thank geoffrey hinton , tony robinson , mitsuo kawato and daniel wolpert for helpful comments on the manuscript . this project was supported in part by a grant from the mcdonnell - pew foundation , by a grant from atr human information processing research laboratories , by a grant from siemens corporation , by by grant iri - #NUM# from the national science foundation , and by grant n #NUM# - #NUM# - j - #NUM# from the office of naval research . the project was also supported by nsf grant asc - #NUM# in support of the center for biological and computational learning at mit , including funds provided by darpa under the hpcc program , and nsf grant ecs - #NUM# to support an initiative in intelligent control at mit . michael i . jordan is a nsf presidential young investigator .
the em algorithm performs maximum likelihood estimation for data in which some variables are unobserved . we present a function that resembles negative free energy and show that the m step maximizes this function with respect to the model parameters and the e step maximizes it with respect to the distribution over the unobserved variables . from this perspective , it is easy to justify an incremental variant of the em algorithm in which the distribution for only one of the unobserved variables is recalculated in each e step . this variant is shown empirically to give faster convergence in a mixture estimation problem . a variant of the algorithm that exploits sparse conditional distributions is also described , and a wide range of other variant algorithms are also seen to be possible .
a network of wilson - cowan oscillators is constructed , and its emergent properties of synchronization and desynchronization are investigated by both computer simulation and formal analysis . the network is a two - dimensional matrix , where each oscillator is coupled only to its neighbors . we show analytically that a chain of locally coupled oscillators ( the piece - wise linear approximation to the wilson - cowan oscillator ) synchronizes , and present a technique to rapidly entrain finite numbers of oscillators . the coupling strengths change on a fast time scale based on a hebbian rule . a global separator is introduced which receives input from and sends feedback to each oscillator in the matrix . the global separator is used to desynchronize different oscillator groups . unlike many other models , the properties of this network emerge from local connections , that preserve spatial relationships among components , and are critical for encoding gestalt principles of feature grouping . the ability to synchronize and desynchronize oscillator groups within this network offers a promising approach for pattern segmentation and figure / ground segregation based on oscillatory correlation .
in this paper i describe the implementation of a probabilistic regression model in bugs . bugs is a program that carries out bayesian inference on statistical problems using a simulation technique known as gibbs sampling . it is possible to implement surprisingly complex regression models in this environment . i demonstrate the simultaneous inference of an interpolant and an input - dependent noise level .
classifying by hand complex data coming from psychology experiments can be a long and difficult task , because of the quantity of data to classify and the amount of training it may require . one way to alleviate this problem is to use machine learning techniques . we built a classifier based on decision trees that reproduces the classifying process used by two humans on a sample of data and that learns how to classify unseen data . the automatic classifier proved to be more accurate , more constant and much faster than classification by hand .
the estimation or training methods in the neural network literature are usually some simple form of gradient descent algorithm suitable for implementation in hardware using massively parallel computations . for ordinary computers that are not massively parallel , optimization algorithms such as those in several sas procedures are usually far more efficient . this talk shows how to fit neural networks using sas / or r fl , sas / ets r fl , and sas / stat r fl software .
we apply reinforcement learning methods to learn domain - specific heuristics for job shop scheduling . a repair - based scheduler starts with a critical - path schedule and incrementally repairs constraint violations with the goal of finding a short conflict - free schedule . the temporal difference algorithm t d ( ) is applied to train a neural network to learn a heuristic evaluation function over states . this evaluation function is used by a one - step looka - head search procedure to find good solutions to new scheduling problems . we evaluate this approach on synthetic problems and on problems from a nasa space shuttle payload processing task . the evaluation function is trained on problems involving a small number of jobs and then tested on larger problems . the td sched - uler performs better than the best known existing algorithm for this task | zweben ' s iterative repair method based on simulated annealing . the results suggest that reinforcement learning can provide a new method for constructing high - performance scheduling systems .
a neural network approach to the classic inverted pendulum task is presented . this task is the task of keeping a rigid pole , hinged to a cart and free to fall in a plane , in a roughly vertical orientation by moving the cart horizontally in the plane while keeping the cart within some maximum distance of its starting position . this task constitutes a difficult control problem if the parameters of the cart - pole system are not known precisely or are variable . it also forms the basis of an even more complex control - learning problem if the controller must learn the proper actions for successfully balancing the pole given only the current state of the system and a failure signal when the pole angle from the vertical becomes too great or the cart exceeds one of the boundaries placed on its position . the approach presented is demonstrated to be effective for the real - time control of a small , self - contained mini - robot , specially outfitted for the task . origins and details of the learning scheme , specifics of the mini - robot hardware , and results of actual learning trials are presented .
platt ' s resource - allocation network ( ran ) ( platt , #NUM# a , #NUM# b ) is modified for a reinforcement - learning paradigm and to " restart " existing hidden units rather than adding new units . after restarting , units continue to learn via back - propagation . the resulting restart algorithm is tested in a q - learning network that learns to solve an inverted pendulum problem . solutions are found faster on average with the restart algorithm than without it .
algorithms based on nested generalized exemplar ( nge ) theory ( salzberg , #NUM# ) classify new data points by computing their distance to the nearest " generalized exemplar " ( i . e . , either a point or an axis - parallel rectangle ) . they combine the distance - based character of nearest neighbor ( nn ) classifiers with the axis - parallel rectangle representation employed in many rule - learning systems . an implementation of nge was compared to the k - nearest neighbor ( knn ) algorithm in #NUM# domains and found to be significantly inferior to knn in #NUM# of them . several modifications of nge were studied to understand the cause of its poor performance . these show that its performance can be substantially improved by preventing nge from creating overlapping rectangles , while still allowing complete nesting of rectangles . performance can be further improved by modifying the distance metric to allow weights on each of the features ( salzberg , #NUM# ) . best results were obtained in this study when the weights were computed using mutual information between the features and the output class . the best version of nge developed is a batch algorithm ( bnge fw mi ) that has no user - tunable parameters . bnge fw mi ' s performance is comparable to the first - nearest neighbor algorithm ( also incorporating feature weights ) . however , the k - nearest neighbor algorithm is still significantly superior to bnge fw mi in #NUM# of the #NUM# domains , and inferior to it in only #NUM# . we conclude that , even with our improvements , the nge approach is very sensitive to the shape of the decision boundaries in classification problems . in domains where the decision boundaries are axis - parallel , the nge approach can produce excellent generalization with interpretable hypotheses . in all domains tested , nge algorithms require much less memory to store generalized exemplars than is required by nn algorithms .
selecting a good model of a set of input points by cross validation is a computationally intensive process , especially if the number of possible models or the number of training points is high . techniques such as gradient descent are helpful in searching through the space of models , but problems such as local minima , and more importantly , lack of a distance metric between various models reduce the applicability of these search methods . hoeffding races is a technique for finding a good model for the data by quickly discarding bad models , and concentrating the computational effort at differentiating between the better ones . this paper focuses on the special case of leave - one - out cross validation applied to memory - based learning algorithms , but we also argue that it is applicable to any class of model selection problems .
in many learning problems , the learning system is presented with values for features that are actually irrelevant to the concept it is trying to learn . the focus algorithm , due to almuallim and dietterich , performs an explicit search for the smallest possible input feature set s that permits a consistent mapping from the features in s to the output feature . the focus algorithm can also be seen as an algorithm for learning determinations or functional dependencies , as suggested in [ #NUM# ] . another algorithm for learning determinations appears in [ #NUM# ] . the focus algorithm has superpolynomial runtime , but almuallim and di - etterich leave open the question of tractability of the underlying problem . in this paper , the problem is shown to be np - complete . we also describe briefly some experiments that demonstrate the benefits of determination learning , and show that finding lowest - cardinality determinations is easier in practice than finding minimal determi define the min - features problem as follows : given a set x of examples ( which are each composed of a a binary value specifying the value of the target feature and a vector of binary values specifying the values of the other features ) and a number n , determine whether or not there exists some feature set s such that : we show that min - features is np - complete by reducing vertex - cover to min - features . #NUM# the vertex - cover problem may be stated as the question : given a graph g with vertices v and edges e , is there a subset v #NUM# of v , of size m , such that each edge in e is connected to at least one vertex in v #NUM# ? we may reduce an instance of vertex - cover to an instance of min - features by mapping each edge in e to an example in x , with one input feature for every vertex in v . #NUM# in [ #NUM# ] , a " proof " is reported for this result by reduction to set covering . the proof therefore fails to show np - completeness . nations .
an unsupervised learning algorithm for a multilayer network of stochastic neurons is described . bottom - up recognition connections convert the input into representations in successive hidden layers and top - down generative connections reconstruct the representation in one layer from the representation in the layer above . in the wake phase , neurons are driven by recognition connections , and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below . in the sleep phase , neurons are driven by generative connections and recognition connections are adapted to increase the probability that they would produce supervised learning algorithms for multilayer neural networks face two problems : they require a teacher to specify the desired output of the network and they require some method of communicating error information to all of the connections . the wake - sleep algorithm avoids both these problems . when there is no external teaching signal to be matched , some other goal is required to force the hidden units to extract underlying structure . in the wake - sleep algorithm the goal is to learn representations that are economical to describe but allow the input to be reconstructed accurately . we can quantify this goal by imagining a communication game in which each vector of raw sensory inputs is communicated to a receiver by first sending its hidden representation and then sending the difference between the input vector and its top - down reconstruction from the hidden representation . the aim of learning is to minimize the description length which is the total number of bits that would be required to communicate the input vectors in this way [ #NUM# ] . no communication actually takes place , but minimizing the description length that would be required forces the network to learn economical representations that capture the underlying regularities in the data [ #NUM# ] . the correct activity vector in the layer above .
properly structured software libraries are crucial for the success of software reuse . specifically , the structure of the software library ought to reect the functional similarity of the stored software components in order to facilitate the retrieval process . we propose the application of artificial neural network technology to achieve such a structured library . in more detail , we utilize an artificial neural network adhering to the unsupervised learning paradigm . the distinctive feature of this very model is to make the semantic relationship between the stored software components geographically explicit . thus , the actual user of the software library gets a notion of the semantic relationship between the components in terms of their geographical closeness .
learning is a fundamental component of intelligence , and a key consideration in designing cognitive architectures such as soar [ laird et al . , #NUM# ] . this chapter considers the question of what constitutes an appropriate general - purpose learning mechanism . we are interested in mechanisms that might explain and reproduce the rich variety of learning capabilities of humans , ranging from learning perceptual - motor skills such as how to ride a bicycle , to learning highly cognitive tasks such as how to play chess . research on learning in fields such as cognitive science , artificial intelligence , neurobiology , and statistics has led to the identification of two distinct classes of learning methods : inductive and analytic . inductive methods , such as neural network backpropagation , learn general laws by finding statistical correlations and regularities among a large set of training examples . in contrast , analytical methods , such as explanation - based learning , acquire general laws from many fewer training examples . they rely instead on prior knowledge to analyze individual training examples in detail , then use this analysis to distinguish relevant example features from the irrelevant . the question considered in this chapter is how to best combine inductive and analytical learning in an architecture that seeks to cover the range of learning exhibited by intelligent systems such as humans . we present a specific learning mechanism , explanation based neural network learning ( ebnn ) , that blends these two types of learning , and present experimental results demonstrating its ability to learn control strategies for a mobile robot using
simulation plays an important role in stochastic geometry and related fields , because all but the simplest random set models tend to be intractable to analysis . many simulation algorithms deliver ( approximate ) samples of such random set models , for example by simulating the equilibrium distribution of a markov chain such as a spatial birth - and - death process . the samples usually fail to be exact because the algorithm simulates the markov chain for a long but finite time , and thus convergence to equilibrium is only approximate . the seminal work by propp and wilson made an important contribution to simulation by proposing a coupling method , coupling from the past ( cftp ) , which delivers perfect , that is to say exact , simulations of markov chains . in this paper we introduce this new idea of perfect simulation and illustrate it using two common models in stochastic geometry : the dead leaves model and a boolean model conditioned to cover a finite set of points .
in recent years , case - based reasoning has been demonstrated to be highly useful for problem solving in complex domains . also , mixed paradigm approaches emerged for combining cbr and induction techniques aiming at verifying the knowledge and / or building an efficient case memory . however , in complex domains induction over the whole problem space is often not possible or too time consuming . in this paper , an approach is presented which ( owing to a close interaction with the cbr part ) attempts to induce rules only for a particular context , i . e . for a problem just being solved by a cbr - oriented system . these rules may then be used for indexing purposes or similarity assessment in order to support the cbr process in the future .
this paper demonstrates the tandem use of a finite automata learning algorithm and a utility planner for an adversarial robotic domain . for many applications , robot agents need to predict the movement of objects in the environment and plan to avoid them . when the robot has no reasoning model of the object , machine learning techniques can be used to generate one . in our project , we learn a dfa model of an adversarial robot and use the automaton to predict the next move of the adversary . the robot agent plans a path to avoid the adversary at the predicted location while fulfilling the goal requirements .
claudia cargnoni is with the dipartimento statistico , universita di firenze , #NUM# firenze , italy . peter muller is assistant professor , and mike west is professor , in the institute of statistics and decision sciences at duke university , durham nc #NUM# - #NUM# . research of cargnoni was performed while visiting isds during #NUM# . muller and west were partially supported by nsf under grant dms - #NUM# .
our theoretical understanding of the properties of genetic algorithms ( gas ) being used for function optimization ( gafos ) is not as strong as we would like . traditional schema analysis provides some first order insights , but doesn ' t capture the non - linear dynamics of the ga search process very well . markov chain theory has been used primarily for steady state analysis of gas . in this paper we explore the use of transient markov chain analysis to model and understand the behavior of finite population gafos observed while in transition to steady states . this approach appears to provide new insights into the circumstances under which gafos will ( will not ) perform well . some preliminary results are presented and an initial evaluation of the merits of this approach is provided .
in this paper we consider the application of training with noise in multi - layer perceptron to input variables relevance determination . noise injection is modified in order to penalize irrelevant features . the proposed algorithm is attractive as it requires the tuning of a single parameter . this parameter controls the penalization of the inputs together with the complexity of the model . after the presentation of the method , experimental evidences are given on simulated data sets .
coins technical report #NUM# - #NUM# january #NUM# abstract in this paper we present a new multivariate decision tree algorithm lmdt , which combines linear machines with decision trees . lmdt constructs each test in a decision tree by training a linear machine and then eliminating irrelevant and noisy variables in a controlled manner . to examine lmdt ' s ability to find good generalizations we present results for a variety of domains . we compare lmdt empirically to a univariate decision tree algorithm and observe that when multivariate tests are the appropriate bias for a given data set , lmdt finds small accurate trees .
reinforcement learning ( rl ) is a model - free tuning and adaptation method for control of dynamic systems . contrary to supervised learning , based usually on gradient descent techniques , rl does not require any model or sensitivity function of the process . hence , rl can be applied to systems that are poorly understood , uncertain , nonlinear or for other reasons untractable with conventional methods . in reinforcement learning , the overall controller performance is evaluated by a scalar measure , called reinforcement . depending on the type of the control task , reinforcement may represent an evaluation of the most recent control action or , more often , of an entire sequence of past control moves . in the latter case , the rl system learns how to predict the outcome of each individual control action . this prediction is then used to adjust the parameters of the controller . the mathematical background of rl is closely related to optimal control and dynamic programming . this paper gives a comprehensive overview of the rl methods and presents an application to the attitude control of a satellite . some well known applications from the literature are reviewed as well .
a biologically motivated mechanism for self - organizing a neural network with modifiable lateral connections is presented . the weight modification rules are purely activity - dependent , unsupervised and local . the lateral interaction weights are initially random but develop into a " mexican hat " shape around each neuron . at the same time , the external input weights self - organize to form a topological map of the input space . the algorithm demonstrates how self - organization can bootstrap itself using input information . predictions of the algorithm agree very well with experimental observations on the development of lateral connections in cortical feature maps .
some of the main users of statistical methods - economists , social scientists , and epidemiologists are discovering that their fields rest not on statistical but on causal foundations . the blurring of these foundations over the years follows from the lack of mathematical notation capable of distinguishing causal from equational relationships . by providing formal and natural explication of such relations , graphical methods have the potential to revolutionize how statistics is used in knowledge - rich applications . statisticians , in response , are beginning to realize that causality is not a metaphysical dead - end but a meaningful concept with clear mathematical underpinning . the paper surveys these developments and outlines future challenges .
this paper describes a new method for inducing logic programs from examples which attempts to integrate the best aspects of existing ilp methods into a single coherent framework . in particular , it combines a bottom - up method similar to golem with a top - down method similar to foil . it also includes a method for predicate invention similar to champ and an elegant solution to the " noisy oracle " problem which allows the system to learn recursive programs without requiring a complete set of positive examples . systematic experimental comparisons to both golem and foil on a range of problems are used to clearly demonstrate the ad vantages of the approach .
we present deterministic techniques for computing upper and lower bounds on marginal probabilities in sigmoid and noisy - or networks . these techniques become useful when the size of the network ( or clique size ) precludes exact computations . we illustrate the tightness of the bounds by numerical experi ments .
mit computational cognitive science technical report #NUM# abstract we develop a recursive node - elimination formalism for efficiently approximating large probabilistic networks . no constraints are set on the network topologies . yet the formalism can be straightforwardly integrated with exact methods whenever they are / become applicable . the approximations we use are controlled : they maintain consistently upper and lower bounds on the desired quantities at all times . we show that boltzmann machines , sigmoid belief networks , or any combination ( i . e . , chain graphs ) can be handled within the same framework . the accuracy of the methods is verified exper imentally .
we prove a lower bound of ( #NUM# * ln #NUM# ffi + vcdim ( c ) * ) on the number of random examples required for distribution - free learning of a concept class c , where vcdim ( c ) is the vapnik - chervonenkis dimension and * and ffi are the accuracy and confidence parameters . this improves the previous best lower bound of ( #NUM# * ln #NUM# ffi + vcdim ( c ) ) , and comes close to the known general upper bound of o ( #NUM# ffi + vcdim ( c ) * ln #NUM# * ) for consistent algorithms . we show that for many interesting concept classes , including kcnf and kdnf , our bound is actually tight to within a constant factor .
the ability to handle temporal variation is important when dealing with real - world dynamic signals . in many applications , inputs do not come in as fixed - rate sequences , but rather as signals with time scales that can vary from one instance to the next ; thus , modeling dynamic signals requires not only the ability to recognize sequences but also the ability to handle temporal changes in the signal . this paper discusses " tau net , " a neural network for modeling dynamic signals , and its application to speech . in tau net , sequence learning is accomplished using a combination of prediction , recurrence and time - delay connections . temporal variability is modeled by having adaptable time constants in the network , which are adjusted with respect to the prediction error . adapting the time constants changes the time scale of the network , and the adapted value of the network ' s time constant provides a measure of temporal variation in the signal . tau net has been applied to several simple signals : sets of sine waves differing in frequency and in phase [ #NUM# ] , a multidimensional signal representing the walking gait of children [ #NUM# ] , and the energy contour of a simple speech utterance [ #NUM# ] . tau net has also been shown to work on a voicing distinction task using synthetic speech data [ #NUM# ] . in this paper , tau net is applied to two speaker - independent tasks , vowel recognition ( of f / ae / , / iy / , / ux / g ) and consonant recognition ( of f / p / , / t / , / k / g ) using speech data taken from the timit database . it is shown that tau nets , trained on medium - rate tokens , achieved about the same performance as networks without time constants trained on tokens at all rates , and performed better than networks without time constants trained on medium - rate tokens . our results demonstrate tau net ' s ability to identify vowels and consonants at variable speech rates by extrapolating to rates not represented in the training set .
back - propagation learning ( bp ) is known for its serious limitations in generalising knowledge from certain types of learning material . bp - som is an extension of bp which overcomes some of these limitations . bp - som is a combination of a multi - layered feed - forward network ( mfn ) trained with bp , and kohonen ' s self - organising maps ( soms ) . in earlier reports , it has been shown that bp - som improved the generalisation performance whereas it decreased simultaneously the number of necessary hidden units without loss of generalisation performance . these are only two effects of the use of som learning during training of mfns . in this paper we focus on two additional effects . first , we show that after bp - som training , activations of hidden units of mfns tend to oscillate among a limited number of discrete values . second , we identify som elements as adequate organisers of instances of the task at hand . we visualise both effects , and argue that they lead to intelligible neural networks and can be employed as a basis for automatic rule extraction .
the discrimination powers of multilayer perceptron ( mlp ) and learning vector quantisation ( lvq ) networks are compared for overlapping gaussian distributions . it is shown , both analytically and with monte carlo studies , that the mlp network handles high dimensional problems in a more efficient way than lvq . this is mainly due to the sigmoidal form of the mlp transfer function , but also to the the fact that the mlp uses hyper - planes more efficiently . both algorithms are equally robust to limited training sets and the learning curves fall off like #NUM# = m , where m is the training set size , which is compared to theoretical predictions from statistical estimates and vapnik - chervonenkis bounds .
in this article we approximate the rate of convergence of the gibbs sampler by a normal approximation of the target distribution . based on this approximation , we consider many implementational issues for the gibbs sampler , e . g . , updating strategy , parameterization and blocking . we give theoretical results to justify our approximation and illustrate our methods in a number of realistic examples .
instance - based learning methods explicitly remember all the data that they receive . they usually have no training phase , and only at prediction time do they perform computation . then , they take a query , search the database for similar datapoints and build an on - line local model ( such as a local average or local regression ) with which to predict an output value . in this paper we review the advantages of instance based methods for autonomous systems , but we also note the ensuing cost : hopelessly slow computation as the database grows large . we present and evaluate a new way of structuring a database and a new algorithm for accessing it that maintains the advantages of instance - based learning . earlier attempts to combat the cost of instance - based learning have sacrificed the explicit retention of all data , or been applicable only to instance - based predictions based on a small number of near neighbors or have had to re - introduce an explicit training phase in the form of an interpolative data structure . our approach builds a multiresolution data structure to summarize the database of experiences at all resolutions of interest simultaneously . this permits us to query the database with the same exibility as a conventional linear search , but at greatly reduced computational cost .
we have implemented a reinforcement learning architecture as the reactive component of a two layer control system for a simulated race car . we have found that separating the layers has expedited gradually improving competition and mult - agent interaction . we ran experiments to test the tuning , decomposition and coordination of the low level behaviors . we then extended our control system to allow passing of other cars and tested its ability to avoid collisions . the best design used reinforcement learning with separate networks for each behavior , coarse coded input and a simple rule based coordination mechanism .
this study is concerned with whether it is possible to detect what information contained in the training data and background knowledge is relevant for solving the learning problem , and whether irrelevant information can be eliminated in preprocessing before starting the learning process . a case study of data preprocessing for a hybrid genetic algorithm shows that the elimination of irrelevant features can substantially improve the efficiency of learning . in addition , cost - sensitive feature elimination can be effective for reducing costs of induced hypotheses .
hierarchical genetic programming ( hgp ) approaches rely on the discovery , modification , and use of new functions to accelerate evolution . this paper provides a qualitative explanation of the improved behavior of hgp , based on an analysis of the evolution process from the dual perspective of diversity and causality . from a static point of view , the use of an hgp approach enables the manipulation of a population of higher diversity programs . higher diversity increases the exploratory ability of the genetic search process , as demonstrated by theoretical and experimental fitness distributions and expanded structural complexity of individuals . from a dynamic point of view , this report analyzes the causality of the crossover operator . causality relates changes in the structure of an object with the effect of such changes , i . e . changes in the properties or behavior of the object . the analyses of crossover causality suggests that hgp discovers and exploits useful structures in a bottom - up , hierarchical manner . diversity and causality are complementary , affecting exploration and exploitation in genetic search . unlike other machine learning techniques that need extra machinery to control the tradeoff between them , hgp automatically trades off exploration and exploitation .
previous neural network learning algorithms for sequence processing are computationally expensive and perform poorly when it comes to long time lags . this paper first introduces a simple principle for reducing the descriptions of event sequences without loss of information . a consequence of this principle is that only unexpected inputs can be relevant . this insight leads to the construction of neural architectures that learn to ` divide and conquer ' by recursively decomposing sequences . i describe two architectures . the first functions as a self - organizing multi - level hierarchy of recurrent networks . the second , involving only two recurrent networks , tries to collapse a multi - level predictor hierarchy into a single recurrent net . experiments show that the system can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets .
a neural network model for the self - organization of ocular dominance and lateral connections from binocular input is presented . the self - organizing process results in a network where ( #NUM# ) afferent weights of each neuron organize into smooth hill - shaped receptive fields primarily on one of the retinas , ( #NUM# ) neurons with common eye preference form connected , intertwined patches , and ( #NUM# ) lateral connections primarily link regions of the same eye preference . similar self - organization of cortical structures has been observed experimentally in strabismic kittens . the model shows how patterned lateral connections in the cortex may develop based on correlated activity and explains why lateral connection patterns follow receptive field properties such as ocular dominance .
a self - organizing model of spiking neurons with dynamic thresholds and lateral excitatory and inhibitory connections is presented and tested in the image segmentation task . the model integrates two previously separate lines of research in modeling the visual cortex . laterally connected self - organizing maps have been used to model how afferent structures and lateral connections could self - organize through input - driven hebbian adaptation . spiking neurons with leaky integrator synapses have been used to model image segmentation and binding by synchronization and desynchronization of neuronal activity . although these approaches differ in how they model the neuron , they have the same overall layout of a laterally connected two - dimensional network . this paper shows how both self - organization and segmentation can be achieved in such a network , thus presenting a unified model of development and func tional dynamics in the primary visual cortex .
the full bayesian method for applying neural networks to a prediction problem is to set up the prior / hyperprior structure for the net and then perform the necessary integrals . however , these integrals are not tractable analytically , and markov chain monte carlo ( mcmc ) methods are slow , especially if the parameter space is high - dimensional . using gaussian processes we can approximate the weight space integral analytically , so that only a small number of hyperparameters need be integrated over by mcmc methods . we have applied this idea to classification problems , obtaining ex cellent results on the real - world problems investigated so far .
recently propp and wilson have proposed an algorithm , called coupling from the past ( cftp ) , which allows not only an approximate but perfect ( i . e . exact ) simulation of the stationary distribution of certain finite state space markov chains . perfect sampling using cftp has been successfully extended to the context of point processes , amongst other authors , by haggstrom et al . [ #NUM# ] . in [ #NUM# ] gibbs sampling is applied to a bivariate point process , the penetrable spheres mixture model [ #NUM# ] . however , in general the running time of cftp in terms of number of transitions is not independent of the state sampled . thus an impatient user who aborts long runs may introduce a subtle bias , the user impatience bias . fill [ #NUM# ] introduced an exact sampling algorithm for finite state space markov chains which , in contrast to cftp , is unbiased for user impatience . fill ' s algorithm is a form of rejection sampling and similar to cftp requires sufficient mono - tonicity properties of the transition kernel used . we show how fill ' s version of rejection sampling can be extended to an infinite state space context to produce an exact sample of the penetrable spheres mixture process and related models . following [ #NUM# ] we use gibbs sampling and make use of the partial order of the mixture model state space .
cells in the visual cortex are selective not only to ocular dominance and orientation of the input , but also to its size and spatial frequency . the simulations reported in this paper show how size selectivity could develop through hebbian self - organization , and how receptive fields of different sizes could organize into columns like those for orientation and ocular dominance . the lateral connections in the network self - organize cooperatively and simultaneously with the receptive field sizes , and produce patterns of lateral connectivity that closely follow the receptive field organization . together with our previous work on ocular dominance and orientation selectivity , these results suggest that a single hebbian self - organizing process can give rise to all the major receptive field properties in the visual cortex , and also to structured patterns of lateral interactions , some of which have been verified experimentally and others predicted by the model . the model also suggests a functional role for the self - organized structures the afferent receptive fields develop a sparse coding of the visual input , and the recurrent lateral interactions eliminate redundancies in cortical activity patterns , allowing the cortex to efficiently process massive amounts of visual information .
a pilot study is described on the practical application of artificial neural networks . the limit cycle of the attitude control of a satellite is selected as the test case . one of the sources of the limit cycle is a position dependent error in the observed attitude . a reinforcement learning method is selected , which is able to adapt a controller such that a cost function is optimised . an estimate of the cost function is learned by a neural ` critic ' . in our approach , the estimated cost function is directly represented as a function of the parameters of a linear controller . the critic is implemented as a cmac network . results from simulations show that the method is able to find optimal parameters without unstable behaviour . in particular in the case of large discontinuities in the attitude measurements , the method shows a clear improvement compared to the conventional approach : the rms attitude error decreases approximately #NUM# % .
in a nutshell we can describe a generic ilp problem as following : given a set e of ( positive and negative ) examples of a target predicate , and some background knowledge b about the world ( usually a logic program including facts and auxiliary predicates ) , the task is to find a logic program h ( our hypothesis ) such that all positive examples can be deduced from b and h , while no negative example can . in this paper we review some of the results achieved in this area and discuss the techniques used . moreover we prove the following new results : * predicates described by non - recursive , local clauses of at most k literals are pac - learnable under any distribution . this generalizes a previous result that was valid only for constrained clauses . * predicates that are described by k non - recursive local clauses are pac - learnable under any distribution . this generalizes a previous result that was non construc tive and valid only under some class of distributions . finally we introduce what we believe is the first theoretical framework for learning prolog clauses in the presence of errors . to this purpose we introduce a new noise model , that we call the fixed attribute noise model , for learning propositional concepts over the boolean domain . this new noise model can be of its own interest .
the expectation - maximization algorithm given by dempster et al ( #NUM# ) has enjoyed considerable popularity for solving map estimation problems . this note gives a simple derivation of the algorithm , due to luttrell ( #NUM# ) , that better illustrates the convergence properties of the algorithm and its variants . the algorithm is illustrated with two examples : pooling data from multiple noisy sources and fitting a mixture density .
we consider the problem of how to incorporate prior knowledge in supervised learning techniques . we set the problem in the framework of regularization theory , and consider the case in which we know that the approximated function has radial symmetry . the problem can be solved in two alternative ways : #NUM# ) use the invariance as a constraint in the regularization theory framework to derive a rotation invariant version of radial basis functions ; #NUM# ) use the radial symmetry to create new , " virtual " examples from a given data set . we show that these two apparently different methods of learning from " hints " ( abu - mostafa , #NUM# ) lead to exactly the same analyt ical solution .
i present computational results suggesting that gain - adaptation algorithms based in part on connectionist learning methods may improve over least squares and other classical parameter - estimation methods for stochastic time - varying linear systems . the new algorithms are evaluated with respect to classical methods along three dimensions : asymptotic error , computational complexity , and required prior knowledge about the system . the new algorithms are all of the same order of complexity as lms methods , o ( n ) , where n is the dimensionality of the system , whereas least - squares methods and the kalman filter are o ( n #NUM# ) . the new methods also improve over the kalman filter in that they do not require a complete statistical model of how the system varies over time . in a simple computational experiment , the new methods are shown to produce asymptotic error levels near that of the optimal kalman filter and significantly below those of least - squares and lms methods . the new methods may perform better even than the kalman filter if there is any error in the filter ' s model of how the system varies over time .
windowing has been proposed as a procedure for efficient memory use in the id #NUM# decision tree learning algorithm . however , previous work has shown that windowing may often lead to a decrease in performance . in this work , we try to argue that separate - and - conquer rule learning algorithms are more appropriate for windowing than divide - and - conquer algorithms , because they learn rules independently and are less susceptible to changes in class distributions . in particular , we will present a new windowing algorithm that achieves additional gains in efficiency by exploiting this property of separate - and - conquer algorithms . while the presented algorithm is only suitable for redundant , noise - free data sets , we will also briefly discuss the problem of noisy data in windowing and present some preliminary ideas how it might be solved with an extension of the algorithm introduced in this paper .
this article describes a comprehensive approach to automatic theory revision . given an imperfect theory , the approach combines explanation attempts for incorrectly classified examples in order to identify the failing portions of the theory . for each theory fault , correlated subsets of the examples are used to inductively generate a correction . because the corrections are focused , they tend to preserve the structure of the original theory . because the system starts with an approximate domain theory , in general fewer training examples are required to attain a given level of performance ( classification accuracy ) compared to a purely empirical system . the approach applies to classification systems employing a propositional horn - clause theory . the system has been tested in a variety of application domains , and results are presented for problems in the domains of molecular biology and plant disease diagnosis .
suppose one wishes to sample from the density ( x ) using markov chain monte carlo ( mcmc ) . an auxiliary variable u and its conditional distribution ( ujx ) can be defined , giving the joint distribution ( x ; u ) = ( x ) ( ujx ) . a mcmc scheme which samples over this joint distribution can lead to substantial gains in efficiency compared to standard approaches . the revolutionary algorithm of swendsen and wang ( #NUM# ) is one such example . in addition to reviewing the swendsen - wang algorithm and its generalizations , this paper introduces a new auxiliary variable method called partial decoupling . two applications in bayesian image analysis are considered . the first is a binary classification problem in which partial decoupling out performs sw and single site metropolis . the second is a pet reconstruction which uses the gray level prior of geman and mcclure ( #NUM# ) . a generalized swendsen - wang algorithm is developed for this problem , which reduces the computing time to the point that mcmc is a viable method of posterior exploration .
in this paper , we analyse theoretical properties of the slice sampler . we find that the algorithm has extremely robust geometric ergodicity properties . for the case of just one auxiliary variable , we demonstrate that the algorithm is stochastically monotone , and deduce analytic bounds on the total variation distance from stationarity of the method using foster - lyapunov drift condition methodology .
this paper studies how well the combination of simulated annealing and adfs solves genetic programming ( gp ) style program discovery problems . on a suite composed of the even - k - parity problems for k = #NUM# , #NUM# , #NUM# , it analyses the performance of simulated annealing with adfs as compared to not using adfs . in contrast to gp results on this suite , when simulated annealing is run with adfs , as problem size increases , the advantage to using them over a standard gp program representation is marginal . when the performance of simulated annealing is compared to gp with both algorithm using adfs on the even - #NUM# - parity problem gp is advantageous , on the even - #NUM# - parity problem sa and gp are equal , and on the even - #NUM# - parity problem sa is advantageous .
most learning algorithms work most effectively when their training data contain completely specified labeled samples . in many diagnostic tasks , however , the data will include the values of only some of the attributes ; we model this as a blocking process that hides the values of those attributes from the learner . while blockers that remove the values of critical attributes can handicap a learner , this paper instead focuses on blockers that remove only irrelevant attribute values , i . e . , values that are not needed to classify an instance , given the values of the other unblocked attributes . we first motivate and formalize this model of " superfluous - value blocking " , and then demonstrate that these omissions can be useful , by proving that certain classes that seem hard to learn in the general pac model | viz . , decision trees and dnf formulae | are trivial to learn in this setting . we also show that this model can be extended to deal with ( #NUM# ) theory revision ( i . e . , modifying an existing formula ) ; ( #NUM# ) blockers that occasionally include superfluous values or exclude required values ; and ( #NUM# ) other cor ruptions of the training data .
this paper presents an approach to automatic discovery of functions in genetic programming . the approach is based on discovery of useful building blocks by analyzing the evolution trace , generalizing blocks to define new functions , and finally adapting the problem representation on - the - fly . adaptating the representation determines a hierarchical organization of the extended function set which enables a restructuring of the search space so that solutions can be found more easily . measures of complexity of solution trees are defined for an adaptive representation framework . the minimum description length principle is applied to justify the feasibility of approaches based on a hierarchy of discovered functions and to suggest alternative ways of defining a problem ' s fitness function . preliminary empirical results are presented .
a decision problem associated with a fundamental nonconvex model for linearly inseparable pattern sets is shown to be np - complete . another nonconvex model that employs an #NUM# norm instead of the #NUM# - norm , can be solved in polynomial time by solving #NUM# n linear programs , where n is the ( usually small ) dimensionality of the pattern space . an effective lp - based finite algorithm is proposed for solving the latter model . the algorithm is employed to obtain a noncon - vex piecewise - linear function for separating points representing measurements made on fine needle aspirates taken from benign and malignant human breasts . a computer program trained on #NUM# samples has correctly diagnosed each of #NUM# new samples encountered and is currently in use at the university of wisconsin hospitals . #NUM# . introduction . the fundamental problem we wish to address is that of
many connectionist approaches to musical expectancy and music composition let the question of what next ? overshadow the equally important question of when next ? . one cannot escape the latter question , one of temporal structure , when considering the perception of musical meter . we view the perception of metrical structure as a dynamic process where the temporal organization of external musical events synchronizes , or entrains , a listeners internal processing mechanisms . this article introduces a novel connectionist unit , based upon a mathematical model of entrainment , capable of phase and frequency - locking to periodic components of incoming rhythmic patterns . networks of these units can self - organize temporally structured responses to rhythmic patterns . the resulting network behavior embodies the perception of metrical structure . the article concludes with a discussion of the implications of our approach for theories of metrical structure and musical expectancy .
over the years there has been several packages developed that provide a workbench for genetic algorithm ( ga ) research . most of these packages use the generational model inspired by genesis . a few have adopted the steady - state model used in genitor . unfortunately , they have some deficiencies when working with order - based problems such as packing , routing , and scheduling . this paper describes libga , which was developed specifically for order - based problems , but which also works easily with other kinds of problems . it offers an easy to use ` user - friendly ' interface and allows comparisons to be made between both generational and steady - state genetic algorithms for a particular problem . it includes a variety of genetic operators for reproduction , crossover , and mutation . libga makes it easy to use these operators in new ways for particular applications or to develop and include new operators . finally , it offers the unique new feature of a dynamic generation gap .
human episodic memory provides a seemingly unlimited storage for everyday experiences , and a retrieval system that allows us to access the experiences with partial activation of their components . this paper presents a neural network model of episodic memory inspired by dama - sio ' s idea of convergence zones . the model consists of a layer of perceptual feature maps and a binding layer . a perceptual feature pattern is coarse coded in the binding layer , and stored on the weights between layers . a partial activation of the stored features activates the binding pattern which in turn reactivates the entire stored pattern . a worst - case analysis shows that with realistic - size layers , the memory capacity of the model is several times larger than the number of units in the model , and could account for the large capacity of human episodic memory .
in this paper , we adopt general - sum stochastic games as a framework for multiagent reinforcement learning . our work extends previous work by littman on zero - sum stochastic games to a broader framework . we design a multiagent q - learning method under this framework , and prove that it converges to a nash equilibrium under specified conditions . this algorithm is useful for finding the optimal strategy when there exists a unique nash equilibrium in the game . when there exist multiple nash equilibria in the game , this algorithm should be combined with other learning techniques to find optimal strategies .
when learning from reasoning failures , knowledge of how a system behaves is a powerful lever for deciding what went wrong with the system and in deciding what the system needs to learn . a number of benefits arise when systems possess knowledge of their own operation and of their own knowledge . abstract knowledge about cognition can be used to select diagnosis and repair strategies from among alternatives . specific kinds of self - knowledge can be used to distinguish between failure hypothesis candidates . making self - knowledge explicit can also facilitate the use of such knowledge across domains and can provide a principled way to incorporate new learning strategies . to illustrate the advantages of self - knowledge for learning , we provide implemented examples from two different systems a plan execution system called rapter and a story understanding system called meta - aqua .
theory revision integrates inductive learning and background knowledge by combining training examples with a coarse domain theory to produce a more accurate theory . there are two challenges that theory revision and other theory - guided systems face . first , a representation language appropriate for the initial theory may be inappropriate for an improved theory . while the original representation may concisely express the initial theory , a more accurate theory forced to use that same representation may be bulky , cumbersome , and difficult to reach . second , a theory structure suitable for a coarse domain theory may be insufficient for a fine - tuned theory . systems that produce only small , local changes to a theory have limited value for accomplishing complex structural alterations that may be required . consequently , advanced theory - guided learning systems require flexible representation and flexible structure . an analysis of various theory revision systems and theory - guided learning systems reveals specific strengths and weaknesses in terms of these two desired properties . designed to capture the underlying qualities of each system , a new system uses theory - guided constructive induction . experiments in three domains show improvement over previous theory - guided systems . this leads to a study of the behavior , limitations , and potential of theory - guided constructive induction .
if an experiment requires statistical analysis to establish a result , then one should do a better experiment . ernest rutherford , #NUM# most proponents of cold fusion reporting excess heat from their electrolysis experiments were claiming that one of the main characteristics of cold fusion was its irreproducibility | j . r . huizenga , cold fusion , #NUM# , p . #NUM# abstract amid the ever increasing research into various aspects of neural computing , much progress is evident both from theoretical advances and from empirical studies . on the empirical side a wealth of data from experimental studies is being reported . it is , however , not clear how best to report neural computing experiments such that they may be replicated by other interested researchers . in particular , the nature of iterative learning on a randomised initial architecture , such as backpropagation training of a multilayer perceptron , is such that precise replication of a reported result is virtually impossible . the outcome is that experimental replication of reported results , a touchstone of " the scientific method " , is not an option for researchers in this most popular subfield of neural computing . in this paper , we address this issue of replicability of experiments based on backpropagation training of multilayer perceptrons ( although many of our results will be applicable to any other subfield that is plagued by the same characteristics ) . first , we attempt to produce a complete abstract specification of such a neural computing experiment . from this specification we identify the full range of parameters needed to support maximum replicability , and we use it to show why absolute replicability is not an option in practice . we propose a statistical framework to support replicability . we demonstrate this framework with some empirical studies of our own on both repli - cability with respect to experimental controls , and validity of implementations of the backpropagation algorithm . finally , we suggest how the degree of replicability of a neural computing experiment can be estimated and reflected in the claimed precision for any empirical results reported .
in this paper , we propose an unsupervised neural network allowing a robot to learn sensori - motor associations with a delayed reward . the robot task is to learn the " meaning " of pictograms in order to " survive " in a maze . first , we introduce a new neural conditioning rule ( pcr : probabilistic conditioning rule ) allowing to test hypotheses ( associations between visual categories and movements ) during a given time span . second , we describe a real maze experiment with our mobile robot . we propose a neural architecture to solve this problem and we discuss the difficulty to build visual categories dynamically while associating them to movements . third , we propose to use our algorithm on a simulation in order to test it exhaustively . we give the results for different kind of mazes and we compare our system to an adapted version of the q - learning algorithm . finally , we conclude by showing the limitations of approaches that do not take into account the intrinsic complexity of a reasonning based on image recognition .
we present a framework for the analysis and synthesis of acoustical instruments based on data - driven probabilistic inference modeling . audio time series and boundary conditions of a played instrument are recorded and the non - linear mapping from the control data into the audio space is inferred using the general inference framework of cluster - weighted modeling . the resulting model is used for real - time synthesis of audio sequences from new input data .
in many real - world domains the task of machine learning algorithms is to learn a theory predicting numerical values . in particular several standard test domains used in inductive logic programming ( ilp ) are concerned with predicting numerical values from examples and relational and mostly non - determinate background knowledge . however , so far no ilp algorithm except one can predict numbers and cope with non - determinate background knowledge . ( the only exception is a covering algorithm called fors . ) in this paper we present structural regression trees ( srt ) , a new algorithm which can be applied to the above class of problems by integrating the statistical method of regression trees into ilp . srt constructs a tree containing a literal ( an atomic formula or its negation ) or a conjunction of literals in each node , and assigns a numerical value to each leaf . srt provides more comprehensible results than purely statistical methods , and can be applied to a class of problems most other ilp systems cannot handle . experiments in several real - world domains demonstrate that the approach is competitive with existing methods , indicating that the advantages are not at the expense of predictive accuracy .
a quantitative and practical bayesian framework is described for learning of mappings in feedforward networks . the framework makes possible : ( #NUM# ) objective comparisons between solutions using alternative network architectures ; ( #NUM# ) objective stopping rules for network pruning or growing procedures ; ( #NUM# ) objective choice of magnitude and type of weight decay terms or additive regularisers ( for penalising large weights , etc . ) ; ( #NUM# ) a measure of the effective number of well - determined parameters in a model ; ( #NUM# ) quantified estimates of the error bars on network parameters and on network output ; ( #NUM# ) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions . the bayesian ` evidence ' automatically embodies ` occam ' s razor , ' penalising over - flexible and over - complex models . the bayesian approach helps detect poor underlying assumptions in learning models . for learning models well matched to a problem , a good correlation between generalisation ability this paper makes use of the bayesian framework for regularisation and model comparison described in the companion paper ` bayesian interpolation ' ( mackay , #NUM# a ) . this framework is due to gull and skilling ( gull , #NUM# a ) . and the bayesian evidence is obtained .
the theory revision problem is the problem of how best to go about revising a deficient domain theory using information contained in examples that expose inaccuracies . in this paper we present our approach to the theory revision problem for propositional domain theories . the approach described here , called ptr , uses probabilities associated with domain theory elements to numerically track the ` ` ow ' ' of proof through the theory . this allows us to measure the precise role of a clause or literal in allowing or preventing a ( desired or undesired ) derivation for a given example . this information is used to efficiently locate and repair awed elements of the theory . ptr is proved to converge to a theory which correctly classifies all examples , and shown experimentally to be fast and accurate even for deep theories .
new methodology for fully bayesian mixture analysis is developed , making use of reversible jump markov chain monte carlo methods , that are capable of jumping between the parameter subspaces corresponding to different numbers of components in the mixture . a sample from the full joint distribution of all unknown variables is thereby generated , and this can be used as a basis for a thorough presentation of many aspects of the posterior distribution . the methodology is applied here to the analysis of univariate normal mixtures , using a hierarchical prior model that offers an approach to dealing with weak prior information while avoiding the mathematical pitfalls of using improper priors in the mixture context .
we gratefully acknowledge the substantial contributions to this effort provided by andy barto , who sparked our original interest in these questions and whose continued encouragement and insightful comments and criticisms have helped us greatly . recent discussions with satinder singh and vijay gullapalli have also had a helpful impact on this work . special thanks also to rich sutton , who has influenced our thinking on this subject in numerous ways . this work was supported by grant iri - #NUM# from the national science foundation and by the u . s . air force .
active learning differs from passive " learning from examples " in that the learning algorithm assumes at least some control over what part of the input domain it receives information about . in some situations , active learning is provably more powerful that learning from examples alone , giving better generalization for a fixed number of training examples . in this paper , we consider the problem of learning a binary concept in the absence of noise ( valiant #NUM# ) . we describe a formalism for active concept learning called selective sampling , and show how it may be approximately implemented by a neural network . in selective sampling , a learner receives distribution information from the environment and queries an oracle on parts of the domain it considers " useful . " we test our implementation , called an sg - network , on three domains , and observe significant improvement in generalization .
this paper describes a model of the complementarity of rules and precedents in the classification task . under this model , precedents assist rule - based reasoning by operationalizing abstract rule antecedents . conversely , rules assist case - based reasoning through case elaboration , the process of inferring case facts in order to increase the similarity between cases , and term reformulation , the process of replacing a term whose precedents only weakly match a case with terms whose precedents strongly match the case . fully exploiting this complementarity requires a control strategy characterized by impartiality , the absence of arbitrary ordering restrictions on the use of rules and precedents . an impartial control strategy was implemented in grebe in the domain of texas worker ' s compensation law . in a preliminary evaluation , grebe ' s performance was found to be as good or slightly better than the performance of law students on the same task . a case is classified as belonging to a particular category by relating its description to the criteria for category membership .
we introduce a model - based average reward reinforcement learning method called h - learning and compare it with its discounted counterpart , adaptive real - time dynamic programming , in a simulated robot scheduling task . we also introduce an extension to h - learning , which automatically explores the unexplored parts of the state space , while always choosing greedy actions with respect to the current value function . we show that this " auto - exploratory h - learning " performs better than the original h - learning under previously studied exploration methods such as random , recency - based , or counter - based exploration .
this paper proposes using fuzzy logic techniques to dynamically control parameter settings of genetic algorithms ( gas ) . we describe the dynamic parametric ga : a ga that uses a fuzzy knowledge - based system to control ga parameters . we then introduce a technique for automatically designing and tuning the fuzzy knowledge - base system using gas . results from initial experiments show a performance improvement over a simple static ga . one dynamic parametric ga system designed by our automatic method demonstrated improvement on an application not included in the design phase , which may indicate the general applicability of the dynamic parametric ga to a wide range of ap plications .
in previous work ( olshausen & field #NUM# ) , an algorithm was described for learning linear sparse codes which , when trained on natural images , produces a set of basis functions that are spatially localized , oriented , and bandpass ( i . e . , wavelet - like ) . this note shows how the algorithm may be interpreted within a maximum - likelihood framework . several useful insights emerge from this connection : it makes explicit the relation to statistical independence ( i . e . , factorial coding ) , it shows a formal relationship to the algorithm of bell and sejnowski ( #NUM# ) , and it suggests how to adapt parameters that were previously fixed . this report describes research done within the center for biological and computational learning in the department of brain and cognitive sciences at the massachusetts institute of technology . this research is sponsored by an individual national research service award to b . a . o . ( nimh f #NUM# - mh #NUM# ) and by a grant from the national science foundation under contract asc - #NUM# ( this award includes funds from arpa provided under the hpcc program ) to cbcl .
we study layered belief networks of binary random variables in which the conditional probabilities pr [ childjparents ] depend monotonically on weighted sums of the parents . for these networks , we give efficient algorithms for computing rigorous bounds on the marginal probabilities of evidence at the output layer . our methods apply generally to the computation of both upper and lower bounds , as well as to generic transfer function parameterizations of the conditional probability tables ( such as sigmoid and noisy - or ) . we also prove rates of convergence of the accuracy of our bounds as a function of network size . our results are derived by applying the theory of large deviations to the weighted sums of parents at each node in the network . bounds on the marginal probabilities are computed from two contributions : one assuming that these weighted sums fall near their mean values , and the other assuming that they do not . this gives rise to an interesting trade - off between probable explanations of the evidence and improbable deviations from the mean . in networks where each child has n parents , the gap between our upper and lower bounds behaves as a sum of two terms , one of order p in addition to providing such rates of convergence for large networks , our methods also yield efficient algorithms for approximate inference in fixed networks .
feature selection has proven to be a valuable technique in supervised learning for improving predictive accuracy while reducing the number of attributes considered in a task . we investigate the potential for similar benefits in an unsupervised learning task , conceptual clustering . the issues raised in feature selection by the absence of class labels are discussed and an implementation of a sequential feature selection algorithm based on an existing conceptual clustering system is described . additionally , we present a second implementation which employs a technique for improving the efficiency of the search for an optimal description and compare the performance of both algorithms .
robust , flexible and sufficiently general vision systems such as those for recognition and description of complex #NUM# - dimensional objects require an adequate armamentarium of representations and learning mechanisms . this paper briefly analyzes the strengths and weaknesses of different learning paradigms such as symbol processing systems , connectionist networks , and statistical and syntactic pattern recognition systems as possible candidates for providing such capabilities and points out several promising directions for integrating multiple such paradigms in a synergistic fashion towards that goal .
a self - organizing neural network for sequence classification called sardnet is described and analyzed experimentally . sardnet extends the kohonen feature map architecture with activation retention and decay in order to create unique distributed response patterns for different sequences . sardnet yields extremely dense yet descriptive representations of sequential input in very few training iterations . the network has proven successful on mapping arbitrary sequences of binary and real numbers , as well as phonemic representations of english words . potential applications include isolated spoken word recognition and cognitive science models of sequence processing .
liacc - technical report #NUM# - #NUM# abstract . in this paper we address the problem of acquiring knowledge by integration . our aim is to construct an integrated knowledge base from several separate sources . the objective of integration is to construct one system that exploits all the knowledge that is available and has good performance . the aim of this paper is to discuss the methodology of knowledge integration and present some concrete results . in our experiments the performance of the integrated theory exceeded the performance of the individual theories by quite a significant amount . also , the performance did not fluctuate much when the experiments were repeated . these results indicate knowledge integration can complement other existing ml methods .
in this introduction , we define the term bias as it is used in machine learning systems . we motivate the importance of automated methods for evaluating and selecting biases using a framework of bias selection as search in bias and meta - bias spaces . recent research in the field of machine learning bias is summarized .
ogi cse technical report #NUM# - #NUM# abstract : smoothing regularizers for radial basis functions have been studied extensively , but no general smoothing regularizers for projective basis functions ( pbfs ) , such as the widely - used sigmoidal pbfs , have heretofore been proposed . we derive new classes of algebraically - simple m th - order smoothing regularizers for networks of projective basis functions f ( w ; x ) = p n fi fl + u #NUM# ; with general transfer functions g [ ] . these simple algebraic forms r ( w ; m ) enable the direct enforcement of smoothness without the need for costly monte carlo integrations of s ( w ; m ) . the regularizers are tested on illustrative sample problems and compared to quadratic weight decay . the new regularizers are shown to yield better generalization errors than
we address the problem of musical variation ( identification of different musical sequences as variations ) and its implications for mental representations of music . according to reductionist theories , listeners judge the structural importance of musical events while forming mental representations . these judgments may result from the production of reduced memory representations that retain only the musical gist . in a study of improvised music performance , pianists produced variations on melodies . analyses of the musical events retained across variations provided support for the reductionist account of structural importance . a neural network trained to produce reduced memory representations for the same melodies represented structurally important events more efficiently than others . agreement among the musicians ' improvisations , the network model , and music - theoretic predictions suggest that perceived constancy across musical variation is a natural result of a reductionist mechanism for producing memory representations .
ensemble learning by variational free energy minimization is a tool introduced to neural networks by hinton and van camp in which learning is described in terms of the optimization of an ensemble of parameter vectors . the optimized ensemble is an approximation to the posterior probability distribution of the parameters . this tool has now been applied to a variety of statistical inference problems . in this paper i study a linear regression model with both parameters and hyper - parameters . i demonstrate that the evidence approximation for the optimization of regularization constants can be derived in detail from a free energy minimization view point .
the self regenerative mcmc is a tool for constructing a markov chain with a given stationary distribution by constructing an auxiliary chain with some other stationary distribution . elements of the auxiliary chain are picked a suitable random number of times so that the resulting chain has the stationary distribution , sahu and zhigljavsky ( #NUM# ) . in this article we provide a generic adaptation scheme for the above algorithm . the adaptive scheme is to use the knowledge of the stationary distribution gathered so far and then to update during the course of the simulation . this method is easy to implement and often leads to considerable improvement . we obtain theoretical results for the adaptive scheme . our proposed methodology is illustrated with a number of realistic examples in bayesian computation and its performance is compared with other available mcmc techniques . in one of our applications we develop a non - linear dynamics model for modeling predator - prey relationships in the wild .
conceptual analogy ( ca ) is an approach that integrates conceptualization , i . e . , memory organization based on prior experiences and analogical reasoning ( borner #NUM# a ) . it was implemented prototypically and tested to support the design process in building engineering ( borner and janetzko #NUM# , borner #NUM# ) . there are a number of features that distinguish ca from standard approaches to cbr and ar . first of all , ca automatically extracts the knowledge needed to support design tasks ( i . e . , complex case representations , the relevance of object features and relations , and proper adaptations ) from attribute - value representations of prior layouts . secondly , it effectively determines the similarity of complex case representations in terms of adaptability . thirdly , implemented and integrated into a highly interactive and adaptive system architecture it allows for incremental knowledge acquisition and user support . this paper surveys the basic assumptions and the psychological results which influenced the development of ca . it sketches the knowledge representation formalisms employed and characterizes the sub - processes needed to integrate memory organization and analogical reasoning .
this paper presents algorithms for robustness analysis of bayesian networks with global neighborhoods . robust bayesian inference is the calculation of bounds on posterior values given perturbations in a probabilistic model . we present algorithms for robust inference ( including expected utility , expected value and variance bounds ) with global perturbations that can be modeled by * - contaminated , constant density ratio , constant density bounded and total variation classes of distributions . c fl #NUM# carnegie mellon university
the paper describes a self - learning control system for a mobile robot . based on sensor information the control system has to provide a steering signal in such a way that collisions are avoided . since in our case no ` examples ' are available , the system learns on the basis of an external reinforcement signal which is negative in case of a collision and zero otherwise . rules from temporal difference learning are used to find the correct mapping between the ( discrete ) sensor input space and the steering signal . we describe the algorithm for learning the correct mapping from the input ( state ) vector to the output ( steering ) signal , and the algorithm which is used for a discrete coding of the input state space .
standard methods for inducing both the structure and weight values of recurrent neural networks fit an assumed class of architectures to every task . this simplification is necessary because the interactions between network structure and function are not well understood . evolutionary computation , which includes genetic algorithms and evolutionary programming , is a population - based search method that has shown promise in such complex tasks . this paper argues that genetic algorithms are inappropriate for network acquisition and describes an evolutionary program , called gnarl , that simultaneously acquires both the structure and weights for recurrent networks . this algorithms empirical acquisition method allows for the emergence of complex behaviors and topologies that are potentially excluded by the artificial architectural constraints imposed in standard network induction methods .
a new mechanism for genetic encoding of neural networks is proposed , which is loosely based on the marker structure of biological dna . the mechanism allows all aspects of the network structure , including the number of nodes and their connectivity , to be evolved through genetic algorithms . the effectiveness of the encoding scheme is demonstrated in an object recognition task that requires artificial creatures ( whose behaviour is driven by a neural network ) to develop high - level finite - state exploration and discrimination strategies . the task requires solving the sensory - motor grounding problem , i . e . developing a functional understanding of the effects that a creature ' s movement has on its sensory input .
we study a multivariate smoothing spline estimate of a function of several variables , based on an anova decomposition as sums of main effect functions ( of one variable ) , two - factor interaction functions ( of two variables ) , etc . we derive the bayesian " confidence intervals " for the components of this decomposition and demonstrate that , even with multiple smoothing parameters , they can be efficiently computed using the publicly available code rkpack , which was originally designed just to compute the estimates . we carry out a small monte carlo study to see how closely the actual properties of these component - wise confidence intervals match their nominal confidence levels . lastly , we analyze some lake acidity data as a function of calcium concentration , latitude , and longitude , using both polynomial and thin plate spline main effects in the same model .
#NUM# to appear in cowan , j . , tesauro , g . , and alspector , j . ( eds . ) , advances in neural information processing systems #NUM# . san francisco , ca morgan kaufmann publishers . this paper has been written tersely to accomodate an #NUM# page limitation . presented as a ( refereed ) poster session at the #NUM# conference on neural information processing systems - natural and synthetic , november #NUM# , denver , co . supported by the nsf under grants dms - #NUM# and dms - #NUM# , and by the national eye institute - nih under grants ey #NUM# and ey #NUM#
in this paper , we define and examine two versions of the bridge problem . the first variant of the bridge problem is a determistic model where the agent knows a superset of the transitions and a priori probabilities that those transitions are intact . in the second variant , transitions can break or be fixed with some probability at each time step . these problems are applicable to planning in uncertain domains as well as packet routing in a computer network . we show how an agent can act optimally in these models by reduction to markov decision processes . we describe methods of solving them but note that these methods are intractable for reasonably sized problems . finally , we suggest neuro - dynamic programming as a method of value function approximation for these types of models .
if several mental states can be reliably distinguished by recognizing patterns in eeg , then a paralyzed person could communicate to a device like a wheelchair by composing sequencesof these mental states . in this article , we report on a study comparing four representations of eeg signals and their classification by a two - layer neural network with sigmoid activation functions . the neural network is implemented on a cnaps server ( #NUM# processor , simd architecture ) by adaptive solutions , inc . , gaining a #NUM# - fold decrease in training time over a sun
recurrent perceptron classifiers generalize the classical perceptron model . they take into account those correlations and dependences among input coordinates which arise from linear digital filtering . this paper provides tight bounds on sample complexity associated to the fitting of such models to experimental data .
i present a general taxonomy of neural net architectures for processing time - varying patterns . this taxonomy subsumes many existing architectures in the literature , and points to several promising architectures that have yet to be examined . any architecture that processes time - varying patterns requires two conceptually distinct components : a short - term memory that holds on to relevant past events and an associator that uses the short - term memory to classify or predict . my taxonomy is based on a characterization of short - term memory models along the dimensions of form , content , and adaptability . experiments on predicting future values of a financial time series ( us dollar - swiss franc exchange rates ) are presented using several alternative memory models . the results of these experiments serve as a baseline against which more sophisticated architectures can be compared . neural networks have proven to be a promising alternative to traditional techniques for nonlinear temporal prediction tasks ( e . g . , curtiss , brandemuehl , & kreider , #NUM# ; lapedes & farber , #NUM# ; weigend , huberman , & rumelhart , #NUM# ) . however , temporal prediction is a particularly challenging problem because conventional neural net architectures and algorithms are not well suited for patterns that vary over time . the prototypical use of neural nets is in structural pattern recognition . in such a task , a collection of features | visual , semantic , or otherwise | is presented to a network and the network must categorize the input feature pattern as belonging to one or more classes . for example , a network might be trained to classify animal species based on a set of attributes describing living creatures such as " has tail " , " lives in water " , or " is carnivorous " ; or a network could be trained to recognize visual patterns over a two - dimensional pixel array as a letter in fa ; b ; . . . ; zg . in such tasks , the network is presented with all relevant information simultaneously . in contrast , temporal pattern recognition involves processing of patterns that evolve over time . the appropriate response at a particular point in time depends not only on the current input , but potentially all previous inputs . this is illustrated in figure #NUM# , which shows the basic framework for a temporal prediction problem . i assume that time is quantized into discrete steps , a sensible assumption because many time series of interest are intrinsically discrete , and continuous series can be sampled at a fixed interval . the input at time t is denoted x ( t ) . for univariate series , this input
dislex is an artificial neural network model of the mental lexicon . it was built to test com - putationally whether the lexicon could consist of separate feature maps for the different lexical modalities and the lexical semantics , connected with ordered pathways . in the model , the orthographic , phonological , and semantic feature maps and the associations between them are formed in an unsupervised process , based on cooccurrence of the lexical symbol and its meaning . after the model is organized , various damage to the lexical system can be simulated , resulting in dyslexic and category - specific aphasic impairments similar to those observed in human patients .
in this paper we describe a new selforganizing decomposition technique for learning high - dimensional mappings . problem decomposition is performed in an error - driven manner , such that the resulting subtasks ( patches ) are equally well approximated . our method combines an unsupervised learning scheme ( feature maps [ koh #NUM# ] ) with a nonlinear approximator ( backpropagation [ rhw #NUM# ] ) . the resulting learning system is more stable and effective in changing environments than plain backpropagation and much more powerful than extended feature maps as proposed by [ rs #NUM# , rms #NUM# ] . extensions of our method give rise to active exploration strategies for autonomous agents facing unknown environments . the appropriateness of our general purpose method will be demonstrated with an ex ample from mathematical function approximation .
irrelevant features and weakly relevant features may reduce the comprehensibility and accuracy of concepts induced by supervised learning algorithms . we formulate the search for a feature subset as an abstract search problem with probabilistic estimates . searching a space using an evaluation function that is a random variable requires trading off accuracy of estimates for increased state exploration . we show how recent feature subset selection algorithms in the machine learning literature fit into this search problem as simple hill climbing approaches , and conduct a small experiment using a best - first search technique .
as the field of genetic programming ( gp ) matures and its breadth of application increases , the need for parallel implementations becomes absolutely necessary . the transputer - based system recently presented by koza ( [ #NUM# ] ) is one of the rare such parallel implementations . until today , no implementation has been proposed for parallel gp using a simd architecture , except for a data - parallel approach ( [ #NUM# ] ) , although others have exploited workstation farms and pipelined supercomputers . one reason is certainly the apparent difficulty of dealing with the parallel evaluation of different s - expressions when only a single instruction can be executed at the same time on every processor . the aim of this paper is to present such an implementation of parallel gp on a simd system , where each processor can efficiently evaluate a different s - expression . we have implemented this approach on a maspar mp - #NUM# computer , and will present some timing results . to the extent that simd machines , like the maspar are available to offer cost - effective cycles for scien tific experimentation , this is a useful approach .
reinforcement learning is the problem of generating optimal behavior in a sequential decision - making environment given the opportunity of interacting with it . many algorithms for solving reinforcement - learning problems work by computing improved estimates of the optimal value function . we extend prior analyses of reinforcement - learning algorithms and present a powerful new theorem that can provide a unified analysis of value - function - based reinforcement - learning algorithms . the usefulness of the theorem lies in how it allows the asynchronous convergence of a complex reinforcement - learning algorithm to be proven by verifying that a simpler synchronous algorithm converges . we illustrate the application of the theorem by analyzing the convergence of q - learning , model - based reinforcement learning , q - learning with multi - state updates , q - learning for markov games , and risk - sensitive reinforcement learning .
hyperspectral image sensors provide images with a large number of contiguous spectral channels per pixel and enable information about different materials within a pixel to be obtained . the problem of spectrally unmixing materials may be viewed as a specific case of the blind source separation problem where data consists of mixed signals ( in this case minerals ) and the goal is to determine the contribution of each mineral to the mix without prior knowledge of the minerals in the mix . the technique of independent component analysis ( ica ) assumes that the spectral components are close to statistically independent and provides an unsupervised method for blind source separation . we introduce contextual ica in the context of hyperspectral data analysis and apply the method to mineral data from synthetically mixed minerals and real image signatures .
partially observable markov decision processes ( pomdps ) allow one to model complex dynamic decision or control problems that include both action outcome uncertainty and imperfect observabil - ity . the control problem is formulated as a dynamic optimization problem with a value function combining costs or rewards from multiple steps . in this paper we propose , analyse and test various incremental methods for computing bounds on the value function for control problems with infinite discounted horizon criteria . the methods described and tested include novel incremental versions of grid - based linear interpolation method and simple lower bound method with sondik ' s updates . both of these can work with arbitrary points of the belief space and can be enhanced by various heuristic point selection strategies . also introduced is a new method for computing an initial upper bound the fast informed bound method . this method is able to improve significantly on the standard and commonly used upper bound computed by the mdp - based method . the quality of resulting bounds are tested on a maze navigation problem with #NUM# states , #NUM# actions and #NUM# observations .
the #NUM# energy prediction competition involved the prediction of a series of building energy loads from a series of environmental input variables . non - linear regression using ` neural networks ' is a popular technique for such modeling tasks . since it is not obvious how large a time - window of inputs is appropriate , or what preprocessing of inputs is best , this can be viewed as a regression problem in which there are many possible input variables , some of which may actually be irrelevant to the prediction of the output variable . because a finite data set will show random correlations between the irrelevant inputs and the output , any conventional neural network ( even with reg - ularisation or ` weight decay ' ) will not set the coefficients for these junk inputs to zero . thus the irrelevant variables will hurt the model ' s performance . the automatic relevance determination ( ard ) model puts a prior over the regression parameters which embodies the concept of relevance . this is done in a simple and ` soft ' way by introducing multiple regularisation constants , one associated with each input . using bayesian methods , the regularisation constants for junk inputs are automatically inferred to be large , preventing those inputs from causing significant overfitting .
several different approaches have been used to describe concepts for supervised learning tasks . in this paper we describe two approaches which are : prototype - based incremental neural networks and case - based reasoning approaches . we show then how we can improve a prototype - based neural network model by storing some specific instances in a cbr memory system . this leads us to propose a co - processing hybrid model for classification . #NUM#
in this paper , we present a performance prediction model for indicating the performance range of mimd parallel processor systems for neural network simulations . the model expresses the total execution time of a simulation as a function of the execution times of a small number of kernel functions , which have to be measured on only one processor and one physical communication link . the functions depend on the type of neural network , its geometry , decomposition and the connection structure of the mimd machine . using the model , the execution time , speedup , scalability and efficiency of large mimd systems can be predicted . the model is validated quantitatively by applying it to two popular neural networks , backpropagation and the kohonen self - organizing feature map , decomposed on a gcel - #NUM# #NUM# , a #NUM# transputer system . measurements are taken from network simulations decomposed via dataset and network decomposition techniques . agreement of the model with the measurements is within #NUM# % - #NUM# % . estimates are given for the performances that can be expected for the new t #NUM# transputer systems . the presented method can also be used for other application areas such as image processing .
algorithms for learning classification trees have had successes in artificial intelligence and statistics over many years . this paper outlines how a tree learning algorithm can be derived using bayesian statistics . this introduces bayesian techniques for splitting , smoothing , and tree averaging . the splitting rule is similar to quinlan ' s information gain , while smoothing and averaging replace pruning . comparative experiments with reimplementations of a minimum encoding approach , quinlan ' s c #NUM# ( quinlan et al . , #NUM# ) and breiman et al . ' s cart ( breiman et al . , #NUM# ) show the full bayesian algorithm can produce publication : this paper is a final draft submitted for publication to the statistics and computing journal ; a version with some minor changes appeared in volume #NUM# , #NUM# , pages #NUM# - #NUM# . more accurate predictions than versions of these other approaches , though pay a computational price .
pomdps are general models of sequential decisions in which both actions and observations can be probabilistic . many problems of interest can be formulated as pomdps , yet the use of pomdps has been limited by the lack of effective algorithms . recently this has started to change and a number of problems such as robot navigation and planning are beginning to be formulated and solved as pomdps . the advantage of the pomdp approach is its clean semantics and its ability to produce principled solutions that integrate physical and information gathering actions . in this paper we pursue this approach in the context of two learning tasks : learning to sort a vector of numbers and learning decision trees from data . both problems are formulated as pomdps and solved by a general pomdp algorithm . the main lessons and results are that #NUM# ) the use of suitable heuristics and representations allows for the solution of sorting and classification pomdps of non - trivial sizes , #NUM# ) the quality of the resulting solutions are competitive with the best algorithms , and #NUM# ) problematic aspects in decision tree learning such as test and mis - classification costs , noisy tests , and missing values are naturally accommodated .
given an arbitrary learning situation , it is difficult to determine the most appropriate learning strategy . the goal of this research is to provide a general representation and processing framework for introspective reasoning for strategy selection . the learning framework for an introspective system is to perform some reasoning task . as it does , the system also records a trace of the reasoning itself , along with the results of such reasoning . if a reasoning failure occurs , the system retrieves and applies an introspective explanation of the failure in order to understand the error and repair the knowledge base . a knowledge structure called a meta - explanation pattern is used to both explain how conclusions are derived and why such conclusions fail . if reasoning is represented in an explicit , declarative manner , the system can examine its own reasoning , analyze its reasoning failures , identify what it needs to learn , and select appropriate learning strategies in order to learn the required knowledge without overreli ance on the programmer .
we describe an ongoing project to develop an adaptive training system ( ats ) that dynamically models a students learning processes and can provide specialized tutoring adapted to a students knowledge state and learning style . the student modeling component of the ats , ml - modeler , uses machine learning ( ml ) techniques to emulate the students novice - to - expert transition . ml - modeler infers which learning methods the student has used to reach the current knowledge state by comparing the students solution trace to an expert solution and generating plausible hypotheses about what misconceptions and errors the student has made . a case - based approach is used to generate hypotheses through incorrectly applying analogy , overgeneralization , and overspecialization . the student and expert models use a network - based representation that includes abstract concepts and relationships as well as strategies for problem solving . fuzzy methods are used to represent the uncertainty in the student model . this paper describes the design of the ats and ml - modeler , and gives a detailed example of how the system would model and tutor the student in a typical session . the domain we use for this example is high - school level chemistry .
many algorithms have parameters that should be set by the user . for most machine learning algorithms parameter setting is a non - trivial task that influence knowledge model returned by the algorithm . parameter values are usually set approximately according to some characteristics of the target problem , obtained in different ways . the usual way is to use background knowledge about the target problem ( if any ) and perform some testing experiments . the paper presents an approach to automated model selection based on local optimization that uses an empirical evaluation of the constructed concept description to guide the search . the approach was tested by using the inductive concept learning system magnus
this paper presents a method for learning logic programs without explicit negative examples by exploiting an assumption of output completeness . a mode declaration is supplied for the target predicate and each training input is assumed to be accompanied by all of its legal outputs . any other outputs generated by an incomplete program implicitly represent negative examples ; however , large numbers of ground negative examples never need to be generated . this method has been incorporated into two ilp systems , chillin and ifoil , both of which use intensional background knowledge . tests on two natural language acquisition tasks , case - role mapping and past - tense learning , illustrate the advantages of the approach .
instance - based learning methods explicitly remember all the data that they receive . they usually have no training phase , and only at prediction time do they perform computation . then , they take a query , search the database for similar datapoints and build an on - line local model ( such as a local average or local regression ) with which to predict an output value . in this paper we review the advantages of instance based methods for autonomous systems , but we also note the ensuing cost : hopelessly slow computation as the database grows large . we present and evaluate a new way of structuring a database and a new algorithm for accessing it that maintains the advantages of instance - based learning . earlier attempts to combat the cost of instance - based learning have sacrificed the explicit retention of all data , or been applicable only to instance - based predictions based on a small number of near neighbors or have had to re - introduce an explicit training phase in the form of an interpolative data structure . our approach builds a multiresolution data structure to summarize the database of experiences at all resolutions of interest simultaneously . this permits us to query the database with the same exibility as a conventional linear search , but at greatly reduced computational cost .
this paper introduces a randomized technique for partitioning examples using oblique hyperplanes . standard decision tree techniques , such as id #NUM# and its descendants , partition a set of points with axis - parallel hyper - planes . our method , by contrast , attempts to find hyperplanes at any orientation . the purpose of this more general technique is to find smaller but equally accurate decision trees than those created by other methods . we have tested our algorithm on both real and simulated data , and found that in some cases it produces surprisingly small trees without losing predictive accuracy . small trees allow us , in turn , to obtain simple qualitative descriptions of each problem domain .
this paper introduces icet , a new algorithm for costsensitive classification . icet uses a genetic algorithm to evolve a population of biases for a decision tree induction algorithm . the fitness function of the genetic algorithm is the average cost of classification when using the decision tree , including both the costs of tests ( features , measurements ) and the costs of classification errors . icet is compared here with three other algorithms for costsensitive classification eg #NUM# , cs - id #NUM# , and idx and also with c #NUM# . #NUM# , which classifies without regard to cost . the five algorithms are evaluated empirically on five real - world medical datasets . three sets of experiments are performed . the first set examines the baseline performance of the five algorithms on the five datasets and establishes that icet performs significantly better than its competitors . the second set tests the robustness of icet under a variety of conditions and shows that icet maintains its advantage . the third set looks at icets search in bias space and discovers a way to improve the search .
this paper highlights the role of mathematical programming , particularly linear programming , in training neural networks . a neural network description is given in terms of separating planes in the input space that suggests the use of linear programming for determining these planes . a more standard description in terms of a mean square error in the output space is also given , which leads to the use of unconstrained minimization techniques for training a neural network . the linear programming approach is demonstrated by a brief description of a system for breast cancer diagnosis that has been in use for the last four years at a major medical facility .
dissatisfaction with existing standard case - based reasoning ( cbr ) systems has prompted us to investigate how we can make these systems more creative and , more broadly , what would it mean for them to be more creative . this paper discusses three research goals : understanding creative processes better , investigating the role of cases and cbr in creative problem solving , and understanding the framework that supports this more interesting kind of case - based reasoning . in addition , it discusses methodological issues in the study of creativity and , in particular , the use of cbr as a research paradigm for exploring creativity .
this work presents an application of a machine learning for characterizing an important property of natural dna sequences compositional inhomogeneity . compositional segments often correspond to meaningful biological units . taking into account such inhomogeneity is a prerequisite of successful recognition of functional features in dna sequences , especially , protein - coding genes . here we present a technique for dna segmentation using hidden markov models . a dna sequence is represented by a chain of homogeneous segments , each described by one of a few statistically discriminated hidden states , whose contents form a first - order markov chain . the technique is used to describe and compare chromosomes i and iv of the completely sequenced saccharomyces cerevisiae ( yeast ) genome . our results indicate the existence of a few well separated states , which gives support to the isochore theory . we also explore the model ' s likelihood landscape and analyze the dynamics of the optimization process , thus addressing the problem of reliability of the obtained optima and efficiency of the algorithms .
weight modifications in traditional neural nets are computed by hard - wired algorithms . without exception , all previous weight change algorithms have many specific limitations . is it ( in principle ) possible to overcome limitations of hard - wired algorithms by allowing neural nets to run and improve their own weight change algorithms ? this paper constructively demonstrates that the answer ( in principle ) is ` yes ' . i derive an initial gradient - based sequence learning algorithm for a ` self - referential ' recurrent network that can ` speak ' about its own weight matrix in terms of activations . it uses some of its input and output units for observing its own errors and for explicitly analyzing and modifying its own weight matrix , including those parts of the weight matrix responsible for analyzing and modifying the weight matrix . the result is the first ` introspective ' neural net with explicit potential control over all of its own adaptive parameters . a disadvantage of the algorithm is its high computational complexity per time step which is independent of the sequence length and equals o ( n conn logn conn ) , where n conn is the number of connections . another disadvantage is the high number of local minima of the unusually complex error surface . the purpose of this paper , however , is not to come up with the most efficient ` introspective ' or ` self - referential ' weight change algorithm , but to show that such algorithms are possible at all .
this paper discusses the problem of how to implement many - to - many , or multi - associative , mapping within connectionist models . traditional symbolic approaches work through explicit representation of all alternatives via stored links , or implicitly through enumerative algorithms . classical pattern association models ignore the issue of generating multiple outputs for a single input pattern , and while recent research on recurrent networks is promising , the field has not clearly focused upon multi - associativity as a goal . in this paper , we define multiassociative memory ( mm ) , and several possible variants , and discuss its utility in general cognitive modeling . we extend sequential cascaded networks ( pollack #NUM# , #NUM# a ) to fit the task , and perform several initial experiments which demonstrate the feasibility of the concept .
human visual systems maintain a stable internal representation of a scene even though the image on the retina is constantly changing because of eye movements . such stabilization can theoretically be effected by dynamic shifts in the receptive field ( rf ) of neurons in the visual system . this paper examines how a neural circuit can learn to generate such shifts . the shifts are controlled by eye position signals and compensate for the movement in the retinal image caused by eye movements . the development of a neural shifter circuit ( olshausen , anderson , & van essen , #NUM# ) is modeled using triadic connections . these connections are gated by signals that indicate the direction of gaze ( eye position signals ) . in simulations , a neural model is exposed to sequences of stimuli paired with appropriate eye position signals . the initially
this paper tries to identify rules and factors that are predictive for the outcome of international conflict management attempts . we use c #NUM# . #NUM# , an advanced machine learning algorithm , for generating decision trees and prediction rules from cases in the confman database . the results show that simple patterns and rules are often not only more understandable , but also more reliable than complex rules . simple decision trees are able to improve the chances of correctly predicting the outcome of a conflict management attempt . this suggests that mediation is more repetitive than conflicts per se , where such results have not been achieved so far .
a technique is described which allows unimodal function optimization methods to be extended to efficiently locate all optima of multimodal problems . we describe an algorithm based on a traditional genetic algorithm ( ga ) . this involves iterating the ga , but uses knowledge gained during one iteration to avoid re - searching , on subsequent iterations , regions of problem space where solutions have already been found . this is achieved by applying a fitness derating function to the raw fitness function , so that fitness values are depressed in the regions of the problem space where solutions have already been found . consequently , the likelihood of discovering a new solution on each iteration is dramatically increased . the technique may be used with various styles of ga , or with other optimization methods , such as simulated annealing . the effectiveness of the algorithm is demonstrated on a number of multimodal test functions . the technique is at least as fast as fitness sharing methods . it provides a speedup of between #NUM# and #NUM# p on a problem with p optima , depending on the value of p and the convergence time complexity .
in this paper , we examine the intuition that td ( ) is meant to operate by approximating asynchronous value iteration . we note that on the important class of discrete acyclic stochastic tasks , value iteration is inefficient compared with the dag - sp algorithm , which essentially performs only one sweep instead of many by working backwards from the goal . the question we address in this paper is whether there is an analogous algorithm that can be used in large stochastic state spaces requiring function approximation . we present such an algorithm , analyze it , and give comparative results to td on several domains . the state ) . using vi to solve mdps belonging to either of these special classes can be quite inefficient , since vi performs backups over the entire space , whereas the only backups useful for improving v fl are those on the " frontier " between already - correct and not - yet - correct v fl values . in fact , there are classical algorithms for both problem classes which compute v fl more efficiently by explicitly working backwards : for the deterministic class , dijkstra ' s shortest - path algorithm ; and for the acyclic class , directed - acyclic - graph - shortest - paths ( dag - sp ) [ #NUM# ] . #NUM# dag - sp first topologically sorts the mdp , producing a linear ordering of the states in which every state x precedes all states reachable from x . then , it runs through that list in reverse , performing one backup per state . worst - case bounds for vi , dijkstra , and dag - sp in deterministic domains with x states and a actions / state are #NUM# although [ #NUM# ] presents dag - sp only for deterministic acyclic problems , it applies straightforwardly to the
automatic design optimization is highly sensitive to problem formulation . the choice of objective function , constraints and design parameters can dramatically impact the computational cost of optimization and the quality of the resulting design . the best formulation varies from one application to another . a design engineer will usually not know the best formulation in advance . we have therefore developed a system that supports interactive formulation , testing and reformulation of design optimization strategies . our system includes an executable , second - order data - flow language for representing optimization strategies . the language allows an engineer to define multiple stages of optimization , and to specify the design parameters , constraints and objectives to be handled at each stage . we have also developed a set of transformations that reformulate strategies represented in our language . the transformations can approximate objective and constraint functions , define new constraints , and re - parameterize or change the dimension of the search space , among other things . the system is applicable to design problems in which the artifact is governed by algebraic or ordinary differential equations . we have tested the system on problems of racing yacht and jet engine nozzle design . we report experimental results demonstrating that our reformulation techniques can significantly improve the performance of automatic design optimization .
the classification performance of a neural network for combined six - band landsat - tm and one - band ers - #NUM# / sar pri imagery from the same scene is carried out . different combinations of the data | either raw , segmented or filtered | , using the available ground truth polygons , training and test sets are created . the training sets are used for learning while the test sets are used for verification of the neural network . the different combinations are evaluated here .
the problem of modeling complicated data sequences , such as dna or speech , often arises in practice . most of the algorithms select a hypothesis from within a model class assuming that the observed sequence is the direct output of the underlying generation process . in this paper we consider the case when the output passes through a memoryless noisy channel before observation . in particular , we show that in the class of markov chains with variable memory length , learning is affected by factors , which , despite being super - polynomial , are still small in some practical cases . markov models with variable memory length , or probabilistic finite suffix automata , were introduced in learning theory by ron , singer and tishby who also described a polynomial time learning algorithm [ #NUM# , #NUM# ] . we present a modification of the algorithm which uses a noise - corrupted sample and has knowledge of the noise structure . the same algorithm is still viable if the noise is not known exactly but a good estimation is available . finally , some experimental results are presented for removing noise from corrupted english text , and to measure how the performance of the learning algorithm is affected by the size of the noisy sample and the noise rate .
we present and evaluate an implemented system with which to rapidly and easily build intelligent software agents for web - based tasks . our design is centered around two basic functions : scorethislink and scorethispage . if given highly accurate such functions , standard heuristic search would lead to efficient retrieval of useful information . our approach allows users to tailor our system ' s behavior by providing approximate advice about the above functions . this advice is mapped into neural network implementations of the two functions . subsequent reinforcements from the web ( e . g . , dead links ) and any ratings of retrieved pages that the user wishes to provide are , respectively , used to refine the link - and page - scoring functions . hence , our architecture provides an appealing middle ground between nonadaptive agent programming languages and systems that solely learn user preferences from the user ' s ratings of pages . we describe our internal representation of web pages , the major predicates in our advice language , how advice is mapped into neural networks , and the mechanisms for refining advice based on subsequent feedback . we also present a case study where we provide some simple advice and specialize our general - purpose system into a " home - page finder " . an empirical study demonstrates that our approach leads to a more effective home - page finder than that of a leading commercial web search site .
in concept learning , objects in a domain are grouped together based on similarity as determined by the attributes used to describe them . existing concept learners require that this set of attributes be known in advance and presented in entirety before learning begins . additionally , most systems do not possess mechanisms for altering the attribute set after concepts have been learned . consequently , a veridical attribute set relevant to the task for which the concepts are to be used must be supplied at the onset of learning , and in turn , the usefulness of the concepts is limited to the task for which the attributes were originally selected . in order to efficiently accommodate changing contexts , a concept learner must be able to alter the set of descriptors without discarding its prior knowledge of the domain . we introduce the notion of attribute - incrementation , the dynamic modification of the attribute set used to describe instances in a problem domain . we have implemented the capability in a concept learning system that has been evaluated along several dimensions using an existing concept formation system for com parison .
this article presents a new reinforcement learning method called sane ( symbiotic , adaptive neuro - evolution ) , which evolves a population of neurons through genetic algorithms to form a neural network capable of performing a task . symbiotic evolution promotes both cooperation and specialization , which results in a fast , efficient genetic search and discourages convergence to suboptimal solutions . in the inverted pendulum problem , sane formed effective networks #NUM# to #NUM# times faster than the adaptive heuristic critic and #NUM# times faster than q - learning and the genitor neuro - evolution approach without loss of generalization . such efficient learning , combined with few domain assumptions , make sane a promising approach to a broad range of reinforcement learning problems , including many real - world applications .
the paper concerns the probabilistic evaluation of plans in the presence of unmeasured variables , each plan consisting of several concurrent or sequential actions . we establish a graphical criterion for recognizing when the effects of a given plan can be predicted from passive observations on measured variables only . when the criterion is satisfied , a closed - form expression is provided for the probability that the plan will achieve a specified goal .
we develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics . our mean field theory provides a tractable approximation to the true probability distribution in these networks ; it also yields a lower bound on the likelihood of evidence . we demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition | the classification of handwritten digits .
many " learning from experience " systems use information extracted from problem solving experiences to modify a performance element pe , forming a new element pe #NUM# that can solve these and similar problems more efficiently . however , as transformations that improve performance on one set of problems can degrade performance on other sets , the new pe #NUM# is not always better than the original pe ; this depends on the distribution of problems . we therefore seek the performance element whose expected performance , over this distribution , is optimal . unfortunately , the actual distribution , which is needed to determine which element is optimal , is usually not known . moreover , the task of finding the optimal element , even knowing the distribution , is intractable for most interesting spaces of elements . this paper presents a method , palo , that side - steps these problems by using a set of samples to estimate the unknown distribution , and by using a set of transformations to hill - climb to a local optimum . this process is based on a mathematically rigorous form of utility analysis : in particular , it uses statistical techniques to determine whether the result of a proposed transformation will be better than the original system . we also present an efficient way of implementing this learning system in the context of a general class of performance elements , and include empirical evidence that this approach can work effectively .
compositional q - learning ( cq - l ) ( singh #NUM# ) is a modular approach to learning to perform composite tasks made up of several elemental tasks by reinforcement learning . skills acquired while performing elemental tasks are also applied to solve composite tasks . individual skills compete for the right to act and only winning skills are included in the decomposition of the composite task . we extend the original cq - l concept in two ways : ( #NUM# ) a more general reward function , and ( #NUM# ) the agent can have more than one actuator . we use the cq - l architecture to acquire skills for performing composite tasks with a simulated two - linked manipulator having large state and action spaces . the manipulator is a non - linear dynamical system and we require its end - effector to be at specific positions in the workspace . fast function approximation in each of the q - modules is achieved through the use of an array of cerebellar model articulation controller ( cmac ) ( albus
the processing performed by a feed - forward neural network is often interpreted through use of decision hyperplanes at each layer . the adaptation process , however , is normally explained using the picture of gradient descent of an error landscape . in this paper the dynamics of the decision hyperplanes is used as the model of the adaptation process . a electro - mechanical analogy is drawn where the dynamics of hyperplanes is determined by interaction forces between hyperplanes and the particles which represent the patterns . relaxation of the system is determined by increasing hyperplane inertia ( mass ) . this picture is used to clarify the dynamics of learning , and to go some way to explaining learning deadlocks and escaping from certain local minima . furthermore network plasticity is introduced as a dynamic property of the system , and its reduction as a necessary consequence of information storage . hyper - plane inertia is used to explain and avoid destructive relearning in trained networks .
modifications to recursive auto - associative memory are presented , which allow it to store deeper and more complex data structures than previously reported . these modifications include adding extra layers to the compressor and reconstructor networks , employing integer rather than real - valued representations , pre - conditioning the weights and pre - setting the representations to be compatible with them . the resulting system is tested on a data set of syntactic trees extracted from the penn treebank .
the problem of combining preferences arises in several applications , such as combining the results of different search engines . this work describes an efficient algorithm for combining multiple preferences . we first give a formal framework for the problem . we then describe and analyze a new boosting algorithm for combining preferences called rankboost . we also describe an efficient implementation of the algorithm for a restricted case . we discuss two experiments we carried out to assess the performance of rankboost . in the first experiment , we used the algorithm to combine different search strategies , each of which is a query expension for a given domain . for this task , we compare the performance of rankboost to the individual search strategies . the second experiment is a collaborative - filtering task , specifically , for making movie recommendations . here , we present results comparing rankboost to nearest neighbor and regression algorithms .
this paper shows that decision trees can be used to improve the performance of case - based learning ( cbl ) systems . we introduce a performance task for machine learning systems called semi - flexible prediction that lies between the classification task performed by decision tree algorithms and the flexible prediction task performed by conceptual clustering systems . in semi - flexible prediction , learning should improve prediction of a specific set of features known a priori rather than a single known feature ( as in classification ) or an arbitrary set of features ( as in conceptual clustering ) . we describe one such task from natural language processing and present experiments that compare solutions to the problem using decision trees , cbl , and a hybrid approach that combines the two . in the hybrid approach , decision trees are used to specify the features to be included in k - nearest neighbor case retrieval . results from the experiments show that the hybrid approach outperforms both the decision tree and case - based approaches as well as two case - based systems that incorporate expert knowledge into their case retrieval algorithms . results clearly indicate that decision trees can be used to improve the performance of cbl systems and do so without reliance on potentially expensive expert knowledge .
technical report no . #NUM# , department of statistics , university of toronto we describe a linear network that models correlations between real - valued visible variables using one or more real - valued hidden variables a factor analysis model . this model can be seen as a linear version of the helmholtz machine , and its parameters can be learned using the wake - sleep method , in which learning of the primary generative model is assisted by a recognition model , whose role is to fill in the values of hidden variables based on the values of visible variables . the generative and recognition models are jointly learned in wake and sleep phases , using just the delta rule . this learning procedure is comparable in simplicity to oja ' s version of hebbian learning , which produces a somewhat different representation of correlations in terms of principal components . we argue that the simplicity of wake - sleep learning makes factor analysis a plau sible alternative to hebbian learning as a model of activity - dependent cortical plasticity .
a bayesian method for estimating the amino acid distributions in the states of a hidden markov model ( hmm ) for a protein family or the columns of a multiple alignment of that family is introduced . this method uses dirichlet mixture densities as priors over amino acid distributions . these mixture densities are determined from examination of previously constructed hmms or multiple alignments . it is shown that this bayesian method can improve the quality of hmms produced from small training sets . specific experiments on the ef - hand motif are reported , for which these priors are shown to produce hmms with higher likelihood on unseen data , and fewer false positives and false negatives in a database search task .
this paper proposes a simple cost model for machine learning applications based on the notion of net present value . the model extends and unifies the models used in ( pazzani et al . , #NUM# ) and ( masand & piatetsky - shapiro , #NUM# ) . it attempts to answer the question " should a given machine learning system now in the prototype stage be fielded ? " the model ' s inputs are the system ' s confusion matrix , the cash flow matrix for the application , the cost per decision , the one - time cost of deploying the system , and the rate of return on investment . like provost and fawcett ' s ( #NUM# ) roc convex hull method , the present model can be used for decision - making even when its input variables are not known exactly . despite its simplicity , it has a number of non - trivial consequences . for example , under it the " no free lunch " theorems of learning theory no longer apply .
this paper demonstrates the use of graphs as a mathematical tool for expressing independenices , and as a formal language for communicating and processing causal information in statistical analysis . we show how complex information about external interventions can be organized and represented graphically and , conversely , how the graphical representation can be used to facilitate quantitative predictions of the effects of interventions . we first review the markovian account of causation and show that directed acyclic graphs ( dags ) offer an economical scheme for representing conditional independence assumptions and for deducing and displaying all the logical consequences of such assumptions . we then introduce the manipulative account of causation and show that any dag defines a simple transformation which tells us how the probability distribution will change as a result of external interventions in the system . using this transformation it is possible to quantify , from non - experimental data , the effects of external interventions and to specify conditions under which randomized experiments are not necessary . finally , the paper offers a graphical interpretation for rubin ' s model of causal effects , and demonstrates its equivalence to the manipulative account of causation . we exemplify the tradeoffs between the two approaches by deriving nonparametric bounds on treatment effects under conditions of imperfect compliance .
in case - based design , the adaptation of a design case to new design requirements plays an important role . if it is sufficient to adapt a predefined set of design parameters , the task is easily automated . if , however , more far - reaching , creative changes are required , current systems provide only limited success . this paper describes an approach to creative design adaptation based on the notion of creativity as ' goal oriented shift of focus of a search process ' . an evolving representation is used to restructure the search space so that designs similar to the example case lie in the focus of the search . this focus is than used as a starting point to create new designs .
we consider a novel non - linear model for time series analysis . the study of this model emphasizes both theoretical aspects as well as practical applicability . the architecture of the model is demonstrated to be sufficiently rich , in the sense of approximating unknown functional forms , yet it retains some of the simple and intuitive characteristics of linear models . a comparison to some more established non - linear models will be emphasized , and theoretical issues are backed by prediction results for benchmark time series , as well as computer generated data sets . efficient estimation algorithms are seen to be applicable , made possible by the mixture based structure of the model . large sample properties of the estimators are discussed as well , in both well specified as well as misspecified settings . we also demonstrate how inference pertaining to the data structure may be made from the parameterization of the model , resulting in a better , more intuitive , understanding of the structure and performance of the model .
the coverage of a learning algorithm is the number of concepts that can be learned by that algorithm from samples of a given size . this paper asks whether good learning algorithms can be designed by maximizing their coverage . the paper extends a previous upper bound on the coverage of any boolean concept learning algorithm and describes two algorithms | multi - balls and large - ball | whose coverage approaches this upper bound . experimental measurement of the coverage of the id #NUM# and fringe algorithms shows that their coverage is far below this bound . further analysis of large - ball shows that although it learns many concepts , these do not seem to be very interesting concepts . hence , coverage maximization alone does not appear to yield practically - useful learning algorithms . the paper concludes with a definition of coverage within a bias , which suggests a way that coverage maximization could be applied to strengthen weak preference biases .
at the previous foga workshop , we presented some initial results on using markov models to analyze the transient behavior of genetic algorithms ( gas ) being used as function optimizers ( gafos ) . in that paper , the states of the markov model were ordered via a simple and mathematically convenient lexicographic ordering used initially by nix and vose . in this paper , we explore alternative orderings of states based on interesting semantic properties such as average fitness , degree of homogeneity , average attractive force , etc . we also explore lumping techniques for reducing the size of the state space . analysis of these reordered and lumped markov models provides new insights into the transient behavior of gas in general and gafos in particular .
an important aspect of creative design is the concept of emergence . though emergence is important , its mechanism is either not well understood or it is limited to the domain of shapes . this deficiency can be compensated by considering definitions of emergent behaviour from the artificial life ( alife ) research community . with these new insights , it is proposed that a computational technique , called evolving representations of design genes , can be extended to emergent behaviour . we demonstrate emergent be - haviour in a co - evolutionary model of design . this co - evolutionary approach to design allows a solution space ( structure space ) to evolve in response to a problem space ( be - haviour space ) . since the behaviour space is now an active participant , behaviour may emerge with new structures at the end of the design process . this paper hypothesizes that emergent behaviour can be identified using the same technique . the floor plan example of ( gero & schnier #NUM# ) is extended to demonstrate how behaviour can emerge in a co - evolutionary design process .
we investigate learnability in the pac model when the data used for learning , attributes and labels , is either corrupted or incomplete . in order to prove our main results , we define a new complexity measure on statistical query ( sq ) learning algorithms . the view of an sq algorithm is the maximum over all queries in the algorithm , of the number of input bits on which the query depends . we show that a restricted view sq algorithm for a class is a general sufficient condition for learnability in both the models of attribute noise and covered ( or missing ) attributes . we further show that since the algorithms in question are statistical , they can also simultaneously tolerate classification noise . classes for which these results hold , and can therefore be learned with simultaneous attribute noise and classification noise , include k - dnf , k - term - dnf by dnf representations , conjunctions with few relevant variables , and over the uniform distribution , decision lists . these noise models are the first pac models in which all training data , attributes and labels , may be corrupted by a random process . previous researchers had shown that the class of k - dnf is learnable with attribute noise if the attribute noise rate is known exactly . we show that all of our attribute noise learnabil - ity results , either with or without classification noise , also hold when the exact noise rate is not appeared in proceedings of the eighth annual acm conference on computational learning theory . acm press , july #NUM# . known , provided that the learner instead has a polynomially good approximation of the noise rate . in addition , we show that the results also hold when there is not one single noise rate , but a distinct noise rate for each attribute . our results for learning with random covering do not require the learner to be told even an approximation of the covering rate and in addition hold in the setting with distinct covering rates for each attribute . finally , we give lower bounds on the number of examples required for learning in the presence of attribute noise or covering .
this study describes a new hidden markov model ( hmm ) system for segmenting uncharacterized genomic dna sequences into exons , introns , and intergenic regions . separate hmm modules were designed and trained for specific regions of dna : exons , introns , intergenic regions , and splice sites . the models were then tied together to form a biologically feasible topology . the integrated hmm was trained further on a set of eukaryotic dna sequences , and tested by using it to segment a separate set of sequences . the resulting hmm system , which is called veil ( viterbi exon - intron locator ) , obtains an overall accuracy on test data of #NUM# % of total bases correctly labelled , with a correlation coefficient of #NUM# . #NUM# . using the more stringent test of exact exon prediction , veil correctly located both ends of #NUM# % of the exons . moreover , more than #NUM# % of the exons it predicts are exactly correct . these results compare favorably to the best previous results for gene structure prediction , and demonstrate the benefits of using hmms for this problem .
recently , there has been an increased interest in lifelong machine learning methods , that transfer knowledge across multiple learning tasks . such methods have repeatedly been found to outperform conventional , single - task learning algorithms when the learning tasks are appropriately related . to increase robustness of such approaches , methods are desirable that can reason about the relatedness of individual learning tasks , in order to avoid the danger arising from tasks that are unrelated and thus potentially misleading . this paper describes the task - clustering ( tc ) algorithm . tc clusters learning tasks into classes of mutually related tasks . when facing a new learning task , tc first determines the most related task cluster , then exploits information selectively from this task cluster only . an empirical study carried out in a mobile robot domain shows that tc outperforms its non - selective counterpart in situations where only a small number of tasks is relevant .
belief revision and belief update have been proposed as two types of belief change serving different purposes . belief revision is intended to capture changes of an agent ' s belief state reflecting new information about a static world . belief update is intended to capture changes of belief in response to a changing world . we argue that both belief revision and belief update are too restrictive ; routine belief change involves elements of both . we present a model for generalized update that allows updates in response to external changes to inform the agent about its prior beliefs . this model of update combines aspects of revision and update , providing a more realistic characterization of belief change . we show that , under certain assumptions , the original update postulates are satisfied . we also demonstrate that plain revision and plain update are special cases of our model , in a way that formally verifies the intuition that revision is suitable for static belief change .
we propose a bayesian framework for regression problems , which covers areas which are usually dealt with by function approximation . an online learning algorithm is derived which solves regression problems with a kalman filter . its solution always improves with increasing model complexity , without the risk of over - fitting . in the infinite dimension limit it approaches the true bayesian posterior . the issues of prior selection and over - fitting are also discussed , showing that some of the commonly held beliefs are misleading . the practical implementation is summarised . simulations using #NUM# popular publicly available data sets are used to demonstrate the method and highlight important issues concerning the choice of priors .
the problem of belief changehow an agent should revise her beliefs upon learning new informationhas been an active area of research in both philosophy and artificial intelligence . many approaches to belief change have been proposed in the literature . our goal is not to introduce yet another approach , but to examine carefully the rationale underlying the approaches already taken in the literature , and to highlight what we view as methodological problems in the literature . the main message is that to study belief change carefully , we must be quite explicit about the ontology or scenario underlying the belief change process . this is something that has been missing in previous work , with its focus on postulates . our analysis shows that we must pay particular attention to two issues which have often been taken for granted : the first is how we model the agent ' s epistemic state . ( do we use a set of beliefs , or a richer structure , such as an ordering on worlds ? and if we use a set of beliefs , in what language are these beliefs are expressed ? ) the second is the status of observations . ( are observations known to be true , or just believed ? in the latter case , how firm is the belief ? ) for example , we argue that even postulates that have been called beyond controversy are unreasonable when the agent ' s beliefs include beliefs about her own epistemic state as well as the external world . issues of the status of observations arise particularly when we consider iterated belief revision , and we must confront the possibility of revising by ' and then by : ' .
the study of belief change has been an active area in philosophy and ai . in recent years , two special cases of belief change , belief revision and belief update , have been studied in detail . roughly speaking , revision treats a surprising observation as a sign that previous beliefs were wrong , while update treats a surprising observation as an indication that the world has changed . in general , we would expect that an agent making an observation may both want to revise some earlier beliefs and assume that some change has occurred in the world . we define a novel approach to belief change that allows us to do this , by applying ideas from probability theory in a qualitative settings . the key idea is to use a qualitative markov assumption , which says that state transitions are independent . we show that a recent approach to modeling qualitative uncertainty using plausibility measures allows us to make such a qualitative markov assumption in a relatively straightforward way , and show how the markov assumption can be used to provide an attractive belief - change model .
in reinforcement learning it is frequently necessary to resort to an approximation to the true optimal value function . here we investigate the benefits of online search in such cases . we examine " local " searches , where the agent performs a finite - depth lookahead search , and " global " searches , where the agent performs a search for a trajectory all the way from the current state to a goal state . the key to the success of these methods lies in taking a value function , which gives a rough solution to the hard problem of finding good trajectories from every single state , and combining that with online search , which then gives an accurate solution to the easier problem of finding a good trajectory specifically from the current state .
probabilistic context - free grammars ( pcfgs ) provide a simple way to represent a particular class of distributions over sentences in a context - free language . efficient parsing algorithms for answering particular queries about a pcfg ( i . e . , calculating the probability of a given sentence , or finding the most likely parse ) have been developed , and applied to a variety of pattern - recognition problems . we extend the class of queries that can be answered in several ways : ( #NUM# ) allowing missing tokens in a sentence or sentence fragment , ( #NUM# ) supporting queries about intermediate structure , such as the presence of particular nonterminals , and ( #NUM# ) flexible conditioning on a variety of types of evidence . our method works by constructing a bayesian network to represent the distribution of parse trees induced by a given pcfg . the network structure mirrors that of the chart in a standard parser , and is generated using a similar dynamic - programming approach . we present an algorithm for constructing bayesian networks from pcfgs , and show how queries or patterns of queries on the network correspond to interesting queries on pcfgs . the network formalism also supports extensions to encode various context sensitivities within the probabilistic dependency structure .
this paper presents recent developments toward a formalism that combines useful properties of both logic and probabilities . like logic , the formalism admits qualitative sentences and provides symbolic machinery for deriving deductively closed beliefs and , like probability , it permits us to express if - then rules with different levels of firmness and to retract beliefs in response to changing observations . rules are interpreted as order - of - magnitude approximations of conditional probabilities which impose constraints over the rankings of worlds . inferences are supported by a unique priority ordering on rules which is syntactically derived from the knowledge base . this ordering accounts for rule interactions , respects specificity considerations and facilitates the construction of coherent states of beliefs . practical algorithms are developed and analyzed for testing consistency , computing rule ordering , and answering queries . imprecise observations are incorporated using qualitative versions of jef - frey ' s rule and bayesian updating , with the result that coherent belief revision is embodied naturally and tractably . finally , causal rules are interpreted as imposing markovian conditions that further constrain world rankings to reflect the modularity of causal organizations . these constraints are shown to facilitate reasoning about causal projections , explanations , actions and change .
clay is an evolutionary architecture for autonomous robots that integrates motor schema - based control and reinforcement learning . robots utilizing clay benefit from the real - time performance of motor schemas in continuous and dynamic environments while taking advantage of adaptive reinforcement learning . clay coordinates assemblages ( groups of motor schemas ) using embedded reinforcement learning modules . the coordination modules activate specific assemblages based on the presently perceived situation . learning occurs as the robot selects assemblages and samples a reinforcement signal over time . experiments in a robot soccer simulation illustrate the performance and utility of the system .
most known learning algorithms for dynamic neural networks in non - stationary environments need global computations to perform credit assignment . these algorithms either are not local in time or not local in space . those algorithms which are local in both time and space usually can not deal sensibly with ` hidden units ' . in contrast , as far as we can judge by now , learning rules in biological systems with many ` hidden units ' are local in both space and time . in this paper we propose a parallel on - line learning algorithm which performs local computations only , yet still is designed to deal with hidden units and with units whose past activations are ` hidden in time ' . the approach is inspired by holland ' s idea of the bucket brigade for classifier systems , which is transformed to run on a neural network with fixed topology . the result is a feedforward or recurrent ` neural ' dissipative system which is consuming ` weight - substance ' and permanently trying to distribute this substance onto its connections in an appropriate way . simple experiments demonstrating the feasability of the algorithm are reported .
although creativity has largely been studied in problem solving contexts , creativity consists of both a generative component and a comprehension component . in particular , creativity is an essential part of reading and understanding of natural language stories . we have formalized the understanding process and have developed an algorithm capable of producing creative understanding behavior . we have also created a novel knowledge organization scheme to assist the process . our model of creativity is implemented as a portion of the isaac ( integrated story analysis and creativity ) reading system , a system which models the creative reading of science fiction stories .
creative designers often see solutions to pending design problems in the everyday objects surrounding them . this can often lead to innovation and insight , sometimes revealing new functions and purposes for common design pieces in the process . we are interested in modeling serendipitous recognition of solutions to pending problems in the context of creative mechanical design . this paper characterizes this ability , analyzing observations we have made of it , and placing it in the context of other forms of recognition . we propose a computational model to capture and explore serendipitous recognition which is based on ideas from reconstructive dynamic memory and situation assessment in case - based reasoning .
we consider learning in situations where the function used to classify examples may switch back and forth between a small number of different concepts during the course of learning . we examine several models for such situations : oblivious models in which switches are made independent of the selection of examples , and more adversarial models in which a single adversary controls both the concept switches and example selection . we show relationships between the more benign models and the p - concepts of kearns and schapire , and present polynomial - time algorithms for learning switches between two k - dnf formulas . for the most adversarial model , we present a model of success patterned after the popular competitive analysis used in studying on - line algorithms . we describe a randomized query algorithm for such adversarial switches between two monotone disjunctions that is " #NUM# - competitive " in that the total number of mistakes plus queries is with high probability bounded by the number of switches plus some fixed polynomial in n ( the number of variables ) . we also use notions described here to provide sufficient conditions under which learning a p - concept class " with a decision rule " implies being able to learn the class " with a model of probability . "
case based systems typically retrieve cases from the case base by applying similarity measures . the measures are usually constructed in an ad hoc manner . this paper presents a theoretical framework for the systematic construction of similarity measures . in addition to paving the way to a design methodology for similarity measures , this systematic approach facilitates the identification of opportunities for par allelisation in case base retrieval .
in real world applications , software engineers recognise the use of memory must be organised via data structures and that software using the data must be independant of the data structures ' implementation details . they achieve this by using abstract data structures , such as records , files and buffers . we demonstrate that genetic programming can automatically implement simple abstract data structures , considering in detail the task of evolving a list . we show general and reasonably efficient implementations can be automatically generated from simple primitives . a model for maintaining evolved code is demonstrated using the list problem . much published work on genetic programming ( gp ) evolves functions without side - effects to learn patterns in test data . in contrast human written programs often make extensive and explicit use of memory . indeed memory in some form is required for a programming system to be turing complete , i . e . for it to be possible to write any ( computable ) program in that system . however inclusion of memory can make the interactions between parts of programs much more complex and so make it harder to produce programs . despite this it has been shown gp can automatically create programs which explicitly use memory [ teller #NUM# ] . in both normal and genetic programming considerable benefits have been found in adopting a structured approach . for example [ koza #NUM# ] shows the introduction of evolvable code modules ( automatically defined functions , adfs ) can greatly help gp to reach a solution . we suggest that a corresponding structured approach to use of data will similarly have significant advantage to gp . earlier work has demonstrated that genetic programming can automatically generate simple abstract data structures , namely stacks and queues [ langdon #NUM# a ] . that is , gp can evolve programs that organise memory ( accessed via simple read and write primitives ) into data structures which can be used by external software without it needing to know how they are implemented . this chapter shows it is possible to evolve a list data structure from basic primitives . [ aho , hopcroft and ullman #NUM# ] suggest three different ways to implement a list but these experiments show gp can evolve its own implementation . this requires all the list components to agree on one implementation as they co - evolve together . section #NUM# . #NUM# describes the gp architecture , including use of pareto multiple component fitness scoring ( #NUM# . #NUM# . #NUM# ) and measures aimed at speeding the gp search ( #NUM# . #NUM# . #NUM# ) . the evolved solutions are described in section #NUM# . #NUM# . section #NUM# . #NUM# presents a candidate model for maintaining evolved software . this is followed by a discussion of what we have learned ( #NUM# . #NUM# ) and conclusions that can be drawn ( #NUM# . #NUM# ) .
we introduce a convergence diagnostic procedure for mcmc which operates by estimating total variation distances for the distribution of the algorithm after certain numbers of iterations . the method has advantages over many existing methods in terms of applicability , utility , computational expense and interpretability . it can be used to assess convergence of both marginal and joint posterior densities , and we show how it can be applied to the two most commonly used mcmc samplers ; the gibbs sampler and the metropolis hastings algorithm . illustrative examples highlight the utility and interpretability of the proposed diagnostic , but also highlight some of its limitations .
because of the distance between the skull and brain and their different resistivities , electroencephalographic ( eeg ) data collected from any point on the human scalp includes activity generated within a large brain area . this spatial smearing of eeg data by volume conduction does not involve significant time delays , however , suggesting that the independent component analysis ( ica ) algorithm of bell and sejnowski [ #NUM# ] is suitable for performing blind source separation on eeg data . the ica algorithm separates the problem of source identification from that of source localization . first results of applying the ica algorithm to eeg and event - related potential ( erp ) data collected during a sustained auditory detection task show : ( #NUM# ) ica training is insensitive to different random seeds . ( #NUM# ) ica may be used to segregate obvious artifactual eeg components ( line and muscle noise , eye movements ) from other sources . ( #NUM# ) ica is capable of isolating overlapping eeg phenomena , including alpha and theta bursts and spatially - separable erp components , to separate ica channels . ( #NUM# ) nonstationarities in eeg and behavioral state can be tracked using ica via changes in the amount of residual correlation between ica - filtered output channels .
the magical number seven , plus or minus two : some limits on our capacity for processing information . the psychological review , #NUM# ( #NUM# ) : #NUM# - #NUM# . schmidhuber , j . ( #NUM# b ) . towards compositional learning with dynamic neural networks . technical report fki - #NUM# - #NUM# , technische universitat munchen , institut fu informatik . servan - schreiber , d . , cleermans , a . , and mcclelland , j . ( #NUM# ) . encoding sequential structure in simple recurrent networks . technical report cmu - cs - #NUM# - #NUM# , carnegie mellon university , computer science department .
the standard approach to decision tree induction is a top - down , greedy algorithm that makes locally optimal , irrevocable decisions at each node of a tree . in this paper , we study an alternative approach , in which the algorithms use limited lookahead to decide what test to use at a node . we systematically compare , using a very large number of decision trees , the quality of decision trees induced by the greedy approach to that of trees induced using lookahead . the main results of our experiments are : ( i ) the greedy approach produces trees that are just as accurate as trees produced with the much more expensive lookahead step ; and ( ii ) decision tree induction exhibits pathology , in the sense that lookahead can produce trees that are both larger and less accurate than trees produced without it .
this thesis presents a machine learning model capable of extracting discrete classes out of continuous valued input features . this is done using a neurally inspired novel competitive classifier ( cc ) which feeds the discrete classifications forward to a supervised machine learning model . the supervised learning model uses the discrete classifications and perhaps other information available to solve a problem . the supervised learner then generates feedback to guide the cc into potentially more useful classifications of the continuous valued input features . two supervised learning models are combined with the cc creating asocs - afe and id #NUM# - afe . both models are simulated and the results are analyzed . based on these results , several areas of future research are proposed .
we are frequently called upon to perform multiple tasks that compete for our attention and resource . often we know the optimal solution to each task in isolation ; in this paper , we describe how this knowledge can be exploited to efficiently find good solutions for doing the tasks in parallel . we formulate this problem as that of dynamically merging multiple markov decision processes ( mdps ) into a composite mdp , and present a new theoretically - sound dynamic programming algorithm for finding an optimal policy for the composite mdp . we analyze various aspects of our algorithm and every day , we are faced with the problem of doing multiple tasks in parallel , each of which competes for our attention and resource . if we are running a job shop , we must decide which machines to allocate to which jobs , and in what order , so that no jobs miss their deadlines . if we are a mail delivery robot , we must find the intended recipients of the mail while simultaneously avoiding fixed obstacles ( such as walls ) and mobile obstacles ( such as people ) , and still manage to keep ourselves sufficiently charged up . frequently we know how to perform each task in isolation ; this paper considers how we can take the information we have about the individual tasks and combine it to efficiently find an optimal solution for doing the entire set of tasks in parallel . more importantly , we describe a theoretically - sound algorithm for doing this merging dynamically ; new tasks ( such as a new job arrival at a job shop ) can be assimilated online into the solution being found for the ongoing set of simultaneous tasks . illustrate its use on a simple merging problem .
we consider the problem of fitting an n fi n distance matrix d by a tree metric t . let " be the distance to the closest tree metric , that is , " = min t fk t ; d k #NUM# g . first we present an o ( n #NUM# ) algorithm for finding an additive tree t such that k t ; d k #NUM# #NUM# " , giving the first algorithm for this problem with a performance guarantee . second we show that it is n p - hard to find a tree t such that k t ; d k #NUM# & lt ; #NUM#
case - based planning ( cbp ) provides a way of scaling up domain - independent planning to solve large problems in complex domains . it replaces the detailed and lengthy search for a solution with the retrieval and adaptation of previous planning experiences . in general , cbp has been demonstrated to improve performance over generative ( from - scratch ) planning . however , the performance improvements it provides are dependent on adequate judgements as to problem similarity . in particular , although cbp may substantially reduce planning effort overall , it is subject to a mis - retrieval problem . the success of cbp depends on these retrieval errors being relatively rare . this paper describes the design and implementation of a replay framework for the case - based planner dersnlp + ebl . der - snlp + ebl extends current cbp methodology by incorporating explanation - based learning techniques that allow it to explain and learn from the retrieval failures it encounters . these techniques are used to refine judgements about case similarity in response to feedback when a wrong decision has been made . the same failure analysis is used in building the case library , through the addition of repairing cases . large problems are split and stored as single goal subproblems . multi - goal problems are stored only when these smaller cases fail to be merged into a full solution . an empirical evaluation of this approach demonstrates the advantage of learning from experienced retrieval failure .
relational learning algorithms are of special interest to members of the machine learning community ; they offer practical methods for extending the representations used in algorithms that solve supervised learning tasks . five approaches are currently being explored to address issues involved with using relational representations . this paper surveys algorithms embodying these approaches , summarizes their empirical evaluations , highlights their commonalities , and suggests potential directions for future research .
the learning process in boltzmann machines is computationally very expensive . the computational complexity of the exact algorithm is exponential in the number of neurons . we present a new approximate learning algorithm for boltzmann machines , which is based on mean field theory and the linear response theorem . the computational complexity of the algorithm is cubic in the number of neurons . in the absence of hidden units , we show how the weights can be directly computed from the fixed point equation of the learning rules . thus , in this case we do not need to use a gradient descent procedure for the learning process . we show that the solutions of this method are close to the optimal solutions and give a significant improvement when correlations play a significant role . finally , we apply the method to a pattern completion task and show good performance for networks up to #NUM# neurons .
this paper introduces a methodology for solving combinatorial optimization problems through the application of reinforcement learning methods . the approach can be applied in cases where several similar instances of a combinatorial optimization problem must be solved . the key idea is to analyze a set of " training " problem instances and learn a search control policy for solving new problem instances . the search control policy has the twin goals of finding high - quality solutions and finding them quickly . results of applying this methodology to a nasa scheduling problem show that the learned search control policy is much more effective than the best known non - learning search procedure | a method based on simulated annealing .
markov decision processes ( mdps ) with undis - counted rewards represent an important class of problems in decision and control . the goal of learning in these mdps is to find a policy that yields the maximum expected return per unit time . in large state spaces , computing these averages directly is not feasible ; instead , the agent must estimate them by stochastic exploration of the state space . in this case , longer exploration times enable more accurate estimates and more informed decision - making . the learning curve for an mdp measures how the agent ' s performance depends on the allowed exploration time , t . in this paper we analyze these learning curves for a simple control problem with undiscounted rewards . in particular , methods from statistical mechanics are used to calculate lower bounds on the agent ' s performance in the thermodynamic limit t ! #NUM# , n ! #NUM# , ff = t = n ( finite ) , where t is the number of time steps allotted per policy evaluation and n is the size of the state space . in this limit , we provide a lower bound on the return of policies that appear optimal based on imperfect statistics .
this paper studies self - directed learning , a variant of the on - line learning model in which the learner selects the presentation order for the instances . we give tight bounds on the complexity of self - directed learning for the concept classes of monomials , k - term dnf formulas , and orthogonal rectangles in f #NUM# ; #NUM# ; ; n #NUM# g d . these results demonstrate that the number of mistakes under self - directed learning can be surprisingly small . we then prove that the model of self - directed learning is more powerful than all other commonly used on - line and query learning models . next we explore the relationship between the complexity of self - directed learning and the vapnik - chervonenkis dimension . finally , we explore a relationship between mitchell ' s version space algorithm and the existence of self - directed learning algorithms that make few mistakes .
in this paper we study a forecasting model based on mixture of experts for predicting the french electric daily consumption energy . we split the task into two parts . using mixture of experts , a first model predicts the electricity demand from the exogenous variables ( such as temperature and degree of cloud cover ) and can be viewed as a nonlinear regression model of mixture of gaussians . using a single neural network , a second model predicts the evolution of the residual error of the first one , and can be viewed as an nonlinear autoregression model . we analyze the splitting of the input space generated by the mixture of experts model , and compare the performance to models presently used .
sutton ' s td ( ) metho d aims to provide a represen tation of the cost function in an absorbing mark ov chain with transition costs . a simple example is given where the represen tation obtained dep ends on . for = #NUM# the represen tation is optimal with resp ect to a least squares error criterion , but as decreases towards #NUM# the represen tation becomes progressiv ely worse and , in some cases , very poor . the example suggests a need to understand better the circumstances under which td ( #NUM# ) and q - learning obtain satisfactory neural net work - based compact represen tations of the cost function . a variation of td ( #NUM# ) is also prop osed , which performs b etter on the example .
case - based reasoning involves reasoning from cases : specific pieces of experience , the reasoner ' s or another ' s , that can be used to solve problems . we use the term " graph - structured " for representations that ( #NUM# ) are capable of expressing the relations between any two objects in a case , ( #NUM# ) allow the set of relations used to vary from case to case , and ( #NUM# ) allow the set of possible relations to be expanded as necessary to describe new cases . such representations can be implemented as , for example , semantic networks or lists of concrete propositions in some logic . we believe that graph - structured representations offer significant advantages , and thus we are investigating ways to implement such representations efficiently . we make a " case - based argument " using examples from two systems , chiron and caper , to show how a graph - structured representation supports two different kinds of case - based planning in two different domains . we discuss the costs associated with graph - structured representations and describe an approach to reducing those costs , imple mented in caper .
the advantage of using linear regression in the leaves of a regression tree is analysed in the paper . it is carried out how this modification affects the construction , pruning and interpretation of a regression tree . the modification is tested on artificial and real - life domains where its impact on classification error and stability of the induced trees is considered . the results show that the modification is beneficial , as it leads to smaller classification errors of induced regression trees . the bayesian approach to estimation of class distributions is used in all experiments .
general theory & quantitative results the human genotype represents at most ten billion binary informations , whereas the human brain contains more than a million times a billion synapses . so a differentiated brain structure is essentially due to self - organization . such self - organization is relevant for areas ranging from medicine to the design of intelligent complex systems . many brain structures emerge as collective phenomenon of a microscopic neurosynaptic dynamics : a stochastic dynamics mimics the neuronal action potentials , while the synaptic dynamics is modeled by a local coupling dynamics of type hebb - rule , that is , a synaptic efficiency increases after coincident spiking of pre - and postsynaptic neuron . the microscopic dynamics is transformed to a collective dynamics reminiscent of hydrodynamics . the theory models empirical findings quantitatively : topology preserving neuronal maps were assumed by descartes in #NUM# ; their self - organization was suggested by weiss in #NUM# ; their empirical observation was reported by marshall in #NUM# ; it is shown that they are neurosynaptically stable due to ubiquitous infinitesimal short range electrical or chemical leakage . in the visual cortex , neuronal stimulus orientation preference emerges ; empirically measured orientation patterns are determined by the poisson equation of electrostatics ; this poisson equation orientation pattern emergence is derived here . complex cognitive abilities emerge when the basic local synaptic changes are regulated by valuation , emergent valuation , attention , attention focus or combination of subnetworks . altogether a general theory is presented for the emergence of functionality from synaptic growth in neuro - biological systems . the theory provides a transformation to a collective dynamics and is used for quantitative modeling of empirical data .
most empirical evaluations of machine learning algorithms are case studies evaluations of multiple algorithms on multiple databases . authors of case studies implicitly or explicitly hypothesize that the pattern of their results , which often suggests that one algorithm performs significantly better than others , is not limited to the small number of databases investigated , but instead holds for some general class of learning problems . however , these hypotheses are rarely supported with additional evidence , which leaves them suspect . this paper describes an empirical method for generalizing results from case studies and an example application . this method yields rules describing when some algorithms significantly outperform others on some dependent measures . advantages for generalizing from case studies and limitations of this particular approach are also described .
learning multiple descriptions for each class in the data has been shown to reduce generalization error but the amount of error reduction varies greatly from domain to domain . this paper presents a novel empirical analysis that helps to understand this variation . our hypothesis is that the amount of error reduction is linked to the " degree to which the descriptions for a class make errors in a correlated manner . " we present a precise and novel definition for this notion and use twenty - nine data sets to show that the amount of observed error reduction is negatively correlated with the degree to which the descriptions make errors in a correlated manner . we empirically show that it is possible to learn descriptions that make less correlated errors in domains in which many ties in the search evaluation measure ( e . g . information gain ) are experienced during learning . the paper also presents results that help to understand when and why multiple descriptions are a help ( irrelevant attributes ) and when they are not as much help ( large amounts of class noise ) .
this paper presents the plannett system , which combines artificial neural networks to achieve expert - level accuracy on the difficult scientific task of recognizing volcanos in radar images of the surface of the planet venus . plannett uses anns that vary along two dimensions the set of input features used to train and the number of hidden units . the anns are combined simply by averaging their output activations . when plannett is used as the classification module of a three - stage image analysis system called jar - tool , the end - to - end accuracy ( sensitivity and specificity ) is as good as that of a human planetary geologist on a four - image test suite . jartool - plannett also achieves the best algorithmic accuracy on these images to date .
planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence . in this paper we summarize an approach to this problem based on the mathematical framework of markov decision processes and reinforcement learning . conventional model - based reinforcement learning uses primitive actions that last one time step and that can be modeled independently of the learning agent . these can be generalized to macro actions , multi - step actions specified by an arbitrary policy and a way of completing . macro actions generalize the classical notion of a macro operator in that they are closed loop , uncertain , and of variable duration . macro actions are needed to represent common - sense higher - level actions such as going to lunch , grasping an object , or traveling to a distant city . this paper generalizes prior work on temporally abstract models ( sutton #NUM# ) and extends it from the prediction setting to include actions , control , and planning . we define a semantics of models of macro actions that guarantees the validity of planning using such models . this paper present new results in the theory of planning with macro actions and illustrates its potential advantages in a gridworld task .
this paper reviews five statistical tests for determining whether one learning algorithm outperforms another on a particular learning task . these tests are compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists ( type #NUM# error ) . two widely - used statistical tests are shown to have high probability of type i error in certain situations and should never be used . these tests are ( a ) a test for the difference of two proportions and ( b ) a paired - differences t test based on taking several random train / test splits . a third test , a paired - differences t test based on #NUM# - fold cross - validation , exhibits somewhat elevated probability of type i error . a fourth test , mcnemar ' s test , is shown to have low type i error . the fifth test is a new test , #NUM# x #NUM# cv , based on #NUM# iterations of #NUM# - fold cross - validation . experiments show that this test also has good type i error . the paper also measures the power ( ability to detect algorithm differences when they do exist ) of these tests . the #NUM# x #NUM# cv test is shown to be slightly more powerful than mcnemar ' s test . the choice of the best test is determined by the computational cost of running the learning algorithm . for algorithms that can be executed only once , mcnemar ' s test is the only test with acceptable type i error . for algorithms that can be executed ten times , the #NUM# x #NUM# cv test is recommended , because it is slightly more powerful and because it directly measures variation due to the choice of training set .
many classification algorithms are " passive " , in that they assign a class - label to each instance based only on the description given , even if that description is incomplete . in contrast , an active classifier can | at some cost | obtain the values of missing attributes , before deciding upon a class label . the expected utility of using an active classifier depends on both the cost required to obtain the additional attribute values and the penalty incurred if it outputs the wrong classification . this paper considers the problem of learning near - optimal active classifiers , using a variant of the probably - approximately - correct ( pac ) model . after defining the framework | which is perhaps the main contribution of this paper | we describe a situation where this task can be achieved efficiently , but then show that the task is often intractable .
probabilistic inference algorithms for finding the most probable explanation , the maximum aposteriori hypothesis , and the maximum expected utility and for updating belief are reformulated as an elimination - type algorithm called bucket elimination . this emphasizes the principle common to many of the algorithms appearing in that literature and clarifies their relationship to nonserial dynamic programming algorithms . we also present a general way of combining conditioning and elimination within this framework . bounds on complexity are given for all the algorithms as a function of the problem ' s struc ture .
this article will be published in sociological methodology #NUM# , edited by peter v . marsden , cambridge , mass . : blackwells . adrian e . raftery is professor of statistics and sociology , department of sociology , dk - #NUM# , university of washington , seattle , wa #NUM# . this research was supported by nih grant no . #NUM# r #NUM# hd #NUM# . i would like to thank robert hauser , michael hout , steven lewis , scott long , diane lye , peter marsden , bruce western , yu xie and two anonymous reviewers for detailed comments on an earlier version . i am also grateful to clem brooks , sir david cox , tom diprete , john goldthorpe , david grusky , jennifer hoeting , robert kass , david madigan , michael sobel and chris volinsky for helpful discussions and correspondence .
in this paper we propose a family of algorithms combining tree - clustering with conditioning that trade space for time . such algorithms are useful for reasoning in probabilistic and deterministic networks as well as for accomplishing optimization tasks . by analyzing the problem structure it will be possible to select from a spectrum the algorithm that best meets a given time - space specifica tion .
in this paper we propose a new approach to probabilistic inference on belief networks , global conditioning , which is a simple generalization of pearl ' s ( #NUM# b ) method of loop - cutset conditioning . we show that global conditioning , as well as loop - cutset conditioning , can be thought of as a special case of the method of lauritzen and spiegelhalter ( #NUM# ) as refined by jensen et al ( #NUM# a ; #NUM# b ) . nonetheless , this approach provides new opportunities for parallel processing and , in the case of sequential processing , a tradeoff of time for memory . we also show how a hybrid method ( suermondt and others #NUM# ) combining loop - cutset conditioning with jensen ' s method can be viewed within our framework . by exploring the relationships between these methods , we develop a unifying framework in which the advantages of each approach can be combined successfully .
the dynamics and collective properties of feedback networks with spiking neurons are investigated . special emphasis is given to the potential computational role of subthreshold oscillations . it is shown that model systems with integrate - and - fire neurons can function as associative memories on two distinct levels . on the first level , binary patterns are represented by the spike activity | " to fire or not to fire . " on the second level , analog patterns are encoded in the relative firing times between individual spikes or between spikes and an underlying subthreshold oscillation . both coding schemes may coexist within the same network . the results suggest that cortical neurons may perform a broad spectrum of associative computations far beyond the scope of the traditional firing - rate picture .
this paper considers a new method for maintaining diversity by creating subpopulations in a standard generational evolutionary algorithm . unlike other methods , it replaces the concept of distance between individuals with tag bits that identify the subpopulation to which an individual belongs . two variations of this method are presented , illustrating the feasibility of this approach .
ideally pattern recognition machines provide constant output when the inputs are transformed under a group g of desired invariances . these invariances can be achieved by enhancing the training data to include examples of inputs transformed by elements of g , while leaving the corresponding targets unchanged . alternatively the cost function for training can include a regularization term that penalizes changes in the output when the input is transformed under the group . this paper relates the two approaches , showing precisely the sense in which the regularized cost function approximates the result of adding transformed ( or distorted ) examples to the training data . the cost function for the enhanced training set is equivalent to the sum of the original cost function plus a regularizer . for unbiased models , the regularizer reduces to the intuitively obvious choice - a term that penalizes changes in the output when the inputs are transformed under the group . for infinitesimal transformations , the coefficient of the regularization term reduces to the variance of the distortions introduced into the training data . this correspondence provides a simple bridge between the two approaches .
a new method is proposed for exploiting causal independencies in exact bayesian network inference . a bayesian network can be viewed as representing a factorization of a joint probability into the multiplication of a set of conditional probabilities . we present a notion of causal independence that enables one to further factorize the conditional probabilities into a combination of even smaller factors and consequently obtain a finer - grain factorization of the joint probability . the new formulation of causal independence lets us specify the conditional probability of a variable given its parents in terms of an associative and commutative operator , such as or , sum or max , on the contribution of each parent . we start with a simple algorithm ve for bayesian network inference that , given evidence and a query variable , uses the factorization to find the posterior distribution of the query . we show how this algorithm can be extended to exploit causal independence . empirical studies , based on the cpcs networks for medical diagnosis , show that this method is more efficient than previous methods and allows for inference in larger networks than previous algorithms .
our goal is to develop a hybrid cognitive model of how humans acquire skills on complex cognitive tasks . we are pursuing this goal by designing hybrid computational architectures for the nrl navigation task , which requires competent sensorimotor coordination . in this paper , we describe results of directly fitting human execution data on this task . we next present and then empirically compare two methods for modeling control knowledge acquisition ( reinforcement learning and a novel variant of action models ) with human learning on the task . the paper concludes with an experimental demonstration of the impact of background knowledge on system performance . our results indicate that the performance of our action models approach more closely approximates the rate of human learning on this task than does reinforcement learning .
the statistical query learning model can be viewed as a tool for creating ( or demonstrating the existence of ) noise - tolerant learning algorithms in the pac model . the complexity of a statistical query algorithm , in conjunction with the complexity of simulating sq algorithms in the pac model with noise , determine the complexity of the noise - tolerant pac algorithms produced . although roughly optimal upper bounds have been shown for the complexity of statistical query learning , the corresponding noise - tolerant pac algorithms are not optimal due to inefficient simulations . in this paper we provide both improved simulations and a new variant of the statistical query model in order to overcome these inefficiencies . we improve the time complexity of the classification noise simulation of statistical query algorithms . our new simulation has a roughly optimal dependence on the noise rate . we also derive a simpler proof that statistical queries can be simulated in the presence of classification noise . this proof makes fewer assumptions on the queries themselves and therefore allows one to simulate more general types of queries . we also define a new variant of the statistical query model based on relative error , and we show that this variant is more natural and strictly more powerful than the standard additive error model . we demonstrate efficient pac simulations for algorithms in this new model and give general upper bounds on both learning with relative error statistical queries and pac simulation . we show that any statistical query algorithm can be simulated in the pac model with malicious errors in such a way that the resultant pac algorithm has a roughly optimal tolerable malicious error rate and sample complexity . finally , we generalize the types of queries allowed in the statistical query model . we discuss the advantages of allowing these generalized queries and show that our results on improved simulations also hold for these queries . this paper is available from the center for research in computing technology , division of applied sciences , harvard university as technical report tr - #NUM# - #NUM# .
this paper outlines some problems that may occur with reduced error pruning in inductive logic programming , most notably efficiency . thereafter a new method , incremental reduced error pruning , is proposed that attempts to address all of these problems . experiments show that in many noisy domains this method is much more efficient than alternative algorithms , along with a slight gain in accuracy . however , the experiments show as well that the use of this algorithm cannot be recommended for domains with a very specific concept description .
one kind of prosodic structure that apparently underlies both music and some examples of speech production is meter . yet detailed measurements of the timing of both music and speech show that the nested periodicities that define metrical structure can be quite noisy in time . what kind of system could produce or perceive such variable metrical timing patterns ? and what would it take to be able to store and reproduce particular metrical patterns from long - term memory ? we have developed a network of coupled oscillators that both produces and perceives patterns of pulses that conform to particular meters . in addition , beginning with an initial state with no biases , it can learn to prefer the particular meter that it has been previously exposed to . meter is an abstract structure in time based on the periodic recurrence of pulses , that is , on equal time intervals between distinct phase zeros . from this point of view , the simplest meter is a regular metronome pulse . but often there appear meters with two or three ( or rarely even more ) nested periodicities with integral frequency ratios . a hierarchy of such metrical structures is implied in standard western musical notation , where different levels of the metrical hierarchy are indicated by kinds of notes ( quarter notes , half notes , etc . ) and by the bars separating measures with an equal number of beats . for example , in a basic waltz - time meter , there are individual beats , all with the same spacing , grouped into sets of three , with every third one receiving a stronger accent at its onset . in this meter there is a hierarchy consisting of both a faster periodic cycle ( at the beat level ) and a slower one ( at the measure level ) that is #NUM# / #NUM# as fast , with its onset ( or zero phase angle ) coinciding with the zero phase angle of every third beat . this essentially temporal view of meter contrasts with the traditional symbol - string theories ( such as hayes , #NUM# for speech and lerdahl and jackendoff , #NUM# for music ) . metrical systems , however they are defined , seem to underlie most of what we call music . indeed , an expanded version of european musical notation is found to be practical for transcribing most music from around the world . that is , most forms of music employ nested periodic temporal patterns ( titon , fujie , & locke , #NUM# ) . musical notation has
we propose a model of abduction based on the revision of the epistemic state of an agent . explanations must be sufficient to induce belief in the sentence to be explained ( for instance , some observation ) , or ensure its consistency with other beliefs , in a manner that adequately accounts for factual and hypothetical sentences . our model will generate explanations that nonmonotonically predict an observation , thus generalizing most current accounts , which require some deductive relationship between explanation and observation . it also provides a natural preference ordering on explanations , defined in terms of normality or plausibility . to illustrate the generality of our approach , we reconstruct two of the key paradigms for model - based diagnosis , abductive and consistency - based diagnosis , within our framework . this reconstruction provides an alternative semantics for both and extends these systems to accommodate our predictive explanations and semantic preferences on explanations . it also illustrates how more general information can be incorporated in a principled manner .
we describe a ranked - model semantics for if - then rules admitting exceptions , which provides a coherent framework for many facets of evidential and causal reasoning . rule priorities are automatically extracted form the knowledge base to facilitate the construction and retraction of plausible beliefs . to represent causation , the formalism incorporates the principle of markov shielding which imposes a stratified set of independence constraints on rankings of interpretations . we show how this formalism resolves some classical problems associated with specificity , prediction and abduction , and how it offers a natural way of unifying belief revision , belief update , and reasoning about actions .
we build up the mathematical connection between the " expectation - maximization " ( em ) algorithm and gradient - based approaches for maximum likelihood learning of finite gaussian mixtures . we show that the em step in parameter space is obtained from the gradient via a projection matrix p , and we provide an explicit expression for the matrix . we then analyze the convergence of em in terms of special properties of p and provide new results analyzing the effect that p has on the likelihood surface . based on these mathematical results , we present a comparative discussion of the advantages and disadvantages of em and other algorithms for the learning of gaussian mixture models .
a first order regression algorithm capable of handling real - valued variables is introduced and some of its applications are presented . regressional learning assumes real - valued ( continuous ) class and discrete or real - valued variables . the algorithm combines regressional learning with standard ilp concepts , such as first order concept description and background knowledge . in particular , our goals were to develop a system which can : induce first - order logic concepts which incorporate real - valued variables , use background knowledge in intensional form , model dynamic systems ( learn from time series ) , partition attribute space to subspaces in order to find a regressional submodel for each subspace separately , end handle noisy data . an outline of the algorithm and the results of the system ' s application in some artificial and real - world domains are presented . real - world domains comprise modelling of the water behavior in a surge tank and modelling of the workpiece roughness in the steel grinding process . the results confirm the advantage of combining of various techniques and again confirm that the comprehensibility of the induced knowledge in crucial for successfull application of ai techniques .
we address the problem of measuring the degree of hemispheric organization and asymmetry of organization in a computational model of a bihemispheric cerebral cortex . a theoretical framework for such measures is developed and used to produce algorithms for measuring the degree of organization , symmetry , and lateralization in topographic map formation . the performance of the resulting measures is tested for several topographic maps obtained by self - organization of an initially random network , and the results are compared with subjective assessments made by humans . it is found that the closest agreement with the human assessments is obtained by using organization measures based on sigmoid - type error averaging . measures are developed which correct for large constant displacements as well as curving of the hemispheric topographic maps .
learning structure in temporally - extended sequences is a difficult computational problem because only a fraction of the relevant information is available at any instant . although variants of back propagation can in principle be used to find structure in sequences , in practice they are not sufficiently powerful to discover arbitrary contingencies , especially those spanning long temporal intervals or involving high order statistics . for example , in designing a connectionist network for music composition , we have encountered the problem that the net is able to learn musical structure that occurs locally in time | e . g . , relations among notes within a musical phrase | but not structure that occurs over longer time periods | e . g . , relations among phrases . to address this problem , we require a means of constructing a reduced description of the sequence that makes global aspects more explicit or more readily detectable . i propose to achieve this using hidden units that operate with different time constants . simulation experiments indicate that slower time - scale hidden units are able to pick up global structure , structure that simply can not be learned by standard many patterns in the world are intrinsically temporal , e . g . , speech , music , the unfolding of events . recurrent neural net architectures have been devised to accommodate time - varying sequences . for example , the architecture shown in figure #NUM# can map a sequence of inputs to a sequence of outputs . learning structure in temporally - extended sequences is a difficult computational problem because the input pattern may not contain all the task - relevant information at any instant . thus , back propagation .
an incremental , higher - order , non - recurrent network combines two properties found to be useful for learning sequential tasks : higher - order connections and incremental introduction of new units . the network adds higher orders when needed by adding new units that dynamically modify connection weights . since the new units modify the weights at the next time - step with information from the previous step , temporal tasks can be learned without the use of feedback , thereby greatly simplifying training . furthermore , a theoretically unlimited number of units can be added to reach into the arbitrarily distant past . experiments with the reber grammar have demonstrated speedups of two orders of magnitude over recurrent networks .
in complex models like hidden markov chains , the convergence of the mcmc algorithms used to approximate the posterior distribution and the bayes estimates of the parameters of interest must be controlled in a robust manner . we propose in this paper a series of on - line controls , which rely on classical non - parametric tests , to evaluate independence from the start - up distribution , stability of the markov chain , and asymptotic normality . these tests lead to graphical control spreadsheets which are presented in the set - up of normal mixture hidden markov chains to compare the full gibbs sampler with an aggregated gibbs sampler based on the forward - backward formulae .
three different methods were investigated to determine their ability to detect and classify various categories of diffuse liver disease . a statistical method , i . e . , discriminant analysis , a supervised neural network called backpropagation and a nonsupervised , self - organizing feature map were examined . the investigation was performed on the basis of a previously selected set of acoustic and image texture parameters . the limited number of patients was successfully extended by generating additional but independent data with identical statistical properties . the generated data were used for training and test sets . the final test was made with the original patient data as a validation set . it is concluded that neural networks are an attractive alternative to traditional statistical techniques when dealing with medical detection and classification tasks . moreover , the use of generated data for training the networks and the discriminant classifier has been shown to be justified and profitable .
nonlinear extensions of one - unit and multi - unit principal component analysis ( pca ) neural networks , introduced earlier by the author , are reviewed . the networks and their nonlinear hebbian learning rules are related to other signal expansions like the projection pursuit ( pp ) and the independent component analysis ( ica ) .
we present high - level , decomposition - based algorithms for large - scale block - angular optimization problems containing integer variables , and demonstrate their effectiveness in the solution of large - scale graph partitioning problems . these algorithms combine the subproblem - coordination paradigm ( and lower bounds ) of price - directive decomposition methods with knapsack and genetic approaches to the utilization of " building blocks " of partial solutions . even for graph partitioning problems requiring billions of variables in a standard #NUM# - #NUM# formulation , this approach produces high - quality solutions ( as measured by deviations from an easily computed lower bound ) , and substantially outperforms widely - used graph partitioning techniques based on heuristics and spectral methods .
maps of regional morbidity and mortality rates are useful tools in determining spatial patterns of disease . combined with socio - demographic census information , they also permit assessment of environmental justice , i . e . , whether certain subgroups suffer disproportionately from certain diseases or other adverse effects of harmful environmental exposures . bayes and empirical bayes methods have proven useful in smoothing crude maps of disease risk , eliminating the instability of estimates in low - population areas while maintaining geographic resolution . in this paper we extend existing hierarchical spatial models to account for temporal effects and spatio - temporal interactions . fitting the resulting highly - parametrized models requires careful implementation of markov chain monte carlo ( mcmc ) methods , as well as novel techniques for model evaluation and selection . we illustrate our approach using a dataset of county - specific lung cancer rates in the state of ohio during the period #NUM# - #NUM# .
a novel unsupervised neural network for dimensionality reduction that seeks directions emphasizing multimodality is presented , and its connection to exploratory projection pursuit methods is discussed . this leads to a new statistical insight into the synaptic modification equations governing learning in bienenstock , cooper , and munro ( bcm ) neurons ( #NUM# ) . the importance of a dimensionality reduction principle based solely on distinguishing features is demonstrated using a phoneme recognition experiment . the extracted features are compared with features extracted using a back - propagation network .
this paper is reprinted from computational learning theory and natural learning systems , vol . #NUM# , t . petsche , s . judd , and s . hanson , ( eds . ) , forthcoming #NUM# . copyrighted #NUM# by mit press the ability of an inductive learning system to find a good solution to a given problem is dependent upon the representation used for the features of the problem . a number of factors , including training - set size and the ability of the learning algorithm to perform constructive induction , can mediate the effect of an input representation on the accuracy of a learned concept description . we present experiments that evaluate the effect of input representation on generalization performance for the real - world problem of finding genes in dna . our experiments that demonstrate that : ( #NUM# ) two different input representations for this task result in significantly different generalization performance for both neural networks and decision trees ; and ( #NUM# ) both neural and symbolic methods for constructive induction fail to bridge the gap between these two representations . we believe that this real - world domain provides an interesting challenge problem for the machine learning subfield of constructive induction because the relationship between the two representations is well known , and because conceptually , the representational shift involved in constructing the better representation should not be too imposing .
self - supervised backpropagation is an unsupervised learning procedure for feedfor - ward networks , where the desired output vector is identical with the input vector . for backpropagation , we are able to use powerful simulators running on parallel machines . topology - preserving maps , on the other hand , can be developed by a variant of the competitive learning procedure . however , in a degenerate case , self - supervised backpropagation is a version of competitive learning . a simple extension of the cost function of backpropagation leads to a competitive version of self - supervised backpropagation , which can be used to produce topographic maps . we demonstrate the approach applied to the traveling salesman problem ( tsp ) . the algorithm was implemented using the backpropagation simulator ( clones ) on a parallel machine ( rap ) .
this paper describes an evolving computational model of the perception and production of simple rhythmic patterns . the model consists of a network of oscillators of different resting frequencies which couple with input patterns and with each other . oscillators whose frequencies match periodicities in the input tend to become activated . metrical structure is represented explicitly in the network in the form of clusters of oscillators whose frequencies and phase angles are constrained to maintain the harmonic relationships that characterize meter . rests in rhythmic patterns are represented by explicit rest oscillators in the network , which become activated when an expected beat in the pattern fails to appear . the model makes predictions about the relative difficulty of patterns and the effect of deviations from periodicity in the input . the nested periodicity that defines musical , and probably also linguistic , meter appears to be fundamental to the way in which people perceive and produce patterns in time . meter by itself , however , is not sufficient to describe patterns which are interesting or memorable because of how they deviate from the metrical hierarchy . the simplest deviations are rests or gaps where one or more levels in the hierarchy would normally have a beat . when beats are removed at regular intervals which match the period of some level of the metrical hierarchy , we have what we will call a simple rhythmic pattern . figure #NUM# shows an example of a simple rhythmic pattern . below it is a grid representation of the meter which is behind the pattern .
in speedup - learning problems , where full descriptions of operators are always known , both explanation - based learning ( ebl ) and reinforcement learning ( rl ) can be applied . this paper shows that both methods involve fundamentally the same process of propagating information backward from the goal toward the starting state . rl performs this propagation on a state - by - state basis , while ebl computes the weakest preconditions of operators , and hence , performs this propagation on a region - by - region basis . based on the observation that rl is a form of asynchronous dynamic programming , this paper shows how to develop a dynamic programming version of ebl , which we call explanation - based reinforcement learning ( ebrl ) . the paper compares batch and online versions of ebrl to batch and online versions of rl and to standard ebl . the results show that ebrl combines the strengths of ebl ( fast learning and the ability to scale to large state spaces ) with the strengths of rl ( learning of optimal policies ) . results are shown in chess endgames and in synthetic maze tasks .
in this paper we present some extensions to the k - means algorithm for vector quantization that permit its efficient use in image segmentation and pattern classification tasks . it is shown that by introducing state variables that correspond to certain statistics of the dynamic behavior of the algorithm , it is possible to find the representative centers of the lower dimensional manifolds that define the boundaries between classes , for clouds of multi - dimensional , multi - class data ; this permits one , for example , to find class boundaries directly from sparse data ( e . g . , in image segmentation tasks ) or to efficiently place centers for pattern classification ( e . g . , with local gaussian classifiers ) . the same state variables can be used to define algorithms for determining adaptively the optimal number of centers for clouds of data with space - varying density . some examples of the application of these extensions are also given . this report describes research done within cimat ( guanajuato , mexico ) , the center for biological and computational learning in the department of brain and cognitive sciences , and at the artificial intelligence laboratory . this research is sponsored by grants from the office of naval research under contracts n #NUM# - #NUM# - j - #NUM# and n #NUM# - #NUM# - j - #NUM# ; by a grant from the national science foundation under contract asc - #NUM# ; and by a grant from the national institutes of health under contract nih #NUM# - s #NUM# - rr #NUM# . additional support is provided by the north atlantic treaty organization , atr audio and visual perception research laboratories , mitsubishi electric corporation , sumitomo metal industries , and siemens ag . support for the a . i . laboratory ' s artificial intelligence research is provided by onr contract n #NUM# - #NUM# - j - #NUM# . j . l . marroquin was supported in part by a grant from the consejo nacional de ciencia y tecnologia , mexico .
the limitations of using self - organizing maps ( som ) for either clustering / vector quantization ( vq ) or multidimensional scaling ( mds ) are being discussed by reviewing recent empirical findings and the relevant theory . som ' s remaining ability of doing both vq and mds at the same time is challenged by a new combined technique of online k - means clustering plus sammon mapping of the cluster centroids . som are shown to perform significantly worse in terms of quantization error , in recovering the structure of the clusters and in preserving the topology in a comprehensive empirical study using a series of multivariate normal clustering problems .
while exploring to find better solutions , an agent performing online reinforcement learning ( rl ) can perform worse than is acceptable . in some cases , exploration might have unsafe , or even catastrophic , results , often modeled in terms of reaching ` failure ' states of the agent ' s environment . this paper presents a method that uses domain knowledge to reduce the number of failures during exploration . this method formulates the set of actions from which the rl agent composes a control policy to ensure that exploration is conducted in a policy space that excludes most of the unacceptable policies . the resulting action set has a more abstract relationship to the task being solved than is common in many applications of rl . although the cost of this added safety is that learning may result in a suboptimal solution , we argue that this is an appropriate tradeoff in many problems . we illustrate this method in the domain of motion planning .
in learning problems where a connectionist network is trained with a finite sized training set , better generalization performance is often obtained when unneeded weights in the network are eliminated . one source of unneeded weights comes from the inclusion of input variables that provide little information about the output variables . we propose a method for identifying and eliminating these input variables . the method first determines the relationship between input and output variables using nonparametric density estimation and then measures the relevance of input variables using the information theoretic concept of mutual information . we present results from our method on a simple toy problem and a nonlinear time series .
this paper introduces the introspection approach , a method by which a learning agent employing reinforcement learning can decide when to ask a training agent for instruction . when using our approach , we find that the same number of trainer ' s responses produced significantly faster learners than by having the learner ask for aid randomly . guidance received via our approach is more informative than random guidance . thus , we can reduce the interaction that the training agent has with the learning agent without reducing the speed with which the learner develops its policy . in fact , by being intelligent about when the learner asks for help , we can even increase the learning speed for the same level of trainer interaction .
we present a method for feature construction and selection that finds a minimal set of conjunctive features that are appropriate to perform the classification task . for problems where this bias is appropriate , the method outperforms other constructive induction algorithms and is able to achieve higher classification accuracy . the application of the method in the search for minimal multi - level boolean expressions is presented and analyzed with the help of some examples .
in this paper we discuss a bayesian approach for finding latent classes in the data . in our approach we use finite mixture models to describe the underlying structure in the data , and demonstrate that the possibility to use full joint probability models raises interesting new prospects for exploratory data analysis . the concepts and methods discussed are illustrated with a case study using a data set from a recent educational study . the bayesian classification approach described has been implemented , and presents an appealing addition to the standard toolbox for exploratory data analysis of educational data .
we present two additions to the hierarchical mixture of experts ( hme ) architecture . we view the hme as a tree structured classifier . firstly , by applying a likelihood splitting criteria to each expert in the hme we " grow " the tree adaptively during training . secondly , by considering only the most probable path through the tree we may " prune " branches away , either temporarily , or permanently if they become redundant . we demonstrate results for the growing and pruning algorithms which show significant speed ups and more efficient use of parameters over the conventional algorithms in discriminating between two interlocking spirals and classifying #NUM# - bit parity patterns .
systems interacting with real - world data must address the issues raised by the possible presence of errors in the observations it makes . in this paper we first present a framework for discussing imperfect data and the resulting problems it may cause . we distinguish between two categories of errors in data random errors or ` noise ' , and systematic errors and examine their relationship to the task of describing observations in a way which is also useful for helping in future problem - solving and learning tasks . secondly we proceed to examine some of the techniques currently used in ai research for recognising such errors .
genetic programming is applied to the task of evolving general iterative sorting algorithms . a connection between size and generality was discovered . adding inverse size to the fitness measure along with correctness not only decreases the size of the resulting evolved algorithms , but also dramatically increases their generality and thus the effectiveness of the evolution process . in addition , a variety of differing problem formulations are investigated and the relative probability of success for each is reported . an example of an evolved sort from each problem formulation is presented , and an initial attempt is made to understand the variations in difficulty resulting from these differing problem formulations .
irrelevant and redundant features may reduce both predictive accuracy and comprehensibility of induced concepts . most common machine learning approaches for selecting a good subset of relevant features rely on cross - validation . as an alternative , we present the application of a particular minimum description length ( mdl ) measure to the task of feature subset selection . using the mdl principle allows taking into account all of the available data at once . the new measure is information - theoretically plausible and yet still simple and therefore efficiently computable . we show empirically that this new method for judging the value of feature subsets is more efficient than and performs at least as well as methods based on cross - validation . domains with both a large number of training examples and a large number of possible features yield the biggest gains in efficiency . thus our new approach seems to scale up better to large learning problems than previous methods .
rules ( rivest #NUM# ) . inductive algorithms such as aq and cn #NUM# learn decision lists incrementally , one rule at a time . such algorithms face the rule overlap problem | the classification accuracy of the decision list depends on the overlap between the learned rules . thus , even though the rules are learned in isolation , they can only be evaluated in concert . existing algorithms solve this problem by adopting a greedy , iterative structure . once a rule is learned , the training examples that match the rule are removed from the training set . we propose a novel solution to the problem : composing decision lists from homogeneous rules , rules whose classification accuracy does not change with their position in the decision list . we prove that the problem of finding a maximally accurate decision list can be reduced to the problem of finding maximally accurate homogeneous rules . we report on the performance of our algorithm on data sets from the uci repository and on the monk ' s problems .
methods to build function approximators from example data have gained considerable interest in the past . especially methodologies that build models that allow an interpretation have attracted attention . most existing algorithms , however , are either complicated to use or infeasible for high - dimensional problems . this article presents an efficient and easy to use algorithm to construct fuzzy graphs from example data . the resulting fuzzy graphs are based on locally independent fuzzy rules that operate solely on selected , important attributes . this enables the application of these fuzzy graphs also to problems in high dimensional spaces . using illustrative examples and a real world data set it is demonstrated how the resulting fuzzy graphs offer quick insights into the structure of the example data , that is , the underlying model .
we describe a methodology for enabling an intelligent teaching system to make high level strategy decisions on the basis of low level student modeling information . this framework is less costly to construct , and superior to hand coding teaching strategies as it is more responsive to the learner ' s needs . in order to accomplish this , reinforcement learning is used to learn to associate superior teaching actions with certain states of the student ' s knowledge . reinforcement learning ( rl ) has been shown to be flexible in handling noisy data , and does not need expert domain knowledge . a drawback of rl is that it often needs a significant number of trials for learning . we propose an off - line learning methodology using sample data , simulated students , and small amounts of expert knowledge to bypass this problem .
any intelligent system , whether human or robotic , must be capable of dealing with patterns over time . temporal pattern processing can be achieved if the system has a short - term memory capacity ( stm ) so that different representations can be maintained for some time . in this work we propose a neural model wherein stm is realized by leaky integrators in a self - organizing system . the model exhibits compo - sitionality , that is , it has the ability to extract and construct progressively complex and structured associations in an hierarchical manner , starting with basic and primitive ( temporal ) elements . an important feature of the proposed model is the use of temporal correlations to express dynamic bindings .
in tasks requiring sustained attention , human alertness varies on a minute time scale . this can have serious consequences in occupations ranging from air traffic control to monitoring of nuclear power plants . changes in the electroencephalographic ( eeg ) power spectrum accompany these fluctuations in the level of alertness , as assessed by measuring simultaneous changes in eeg and performance on an auditory monitoring task . by combining power spectrum estimation , principal component analysis and artificial neural networks , we show that continuous , accurate , noninvasive , and near real - time estimation of an operator ' s global level of alertness is feasible using eeg measures recorded from as few as two central scalp sites . this demonstration could lead to a practical system for noninvasive monitoring of the cognitive state of human operators in attention - critical settings .
this paper presents exact solutions and convergent approximations for inferences in bayesian networks associated with finitely generated convex sets of distributions . robust bayesian inference is the calculation of bounds on posterior values given perturbations in a probabilistic model . the paper presents exact inference algorithms and analyzes the circumstances where exact inference becomes intractable . two classes of algorithms for numeric approximations are developed through transformations on the original model . the first transformation reduces the robust inference problem to the estimation of probabilistic parameters in a bayesian network . the second transformation uses lavine ' s bracketing algorithm to generate a sequence of maximization problems in a bayesian network . the analysis is extended to the * - contaminated , the lower density bounded , the belief function , the sub - sigma , the density bounded , the total variation and the density ratio classes of distributions . c fl #NUM# carnegie mellon university
one of the fundamental problems in learning is identifying members of two different classes . for example , to diagnose cancer , one must learn to discriminate between benign and malignant tumors . through examination of tumors with previously determined diagnosis , one learns some function for distinguishing the benign and malignant tumors . then the acquired knowledge is used to diagnose new tumors . the perceptron is a simple biologically inspired model for this two - class learning problem . the perceptron is trained or constructed using examples from the two classes . then the perceptron is used to classify new examples . we describe geometrically what a perceptron is capable of learning . using duality , we develop a framework for investigating different methods of training a perceptron . depending on how we define the " best " perceptron , different minimization problems are developed for training the perceptron . the effectiveness of these methods is evaluated empirically on four practical applications : breast cancer diagnosis , detection of heart disease , political voting habits , and sonar recognition . this paper does not assume prior knowledge of machine learning or pattern recognition .
i define a latent variable model in the form of a neural network for which only target outputs are specified ; the inputs are unspecified . although the inputs are missing , it is still possible to train this model by placing a simple probability distribution on the unknown inputs and maximizing the probability of the data given the parameters . the model can then discover for itself a description of the data in terms of an underlying latent variable space of lower dimensionality . i present preliminary results of the application of these models to protein data .
this paper #NUM# . ) studies aspects on which these two categories usually differ , like their relevance for generalization and their role in the loss function , #NUM# . ) presents a unifying formalism , where both types of information are identified with answers to generalized questions , #NUM# . ) shows what kind of generalized information is necessary to enable learning , #NUM# . ) aims to put usual training data and prior information on a more equal footing by discussing possibilities and variants of measurement and control for generalized questions , including the examples of smoothness and symmetries , #NUM# . ) reviews shortly the measurement of linguistic concepts based on fuzzy priors , and principles to combine preprocessors , #NUM# . ) uses a bayesian decision theoretic framework , contrasting parallel and inverse decision problems , #NUM# . ) proposes , for problems with non - approximation aspects , a bayesian two step approximation consisting of posterior maximization and a subsequent risk minimization , #NUM# . ) analyses empirical risk minimization under the aspect of nonlocal information #NUM# . ) compares the bayesian two step approximation with empirical risk minimization , including their interpretations of occam ' s razor , #NUM# . ) formulates examples of stationarity conditions for the maximum posterior approximation with nonlocal and nonconvex priors , leading to inhomogeneous nonlinear equations , similar for example to equations in scattering theory in physics . in summary , this paper focuses on the dependencies between answers to different questions . because not training examples alone but such dependencies enable generalization , it emphasizes the need of their empirical measurement and control and of a more explicit treatment in theory . this report describes research done within the center for biological and computational learning in the department of brain and cognitive sciences at the massachusetts institute of technology . this research is sponsored by a grant from national science foundation under contract asc - #NUM# and a grant from onr / arpa under contract n #NUM# - #NUM# - j - #NUM# . the author was supported by a postdoctoral fellowship ( le #NUM# / #NUM# - #NUM# ) from the deutsche forschungsgemeinschaft and a nsf / cise postdoctoral fellowship .
we present an alternative to the cellular encoding technique [ gruau #NUM# ] for evolving graph and network structures via genetic programming . the new technique , called edge encoding , uses edge operators rather than the node operators of cellular encoding . while both cellular encoding and edge encoding can produce all possible graphs , the two encodings bias the genetic search process in different ways ; each may therefore be most useful for a different set of problems . the problems for which these techniques may be used , and for which we think edge encoding may be particularly useful , include the evolution of recurrent neural networks , finite automata , and graph - based queries to symbolic knowledge bases . in this preliminary report we present a technical description of edge encoding and an initial comparison to cellular encoding . experimental investigation of the relative merits of these encoding schemes is currently in progress .
we present a technique for evaluating classifications by geometric comparison of rule sets . rules are represented as objects in an n - dimensional hyperspace . the similarity of classes is computed from the overlap of the geometric class descriptions . the system produces a correlation matrix that indicates the degree of similarity between each pair of classes . the technique can be applied to classifications generated by different algorithms , with different numbers of classes and different attribute sets . experimental results from a case study in a medical domain are included .
as natural resources become less abundant , we naturally become more interested in , and more adept at utilisation of waste materials . in doing this we are bringing to bear a ploy which is of key importance in learning | or so i argue in this paper . in the ` truth from trash ' model , learning is viewed as a process which uses environmental feedback to assemble fortuitous sensory predispositions ( sensory ` trash ' ) into useful , information vehicles , i . e . , ` truthful ' indicators of salient phenomena . the main aim will be to show how a computer implementation of the model has been used to enhance ( through learning ) the strategic abilities of a simulated , football playing mobot .
this paper concerns the probabilistic evaluation of the effects of actions in the presence of unmeasured variables . we show that the identification of causal effect between a singleton variable x and a set of variables y can be accomplished systematically , in time polynomial in the number of variables in the graph . when the causal effect is identifiable , a closed - form expression can be obtained for the probability that the action will achieve a specified goal , or a set of goals .
visor is a large connectionist system that shows how visual schemas can be learned , represented , and used through mechanisms natural to neural networks . processing in visor is based on cooperation , competition , and parallel bottom - up and top - down activation of schema representations . simulations show that visor is robust against noise and variations in the inputs and parameters . it can indicate the confidence of its analysis , pay attention to important minor differences , and use context to recognize ambiguous objects . experiments also suggest that the representation and learning are stable , and its behavior is consistent with human processes such as priming , perceptual reversal , and circular reaction in learning . the schema mechanisms of visor can serve as a starting point for building robust high - level vision systems , and perhaps for schema - based motor control and natural language processing systems as well .
we present a framework for characterizing bayesian classification methods . this framework can be thought of as a spectrum of allowable dependence in a given probabilistic model with the naive bayes algorithm at the most restrictive end and the learning of full bayesian networks at the most general extreme . while much work has been carried out along the two ends of this spectrum , there has been surprising little done along the middle . we analyze the assumptions made as one moves along this spectrum and show the tradeoffs between model accuracy and learning speed which become critical to consider in a variety of data mining domains . we then present a general induction algorithm that allows for traversal of this spectrum depending on the available computational power for carrying out induction and show its application in a number of domains with different properties .
traits that are acquired by members of an evolving population during their lifetime , through adaptive processes such as learning , can become genetically specified in later generations . thus there is a change in the level of learning in the population over evolutionary time . this paper explores the idea that as well as the benefits to be gained from learning , there may also be costs to be paid for the ability to learn . it is these costs that supply the selection pressure for the genetic assimilation of acquired traits . two models are presented that attempt to illustrate this assertion . the first uses kauffman ' s nk fitness landscapes to show the effect that both explicit and implicit costs have on the assimilation of learnt traits . a characteristic ` hump ' is observed in the graph of the level of plasticity in the population showing that learning is first selected for and then against as evolution progresses . the second model is a practical example in which neural network controllers are evolved for a small mobile robot . results from this experiment also show the hump .
the evolution of a population can be guided by phenotypic traits acquired by members of that population during their lifetime . this phenomenon , known as the baldwin effect , can speed the evolutionary process as traits that are initially acquired become genetically specified in later generations . this paper presents conditions under which this genetic assimilation can take place . as well as the benefits that lifetime adaptation can give a population , there may be a cost to be paid for that adaptive ability . it is the evolutionary trade - off between these costs and benefits that provides the selection pressure for acquired traits to become genetically specified . it is also noted that genotypic space , in which evolution operates , and phenotypic space , on which adaptive processes ( such as learning ) operate , are , in general , of a different nature . to guarantee an acquired characteristic can become genetically specified , then these spaces must have the property of neighbourhood correlation which means that a small distance between two individuals in phenotypic space implies that there is a small distance between the same two individuals in genotypic space .
decision trees have been widely used for classification / regression tasks . they are relatively much faster to build as compared to neural networks and are understandable by humans . in normal decision trees , based on the input vector , only one branch is followed . in probabilistic option trees , based on the input vector we follow all of the subtrees with some probability . these probabilities are learned by the system . probabilistic decisions are likely to be useful , when the boundary of classes submerge in each other , or when there is noise in the input data . in addition they provide us with a confidence measure . we allow option nodes in our trees , again , instead of uniform voting , we learn the weightage of every subtree .
the fundamental backpropagation ( bp ) algorithm for training artificial neural networks is cast as a deterministic nonmonotone perturbed gradient method . under certain natural assumptions , such as the series of learning rates diverging while the series of their squares converging , it is established that every accumulation point of the online bp iterates is a stationary point of the bp error function . the results presented cover serial and parallel online bp , modified bp with a momentum term , and bp with weight decay .
recurrent neural networks that are trained to behave like deterministic finite - state automata ( dfas ) can show deteriorating performance when tested on long strings . this deteriorating performance can be attributed to the instability of the internal representation of the learned dfa states . the use of a sigmoidal discriminant function together with the recurrent structure contribute to this instability . we prove that a simple algorithm can construct second - order recurrent neural networks with a sparse interconnection topology and sigmoidal discriminant function such that the internal dfa state representations are stable , i . e . the constructed network correctly classifies strings of arbitrary length . the algorithm is based on encoding strengths of weights directly into the neural network . we derive a relationship between the weight strength and the number of dfa states for robust string classification . for a dfa with n states and m input alphabet symbols , the constructive algorithm generates a " programmed " neural network with o ( n ) neurons and o ( mn ) weights . we compare our algorithm to other methods proposed in the literature .
the extraction of symbolic knowledge from trained neural networks and the direct encoding of ( partial ) knowledge into networks prior to training are important issues . they allow the exchange of information between symbolic and connectionist knowledge representations . the focus of this paper is on the quality of the rules that are extracted from recurrent neural networks . discrete - time recurrent neural networks can be trained to correctly classify strings of a regular language . rules defining the learned grammar can be extracted from networks in the form of deterministic finite - state automata ( dfa ' s ) by applying clustering algorithms in the output space of recurrent state neurons . our algorithm can extract different finite - state automata that are consistent with a training set from the same network . we compare the generalization performances of these different models and the trained network and we introduce a heuristic that permits us to choose among the consistent dfa ' s the model which best approximates the learned regular grammar .
job - shop scheduling is an important task for manufacturing industries . we are interested in the particular task of scheduling payload processing for nasa ' s space shuttle program . this paper summarizes our previous work on formulating this task for solution by the reinforcement learning algorithm t d ( ) . a shortcoming of this previous work was its reliance on hand - engineered input features . this paper shows how to extend the time - delay neural network ( tdnn ) architecture to apply it to irregular - length schedules . experimental tests show that this tdnn - t d ( ) network can match the performance of our previous hand - engineered system . the tests also show that both neural network approaches significantly outperform the best previous ( non - learning ) solution to this problem in terms of the quality of the resulting schedules and the number of search steps required to construct them .
this paper deals with the simulation of turing machines by neural networks . such networks are made up of interconnections of synchronously evolving processors , each of which updates its state according to a " sigmoidal " linear combination of the previous states of all units . the main result states that one may simulate all turing machines by nets , in linear time . in particular , it is possible to give a net made up of about #NUM# , #NUM# processors which computes a universal partial - recursive function . ( this is an update of report sycon - #NUM# - #NUM# ; new results include the simulation in linear time of binary - tape machines , as opposed to the unary alphabets used in the previous version . )
in this paper , we develop new lrta * - based algorithms for a variety of tasks , and analyze their complexity . the lrta * algorithm is a real - time search algorithm developed by korf . it can be used to reach a stationary or moving goal state or to identify all shortest paths from a given start state to a stationary goal state , if the algorithm is reset to the start state when it reaches a goal state . our algorithms have a search horizon of one and require no internal memory ( but must be able to store some information in the states ) . for example , the bi - directional lrts algorithm determines optimal universal plans ( i . e . finds all optimal paths from all states to a set of stationary goal states ) , even if reset actions are not available . we show that all tasks studied in this paper can be solved by such lrta * - based algorithms with only o ( n #NUM# ) action executions for state spaces of size n .
in this paper we consider the problem of approximating a function belonging to some function space by a linear combination of n translates of a given function g . using a lemma by jones ( #NUM# ) and barron ( #NUM# ) we show that it is possible to define function spaces and functions g for which the rate of convergence to zero of the error is o ( #NUM# p n ) in any number of dimensions . the apparent avoidance of the " curse of dimensionality " is due to the fact that these function spaces are more and more constrained as the dimension increases . examples include spaces of the sobolev type , in which the number of weak derivatives is required to be larger than the number of dimensions . we give results both for approximation in the l #NUM# norm and in the l #NUM# norm . the interesting feature of these results is that , thanks to the constructive nature of jones ' and barron ' s lemma , an iterative procedure is defined that can achieve this rate .
university of wisconsin computer sciences technical report #NUM# ( september #NUM# ) abstract in explanation - based learning , a specific problem ' s solution is generalized into a form that can be later used to solve conceptually similar problems . most research in explanation - based learning involves relaxing constraints on the variables in the explanation of a specific example , rather than generalizing the graphical structure of the explanation itself . however , this precludes the acquisition of concepts where an iterative or recursive process is implicitly represented in the explanation by a fixed number of applications . this paper presents an algorithm that generalizes explanation structures and reports empirical results that demonstrate the value of acquiring recursive and iterative concepts . the bagger #NUM# algorithm learns recursive and iterative concepts , integrates results from multiple examples , and extracts useful subconcepts during generalization . on problems where learning a recursive rule is not appropriate , the system produces the same result as standard explanation - based methods . applying the learned recursive rules only requires a minor extension to a prolog - like problem solver , namely , the ability to explicitly call a specific rule . empirical studies demonstrate that generalizing the structure of explanations helps avoid the recently reported negative effects of learning .
we consider a gibbs sampler applied to the uniform distribution on a bounded region r r d . we show that the convergence properties of the gibbs sampler depend greatly on the smoothness of the boundary of r . indeed , for sufficiently smooth boundaries the sampler is uniformly ergodic , while for jagged boundaries the sampler could fail to even be geometrically ergodic .
uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances . these methods can greatly reduce the number of instances that an expert need label . one problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances . we test the use of one classifier ( a highly efficient probabilistic one ) to select examples for training another ( the c #NUM# . #NUM# rule induction program ) . despite being chosen by this heterogeneous approach , the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger .
certain causal models involving unmeasured variables induce no independence constraints among the observed variables but imply , nevertheless , inequality constraints on the observed distribution . this paper derives a general formula for such inequality constraints as induced by instrumental variables , that is , exogenous variables that directly affect some variables but not all . with the help of this formula , it is possible to test whether a model involving instrumental variables may account for the data , or , conversely , whether a given vari able can be deemed instrumental .
bayesian confidence intervals of a smoothing spline are often used to distinguish two curves . in this paper , we provide an asymptotic formula for sample size calculations based on bayesian confidence intervals . approximations and simulations on special functions indicate that this asymptotic formula is reasonably accurate . key words : bayesian confidence intervals ; sample size ; smoothing spline .
we describe several improvements to freund and schapire ' s adaboost boosting algorithm , particularly in a setting in which hypotheses may assign confidences to each of their predictions . we give a simplified analysis of adaboost in this setting , and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses . we give a specific method for assigning confidences to the predictions of decision trees , a method closely related to one used by quinlan . this method also suggests a technique for growing decision trees which turns out to be identical to one proposed by kearns and mansour . we focus next on how to apply the new boosting algorithms to multiclass classification problems , particularly to the multi - label case in which each example may belong to more than one class . we give two boosting methods for this problem . one of these leads to a new method for handling the single - label case which is simpler but as effective as techniques suggested by freund and schapire . finally , we give some experimental results comparing a few of the algorithms discussed in this paper .
we examine a novel addition to the known methods for learning bayesian networks from data that improves the quality of the learned networks . our approach explicitly represents and learns the local structure in the conditional probability distributions ( cpds ) that quantify these networks . this increases the space of possible models , enabling the representation of cpds with a variable number of parameters . the resulting learning procedure induces models that better emulate the interactions present in the data . we describe the theoretical foundations and practical aspects of learning local structures and provide an empirical evaluation of the proposed learning procedure . this evaluation indicates that learning curves characterizing this procedure converge faster , in the number of training instances , than those of the standard procedure , which ignores the local structure of the cpds . our results also show that networks learned with local structures tend to be more complex ( in terms of arcs ) , yet require fewer parameters .
an accurate simulation of a heating coil is used to compare the performance of a proportional plus integral ( pi ) controller , a neural network trained to predict the steady - state output of the pi controller , a neural network trained to minimize the n - step ahead error between the coil output and the set point , and a reinforcement learning agent trained to minimize the sum of the squared error over time . although the pi controller works very well for this task , the neural networks produce improved performance . the reinforcement learning agent , when combined with a pi controller , learned to augment the pi control output for a small number of states for which control can be improved . keywords : neural networks , reinforcement learning , pi control , hvac
the cn #NUM# algorithm induces an ordered list of classification rules from examples using entropy as its search heuristic . in this short paper , we describe two improvements to this algorithm . firstly , we present the use of the laplacian error estimate as an alternative evaluation function and secondly , we show how unordered as well as ordered rules can be generated . we experimentally demonstrate significantly improved performances resulting from these changes , thus enhancing the usefulness of cn #NUM# as an inductive tool . comparisons with quinlan ' s c #NUM# . #NUM# are also made .
neural computation , also called connectionism , parallel distributed processing , neural network modeling or brain - style computation , has grown rapidly in the last decade . despite this explosion , and ultimately because of impressive applications , there has been a dire need for a concise introduction from a theoretical perspective , analyzing the strengths and weaknesses of connectionist approaches and establishing links to other disciplines , such as statistics or control theory . the introduction to the theory of neural computation by hertz , krogh and palmer ( subsequently referred to as hkp ) is written from the perspective of physics , the home discipline of the authors . the book fulfills its mission as an introduction for neural network novices , provided that they have some background in calculus , linear algebra , and statistics . it covers a number of models that are often viewed as disjoint . critical analyses and fruitful comparisons between these models
this paper describes how a competitive tree learning algorithm can be derived from first principles . the algorithm approximates the bayesian decision theoretic solution to the learning task . comparative experiments with the algorithm and the several mature ai and statistical families of tree learning algorithms currently in use show the derived bayesian algorithm is consistently as good or better , although sometimes at computational cost . using the same strategy , we can design algorithms for many other supervised and model learning tasks given just a probabilistic representation for the kind of knowledge to be learned . as an illustration , a second learning algorithm is derived for learning bayesian networks from data . implications to incremental learning and the use of multiple models are also discussed .
we address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high - accuracy concepts . we examine notions of relevance and irrelevance , and show that the definitions used in the machine learning literature do not adequately partition the features into useful categories of relevance . we present definitions for irrelevance and for two degrees of relevance . these definitions improve our understanding of the behavior of previous subset selection algorithms , and help define the subset of features that should be sought . the features selected should depend not only on the features and the target concept , but also on the induction algorithm . we describe a method for feature subset selection using cross - validation that is applicable to any induction algorithm , and discuss experiments conducted with id #NUM# and c #NUM# . #NUM# on artificial and real datasets .
we describe a machine learning method for predicting the value of a real - valued function , given the values of multiple input variables . the method induces solutions from samples in the form of ordered disjunctive normal form ( dnf ) decision rules . a central objective of the method and representation is the induction of compact , easily interpretable solutions . this rule - based decision model can be extended to search efficiently for similar cases prior to approximating function values . experimental results on real - world data demonstrate that the new techniques are competitive with existing machine learning and statistical methods and can sometimes yield superior regression performance .
in this paper we review research on machine learning and its relation to computational models of human learning . we focus initially on concept induction , examining five main approaches to this problem , then consider the more complex issue of learning sequential behaviors . after this , we compare the rhetoric that sometimes appears in the machine learning and psychological literature with the growing evidence that different theoretical paradigms typically produce similar results . in response , we suggest that concrete computational models , which currently dominate the field , may be less useful than simulations that operate at a more abstract level . we illustrate this point with an abstract simulation that explains a challenging phenomenon in the area of category learning , and we conclude with some general observations about such abstract models .
the function of an unknown biological sequence can often be accurately inferred by identifying sequences homologous to the original sequence . given a query set of known homologs , there exist at least three general classes of techniques for finding additional homologs : pairwise sequence comparisons , motif analysis , and hidden markov modeling . pairwise sequence comparisons are typically employed when only a single query sequence is known . hidden markov models ( hmms ) , on the other hand , are usually trained with sets of more than #NUM# sequences . motif - based methods fall in between these two extremes .
prerequisites . an understanding of the dynamic programming ( edit distance ) approach to pairwise sequence alignment is useful for parts #NUM# . #NUM# , #NUM# . #NUM# , and #NUM# . also , familiarity with the use of internet resources would be helpful for part #NUM# . for the former , see chapters #NUM# . #NUM# - #NUM# . #NUM# , and for the latter , see chapter #NUM# of the hypertext book of the gna - vsns biocomputing course at url . general rationale . you will understand why multiple alignment is considered a challenging problem , you will study approaches that try to reduce the number of steps needed to calculate the optimal solution , and you will study fast heuristics . in a case study involving immunoglobulin sequences , you will study multiple alignments obtained from www servers , recapitulating results from an original paper . revision history . version #NUM# . #NUM# on #NUM# sep #NUM# . expanded ex . #NUM# . updated ex . #NUM# . revised solution sheet - re - ex . #NUM# + #NUM# . marked more exercises by " a " ( to be submitted to the instructor ) . various minor clarifications in content
this article describes a new system for induction of oblique decision trees . this system , oc #NUM# , combines deterministic hill - climbing with two forms of randomization to find a good oblique split ( in the form of a hyperplane ) at each node of a decision tree . oblique decision tree methods are tuned especially for domains in which the attributes are numeric , although they can be adapted to symbolic or mixed symbolic / numeric attributes . we present extensive empirical studies , using both real and artificial data , that analyze oc #NUM# ' s ability to construct oblique trees that are smaller and more accurate than their axis - parallel counterparts . we also examine the benefits of randomization for the construction of oblique decision trees .
explanation - based reinforcement learning ( ebrl ) was introduced by dietterich and flann as a way of combining the ability of reinforcement learning ( rl ) to learn optimal plans with the generalization ability of explanation - based learning ( ebl ) ( di - etterich & flann , #NUM# ) . we extend this work to domains where the agent must order and achieve a sequence of subgoals in an optimal fashion . hierarchical ebrl can effectively learn optimal policies in some of these sequential task domains even when the subgoals weakly interact with each other . we also show that when a planner that can achieve the individual subgoals is available , our method converges even faster .
discretization of continuously valued data is a useful and necessary tool because many learning paradigms assume nominal data . a list of objectives for efficient and effective discretization is presented . a paradigm called brace ( boundary ranking and classification evaluation ) that attempts to meet the objectives is presented along with an algorithm that follows the paradigm . the paradigm meets many of the objectives , with potential for extension to meet the remainder . empirical results have been promising . for these reasons brace has potential as an effective and efficient method for discretization of continuously valued data . a further advantage of brace is that it is general enough to be extended to other types of clustering / unsupervised learning .
naive bayesian classifiers which make independence assumptions perform remarkably well on some data sets but poorly on others . we explore ways to improve the bayesian classifier by searching for dependencies among attributes . we propose and evaluate two algorithms for detecting dependencies among attributes and show that the backward sequential elimination and joining algorithm provides the most improvement over the naive bayesian classifier . the domains on which the most improvement occurs are those domains on which the naive bayesian classifier is significantly less accurate than a decision tree learner . this suggests that the attributes used in some common databases are not independent conditioned on the class and that the violations of the independence assumption that affect the accuracy of the classifier the bayesian classifier ( duda & hart , #NUM# ) is a probabilistic method for classification . it can be used to determine the probability that an example j belongs to class c i given values of attributes of an example represented as a set of n nominally - valued attribute - value pairs of the form a #NUM# = v #NUM# j : ^ p ( a k = v k j jc i ) may be estimated from the training data . to determine the most likely class of a test example , the probability of each class is computed with equation #NUM# . a classifier created in this manner is sometimes called a simple ( langley , #NUM# ) or naive ( kononenko , #NUM# ) bayesian classifier . one important evaluation metric for machine learning methods is the predictive accuracy on unseen examples . this is measured by randomly selecting a subset of the examples in a database to use as training examples and reserving the remainder to be used as test examples . in the case of the simple bayesian classifier , the training examples are used to estimate probabilities and equation #NUM# . #NUM# is then used can be detected from training data .
multiple sequence alignment of distantly related viral proteins remains a challenge to all currently available alignment methods . the hidden markov model approach offers a new , flexible method for the generation of multiple sequence alignments . the results of studies attempting to infer appropriate parameter constraints for the generation of de novo hmms for globin , kinase , aspartic acid protease , and ribonuclease h sequences by both the sam and hmmer methods are described .
several recurrent networks have been proposed as representations for the task of formal language learning . after training a recurrent network , the next step is to understand the information processing carried out by the network . some researchers ( giles et al . , #NUM# ; watrous & kuhn , #NUM# ; cleeremans et al . , #NUM# ) have resorted to extracting finite state machines from the internal state trajectories of their recurrent networks . this paper describes two conditions , sensitivity to initial conditions and frivolous computational explanations due to discrete measurements ( kolen & pollack , #NUM# ) , which allow these extraction methods to return illusionary finite state descriptions .
in order to be useful , a learning algorithm must be able to generalize well when faced with inputs not previously presented to the system . a bias is necessary for any generalization , and as shown by several researchers in recent years , no bias can lead to strictly better generalization than any other when summed over all possible functions or applications . this paper provides examples to illustrate this fact , but also explains how a bias or learning algorithm can be better than another in practice when the probability of the occurrence of functions is taken into account . it shows how domain knowledge and an understanding of the conditions under which each learning algorithm performs well can be used to increase the probability of accurate generalization , and identifies several of the conditions that should be considered when attempting to select an appropriate bias for a particular problem .
in this paper , we introduce a model - based reinforcement learning method called h - learning , which optimizes undiscounted average reward . we compare it with three other reinforcement learning methods in the domain of scheduling automatic guided vehicles , transportation robots used in modern manufacturing plants and facilities . the four methods differ along two dimensions . they are either model - based or model - free , and optimize discounted total reward or undiscounted average reward . our experimental results indicate that h - learning is more robust with respect to changes in the domain parameters , and in many cases , converges in fewer steps to better average reward per time step than all the other methods . an added advantage is that unlike the other methods it does not have any parameters to tune .
an important but often neglected problem in the field of artificial intelligence is that of grounding systems in their environment such that the representations they manipulate have inherent meaning for the system . since humans rely so heavily on semantics , it seems likely that the grounding is crucial to the development of truly intelligent behavior . this study investigates the use of simulated robotic agents with neural network processors as part of a method to ensure grounding . both the topology and weights of the neural networks are optimized through genetic algorithms . although such comprehensive optimization is difficult , the empirical evidence gathered here shows that the method is not only tractable but quite fruitful . in the experiments , the agents evolved a wall - following control strategy and were able to transfer it to novel environments . their behavior suggests that they were also learning to build cognitive maps .
explanation - based learning has shown promise as a powerful analytical learning technique . however , ebl is severely hampered by the requirement of a complete and correct domain theory for successful learning to occur . clearly , in non - trivial domains , developing such a domain theory is a nearly impossible task . therefore , much research has been devoted to understanding how an imperfect domain theory can be corrected and extended during system performance . in this paper , we present a characterization of this problem , and use it to analyze past research in the area . past characterizations of the problem ( e . g , [ mitchell et al . , #NUM# ; rajamoney and dejong , #NUM# ] ) have viewed the types of performance errors caused by a faulty domain theory as primary . in contrast , we focus primarily on the types of knowledge deficiencies present in the theory , and from these derive the types of performance errors that can result . correcting the theory can be viewed as a search through the space of possible domain theories , with a variety of knowledge sources that can be used to guide the search . we examine the types of knowledge used by a variety of past systems for this purpose . the hope is that this analysis will indicate the need for a " universal weak method " of domain theory correction , in which different sources of knowledge for theory correction can be freely and flexibly combined .
we study the task of tnding a maximal a posteriori ( map ) instantiation of bayesian network variables , given a partial value assignment as an initial constraint . this problem is known to be np - hard , so we concentrate on a stochastic approximation algorithm , simulated annealing . this stochastic algorithm can be realized as a sequential process on the set of bayesian network variables , where only one variable is allowed to change at a time . consequently , the method can become impractically slow as the number of variables increases . we present a method for mapping a given bayesian network to a massively parallel bolztmann machine neural network architecture , in the sense that instead of using the normal sequential simulated annealing algorithm , we can use a massively parallel stochastic process on the boltzmann machine architecture . the neural network updating process provably converges to a state which solves a given map task .
parameterized heuristics offers an elegant and powerful theoretical framework for design and analysis of autonomous adaptive communication networks . routing of messages in such networks presents a real - time instance of a multi - criterion optimization problem in a dynamic and uncertain environment . this paper describes a framework for heuristic routing in large networks . the effectiveness of the heuristic routing mechanism upon which quo vadis is based is described as part of a simulation study within a network with grid topology . a formal analysis of the underlying principles is presented through the incremental design of a set of heuristic decision functions that can be used to guide messages along a near - optimal ( e . g . , minimum delay ) path in a large network . this paper carefully derives the properties of such heuristics under a set of simplifying assumptions about the network topology and load dynamics and identify the conditions under which they are guaranteed to route messages along an optimal path . the paper concludes with a discussion of the relevance of the theoretical results presented in the paper to the design of intelligent autonomous adaptive communication networks and an outline of some directions of future research .
we analyze algorithms that predict a binary value by combining the predictions of several prediction strategies , called experts . our analysis is for worst - case situations , i . e . , we make no assumptions about the way the sequence of bits to be predicted is generated . we measure the performance of the algorithm by the difference between the expected number of mistakes it makes on the bit sequence and the expected number of mistakes made by the best expert on this sequence , where the expectation is taken with respect to the randomization in the predictions . we show that the minimum achievable difference is on the order of the square root of the number of mistakes of the best expert , and we give efficient algorithms that achieve this . our upper and lower bounds have matching leading constants in most cases . we then show how this leads to certain kinds of pattern recognition / learning algorithms with performance bounds that improve on the best results currently known in this context . we also compare our analysis to the case in which log loss is used instead of the expected number of mistakes .
this paper presents the formalization of a novel approach to structural similarity assessment and adaptation in case - based reasoning ( cbr ) for synthesis . the approach has been informally presented , exemplified , and implemented for the domain of industrial building design ( borner #NUM# ) . by relating the approach to existing theories we provide the foundation of its systematic evaluation and appropriate usage . cases , the primary repository of knowledge , are represented structurally using an algebraic approach . similarity relations provide structure preserving case modifications modulo the underlying algebra and an equational theory over the algebra ( so available ) . this representation of a modeled universe of discourse enables theory - based inference of adapted solutions . the approach enables us to incorporate formally generalization , abstraction , geometrical transformation , and their combinations into cbr .
a learning agent employing reinforcement learning is hindered because it only receives the critic ' s sparse and weakly informative training information . we present an approach in which an automated training agent may also provide occasional instruction to the learner in the form of actions for the learner to perform . the learner has access to both the critic ' s feedback and the trainer ' s instruction . in the experiments , we vary the level of the trainer ' s interaction with the learner , from allowing the trainer to instruct the learner at almost every time step , to not allowing the trainer to respond at all . we also vary a parameter that controls how the learner incorporates the trainer ' s actions . the results show significant reductions in the average number of training trials necessary to learn to perform the task .
we present an algorithm for improving the accuracy of algorithms for learning binary concepts . the improvement is achieved by combining a large number of hypotheses , each of which is generated by training the given learning algorithm on a different set of examples . our algorithm is based on ideas presented by schapire in his paper " the strength of weak learnability " , and represents an improvement over his results . the analysis of our algorithm provides general upper bounds on the resources required for learning in valiant ' s polynomial pac learning framework , which are the best general upper bounds known today . we show that the number of hypotheses that are combined by our algorithm is the smallest number possible . other outcomes of our analysis are results regarding the representational power of threshold circuits , the relation between learnability and compression , and a method for parallelizing pac learning algorithms . we provide extensions of our algorithms to cases in which the concepts are not binary and to the case where the accuracy of the learning algorithm depends on the distribution of the instances .
this paper proposes a model of ratio decidendi as a justification structure consisting of a series of reasoning steps , some of which relate abstract predicates to other abstract predicates and some of which relate abstract predicates to specific facts . this model satisfies an important set of characteristics of ratio decidendi identified from the jurisprudential literature . in particular , the model shows how the theory under which a case is decided controls its precedential effect . by contrast , a purely exemplar - based model of ratio decidendi fails to account for the dependency of prece - dential effect on the theory of decision .
in this paper , the abstract computational principles underlying topographic maps are discussed . we give a definition of a perfectly neighbourhood preserving map , which we call a topographic homeomorphism , and we prove that this has certain desirable properties . it is argued that when a topographic homeomorphism does not exist ( the usual case ) , many equally valid choices are available for quantifying the quality of a map . we introduce a particular measure that encompasses several previous proposals , and discuss its relation to other work . this formulation of the problem sets it within the well - known class of quadratic assignment problems .
this paper studies the robustness of pac learning algorithms when the instance space is f #NUM# ; #NUM# g n , and the examples are corrupted by purely random noise affecting only the instances ( and not the labels ) . in the past , conflicting results on this subject have been obtained | the " best agreement " rule can only tolerate small amounts of noise , yet in some cases large amounts of noise can be tolerated . we show that the truth lies somewhere between these two alternatives . for uniform attribute noise , in which each attribute is flipped independently at random with the same probability , we present an algorithm that pac learns monomials for any ( unknown ) noise rate less than #NUM# = #NUM# . contrasting this positive result , we show that nonuniform random attribute noise , where each attribute i is flipped randomly and independently with its own probability p i , is nearly as harmful as malicious noise | no algorithm can tolerate more than a very small amount of such noise .
this paper describes research investigating behavioral specialization in learning robot teams . each agent is provided a common set of skills ( motor schema - based behavioral assemblages ) from which it builds a task - achieving strategy using reinforcement learning . the agents learn individually to activate particular behavioral assemblages given their current situation and a reward signal . the experiments , conducted in robot soccer simulations , evaluate the agents in terms of performance , policy convergence , and behavioral diversity . the results show that in many cases , robots will automatically diversify by choosing heterogeneous behaviors . the degree of diversification and the performance of the team depend on the reward structure . when the entire team is jointly rewarded or penalized ( global reinforcement ) , teams tend towards heterogeneous behavior . when agents are provided feedback individually ( local reinforcement ) , they converge to identical policies .
product units provide a method of automatically learning the higher - order input combinations required for the efficient synthesis of boolean logic functions by neural networks . product units also have a higher information capacity than sigmoidal networks . however , this activation function has not received much attention in the literature . a possible reason for this is that one encounters some problems when using standard backpropagation to train networks containing these units . this report examines these problems , and evaluates the performance of three training algorithms on networks of this type . empirical results indicate that the error surface of networks containing product units have more local minima than corresponding networks with summation units . for this reason , a combination of local and global training algorithms were found to provide the most reliable convergence . we then investigate how ` hints ' can be added to the training algorithm . by extracting a common frequency from the input weights , and training this frequency separately , we show that convergence can be accelerated . in order to compare their performance with other transfer functions , product units were implemented as candidate units in the cascade correlation ( cc ) [ #NUM# ] system . using these candidate units resulted in smaller networks which trained faster than when the any of the standard ( three sigmoidal types and one gaussian ) transfer functions were used . this superiority was confirmed when a pool of candidate units of four different nonlinear activation functions were used , which have to compete for addition to the network . extensive simulations showed that for the problem of implementing random boolean logic functions , product units are always chosen above any of the other transfer functions .
our goal is to develop a hybrid cognitive model of how humans acquire skills on complex cognitive tasks . we are pursuing this goal by designing hybrid computational architectures for the nrl navigation task , which requires competent sensorimotor coordination . in this paper , we describe results of directly fitting human execution data on this task . we next present and then empirically compare two methods for modeling control knowledge acquisition ( reinforcement learning and a novel variant of action models ) with human learning on the task . the paper concludes with an experimental demonstration of the impact of background knowledge on system performance . our results indicate that the performance of our action models approach more closely approximates the rate of human learning on this task than does reinforcement learning .
we show in this paper that the agm postulates are too week to ensure the rational preservation of conditional beliefs during belief revision , thus permitting improper responses to sequences of observations . we remedy this weakness by proposing four additional postulates , which are sound relative to a qualitative version of probabilistic conditioning . contrary to the agm framework , the proposed postulates characterize belief revision as a process which may depend on elements of an epistemic state that are not necessarily captured by a belief set . we also show that a simple modification to the agm framework can allow belief revision to be a function of epistemic states . we establish a model - based representation theorem which characterizes the proposed postulates and constrains , in turn , the way in which entrenchment orderings may be transformed under iterated belief revision .
results are presented that demonstrate the learning and fine - tuning of search strategies using connectionist mechanisms . previous studies of strategy learning within the symbolic , production - rule formalism have not addressed fine - tuning behavior . here a two - layer connectionist system is presented that develops its search from a weak to a task - specific strategy and fine - tunes its performance . the system is applied to a simulated , real - time , balance - control task . we compare the performance of one - layer and two - layer networks , showing that the ability of the two - layer network to discover new features and thus enhance the original representation is critical to solving the balancing task .
following terminology used in adaptive control , we distinguish between indirect learning methods , which learn explicit models of the dynamic structure of the system to be controlled , and direct learning methods , which do not . we compare an existing indirect method , which uses a conventional dynamic programming algorithm , with a closely related direct reinforcement learning method by applying both methods to an infinite horizon markov decision problem with unknown state - transition probabilities . the simulations show that although the direct method requires much less space and dramatically less computation per control action , its learning ability in this task is superior to , or compares favorably with , that of the more complex indirect method . although these results do not address how the methods ' performances compare as problems become more difficult , they suggest that given a fixed amount of computational power available per control action , it may be better to use a direct reinforcement learning method augmented with indirect techniques than to devote all available resources to a computation - ally costly indirect method . comprehensive answers to the questions raised by this study depend on many factors making up the eco nomic context of the computation .
we propose a general framework in which to study belief change . we begin by defining belief in terms of knowledge and plausibility : an agent believes ' if he knows that ' is true in all the worlds he considers most plausible . we then consider some properties defining the interaction between knowledge and plausibility , and show how these properties affect the properties of belief . in particular , we show that by assuming two of the most natural properties , belief becomes a kd #NUM# operator . finally , we add time to the picture . this gives us a framework in which we can talk about knowledge , plausibility ( and hence belief ) , and time , which extends the framework of halpern and fagin [ hf #NUM# ] for modeling knowledge in multi - agent systems . we show that our framework is quite expressive and lets us model in a natural way a number of different scenarios for belief change . for example , we show how we can capture an analogue to prior probabilities , which can be updated by " conditioning " . in a related paper , we show how the two best studied scenarios , belief revision and belief update , fit into the framework .
markov chain monte carlo ( mcmc ) is used for evaluating expectations of functions of interest under a target distribution . this is done by calculating averages over the sample path of a markov chain having as its stationary distribution . for computational efficiency , the markov chain should be rapidly mixing . this can sometimes be achieved only by careful design of the transition kernel of the chain , on the basis of a detailed preliminary exploratory analysis of . an alternative approach might be to allow the transition kernel to adapt whenever new features of are encountered during the mcmc run . however , if such adaptation occurs infinitely often , the stationary distribution of the chain may be disturbed . we describe a framework , based on the concept of markov chain regeneration , which allows adaptation to occur infinitely often , but which does not disturb the stationary distribution of the chain or the consistency of sample - path averages . key words : adaptive method ; bayesian inference ; gibbs sampling ; markov chain monte carlo ;
a traditional interpolation model is characterized by the choice of reg - ularizer applied to the interpolant , and the choice of noise model . typically , the regularizer has a single regularization constant ff , and the noise model has a single parameter fi . the ratio ff = fi alone is responsible for determining globally all these attributes of the interpolant : its ` complexity ' , ` flexibility ' , ` smoothness ' , ` characteristic scale length ' , and ` characteristic amplitude ' . we suggest that interpolation models should be able to capture more than just one flavour of simplicity and complexity . we describe bayesian models in which the interpolant has a smoothness that varies spatially . we emphasize the importance , in practical implementation , of the concept of ` conditional convexity ' when designing models with many hyperparameters . we apply the new models to the interpolation of neuronal spike data and demonstrate a substantial improvement in generalization error .
author of this paper was co - ordinator of the machine learning project statlog during #NUM# - #NUM# . this project was supported financially by the european community . the main aim of statlog was to evaluate different learning algorithms using real industrial and commercial applications . as an industrial partner and contributor , daimler - benz has introduced different applications to stat - log among them fault diagnosis , letter and digit recognition , credit - scoring and prediction of the number of registered trucks . we have learned a lot of lessons from this project which have effected our application oriented research in the field of machine learning ( ml ) in daimler - benz . we have distinguished that , especially , more research is necessary to prepare the ml - algorithms to handle the real industrial and commercial applications . in this paper we describe , shortly , the daimler - benz applications in statlog , we discuss shortcomings of the applied ml - algorithms and finally we outline the fields where we think further research is necessary .
this paper describes the application of reinforcement learning ( rl ) to the difficult real world problem of elevator dispatching . the elevator domain poses a combination of challenges not seen in most rl research to date . elevator systems operate in continuous state spaces and in continuous time as discrete event dynamic systems . their states are not fully observable and they are nonstationary due to changing passenger arrival rates . in addition , we use a team of rl agents , each of which is responsible for controlling one elevator car . the team receives a global reinforcement signal which appears noisy to each agent due to the effects of the actions of the other agents , the random nature of the arrivals and the incomplete observation of the state . in spite of these complications , we show results that in simulation surpass the best of the heuristic elevator control algorithms of which we are aware . these results demonstrate the power of rl on a very large scale stochastic dynamic optimization problem of practical utility .
this paper presents fringe exploration , a technique for efficient exploration in partially observable domains . the key idea , ( applicable to many exploration techniques ) , is to keep statistics in the space of possible short - term memories , instead of in the agent ' s current state space . experimental results in a partially observable maze and in a difficult driving task with visual routines show dramatic performance improvements .
performing policy iteration in dynamic programming should only require knowledge of relative rather than absolute measures of the utility of actions what baird ( #NUM# ) calls the advantages of actions at states . nevertheless , existing methods in dynamic programming ( including baird ' s ) compute some form of absolute utility function . for smooth problems , advantages satisfy two differential consistency conditions ( including the requirement that they be free of curl ) , and we show that enforcing these can lead to appropriate policy improvement solely in terms of advantages .
we introduce a parallel approach , " dt - select , " for selecting features used by inductive learning algorithms to predict protein secondary structure . dt - select is able to rapidly choose small , nonredundant feature sets from pools containing hundreds of thousands of potentially useful features . it does this by building a decision tree , using features from the pool , that classifies a set of training examples . the features included in the tree provide a compact description of the training data and are thus suitable for use as inputs to other inductive learning algorithms . empirical experiments in the protein secondary - structure task , in which sets of complex features chosen by dt - select are used to augment a standard artificial neural network representation , yield surprisingly little performance gain , even though features are selected from very large feature pools . we discuss some possible reasons for this result . #NUM#
this paper presents the extension package developed by the author at the faculty of sciences and technology of the new university of lisbon , designed for experimentation with coarse - grained distributed genetic algorithms ( dga ) . the package was implemented as an extension to the basic sugal system , developed by andrew hunter at the university of sunderland , u . k . , which is primarily intended to be used in the research of sequential or serial genetic algorithms ( sga ) .
we explore representation of #NUM# d objects in which several distinct #NUM# d views are stored for each object . we demonstrate the ability of a two - layer network of thresholded summation units to support such representations . using unsupervised hebbian relaxation , the network learned to recognize ten objects from different viewpoints . the training process led to the emergence of compact representations of the specific input views . when tested on novel views of the same objects , the network exhibited a substantial generalization capability . in simulated psychophysical experiments , the network ' s behavior was qualitatively similar to that of human subjects .
internal models of the environment have an important role to play in adaptive systems in general and are of particular importance for the supervised learning paradigm . in this paper we demonstrate that certain classical problems associated with the notion of the " teacher " in supervised learning can be solved by judicious use of learned internal models as components of the adaptive system . in particular , we show how supervised learning algorithms can be utilized in cases in which an unknown dynamical system intervenes between actions and desired outcomes . our approach applies to any supervised learning algorithm that is capable of learning in multi - layer networks . * this paper is a revised version of mit center for cognitive science occasional paper # #NUM# . we wish to thank michael mozer , andrew barto , robert jacobs , eric loeb , and james mcclelland for helpful comments on the manuscript . this project was supported in part by brsg #NUM# s #NUM# rr #NUM# - #NUM# awarded by the biomedical research support grant program , division of research resources , national institutes of health , by a grant from atr auditory and visual perception research laboratories , by a grant from siemens corporation , by a grant from the human frontier science program , and by grant n #NUM# - #NUM# - j - #NUM# awarded by the office of naval research .
this paper will appear in proceedings of the eleventh international conference on machine learning . abstract this paper presents an algorithm for incremental induction of decision trees that is able to handle both numeric and symbolic variables . in order to handle numeric variables , a new tree revision operator called ` slewing ' is introduced . finally , a non - incremental method is given for finding a decision tree based on a direct metric of a candidate tree .
this research was primarily conducted while this author was at the university of calif . at santa cruz with support from onr grant n #NUM# - #NUM# - k - #NUM# , and at harvard university , supported by onr grant n #NUM# - #NUM# - k - #NUM# and darpa grant afosr - #NUM# - #NUM# . current address : nec research institute , #NUM# independence way , princeton , nj #NUM# . supported by onr grants n #NUM# - #NUM# - k - #NUM# and n #NUM# - #NUM# - j - #NUM# . part of this research was done while this author was on sabbatical at aiken computation laboratory , harvard , with partial support from the onr grants n #NUM# - #NUM# - k - #NUM# and n #NUM# - #NUM# - k - #NUM# . address : department of computer science , university of california at santa cruz .
many recent approaches to avoiding the utility problem in speedup learning ( the eventual degradation of performance due to increasing amounts of learned problem - solver control knowledge ) rely on sophisticated utility measures and significant numbers of training problems to accurately estimate the utility of control knowledge . empirical results presented here and elsewhere indicate that a simple selection strategy of retaining all control rules derived from a training problem solution quickly defines an efficient set of control knowledge from few training problems . this simple selection strategy provides a low - cost alternative to example - intensive approaches for improving the speed of a problem solver . experimentation illustrates the existence of a minimum ( representing least cost ) in the learning curve which is reached after a few training examples . stress is placed on controlling the amount of learned knowledge as opposed to which knowledge . an attempt is also made to relate domain characteristics to the shape of the learning curve .
parti - game is a new algorithm for learning feasible trajectories to goal regions in high dimensional continuous state - spaces . in high dimensions it is essential that learning does not plan uniformly over a state - space . parti - game maintains a decision - tree partitioning of state - space and applies techniques from game - theory and computational geometry to efficiently and adaptively concentrate high resolution only on critical areas . the current version of the algorithm is designed to find feasible paths or trajectories to goal regions in high dimensional spaces . future versions will be designed to find a solution that optimizes a real - valued criterion . many simulated problems have been tested , ranging from two - dimensional to nine - dimensional state - spaces , including mazes , path planning , non - linear dynamics , and planar snake robots in restricted spaces . in all cases , a good solution is found in less than ten trials and a few minutes .
predictive inference is seen here as the process of determining the predictive distribution of a discrete variable , given a data set of training examples and the values for the other problem domain variables . we consider three approaches for computing this predictive distribution , and assume that the joint probability distribution for the variables belongs to a set of distributions determined by a set of parametric models . in the simplest case , the predictive distribution is computed by using the model with the maximum a posteriori ( map ) posterior probability . in the evidence approach , the predictive distribution is obtained by averaging over all the individual models in the model family . in the third case , we define the predictive distribution by using rissanen ' s new definition of stochastic complexity . our experiments performed with the family of naive bayes models suggest that when using all the data available , the stochastic complexity approach produces the most accurate predictions in the log - score sense . however , when the amount of available training data is decreased , the evidence approach clearly outperforms the two other approaches . the map predictive distribution is clearly inferior in the log - score sense to the two more sophisticated approaches , but for the #NUM# / #NUM# - score the map approach may still in some cases produce the best results .
given a problem , a case - based reasoning ( cbr ) system will search its case memory and use the stored cases to find the solution , possibly modifying retrieved cases to adapt to the required input specifications . in this paper we introduce a neural network architecture for efficient case - based reasoning . we show how a rigorous bayesian probability propagation algorithm can be implemented as a feedforward neural network and adapted for cbr . in our approach the efficient indexing problem of cbr is naturally implemented by the parallel architecture , and heuristic matching is replaced by a probability metric . this allows our cbr to perform theoretically sound bayesian reasoning . we also show how the probability propagation actually offers a solution to the adaptation problem in a very natural way .
we present a new general - purpose algorithm for learning classes of [ #NUM# ; #NUM# ] - valued functions in a generalization of the prediction model , and prove a general upper bound on the expected absolute error of this algorithm in terms of a scale - sensitive generalization of the vapnik dimension proposed by alon , ben - david , cesa - bianchi and haussler . we give lower bounds implying that our upper bounds cannot be improved by more than a constant factor in general . we apply this result , together with techniques due to haussler and to benedek and itai , to obtain new upper bounds on packing numbers in terms of this scale - sensitive notion of dimension . using a different technique , we obtain new bounds on packing numbers in terms of kearns and schapire ' s fat - shattering function . we show how to apply both packing bounds to obtain improved general bounds on the sample complexity of agnostic learning . for each * & gt ; #NUM# , we establish weaker sufficient and stronger necessary conditions for a class of [ #NUM# ; #NUM# ] - valued functions to be agnostically learnable to within * , and to be an * - uniform glivenko - cantelli class .
it is widely considered an ultimate connectionist objective to incorporate neural networks into intelligent systems . these systems are intended to possess a varied repertoire of functions enabling adaptable interaction with a non - static environment . the first step in this direction is to develop various neural network algorithms and models , the second step is to combine such networks into a modular structure that might be incorporated into a workable system . in this paper we consider one aspect of the second point , namely processing reliability and hiding of wetware details . pre - sented is an architecture for a type of neural expert module , named an authority . an authority consists of a number of minos modules . each of the minos modules in an authority has the same processing capabilities , but varies with respect to its particular specialization to aspects of the problem domain . the authority employs the collection of minoses like a panel of experts . the expert with the highest confidence is believed , and it is the answer and confidence quotient that are transmitted to other levels in a system hierarchy .
partially observable markov decision processes ( pomdp ' s ) model decision problems in which an agent tries to maximize its reward in the face of limited and / or noisy sensor feedback . while the study of pomdp ' s is motivated by a need to address realistic problems , existing techniques for finding optimal behavior do not appear to scale well and have been unable to find satisfactory policies for problems with more than a dozen states . after a brief review of pomdp ' s , this paper discusses several simple solution methods and shows that all are capable of finding near - optimal policies for a selection of extremely small pomdp ' s taken from the learning literature . in contrast , we show that none are able to solve a slightly larger and noisier problem based on robot navigation . we find that a combination of two novel approaches performs well on these problems and suggest methods for scaling to even larger and more complicated domains .
we propose a new method of construction of markov chains with a given stationary distribution . this method is based on construction of an auxiliary chain with some other stationary distribution and picking elements of this auxiliary chain a suitable number of times . the proposed method has many advantages over its rivals . it is easy to implement ; it provides a simple analysis ; it can be faster and more efficient than the currently available techniques and it can also be adapted during the course of the simulation . we make theoretical and numerical comparisons of the characteristics of the proposed algorithm with some other mcmc techniques .
the problem of making optimal decisions in uncertain conditions is central to artificial intelligence . if the state of the world is known at all times , the world can be modeled as a markov decision process ( mdp ) . mdps have been studied extensively and many methods are known for determining optimal courses of action , or policies . the more realistic case where state information is only partially observable , partially observable markov decision processes ( pomdps ) , have received much less attention . the best exact algorithms for these problems can be very inefficient in both space and time . we introduce smooth partially observable value approximation ( spova ) , a new approximation method that can quickly yield good approximations which can improve over time . this method can be combined with reinforcement learning methods , a combination that was very effective in our test cases .
the katsuno and mendelzon ( km ) theory of belief update has been proposed as a reasonable model for revising beliefs about a changing world . however , the semantics of update relies on information which is not readily available . we describe an alternative semantical view of update in which observations are incorporated into a belief set by : a ) explaining the observation in terms of a set of plausible events that might have caused that observation ; and b ) predicting further consequences of those explanations . we also allow the possibility of conditional explanations . we show that this picture naturally induces an update operator conforming to the km postulates under certain assumptions . however , we argue that these assumptions are not always reasonable , and they restrict our ability to integrate update with other forms of revision when reasoning about action .
this paper specifies the main features of brain - like , neuronal , and connectionist models ; argues for the need for , and usefulness of , appropriate successively larger brain - like structures ; and examines parallel - hierarchical recognition cone models of perception from this perspective , as examples of such structures . the anatomy , physiology , behavior , and development of the visual system are briefly summarized to motivate the architecture of brain - structured networks for perceptual recognition . results are presented from simulations of carefully pre - designed recognition cone structures that perceive objects ( e . g . , houses ) in digitized photographs . a framework for perceptual learning is introduced , including mechanisms for generation - discovery ( feedback - guided growth of new links and nodes , subject to brain - like constraints ( e . g . , local receptive fields , global convergence - divergence ) . the information processing transforms discovered through generation are fine - tuned by feedback - guided reweight - ing of links . some preliminary results are presented of brain - structured networks that learn to recognize simple objects ( e . g . , letters of the alphabet , cups , apples , bananas ) through feedback - guided generation and reweighting . these show large improvements over networks that either lack brain - like structure or / and learn by reweighting of links alone .
the ability to restructure a decision tree efficiently enables a variety of approaches to decision tree induction that would otherwise be prohibitively expensive . two such approaches are described here , one being incremental tree induction ( iti ) , and the other being non - incremental tree induction using a measure of tree quality instead of test quality ( dmti ) . these approaches and several variants offer new computational and classifier characteristics that lend themselves to particular applications .
we consider a logistic regression model with a gaussian prior distribution over the parameters . we show that accurate variational techniques can be used to obtain a closed form posterior distribution over the parameters given the data thereby yielding a posterior predictive model . the results are readily extended to ( binary ) belief networks . for belief networks we also derive closed form posteriors in the presence of missing values . finally , we show that the dual of the regression problem gives a latent variable density model , the variational formulation of which leads to exactly solvable em updates .
mean field methods provide computationally efficient approximations to posterior probability distributions for graphical models . simple mean field methods make a completely factorized approximation to the posterior , which is unlikely to be accurate when the posterior is multimodal . indeed , if the posterior is multi - modal , only one of the modes can be captured . to improve the mean field approximation in such cases , we employ mixture models as posterior approximations , where each mixture component is a factorized distribution . we describe efficient methods for optimizing the parameters in these models .
the success of evolutionary methods on standard control learning tasks has created a need for new benchmarks . the classic pole balancing problem is no longer difficult enough to serve as a viable yardstick for measuring the learning efficiency of these systems . in this paper we present a more difficult version to the classic problem where the cart and pole can move in a plane . we demonstrate a neuroevolution system ( enforced sub - populations , or esp ) that can solve this difficult problem without velocity information .
this paper introduces and explores some representational biases for efficient learning of spatial , temporal , or spatio - temporal patterns in connectionist networks ( cn ) massively parallel networks of simple computing elements . it examines learning mechanisms that constructively build up network structures that encode information from environmental stimuli at successively higher resolutions as needed for the tasks ( e . g . , perceptual recognition ) that the network has to perform . some simple examples are presented to illustrate the the basic structures and processes used in such networks to ensure the parsimony of learned representations by guiding the system to focus its efforts at the minimal adequate resolution . several extensions of the basic algorithm for efficient learning using multi - resolution representations of spatial , temporal , or spatio - temporal patterns are discussed .
q ( ) - learning uses td ( ) - methods to accelerate q - learning . the update complexity of previous online q ( ) implementations based on lookup - tables is bounded by the size of the state / action space . our faster algorithm ' s update complexity is bounded by the number of actions . the method is based on the observation that q - value updates may be postponed until they are needed .
massively parallel networks of relatively simple computing elements offer an attractive and versatile framework for exploring a variety of learning structures and processes for intelligent systems . this paper briefly summarizes some popular learning structures and processes used in such networks . it outlines a range of potentially more powerful alternatives for pattern - directed inductive learning in such systems . it motivates and develops a class of new learning algorithms for massively parallel networks of simple computing elements . we call this class of learning processes generative for they offer a set of mechanisms for constructive and adaptive determination of the network architecture the number of processing elements and the connectivity among them as a function of experience . generative learning algorithms attempt to overcome some of the limitations of some approaches to learning in networks that rely on modification of weights on the links within an otherwise fixed network topology e . g . , rather slow learning and the need for an a - priori choice of a network architecture . several alternative designs as well as a range of control structures and processes which can be used to regulate the form and content of internal representations learned by such networks are examined . empirical results from the study of some generative learning algorithms are briefly summarized and several extensions and refinements of such algorithms , and directions for future research are outlined .
we introduce a constructive , incremental learning system for regression problems that models data by means of locally linear experts . in contrast to other approaches , the experts are trained independently and do not compete for data during learning . only when a prediction for a query is required do the experts cooperate by blen ding their individual predictions . each expert is trained by minimizing a penalized local cross validation error using second order methods . in this way , an expert is able to adjust the size and shape of the receptive field in which its predictions are valid , and also to adjust its bias on the importance of individual input dimensions . the size and shape adjustment corresponds to finding a local distance metric , while the bias adjustment accomplishes local dimensio n - ality reduction . we derive asymptotic results for our method . in a variety of simulations we demonstrate the properties of the algorithm with respect to interference , learning speed , prediction accuracy , feature detection , and task or i - ented incremental learning .
the model of a non - bayesian agent who faces a repeated game with incomplete information against nature is an appropriate tool for modeling general agent - environment interactions . in such a model the environment state ( controlled by nature ) may change arbitrarily , and the feedback / reward function is initially unknown . the agent is not bayesian , that is he does not form a prior probability neither on the state selection strategy of nature , nor on his reward function . a policy for the agent is a function which assigns an action to every history of observations and actions . two basic feedback structures are considered . in one of them the perfect monitoring case the agent is able to observe the previous environment state as part of his feedback , while in the other the imperfect monitoring case all that is available to the agent is the reward obtained . both of these settings refer to partially observable processes , where the current environment state is unknown . our main result refers to the competitive ratio criterion in the perfect monitoring case . we prove the existence of an efficient stochastic policy that ensures that the competitive ratio is obtained at almost all stages with an arbitrarily high probability , where efficiency is measured in terms of rate of convergence . it is further shown that such an optimal policy does not exist in the imperfect monitoring case . moreover , it is proved that in the perfect monitoring case there does not exist a deterministic policy that satisfies our long run optimality criterion . in addition , we discuss the maxmin criterion and prove that a deterministic efficient optimal strategy does exist in the imperfect monitoring case under this criterion . finally we show that our approach to long - run optimality can be viewed as qualitative , which distinguishes it from previous work in this area .
we describe a polynomial - time algorithm for learning axis - aligned rectangles in q d with respect to product distributions from multiple - instance examples in the pac model . here , each example consists of n elements of q d together with a label indicating whether any of the n points is in the rectangle to be learned . we assume that there is an unknown product distribution d over q d such that all instances are independently drawn according to d . the accuracy of a hypothesis is measured by the probability that it would incorrectly predict whether one of n more points drawn from d was in the rectangle to be learned . our algorithm achieves accuracy * with probability #NUM# ffi in
in the context of inductive learning , the bayesian approach turned out to be very successful in estimating probabilities of events when there are only a few learning examples . the m - probability estimate was developed to handle such situations . in this paper we present the m - distribution estimate , an extension to the m - probability estimate which , besides the estimation of probabilities , covers also the estimation of probability distributions . we focus on its application in the construction of regression trees . the theoretical results were incorporated into a system for automatic induction of regression trees . the results of applying the upgraded system to several domains are presented and compared to previous results .
real - world learning tasks often involve high - dimensional data sets with complex patterns of missing features . in this paper we review the problem of learning from incomplete data from two statistical perspectives | the likelihood - based and the bayesian . the goal is two - fold : to place current neural network approaches to missing data within a statistical framework , and to describe a set of algorithms , derived from the likelihood - based framework , that handle clustering , classification , and function approximation from incomplete data in a principled and efficient manner . these algorithms are based on mixture modeling and make two distinct appeals to the expectation - maximization ( em ) principle ( dempster et al . , #NUM# ) | both for the estimation of mixture components and for coping with the missing data . this report describes research done at the center for biological and computational learning and the artificial intelligence laboratory of the massachusetts institute of technology . support for the center is provided in part by a grant from the national science foundation under contract asc - #NUM# . support for the laboratory ' s artificial intelligence research is provided in part by the advanced research projects agency of the department of defense . the authors were supported in part by a grant from atr auditory and visual perception research laboratories , by a grant from siemens corporation , by grant iri - #NUM# from the national science foundation , and by grant n #NUM# - #NUM# - j - #NUM# from the office of naval research . zoubin ghahramani was supported by a grant from the mcdonnell - pew foundation . michael i . jordan is a nsf presidential young investigator .
recently , we have proven that the dynamics of any deterministic finite - state automata ( dfa ) with n states and m input symbols can be implemented in a sparse second - order recurrent neural network ( sornn ) with n + #NUM# state neurons and o ( mn ) second - order weights and sigmoidal discriminant functions [ #NUM# ] . we investigate how that constructive algorithm can be extended to fault - tolerant neural dfa implementations where faults in an analog implementation of neurons or weights do not affect the desired network performance . we show that tolerance to weight perturbation can be achieved easily ; tolerance to weight and / or neuron stuck - at - zero faults , however , requires duplication of the network resources . this result has an impact on the construction of neural dfas with a dense internal representation of dfa states .
in the multi - armed bandit problem , a gambler must decide which arm of k non - identical slot machines to play in a sequence of trials so as to maximize his reward . this classical problem has received much attention because of the simple model it provides of the trade - off between exploration ( trying out each arm to find the best one ) and exploitation ( playing the arm believed to give the best payoff ) . past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines . in this work , we make no statistical assumptions whatsoever about the nature of the process generating the payoffs of the slot machines . we give a solution to the bandit problem in which an adversary , rather than a well - behaved stochastic process , has complete control over the payoffs . in a sequence of t plays , we prove that the expected per - round payoff of our algorithm approaches that of the best arm at the rate o ( t #NUM# = #NUM# ) , and we give an improved rate of convergence when the best arm has fairly low payoff . we also prove a general matching lower bound on the best possible performance of any algorithm in our setting . in addition , we consider a setting in which the player has a team of experts advising him on which arm to play ; here , we give a strategy that will guarantee expected payoff close to that of the best expert . finally , we apply our result to the problem of learning to play an unknown repeated matrix game against an all - powerful adversary .
we show an alternative way of representing a bayesian belief network by sensitivities and probability distributions . this representation is equivalent to the traditional representation by conditional probabilities , but makes dependencies between nodes apparent and intuitively easy to understand . we also propose a qr matrix representation for the sensitivities and / or conditional probabilities which is more efficient , in both memory requirements and computational speed , than the traditional representation for computer - based implementations of probabilistic inference . we use sensitivities to show that for a certain class of binary networks , the computation time for approximate probabilistic inference with any positive upper bound on the error of the result is independent of the size of the network . finally , as an alternative to traditional algorithms that use conditional probabilities , we describe an exact algorithm for probabilistic inference that uses the qr - representation for sensitivities and updates probability distributions of nodes in a network according to messages from the neigh bors .
in many real - world domains , supervised learning requires a large number of training examples . in this paper , we describe an active learning method that uses a committee of learners to reduce the number of training examples required for learning . our approach is similar to the query by committee framework , where disagreement among the committee members on the predicted label for the input part of the example is used to signal the need for knowing the actual value of the label . our experiments are conducted in the text categorization domain , which is characterized by a large number of features , many of which are irrelevant . we report here on experiments using a committee of winnow - based learners and demonstrate that this approach can reduce the number of labeled training examples required over that used by a single winnow learner by #NUM# - #NUM# orders of magnitude .
covering has been formalized and used extensively . in this work , the divide - and - conquer technique is formalized as well and compared to the covering technique in a logic programming framework . covering works by repeatedly specializing an overly general hypothesis , on each iteration focusing on finding a clause with a high coverage of positive examples . divide - and - conquer works by specializing an overly general hypothesis once , focusing on discriminating positive from negative examples . experimental results are presented demonstrating that there are cases when more accurate hypotheses can be found by divide - and - conquer than by covering . moreover , since covering considers the same alternatives repeatedly it tends to be less efficient than divide - and - conquer , which never considers the same alternative twice . on the other hand , covering searches a larger hypothesis space , which may result in that more compact hypotheses are found by this technique than by divide - and - conquer . furthermore , divide - and - conquer is , in contrast to covering , not applicable to learn ing recursive definitions .
this paper introduces the recurrence surface approximation , an inductive learning method based on linear programming that predicts recurrence times using censored training examples , that is , examples in which the available training output may be only a lower bound on the " right answer . " this approach is augmented with a feature selection method that chooses an appropriate feature set within the context of the linear programming generalizer . computational results in the field of breast cancer prognosis are shown . a straightforward translation of the prediction method to an artificial neural network model is also proposed .
minimum message length ( mml ) is an invariant bayesian point estimation technique which is also consistent and efficient . we provide a brief overview of mml inductive inference ( wallace and boulton ( #NUM# ) , wallace and freeman ( #NUM# ) ) , and how it has both an information - theoretic and a bayesian interpretation . we then outline how mml is used for statistical parameter estimation , and how the mml mixture mod - elling program , snob ( wallace and boulton ( #NUM# ) , wal - lace ( #NUM# ) , wallace and dowe ( #NUM# ) ) uses the message lengths from various parameter estimates to enable it to combine parameter estimation with selection of the number of components . the message length is ( to within a constant ) the logarithm of the posterior probability of the theory . so , the mml theory can also be regarded as the theory with the highest posterior probability . snob currently assumes that variables are uncorrelated , and permits multi - variate data from gaussian , discrete multi - state , poisson and von mises circular distributions .
one of the challenges for models of cognitive phenomena is the development of efficient and exible interfaces between low level sensory information and high level processes . for visual processing , researchers have long argued that an attentional mechanism is required to perform many of the tasks required by high level vision . this thesis presents visit , a connectionist model of covert visual attention that has been used as a vehicle for studying this interface . the model is efficient , exible , and is biologically plausible . the complexity of the network is linear in the number of pixels . effective parallel strategies are used to minimize the number of iterations required . the resulting system is able to efficiently solve two tasks that are particularly difficult for standard bottom - up models of vision : computing spatial relations and visual search . simulations show that the networks behavior matches much of the known psychophysical data on human visual attention . the general architecture of the model also closely matches the known physiological data on the human attention system . various extensions to visit are discussed , including methods for learning the component modules .
in bayesian inference , a bayes factor is defined as the ratio of posterior odds versus prior odds where posterior odds is simply a ratio of the normalizing constants of two posterior densities . in many practical problems , the two posteriors have different dimensions . for such cases , the current monte carlo methods such as the bridge sampling method ( meng and wong #NUM# ) , the path sampling method ( gelman and meng #NUM# ) , and the ratio importance sampling method ( chen and shao #NUM# ) cannot directly be applied . in this article , we extend importance sampling , bridge sampling , and ratio importance sampling to problems of different dimensions . then we find global optimal importance sampling , bridge sampling , and ratio importance sampling in the sense of minimizing asymptotic relative mean - square errors of estimators . implementation algorithms , which can asymptotically achieve the optimal simulation errors , are developed and two illustrative examples are also provided .
as knowledge bases used for ai systems increase in size , access to relevant information is the dominant factor in the cost of inference . this is especially true for analogical ( or case - based ) reasoning , in which the ability of the system to perform inference is dependent on efficient and flexible access to a large base of exemplars ( cases ) judged likely to be relevant to solving a problem at hand . in this chapter we discuss a novel algorithm for efficient associative matching of relational structures in large semantic networks . the structure matching algorithm uses massively parallel hardware to search memory for knowledge structures matching a given probe structure . the algorithm is built on top of parka , a massively parallel knowledge representation system which runs on the connection machine . we are currently exploring the utility of this algorithm in caper , a case - based planning system .
we consider the use of " on - line " stopping rules to reduce the number of training examples needed to pac - learn . rather than collect a large training sample that can be proved sufficient to eliminate all bad hypotheses a priori , the idea is instead to observe training examples one - at - a - time and decide " on - line " whether to stop and return a hypothesis , or continue training . the primary benefit of this approach is that we can detect when a hypothesizer has actually " converged , " and halt training before the standard fixed - sample - size bounds . this paper presents a series of such sequential learning procedures for : distribution - free pac - learning , " mistake - bounded to pac " conversion , and distribution - specific pac - learning , respectively . we analyze the worst case expected training sample size of these procedures , and show that this is often smaller than existing fixed sample size bounds | while providing the exact same worst case pac - guarantees . we also provide lower bounds that show these reductions can at best involve constant ( and possibly log ) factors . however , empirical studies show that these sequential learning procedures actually use many times fewer training examples in prac tice .
this paper presents a novel approach to determine structural similarity as guidance for adaptation in case - based reasoning ( cbr ) . we advance structural similarity assessment which provides not only a single numeric value but the most specific structure two cases have in common , inclusive of the modification rules needed to obtain this structure from the two cases . our approach treats retrieval , matching and adaptation as a group of dependent processes . this guarantees the retrieval and matching of not only similar but adaptable cases . both together enlarge the overall problem solving performance of cbr and the explainability of case selection and adaptation considerably . although our approach is more theoretical in nature and not restricted to a specific domain , we will give an example taken from the domain of industrial building design . additionally , we will sketch two prototypical implementations of this approach .
we analyze the blame - assignment task in the context of experience - based design and redesign of physical devices . we identify three types of blame - assignment tasks that differ in the types of information they take as input : the design does not achieve a desired behavior of the device , the design results in an undesirable behavior , a specific structural element in the design misbehaves . we then describe a model - based approach for solving the blame - assignment task . this approach uses structure - behavior - function models that capture a designer ' s comprehension of the way a device works in terms of causal explanations of how its structure results in its behaviors . we also address the issue of indexing the models in memory . we discuss how the three types of blame - assignment tasks require different types of indices for accessing the models . finally we describe the kritik #NUM# system that implements and evaluates this model - based approach to blame assignment .
we present a framework for task - driven knowledge acquisition in the development of design support systems . different types of knowledge that enter the knowledge base of a design support system are defined and illustrated both from a formal and from a knowledge acquisition vantage point . special emphasis is placed on the task - structure , which is used to guide both acquisition and application of knowledge . starting with knowledge for planning steps in design and augmenting this with problem - solving knowledge that supports design , a formal integrated model of knowledge for design is constructed . based on the notion of knowledge acquisition as an incremental process we give an account of possibilities for problem solving depending on the knowledge that is at the disposal of the system . finally , we depict how different kinds of knowledge interact in a design support system . ? this research was supported by the german ministry for research and technology ( bmft ) within the joint project fabel under contract no . #NUM# - #NUM# - #NUM# iw #NUM# . project partners in fabel are german national research center of computer science ( gmd ) , sankt augustin , bsr consulting gmbh , munchen , technical university of dresden , htwk leipzig , university of freiburg , and university of karlsruhe .
the activity of sorting like objects into classes without any help from an omniscient supervisor is known as unsupervised classification . in ai both symbolic and connectionist camps study classification . the statistical classifiers such as autoclass and snob search for the theory that can best explain the distribution of given data , whereas neural network classifiers such as kohonen ' s networks and art #NUM# use the vector quantization principle for classifying data . previously , many studies have compared supervised classification algorithms , but the more challenging problem of comparing unsupervised classifiers has largely been ignored . we performed an empirical comparison of art #NUM# , autoclass and snob . we highlight the strengths and weaknesses of the various classifiers . overall , statistical classifiers , especially snob , perform better than their neural network counterpart art #NUM# .
ai research on case - based reasoning has led to the development of many laboratory case - based systems . as we move towards introducing these systems into work environments , explaining the processes of case - based reasoning is becoming an increasingly important issue . in this paper we describe the notion of a meta - case for illustrating , explaining and justifying case - based reasoning . a meta - case contains a trace of the processing in a problem - solving episode , and provides an explanation of the problem - solving decisions and a ( partial ) justification for the solution . the language for representing the problem - solving trace depends on the model of problem solving . we describe a task - method - knowledge ( tmk ) model of problem - solving and describe the representation of meta - cases in the tmk language . we illustrate this explanatory scheme with examples from interactive kritik , a computer - based de
statistical decision theory provides a principled way to estimate amino acid frequencies in conserved positions of a protein family . the goal is to minimize the risk function , or the expected squared - error distance between the estimates and the true population frequencies . the minimum - risk estimates are obtained by adding an optimal number of pseudocounts to the observed data . two formulas are presented , one for pseudocounts based on marginal amino acid frequencies and one for pseudocounts based on the observed data . experimental results show that profiles constructed using minimal - risk estimates are more discriminating than those constructed using existing methods .
the purpose of this paper is to propose a refinement of the notion of innateness . if we merely identify innateness with bias , then we obtain a poor characterisation of this notion , since any learning device relies on a bias that makes it choose a given hypothesis instead of another . we show that our intuition of innateness is better captured by a characteristic of bias , related to isotropy . generalist models of learning are shown to rely on an isotropic bias , whereas the bias of specialised models , which include some specific a priori knowledge about what is to be learned , is necessarily anisotropic . the socalled generalist models , however , turn out to be specialised in some way : they learn symmetrical forms preferentially , and have strictly no deficiencies in their learning ability . because some learning beings do not always show these two properties , such generalist models may be sometimes ruled out as bad candidates for cognitive modelling .
reliable vision - based control of an autonomous vehicle requires the ability to focus attention on the important features in an input scene . previous work with an autonomous lane following system , alvinn [ pomerleau , #NUM# ] , has yielded good results in uncluttered conditions . this paper presents an artificial neural network based learning approach for handling difficult scenes which will confuse the alvinn system . this work presents a mechanism for achieving task - specific focus of attention by exploiting temporal coherence . a saliency map , which is based upon a computed expectation of the contents of the inputs in the next time step , indicates which regions of the input retina are important for performing the task . the saliency map can be used to accentuate the features which are important for the task , and de - emphasize those which are not .
production scheduling , the problem of sequentially configuring a factory to meet forecasted demands , is a critical problem throughout the manufacturing industry . the requirement of maintaining product inventories in the face of unpredictable demand and stochastic factory output makes standard scheduling models , such as job - shop , inadequate . currently applied algorithms , such as simulated annealing and constraint propagation , must employ ad - hoc methods such as frequent replanning to cope with uncertainty . in this paper , we describe a markov decision process ( mdp ) formulation of production scheduling which captures stochasticity in both production and demands . the solution to this mdp is a value function which can be used to generate optimal scheduling decisions online . a simple example illustrates the theoretical superiority of this approach over replanning - based methods . we then describe an industrial application and two reinforcement learning methods for generating an approximate value function on this domain . our results demonstrate that in both deterministic and noisy scenarios , value function approx imation is an effective technique .
in this paper we investigate a new formal model of machine learning in which the concept ( boolean function ) to be learned may exhibit uncertain or probabilistic behavior | thus , the same input may sometimes be classified as a positive example and sometimes as a negative example . such probabilistic concepts ( or p - concepts ) may arise in situations such as weather prediction , where the measured variables and their accuracy are insufficient to determine the outcome with certainty . we adopt from the valiant model of learning [ #NUM# ] the demands that learning algorithms be efficient and general in the sense that they perform well for a wide class of p - concepts and for any distribution over the domain . in addition to giving many efficient algorithms for learning natural classes of p - concepts , we study and develop in detail an underlying theory of learning p - concepts .
this paper highlights a phenomenon that causes deductively learned knowledge to be harmful when used for problem solving . the problem occurs when deductive problem solvers encounter a failure branch of the search tree . the backtracking mechanism of such problem solvers will force the program to traverse the whole subtree thus visiting many nodes twice - once by using the deductively learned rule and once by using the rules that generated the learned rule in the first place . we suggest an approach called utilization filtering to solve that problem . learners that use this approach submit to the problem solver a filter function together with the knowledge that was acquired . the function decides for each problem whether to use the learned knowledge and what part of it to use . we have tested the idea in the context of a lemma learning system , where the filter uses the probability of a subgoal failing to decide whether to turn lemma usage off . experiments show an improvement of performance by a factor of #NUM# . this paper is concerned with a particular type of harmful redundancy that occurs in deductive problem solvers that employ backtracking in their search procedure , and use deductively learned knowledge to accelerate the search . the problem is that in failure branches of the search tree , the backtracking mechanism of the problem solver forces exploration of the whole subtree . thus , the search procedure will visit many states twice - once by using the deductively learned rule , and once by using the search path that produced the rule in the first place .
the authors thank rich yee , vijay gullapalli , brian pinette , and jonathan bachrach for helping to clarify the relationships between heuristic search and control . we thank rich sutton , chris watkins , paul werbos , and ron williams for sharing their fundamental insights into this subject through numerous discussions , and we further thank rich sutton for first making us aware of korf ' s research and for his very thoughtful comments on the manuscript . we are very grateful to dimitri bertsekas and steven sullivan for independently pointing out an error in an earlier version of this article . finally , we thank harry klopf , whose insight and persistence encouraged our interest in this class of learning problems . this research was supported by grants to a . g . barto from the national science foundation ( ecs - #NUM# and ecs - #NUM# ) and the air force office of scientific research , bolling afb ( afosr - #NUM# - #NUM# ) .
reinforcement learning ( rl ) has become a central paradigm for solving learning - control problems in robotics and artificial intelligence . rl researchers have focussed almost exclusively on problems where the controller has to maximize the discounted sum of payoffs . however , as emphasized by schwartz ( #NUM# ) , in many problems , e . g . , those for which the optimal behavior is a limit cycle , it is more natural and com - putationally advantageous to formulate tasks so that the controller ' s objective is to maximize the average payoff received per time step . in this paper i derive new average - payoff rl algorithms as stochastic approximation methods for solving the system of equations associated with the policy evaluation and optimal control questions in average - payoff rl tasks . these algorithms are analogous to the popular td and q - learning algorithms already developed for the discounted - payoff case . one of the algorithms derived here is a significant variation of schwartz ' s r - learning algorithm . preliminary empirical results are presented to validate these new algorithms .
we present algorithms for exactly learning unknown environments that can be described by deterministic finite automata . the learner performs a walk on the target automaton , where at each step it observes the output of the state it is at , and chooses a labeled edge to traverse to the next state . we assume that the learner has no means of a reset , and we also assume that the learner does not have access to a teacher that answers equivalence queries and gives the learner counterexamples to its hypotheses . we present two algorithms , one assumes that the outputs observed by the learner are always correct and the other assumes that the outputs might be erroneous . the running times of both algorithms are polynomial in the cover time of the underlying graph of the target automaton .
exploring and mapping an unknown environment is a fundamental problem , which is studied in a variety of contexts . many works have focused on finding efficient solutions to restricted versions of the problem . in this paper , we consider a model that makes very limited assumptions on the environment and solve the mapping problem in this general setting . we model the environment by an unknown directed graph g , and consider the problem of a robot exploring and mapping g . we do not assume that the vertices of g are labeled , and thus the robot has no hope of succeeding unless it is given some means of distinguishing between vertices . for this reason we provide the robot with a pebble a device that it can place on a vertex and use to identify the vertex later . in this paper we show : ( #NUM# ) if the robot knows an upper bound on the number of vertices then it can learn the graph efficiently with only one pebble . ( #NUM# ) if the robot does not know an upper bound on the number of vertices n , then fi ( log log n ) pebbles are both necessary and sufficient . in both cases our algorithms are deterministic .
in recent years there has been an increasing interest in learning bayesian networks from data . one of the most effective methods for learning such networks is based on the minimum description length ( mdl ) principle . previous work has shown that this learning procedure is asymptotically successful : with probability one , it will converge to the target distribution , given a sufficient number of samples . however , the rate of this convergence has been hitherto unknown . in this work we examine the sample complexity of mdl based learning procedures for bayesian networks . we show that the number of samples needed to learn an * - close approximation ( in terms of entropy distance ) with confidence ffi is o * ) #NUM# log #NUM# ffi log log #NUM# . this means that the sample complexity is a low - order polynomial in the error threshold and sub - linear in the confidence bound . we also discuss how the constants in this term depend on the complexity of the target distribution . finally , we address questions of asymptotic minimality and propose a method for using the sample complexity results to speed up the learning process .
almost all the work in average - reward reinforcement learning ( arl ) so far has focused on table - based methods which do not scale to domains with large state spaces . in this paper , we propose two extensions to a model - based arl method called h - learning to address the scale - up problem . we extend h - learning to learn action models and reward functions in the form of bayesian networks , and approximate its value function using local linear regression . we test our algorithms on several scheduling tasks for a simulated automatic guided vehicle ( agv ) and show that they are effective in significantly reducing the space requirement of h - learning and making it converge faster . to the best of our knowledge , our results are the first in apply ing function approximation to arl .
understanding high - dimensional real world data usually requires learning the structure of the data space . the structure may contain high - dimensional clusters that are related in complex ways . methods such as merge clustering and self - organizing maps are designed to aid the visualization and interpretation of such data . however , these methods often fail to capture critical structural properties of the input . although self - organizing maps capture high - dimensional topology , they do not represent cluster boundaries or discontinu - ities . merge clustering extracts clusters , but it does not capture local or global topology . this paper proposes an algorithm that combines the topology - preserving characteristics of self - organizing maps with a flexible , adaptive structure that learns the cluster bound aries in the data .
although building sophisticated learning agents that operate in complex environments will require learning to perform multiple tasks , most applications of reinforcement learning have focussed on single tasks . in this paper i consider a class of sequential decision tasks ( sdts ) , called composite sequential decision tasks , formed by temporally concatenating a number of elemental sequential decision tasks . elemental sdts cannot be decomposed into simpler sdts . i consider a learning agent that has to learn to solve a set of elemental and composite sdts . i assume that the structure of the composite tasks is unknown to the learning agent . the straightforward application of reinforcement learning to multiple tasks requires learning the tasks separately , which can waste computational resources , both memory and time . i present a new learning algorithm and a modular architecture that learns the decomposition of composite sdts , and achieves transfer of learning by sharing the solutions of elemental sdts across multiple composite sdts . the solution of a composite sdt is constructed by computationally inexpensive modifications of the solutions of its constituent elemental sdts . i provide a proof of one aspect of the learning algorithm .
existing approaches for learning to control a robot arm rely on supervised methods where correct behavior is explicitly given . it is difficult to learn to avoid obstacles using such methods , however , because examples of obstacle avoidance behavior are hard to generate . this paper presents an alternative approach that evolves neural network controllers through genetic algorithms . no input / output examples are necessary , since neuro - evolution learns from a single performance measurement over the entire task of grasping an object . the approach is tested in a simulation of the oscar - #NUM# robot arm which receives both visual and sensory input . neural networks evolved to effectively avoid obstacles at various locations to reach random target locations .
it is widely accepted that the use of more compact representations than lookup tables is crucial to scaling reinforcement learning ( rl ) algorithms to real - world problems . unfortunately almost all of the theory of reinforcement learning assumes lookup table representations . in this paper we address the pressing issue of combining function approximation and rl , and present #NUM# ) a function approx - imator based on a simple extension to state aggregation ( a commonly used form of compact representation ) , namely soft state aggregation , #NUM# ) a theory of convergence for rl with arbitrary , but fixed , soft state aggregation , #NUM# ) a novel intuitive understanding of the effect of state aggregation on online rl , and #NUM# ) a new heuristic adaptive state aggregation algorithm that finds improved compact representations by exploiting the non - discrete nature of soft state aggregation . preliminary empirical results are also presented .
this article introduces a class of incremental learning procedures specialized for prediction | that is , for using past experience with an incompletely known system to predict its future behavior . whereas conventional prediction - learning methods assign credit by means of the difference between predicted and actual outcomes , the new methods assign credit by means of the difference between temporally successive predictions . although such temporal - difference methods have been used in samuel ' s checker player , holland ' s bucket brigade , and the author ' s adaptive heuristic critic , they have remained poorly understood . here we prove their convergence and optimality for special cases and relate them to supervised - learning methods . for most real - world prediction problems , temporal - difference methods require less memory and less peak computation than conventional methods ; and they produce more accurate predictions . we argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal - difference methods can be applied to advantage .
this paper extends previous work with dyna , a class of architectures for intelligent systems based on approximating dynamic programming methods . dyna architectures integrate trial - and - error ( reinforcement ) learning and execution - time planning into a single process operating alternately on the world and on a learned model of the world . in this paper , i present and show results for two dyna architectures . the dyna - pi architecture is based on dynamic programming ' s policy iteration method and can be related to existing ai ideas such as evaluation functions and universal plans ( reactive systems ) . using a navigation task , results are shown for a simple dyna - pi system that simultaneously learns by trial and error , learns a world model , and plans optimal routes using the evolving world model . the dyna - q architecture is based on watkins ' s q - learning , a new kind of reinforcement learning . dyna - q uses a less familiar set of data structures than does dyna - pi , but is arguably simpler to implement and use . we show that dyna - q architectures are easy to adapt for use in changing environments .
on large problems , reinforcement learning systems must use parameterized function approximators such as neural networks in order to generalize between similar situations and actions . in these cases there are no strong theoretical results on the accuracy of convergence , and computational results have been mixed . in particular , boyan and moore reported at last year ' s meeting a series of negative results in attempting to apply dynamic programming together with function approximation to simple control problems with continuous state spaces . in this paper , we present positive results for all the control tasks they attempted , and for one that is significantly larger . the most important differences are that we used sparse - coarse - coded function approximators ( cmacs ) whereas they used mostly global function approximators , and that we learned online whereas they learned o * ine . boyan and moore and others have suggested that the problems they encountered could be solved by using actual outcomes ( " rollouts " ) , as in classical monte carlo methods , and as in the td ( ) algorithm when = #NUM# . however , in our experiments this always resulted in substantially poorer performance . we conclude that reinforcement learning can work robustly in conjunction with function approximators , and that there is little justification at present for avoiding the case of general .
we consider the requirements of online learning | learning which must be done incrementally and in realtime , with the results of learning available soon after each new example is acquired . despite the abundance of methods for learning from examples , there are few that can be used effectively for online learning , e . g . , as components of reinforcement learning systems . most of these few , including radial basis functions , cmacs , ko - honen ' s self - organizing maps , and those developed in this paper , share the same structure . all expand the original input representation into a higher dimensional representation in an unsupervised way , and then map that representation to the final answer using a relatively simple supervised learner , such as a perceptron or lms rule . such structures learn very rapidly and reliably , but have been thought either to scale poorly or to require extensive domain knowledge . to the contrary , some researchers ( rosenblatt , #NUM# ; gallant & smith , #NUM# ; kanerva , #NUM# ; prager & fallside , #NUM# ) have argued that the expanded representation can be chosen largely at random with good results . the main contribution of this paper is to develop and test this hypothesis . we show that simple random - representation methods can perform as well as nearest - neighbor methods ( while being more suited to online learning ) , and significantly better than backpropagation . we find that the size of the random representation does increase with the dimensionality of the problem , but not unreasonably so , and that the required size can be reduced substantially using unsupervised - learning techniques . our results suggest that randomness has a useful role to play in online supervised learning and constructive induction .
we consider the problem of dynamically apportioning resources among a set of options in a worst - case on - line framework . the model we study can be interpreted as a broad , abstract extension of the well - studied on - line prediction model to a general decision - theoretic setting . we show that the multiplicative weight - update rule of littlestone and warmuth [ #NUM# ] can be adapted to this model yielding bounds that are slightly weaker in some cases , but applicable to a considerably more general class of learning problems . we show how the resulting learning algorithm can be applied to a variety of problems , including gambling , multiple - outcome prediction , repeated games and prediction of points in rn .
a new on - line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals . the dependency is measured by the average mutual information ( mi ) of the outputs . the source signals and the mixing matrix are unknown except for the number of the sources . the gram - charlier expansion instead of the edgeworth expansion is used in evaluating the mi . the natural gradient approach is used to minimize the mi . a novel activation function is proposed for the on - line learning algorithm which has an equivariant property and is easily implemented on a neural network like model . the validity of the new learning algorithm is verified by computer simulations .
overfitting is a well - known problem in the fields of symbolic and connectionist machine learning . it describes the deterioration of gen - eralisation performance of a trained model . in this paper , we investigate the ability of a novel artificial neural network , bp - som , to avoid overfitting . bp - som is a hybrid neural network which combines a multi - layered feed - forward network ( mfn ) with kohonen ' s self - organising maps ( soms ) . during training , supervised back - propagation learning and unsupervised som learning cooperate in finding adequate hidden - layer representations . we show that bp - som outperforms standard backpropagation , and also back - propagation with a weight decay when dealing with the problem of overfitting . in addition , we show that bp - som succeeds in preserving generalisation performance under hidden - unit pruning , where both other methods fail .
we describe a model of iterated belief revision that extends the agm theory of revision to account for the effect of a revision on the conditional beliefs of an agent . in particular , this model ensures that an agent makes as few changes as possible to the conditional component of its belief set . adopting the ramsey test , minimal conditional revision provides acceptance conditions for arbitrary right - nested conditionals . we show that problem of determining acceptance of any such nested conditional can be reduced to acceptance tests for unnested conditionals . thus , iterated revision can be accomplished in a virtual manner , using uniterated revision .
reinforcement learning techniques address the problem of learning to select actions in unknown , dynamic environments . it is widely acknowledged that to be of use in complex domains , reinforcement learning techniques must be combined with generalizing function approximation methods such as artificial neural networks . little , however , is understood about the theoretical properties of such combinations , and many researchers have encountered failures in practice . in this paper we identify a prime source of such failuresnamely , a systematic overestimation of utility values . using watkins ' q - learning [ #NUM# ] as an example , we give a theoretical account of the phenomenon , deriving conditions under which one may expected it to cause learning to fail . employing some of the most popular function approximators , we present experimental results which support the theoretical findings .
we derive a new self - organising learning algorithm which maximises the information transferred in a network of non - linear units . the algorithm does not assume any knowledge of the input distributions , and is defined here for the zero - noise limit . under these conditions , information maximisation has extra properties not found in the linear case ( linsker #NUM# ) . the non - linearities in the transfer function are able to pick up higher - order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation . this enables the network to separate statistically independent components in the inputs : a higher - order generalisation of principal components analysis . we apply the network to the source separation ( or cocktail party ) problem , successfully separating unknown mixtures of up to ten speakers . we also show that a variant on the network architecture is able to perform blind deconvolution ( cancellation of unknown echoes and reverberation in a speech signal ) . finally , we derive dependencies of information transfer on time delays . we suggest that information max - imisation provides a unifying framework for problems in ` blind ' signal processing .
this paper is a multidisciplinary review of empirical , statistical learning from a graphical model perspective . well - known examples of graphical models include bayesian networks , directed graphs representing a markov chain , and undirected networks representing a markov field . these graphical models are extended to model data analysis and empirical learning using the notation of plates . graphical operations for simplifying and manipulating a problem are provided including decomposition , differentiation , and the manipulation of probability models from the exponential family . two standard algorithm schemas for learning are reviewed in a graphical framework : gibbs sampling and the expectation maximization algorithm . using these operations and schemas , some popular algorithms can be synthesized from their graphical specification . this includes versions of linear regression , techniques for feed - forward networks , and learning gaussian and discrete bayesian networks from data . the paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented .
the utility problem in speedup learning describes a common behavior of machine learning methods : the eventual degradation of performance due to increasing amounts of learned knowledge . the shape of the learning curve ( cost of using a learning method vs . number of training examples ) over several domains suggests a parameterized model relating performance to the amount of learned knowledge and a mechanism to limit the amount of learned knowledge for optimal performance . many recent approaches to avoiding the utility problem in speedup learning rely on sophisticated utility measures and significant numbers of training data to accurately estimate the utility of control knowledge . empirical results presented here and elsewhere indicate that a simple selection strategy of retaining all control rules derived from a training problem explanation quickly defines an efficient set of control knowledge from few training problems . this simple selection strategy provides a low - cost alternative to example - intensive approaches for improving the speed of a problem solver . experimentation illustrates the existence of a minimum ( representing least cost ) in the learning curve which is reached after a few training examples . stress is placed on controlling the amount of learned knowledge as opposed to which knowledge . an attempt is also made to relate domain characteristics to the shape of the learning curve .
we compare kernel estimators , single and multi - layered perceptrons and radial - basis functions for the problems of classification of handwritten digits and speech phonemes . by taking two different applications and employing many techniques , we report here a two - dimensional study whereby a domain - independent assessment of these learning methods can be possible . we consider a feed - forward network with one hidden layer . as examples of the local methods , we use kernel estimators like k - nearest neighbor ( k - nn ) , parzen windows , generalized k - nn , and grow and learn ( condensed nearest neighbor ) . we have also considered fuzzy k - nn due to its similarity . as distributed networks , we use linear perceptron , pairwise separating linear perceptron , and multilayer perceptrons with sigmoidal hidden units . we also tested the radial - basis function network which is a combination of local and distributed networks . four criteria are taken for comparison : correct classification of the test set , network size , learning time , and the operational complexity . we found that perceptrons when the architecture is suitable , generalize better than local , memory - based kernel estimators but require longer training and more precise computation . local networks are simple , learn very quickly and acceptably , but use more memory .
in current cbr systems , case adaptation is usually performed by rule - based methods that use task - specific rules hand - coded by the system developer . the ability to define those rules depends on knowledge of the task and domain that may not be available a priori , presenting a serious impediment to endowing cbr systems with the needed adaptation knowledge . this paper describes ongoing research on a method to address this problem by acquiring adaptation knowledge from experience . the method uses reasoning from scratch , based on introspective reasoning about the requirements for successful adaptation , to build up a library of adaptation cases that are stored for future reuse . we describe the tenets of the approach and the types of knowledge it requires . we sketch initial computer implementation , lessons learned , and open questions for further study .
this position paper sketches a framework for modeling introspective reasoning and discusses the relevance of that framework for modeling introspective reasoning about memory search . it argues that effective and flexible memory processing in rich memories should be built on five types of explicitly represented self - knowledge : knowledge about information needs , relationships between different types of information , expectations for the actual behavior of the information search process , desires for its ideal behavior , and representations of how those expectations and desires relate to its actual performance . this approach to modeling memory search is both an illustration of general principles for modeling introspective reasoning and a step towards addressing the problem of how a reasoner human or machinecan acquire knowledge about the properties of its own knowledge base .
machine learning techniques are perceived to have a great potential as means for the acquisition of knowledge ; nevertheless , their use in complex engineering domains is still rare . most machine learning techniques have been studied in the context of knowledge acquisition for well defined tasks , such as classification . learning for these tasks can be handled by relatively simple algorithms . complex domains present difficulties that can be approached by combining the strengths of several complementing learning techniques , and overcoming their weaknesses by providing alternative learning strategies . this study presents two perspectives , the macro and the micro , for viewing the issue of multistrategy learning . the macro perspective deals with the decomposition of an overall complex learning task into relatively well - defined learning tasks , and the micro perspective deals with designing multistrategy learning techniques for supporting the acquisition of knowledge for each task . the two perspectives are discussed in the context of
in order to learn effectively , a system must not only possess knowledge about the world and be able to improve that knowledge , but it also must introspectively reason about how it performs a given task and what particular pieces of knowledge it needs to improve its performance at the current task . introspection requires a declaratflive representation of the reasoning performed by the system during the performance task . this paper presents a taxonomy of possible reasoning failures that can occur during this task , their declarative representations , and their associations with particular learning strategies . we propose a theory of meta - xps , which are explanation structures that help the system identify failure types and choose appropriate learning strategies in order to avoid similar mistakes in the future . a program called meta - aqua embodies the theory and processes examples in the domain of drug smuggling .
we introduce a learning algorithm for unsupervised neural networks based on ideas from statistical mechanics . the algorithm is derived from a mean field approximation for large , layered sigmoid belief networks . we show how to ( approximately ) infer the statistics of these networks without resort to sampling . this is done by solving the mean field equations , which relate the statistics of each unit to those of its markov blanket . using these statistics as target values , the weights in the network are adapted by a local delta rule . we evaluate the strengths and weaknesses of these networks for problems in statistical pattern recognition .
the paper describes a self - learning control system for a mobile robot . based on sensor information the control system has to provide a steering signal in such a way that collisions are avoided . since in our case no ` examples ' are available , the system learns on the basis of an external reinforcement signal which is negative in case of a collision and zero otherwise . rules from temporal difference learning are used to find the correct mapping between the ( discrete ) sensor input space and the steering signal . we describe the algorithm for learning the correct mapping from the input ( state ) vector to the output ( steering ) signal , and the algorithm which is used for a discrete coding of the input state space .
in this paper we initiate an investigation of generalizations of the probably approximately correct ( pac ) learning model that attempt to significantly weaken the target function assumptions . the ultimate goal in this direction is informally termed agnostic learning , in which we make virtually no assumptions on the target function . the name derives from the fact that as designers of learning algorithms , we give up the belief that nature ( as represented by the target function ) has a simple or succinct explanation . we give a number of positive and negative results that provide an initial outline of the possibilities for agnostic learning . our results include hardness results for the most obvious generalization of the pac model to an agnostic setting , an efficient and general agnostic learning method based on dynamic programming , relationships between loss functions for agnostic learning , and an algorithm for a learning problem that involves hidden variables .
in this paper we describe the design and implementation of the derivation replay framework , dersnlp + ebl ( derivational snlp + ebl ) , which is based within a partial order planner . dersnlp + ebl replays previous plan derivations by first repeating its earlier decisions in the context of the new problem situation , then extending the replayed path to obtain a complete solution for the new problem . when the replayed path cannot be extended into a new solution , explanation - based learning ( ebl ) techniques are employed to identify the features of the new problem which prevent this extension . these features are then added as censors on the retrieval of the stored case . to keep retrieval costs low , dersnlp + ebl normally stores plan derivations for individual goals , and replays one or more of these derivations in solving multi - goal problems . cases covering multiple goals are stored only when subplans for individual goals cannot be successfully merged . the aim in constructing the case library is to predict these goal interactions and to store a multi - goal case for each set of negatively interacting goals . we provide empirical results demonstrating the effectiveness of dersnlp + ebl in improving planning performance on randomly - generated problems drawn from a complex domain .
previous algorithms for supervised sequence learning are based on dynamic recurrent networks . this paper describes an alternative class of gradient - based systems consisting of two feedforward nets that learn to deal with temporal sequences using fast weights : the first net learns to produce context dependent weight changes for the second net whose weights may vary very quickly . the method offers the potential for stm storage efficiency : a single weight ( instead of a full - fledged unit ) may be sufficient for storing temporal information . various learning methods are derived . two experiments with unknown time delays illustrate the approach . one experiment shows how the system can be used for adaptive temporary variable binding .
evolutionary tree reconstruction is a very important step in many biological research problems , and yet is extremely difficult for a variety of computational , statistical , and scientific reasons . in particular , the reconstruction of very large trees containing significant amounts of divergence is especially challenging . we present in this paper a new tree reconstruction method , which we call the disk - covering method , which can be used to recover accurate estimations of the evolutionary tree for otherwise intractable datasets . dcm obtains a decomposition of the input dataset into small overlapping sets of closely related taxa , reconstructs trees on these subsets ( using a " base " phylogenetic method of choice ) , and then combines the subtrees into one tree on the entire set of taxa . because the subproblems analyzed by dcm are smaller , com - putationally expensive methods such as maximum likelihood estimation can be used without incurring too much cost . at the same time , because the taxa within each subset are closely related , even very simple methods ( such as neighbor - joining ) are much more likely to be highly accurate . the result is that dcm - boosted methods are typically faster and more accurate as compared to " naive " use of the same method . in this paper we describe the basic ideas and techniques in dcm , and demonstrate the advantages of dcm experimentally by simulating sequence evolution on a variety of trees .
automating the construction of semantic grammars is a difficult and interesting problem for machine learning . this paper shows how the semantic - grammar acquisition problem can be viewed as the learning of search - control heuristics in a logic program . appropriate control rules are learned using a new first - order induction algorithm that automatically invents useful syntactic and semantic categories . empirical results show that the learned parsers generalize well to novel sentences and out - perform previous approaches based on connectionist techniques .
in the barn owl , the self - organization of the auditory map of space in the external nucleus of the inferior colliculus ( icx ) is strongly influenced by vision , but the nature of this interaction is unknown . in this paper a biologically plausible and mini - malistic model of icx self - organization is proposed where the icx receives a learn signal based on the owl ' s visual attention . when the visual attention is focused in the same spatial location as the auditory input , the learn signal is turned on , and the map is allowed to adapt . a two - dimensional kohonen map is used to model the icx , and simulations were performed to evaluate how the learn signal would affect the auditory map . when primary area of visual attention was shifted at different spatial locations , the auditory map shifted to the corresponding location . the shift was complete when done early in the development and partial when done later . similar results have been observed in the barn owl with its visual field modified with prisms . therefore , the simulations suggest that a learn signal , based on visual attention , is a possible explanation for the auditory plasticity .
the place fields of hippocampal cells in old animals sometimes change when an animal is removed from and then returned to an environment [ barnes et al . , #NUM# ] . the ensemble correlation between two sequential visits to the same environment shows a strong bimodality for old animals ( near #NUM# , indicative of remapping , and greater than #NUM# . #NUM# , indicative of a similar representation between experiences ) , but a strong unimodality for young animals ( greater than #NUM# . #NUM# , indicative of a similar representation between experiences ) . one explanation for this is the multi - map hypothesis in which multiple maps are encoded in the hippocampus : old animals may sometimes be returning to the wrong map . a theory proposed by samsonovich and mcnaughton ( #NUM# ) suggests that the barnes et al . experiment implies that the maps are pre - wired in the ca #NUM# region of hippocampus . here , we offer an alternative explanation in which orthogonalization properties in the dentate gyrus ( dg ) region of hippocampus interact with errors in self - localization ( reset of the path integrator on re - entry into the environment ) to produce the bimodality .
m . i . t media laboratory perceptual computing section technical report no . #NUM# appeared #NUM# th ieee intl . conference on pattern recognition ( icpr ' #NUM# ) , vienna , austria . we present a foveated gesture recognition system that guides an active camera to foveate salient features based on a reinforcement learning paradigm . using vision routines previously implemented for an interactive environment , we determine the spatial location of salient body parts of a user and guide an active camera to obtain images of gestures or expressions . a hidden - state reinforcement learning paradigm based on the partially observable markov decision process ( pomdp ) is used to implement this visual attention . the attention module selects targets to foveate based on the goal of successful recognition , and uses a new multiple - model q - learning formulation . given a set of target and distractor gestures , our system can learn where to foveate to maximally discriminate a particular gesture .
various extensions to the genetic algorithm ( ga ) attempt to find all or most optima in a search space containing several optima . many of these emulate natural speciation . for co - evolutionary learning to succeed in a range of management and control problems , such as learning game strategies , such methods must find all or most optima . however , suitable comparison studies are rare . we compare two similar ga specia - tion methods , fitness sharing and implicit sharing . using a realistic letter classification problem , we find they have advantages under different circumstances . implicit sharing covers optima more comprehensively , when the population is large enough for a species to form at each optimum . with a population not large enough to do this , fitness sharing can find the optima with larger basins of attraction , and ignore the peaks with narrow bases , while implicit sharing is more easily distracted . this indicates that for a speciated ga trying to find as many near - global optima as possible , implicit sharing works well only if the population is large enough . this requires prior knowledge of how many peaks exist .
modern knowledge systems for design typically employ multiple problem - solving methods which in turn use different kinds of knowledge . the construction of a heterogeneous knowledge system that can support practical design thus raises two fundamental questions : how to accumulate huge volumes of design information , and how to support heterogeneous design processing ? fortunately , partial answers to both questions exist separately . legacy databases already contain huge amounts of general - purpose design information . in addition , modern knowledge systems typically characterize the kinds of knowledge needed by specific problem - solving methods quite precisely . this leads us to hypothesize method - specific data - to - knowledge compilation as a potential mechanism for integrating heterogeneous knowledge systems and legacy databases for design . in this paper , first we outline a general computational architecture called hiped for this integration . then , we focus on the specific issue of how to convert data accessed from a legacy database into a form appropriate to the problem - solving method used in a heterogeneous knowledge system . we describe an experiment in which a legacy knowledge system called interactive kritik is integrated with an oracle database using idi as the communication tool . the limited experiment indicates the computational feasibility of method - specific data - to - knowledge compilation , but also raises additional research issues .
this paper investigates the advantages and disadvantages of the mixture of experts ( me ) model ( introduced to the connectionist community in [ jjnh #NUM# ] and applied to time series analysis in [ wm #NUM# ] ) on two time series where the dynamics is well understood . the first series is a computer - generated series , consisting of a mixture between a noise - free process ( the quadratic map ) and a noisy process ( a composition of a noisy linear autoregressive and a hyperbolic tangent ) . there are three main results : ( #NUM# ) the me model produces significantly better results than single networks ; ( #NUM# ) it discovers the regimes correctly and also allows us to characterize the sub - processes through their variances . ( #NUM# ) due to the correct matching of the noise level of the model to that of the data it avoids overfitting . the second series is the laser series used in the santa fe competition ; the me model also obtains excellent out - of - sample predictions , allows for analysis and shows no overfitting .
in natural visual experience , different views of an object tend to appear in close temporal proximity as an animal manipulates the object or navigates around it . we investigated the ability of an attractor network to acquire view invariant visual representations by associating first neighbors in a pattern sequence . the pattern sequence contains successive views of faces of ten individuals as they change pose . under the network dynamics developed by griniasty , tsodyks & amit ( #NUM# ) , multiple views of a given subject fall into the same basin of attraction . we use an independent component ( ica ) representation of the faces for the input patterns ( bell & sejnowski , #NUM# ) . the ica representation has advantages over the principal component representation ( pca ) for viewpoint - invariant recognition both with and without the attractor network , suggesting that ica is a better representation than pca for object recognition .
key ideas from statistical learning theory and support vector machines are generalized to decision trees . a support vector machine is used for each decision in the tree . the " optimal " decision tree is characterized , and both a primal and dual space formulation for constructing the tree are proposed . the result is a method for generating logically simple decision trees with multivariate linear or nonlinear decisions . the preliminary results indicate that the method produces simple trees that generalize well with respect to other decision tree algorithms and single support vector machines .
we had previously shown that regularization principles lead to approximation schemes which are equivalent to networks with one layer of hidden units , called regularization networks . in particular , standard smoothness functionals lead to a subclass of regularization networks , the well known radial basis functions approximation schemes . this paper shows that regularization networks encompass a much broader range of approximation schemes , including many of the popular general additive models and some of the neural networks . in particular , we introduce new classes of smoothness functionals that lead to different classes of basis functions . additive splines as well as some tensor product splines can be obtained from appropriate classes of smoothness functionals . furthermore , the same generalization that extends radial basis functions ( rbf ) to hyper basis functions ( hbf ) also leads from additive models to ridge approximation models , containing as special cases breiman ' s hinge functions , some forms of projection pursuit regression and several types of neural networks . we propose to use the term generalized regularization networks for this broad class of approximation schemes that follow from an extension of regularization . in the probabilistic interpretation of regularization , the different classes of basis functions correspond to different classes of prior probabilities on the approximating function spaces , and therefore to different types of smoothness assumptions . in summary , different multilayer networks with one hidden layer , which we collectively call generalized regularization networks , correspond to different classes of priors and associated smoothness functionals in a classical regularization principle . three broad classes are a ) radial basis functions that can be generalized to hyper basis functions , b ) some tensor product splines , and c ) additive splines that can be generalized to schemes of the type of ridge approximation , hinge functions and several perceptron - like neural networks with one - hidden layer .
learning an input - output mapping from a set of examples , of the type that many neural networks have been constructed to perform , can be regarded as synthesizing an approximation of a multi - dimensional function , that is solving the problem of hy - persurface reconstruction . from this point of view , this form of learning is closely related to classical approximation techniques , such as generalized splines and regularization theory . this paper considers the problems of an exact representation and , in more detail , of the approximation of linear and nonlinear mappings in terms of simpler functions of fewer variables . kolmogorov ' s theorem concerning the representation of functions of several variables in terms of functions of one variable turns out to be almost irrelevant in the context of networks for learning . we develop a theoretical framework for approximation based on regularization techniques that leads to a class of three - layer networks that we call generalized radial basis functions ( grbf ) , since they are mathematically related to the well - known radial basis functions , mainly used for strict interpolation tasks . grbf networks are not only equivalent to generalized splines , but are also closely related to pattern recognition methods such as parzen windows and potential functions and to several neural network algorithms , such as kanerva ' s associative memory , backpropagation and kohonen ' s topology preserving map . they also have an interesting interpretation in terms of prototypes that are synthesized and optimally combined during the learning stage . the paper introduces several extensions and applications of the technique and discusses intriguing analogies with neurobiological data .
this paper describes how a reasoner can improve its understanding of an incompletely understood domain through the application of what it already knows to novel problems in that domain . recent work in ai has dealt with the issue of using past explanations stored in the reasoner ' s memory to understand novel situations . however , this process assumes that past explanations are well understood and provide good " lessons " to be used for future situations . this assumption is usually false when one is learning about a novel domain , since situations encountered previously in this domain might not have been understood completely . instead , it is reasonable to assume that the reasoner would have gaps in its knowledge base . by reasoning about a new situation , the reasoner should be able to fill in these gaps as new information came in , reorganize its explanations in memory , and gradually evolve a better understanding of its domain . we present a story understanding program that retrieves past explanations from situations already in memory , and uses them to build explanations to understand novel stories about terrorism . in doing so , the system refines its understanding of the domain by filling in gaps in these explanations , by elaborating the explanations , or by learning new indices for the explanations . this is a type of incremental learning since the system improves its explanatory knowledge of the domain in an incremental fashion rather than by learning new xps as a whole .
we present a statistical model of genes in dna . a generalized hidden markov model ( ghmm ) provides the framework for describing the grammar of a legal parse of a dna sequence ( stormo & haussler #NUM# ) . probabilities are assigned to transitions between states in the ghmm and to the generation of each nucleotide base given a particular state . machine learning techniques are applied to optimize these probabilities using a standardized training set . given a new candidate sequence , the best parse is deduced from the model using a dynamic programming algorithm to identify the path through the model with maximum probability . the ghmm is flexible and modular , so new sensors and additional states can be inserted easily . in addition , it provides simple solutions for integrating cardinality constraints , reading frame constraints , " indels " , and homology searching . the description and results of an implementation of such a gene - finding model , called genie , is presented . the exon sensor is a codon frequency model conditioned on windowed nucleotide frequency and the preceding codon . two neural networks are used , as in ( brunak , engelbrecht , & knudsen #NUM# ) , for splice site prediction . we show that this simple model performs quite well . for a cross - validated standard test set of #NUM# genes in human dna , our gene - finding system identified up to #NUM# % of protein - coding bases correctly with a specificity of #NUM# % . #NUM# % of exons were exactly identified with a specificity of #NUM# % . genie is shown to perform favorably compared with several other gene - finding systems .
we examine questions of optimality and domination in repeated stage games where one or both players may draw their strategies only from ( perhaps different ) computationally bounded sets . we also consider optimality and domination when bounded convergence rates of the infinite payoff . we develop a notion of a " grace period " to handle the problem of vengeful strategies .
we study the problem of efficiently learning to play a game optimally against an unknown adversary chosen from a computationally bounded class . we both contribute to the line of research on playing games against finite automata , and expand the scope of this research by considering new classes of adversaries . we introduce the natural notions of games against recent history adversaries ( whose current action is determined by some simple boolean formula on the recent history of play ) , and games against statistical adversaries ( whose current action is determined by some simple function of the statistics of the entire history of play ) . in both cases we give efficient algorithms for learning to play penny - matching and a more difficult game called contract . we also give the most powerful positive result to date for learning to play against finite automata , an efficient algorithm for learning to play any game against any finite automata with probabilistic actions and low cover time .
morgan is an integrated system for finding genes in vertebrate dna sequences . morgan uses a variety of techniques to accomplish this task , the most distinctive of which is a decision tree classifier . the decision tree system is combined with new methods for identifying start codons , donor sites , and acceptor sites , and these are brought together in a frame - sensitive dynamic programming algorithm that finds the optimal segmentation of a dna sequence into coding and noncoding regions ( exons and introns ) . the optimal segmentation is dependent on a separate scoring function that takes a subsequence and assigns to it a score reflecting the probability that the sequence is an exon . the scoring functions in morgan are sets of decision trees that are combined to give a probability estimate . experimental results on a database of #NUM# vertebrate dna sequences show that morgan has excellent performance by many different measures . on a separate test set , it achieves an overall accuracy of #NUM# % , with a correlation coefficient of #NUM# . #NUM# and a sensitivity and specificity for coding bases of #NUM# % and #NUM# % . in addition , morgan identifies #NUM# % of coding exons exactly ; i . e . , both the beginning and end of the coding regions are predicted correctly . this paper describes the morgan system , including its decision tree routines and the algorithms for site recognition , and its performance on a benchmark database of vertebrate dna .
a method is described which reduces the hypotheses space with an efficient and easily interpretable reduction criteria called a - reduction . a learning algorithm is described based on a - reduction and analyzed by using probability approximate correct learning results . the results are obtained by reducing a rule set to an equivalent set of kdnf formulas . the goal of the learning algorithm is to induce a compact rule set describing the basic dependencies within a set of data . the reduction is based on criterion which is very exible and gives a semantic interpretation of the rules which fulfill the criteria . comparison with syntactical hypotheses reduction show that the a reduction improves search and has a smaller probability of missclassification .
we identify the three principle factors affecting the performance of learning by networks with localized units : unit noise , sample density , and the structure of the target function . we then analyze the effect of unit receptive field parameters on these factors and use this analysis to propose a new learning algorithm which dynamically alters receptive field properties during learning .
semi - markov decision problems are continuous time generalizations of discrete time markov decision problems . a number of reinforcement learning algorithms have been developed recently for the solution of markov decision problems , based on the ideas of asynchronous dynamic programming and stochastic approximation . among these are td ( ) , q - learning , and real - time dynamic programming . after reviewing semi - markov decision problems and bellman ' s optimality equation in that context , we propose algorithms similar to those named above , adapted to the solution of semi - markov decision problems . we demonstrate these algorithms by applying them to the problem of determining the optimal control for a simple queueing system . we conclude with a discussion of circumstances under which these algorithms may be usefully ap plied .
there has recently been widespread interest in the use of multiple models for classification and regression in the statistics and neural networks communities . the hierarchical mixture of experts ( hme ) [ #NUM# ] has been successful in a number of regression problems , yielding significantly faster training through the use of the expectation maximisation algorithm . in this paper we extend the hme to classification and results are reported for three common classification benchmark tests : exclusive - or , n - input parity and two spirals .
one important factor determining the computa - tional complexity of evaluating a probabilistic network is the cardinality of the state spaces of the nodes . by varying the granularity of the state spaces , one can trade off accuracy in the result for computational efficiency . we present an any - time procedure for approximate evaluation of probabilistic networks based on this idea . on application to some simple networks , the proce - dure exhibits a smooth improvement in approxi - mation quality as computation time increases . this suggests that statespace abstraction is one more useful control parameter for designing real time probabilistic reasoners .
existing complexity measures from contemporary learning theory cannot be conveniently applied to specific learning problems ( e . g . , training sets ) . moreover , they are typically non - generic , i . e . , they necessitate making assumptions about the way in which the learner will operate . the lack of a satisfactory , generic complexity measure for learning problems poses difficulties for researchers in various areas ; the present paper puts forward an idea which may help to alleviate these . it shows that supervised learning problems fall into two , generic , complexity classes only one of which is associated with computational tractability . by determining which class a particular problem belongs to , we can thus effectively evaluate its degree of generic difficulty .
the paper investigates the statistical effects which may need to be exploited in supervised learning . it notes that these effects can be classified according to their conditionality and their order and proposes that learning algorithms will typically have some form of bias towards particular classes of effect . it presents the results of an empirical study of the statistical bias of backpropagation . the study involved applying the algorithm to a wide range of learning problems using a variety of different internal architectures . the results of the study revealed that backpropagation has a very specific bias in the general direction of statistical rather than relational effects . the paper shows how the existence of this bias effectively constitutes a weakness in the algorithm ' s ability to discount noise .
segmentation : preliminary results abstract . scatter - partitioning radial basis function ( rbf ) networks increase their number of degrees of freedom with the complexity of an input - output mapping to be estimated on the basis of a supervised training data set . due to its superior expressive power a scatter - partitioning gaussian rbf ( grbf ) model , termed supervised growing neural gas ( sgng ) , is selected from the literature . sgng employs a one - stage error - driven learning strategy and is capable of generating and removing both hidden units and synaptic connections . a slightly modified sgng version is tested as a function estimator when the training surface to be fitted is an image , i . e . , a #NUM# - d signal whose size is finite . the relationship between the generation , by the learning system , of disjointed maps of hidden units and the presence , in the image , of pictorially homogeneous subsets ( segments ) is investigated . unfortunately , the examined sgng version performs poorly both as function estimator and image segmenter . this may be due to an intrinsic inadequacy of the one - stage error - driven learning strategy to adjust structural parameters and output weights simultaneously but consistently . in the framework of rbf networks , further studies should investigate the combination of two - stage error - driven learning strategies with synapse generation and removal criteria . y internal report of the paper entitled " image segmentation with scatter - partitioning rbf networks : a feasibility study , " to be presented at the conference applications and science of neural networks , fuzzy systems , and evolutionary computation , part of spie ' s international symposium on optical science , engineering and instrumentation , #NUM# - #NUM# july #NUM# , san diego , ca .
a distinct advantage of symbolic learning algorithms over artificial neural networks is that typically the concept representations they form are more easily understood by humans . one approach to understanding the representations formed by neural networks is to extract symbolic rules from trained networks . in this paper we describe and investigate an approach for extracting rules from networks that uses ( #NUM# ) the nofm extraction algorithm , and ( #NUM# ) the network training method of soft weight - sharing . previously , the nofm algorithm had been successfully applied only to knowledge - based neural networks . our experiments demonstrate that our extracted rules generalize better than rules learned using the c #NUM# . #NUM# system . in addition to being accurate , our extracted rules are also reasonably comprehensible .
our experience showed us that exibility in expressing a parallel algorithm for simulating neural networks is desirable even if it is not possible then to obtain the most efficient solution for any single training algorithm . we believe that the advantages of a clear and easy to understand program predominates the disadvantages of approaches allowing only for a specific machine or neural network algorithm . we currently investigate if other neural network models are worth while being parallelized , and how the resulting parallel algorithms can be composed of a few common basic building blocks and the logarithmic tree as efficient communication structure . #NUM# #NUM# #NUM# #NUM# #NUM# #NUM# connections #NUM# #NUM# connections [ #NUM# ] d . ackley , g . hinton , t . sejnowski : a learning algorithm for boltzmann machines , cognitive science #NUM# , pp . #NUM# - #NUM# , #NUM# [ #NUM# ] b . m . forrest et al . : implementing neural network models on parallel computers , the computer journal , vol . #NUM# , no . #NUM# , #NUM# [ #NUM# ] w . giloi : latency hiding in message passing architectures , international parallel processing symposium , april #NUM# , cancun , mexico , ieee computer society press [ #NUM# ] t . nordstrm , b . svensson : using and designing massively parallel computers for artificial neural networks , journal of parallel and distributed computing , vol . #NUM# , pp . #NUM# - #NUM# , #NUM# [ #NUM# ] a . kramer , a . vincentelli : efficient parallel learning algorithms for neural networks , advances in neural information processing systems i , d . touretzky ( ed . ) , pp . #NUM# - #NUM# , #NUM# [ #NUM# ] t . kohonen : self - organization and associative memory , springer - verlag , berlin , #NUM# [ #NUM# ] d . a . pomerleau , g . l . gusciora , d . l . touretzky , h . t . kung : neural network simulation at warp speed : how we got #NUM# million connections per second , ieee intern . conf . neural networks , july #NUM# [ #NUM# ] a . rbel : dynamic selection of training patterns for neural networks : a new method to control the generalization , technical report #NUM# - #NUM# , technical university of berlin , #NUM# [ #NUM# ] d . e . rumelhart , d . e . hinton , r . j . williams : learning internal representations by error propagation , rumelhart & mcclelland ( eds . ) , parallel distributed processing : explorations in the microstructure of cognition , vol . i , pp . #NUM# - #NUM# , bradford books / mit press , cambridge , ma , #NUM# [ #NUM# ] w . schiffmann , m . joost , r . werner : comparison of optimized backpropagation algorithms , proc . of the european symposium on artificial neural networks , esann ' #NUM# , brussels , pp . #NUM# - #NUM# , #NUM# [ #NUM# ] j . schmidhuber : accelerated learning in backpropagation nets , connectionism in perspective , elsevier science publishers b . v . ( north - holland ) , pp #NUM# - #NUM# , #NUM# [ #NUM# ] m . taylor , p . lisboa ( eds . ) : techniques and applications of neural networks , ellis horwood , #NUM# [ #NUM# ] m . witbrock , m . zagha : an implementation of backpropagation learning on gf #NUM# , a large simd parallel computer , parallel computing , vol . #NUM# , pp . #NUM# - #NUM# , #NUM# [ #NUM# ] x . zhang , m . mckenna , j . p . mesirov , d . l . waltz : the backpropagation algorithm on grid and hypercube architectures , parallel computing , vol . #NUM# , pp . #NUM# - #NUM# , #NUM#
in order to learn effectively , a system must not only possess knowledge about the world and be able to improve that knowledge , but it also must introspectively reason about how it performs a given task and what particular pieces of knowledge it needs to improve its performance at the current task . introspection requires a declaratflive representation of the reasoning performed by the system during the performance task . this paper presents a taxonomy of possible reasoning failures that can occur during this task , their declarative representations , and their associations with particular learning strategies . we propose a theory of meta - xps , which are explanation structures that help the system identify failure types and choose appropriate learning strategies in order to avoid similar mistakes in the future . a program called meta - aqua embodies the theory and processes examples in the domain of drug smuggling .
although artificial neural networks have been applied in a variety of real - world scenarios with remarkable success , they have often been criticized for exhibiting a low degree of human comprehensibility . techniques that compile compact sets of symbolic rules out of artificial neural networks offer a promising perspective to overcome this obvious deficiency of neural network representations . this paper presents an approach to the extraction of if - then rules from artificial neural networks . its key mechanism is validity interval analysis , which is a generic tool for extracting symbolic knowledge by propagating rule - like knowledge through backpropagation - style neural networks . empirical studies in a robot arm domain illustrate the appropriateness of the proposed method for extracting rules from networks with real - valued and distributed representations .
in this paper , we examine a method for feature subset selection based on information theory . initially , a framework for defining the theoretically optimal , but computation - ally intractable , method for feature subset selection is presented . we show that our goal should be to eliminate a feature if it gives us little or no additional information beyond that subsumed by the remaining features . in particular , this will be the case for both irrelevant and redundant features . we then give an efficient algorithm for feature selection which computes an approximation to the optimal feature selection criterion . the conditions under which the approximate algorithm is successful are examined . empirical results are given on a number of data sets , showing that the algorithm effectively han dles datasets with large numbers of features .
in this paper , we address the problem of case - based learning in the presence of irrelevant features . we review previous work on attribute selection and present a new algorithm , oblivion , that carries out greedy pruning of oblivious decision trees , which effectively store a set of abstract cases in memory . we hypothesize that this approach will efficiently identify relevant features even when they interact , as in parity concepts . we report experimental results on artificial domains that support this hypothesis , and experiments with natural domains that show improvement in some cases but not others . in closing , we discuss the implications of our experiments , consider additional work on irrelevant features , and outline some directions for future research .
learning plays a vital role in the development of situated agents . in this paper , we explore the use of reinforcement learning to " shape " a robot to perform a predefined target behavior . we connect both simulated and real robots to a lecsys , a parallel implementation of a learning classifier system with an extended genetic algorithm . after classifying different kinds of animat - like behaviors , we explore the effects on learning of different types of agent ' s architecture ( monolithic , flat and hierarchical ) and of training strategies . in particular , hierarchical architecture requires the agent to learn how to coordinate basic learned responses . we show that the best results are achieved when both the agent ' s architecture and the training strategy match the structure of the behavior pattern to be learned . we report the results of a number of experiments carried out both in simulated and in real environments , and show that the results of simulations carry smoothly to real robots . while most of our experiments deal with simple reactive behavior , in one of them we demonstrate the use of a simple and general memory mechanism . as a whole , our experimental activity demonstrates that classifier systems with genetic algorithms can be practically employed to develop autonomous agents .
although probabilistic inference in a general bayesian belief network is an np - hard problem , inference computation time can be reduced in most practical cases by exploiting domain knowledge and by making appropriate approximations in the knowledge representation . in this paper we introduce the property of similarity of states and a new method for approximate knowledge representation which is based on this property . we define two or more states of a node to be similar when the likelihood ratio of their probabilities does not depend on the instantiations of the other nodes in the network . we show that the similarity of states exposes redundancies in the joint probability distribution which can be exploited to reduce the computational complexity of probabilistic inference in networks with multiple similar states . for example , we show that a bn #NUM# o network | a two layer networks often used in diagnostic problems | can be reduced to a very close network with multiple similar states . probabilistic inference in the new network can be done in only polynomial time with respect to the size of the network , and the results for queries of practical importance are very close to the results that can be obtained in exponential time with the original network . the error introduced by our reduction converges to zero faster than exponentially with respect to the degree of the polynomial describing the resulting computational complexity .
multilayer architectures such as those used in bayesian belief networks and helmholtz machines provide a powerful framework for representing and learning higher order statistical relations among inputs . because exact probability calculations with these models are often intractable , there is much interest in finding approximate algorithms . we present an algorithm that efficiently discovers higher order structure using em and gibbs sampling . the model can be interpreted as a stochastic recurrent network in which ambiguity in lower - level states is resolved through feedback from higher levels . we demonstrate the performance of the algorithm on bench mark problems .
in this paper we study an extension of the distribution - free model of learning introduced by valiant [ #NUM# ] ( also known as the probably approximately correct or pac model ) that allows the presence of malicious errors in the examples given to a learning algorithm . such errors are generated by an adversary with unbounded computational power and access to the entire history of the learning algorithm ' s computation . thus , we study a worst - case model of errors . our results include general methods for bounding the rate of error tolerable by any learning algorithm , efficient algorithms tolerating nontrivial rates of malicious errors , and equivalences between problems of learning with errors and standard combinatorial optimization problems .
we investigate the problem of computing the posterior probability of a model class , given a data sample and a prior distribution for possible parameter settings . by a model class we mean a group of models which all share the same parametric form . in general this posterior may be very hard to compute for high - dimensional parameter spaces , which is usually the case with real - world applications . in the literature several methods for computing the posterior approximately have been proposed , but the quality of the approximations may depend heavily on the size of the available data sample . in this work we are interested in testing how well the approximative methods perform in real - world problem domains . in order to conduct such a study , we have chosen the model family of finite mixture distributions . with certain assumptions , we are able to derive the model class posterior analytically for this model family . we report a series of model class selection experiments on real - world data sets , where the true posterior and the approximations are compared . the empirical results support the hypothesis that the approximative techniques can provide good estimates of the true posterior , especially when the sample size grows large .
in this paper we explore the use of finite mixture models for building decision support systems capable of sound probabilistic inference . finite mixture models have many appealing properties : they are computationally efficient in the prediction ( reasoning ) phase , they are universal in the sense that they can approximate any problem domain distribution , and they can handle multimod - ality well . we present a formulation of the model construction problem in the bayesian framework for finite mixture models , and describe how bayesian inference is performed given such a model . the model construction problem can be seen as missing data estimation and we describe a realization of the expectation - maximization ( em ) algorithm for finding good models . to prove the feasibility of our approach , we report crossvalidated empirical results on several publicly available classification problem datasets , and compare our results to corresponding results obtained by alternative techniques , such as neural networks and decision trees . the comparison is based on the best results reported in the literature on the datasets in question . it appears that using the theoretically sound bayesian framework suggested here the other reported results can be outperformed with a relatively small effort .
one application of models of reasoning behavior is to allow a reasoner to introspectively detect and repair failures of its own reasoning process . we address the issues of the transferability of such models versus the specificity of the knowledge in them , the kinds of knowledge needed for self - modeling and how that knowledge is structured , and the evaluation of introspective reasoning systems . we present the robbie system which implements a model of its planning processes to improve the planner in response to reasoning failures . we show how robbie ' s hierarchical model balances model generality with access to implementation - specific details , and discuss the qualitative and quantitative measures we have used for evaluating its introspective component .
we consider multi - criteria sequential decision making problems , where the criteria are ordered according to their importance . structural properties of these problems are touched and reinforcement learning algorithms , which learn asymptotically optimal decisions , are derived . computer experiments confirm the theoretical results and provide further insight in the learning processes .
acyclic digraphs ( adgs ) are widely used to describe dependences among variables in multivariate distributions . in particular , the likelihood functions of adg models admit convenient recursive factorizations that often allow explicit maximum likelihood estimates and that are well suited to building bayesian networks for expert systems . there may , however , be many adgs that determine the same dependence ( = markov ) model . thus , the family of all adgs with a given set of vertices is naturally partitioned into markov - equivalence classes , each class being associated with a unique statistical model . statistical procedures , such as model selection or model averaging , that fail to take into account these equivalence classes , may incur substantial computational or other inefficiencies . recent results have shown that each markov - equivalence class is uniquely determined by a single chain graph , the essential graph , that is itself markov - equivalent simultaneously to all adgs in the equivalence class . here we propose two stochastic bayesian model averaging and selection algorithms for essential graphs and apply them to the analysis of three discrete - variable data sets .
given a set of samples of an unknown probability distribution , we study the problem of constructing a good approximative bayesian network model of the probability distribution in question . this task can be viewed as a search problem , where the goal is to find a maximal probability network model , given the data . in this work , we do not make an attempt to learn arbitrarily complex multi - connected bayesian network structures , since such resulting models can be unsuitable for practical purposes due to the exponential amount of time required for the reasoning task . instead , we restrict ourselves to a special class of simple tree - structured bayesian networks called bayesian prototype trees , for which a polynomial time algorithm for bayesian reasoning exists . we show how the probability of a given bayesian prototype tree model can be evaluated , given the data , and how this evaluation criterion can be used in a stochastic simulated annealing algorithm for searching the model space . the simulated annealing algorithm provably finds the maximal probability model , provided that a sufficient amount of time is used .
this paper presents u - tree , a reinforcement learning algorithm that uses selective attention and short - term memory to simultaneously address the intertwined problems of large perceptual state spaces and hidden state . by combining the advantages of work in instance - based ( or memory - based ) learning and work with robust statistical tests for separating noise from task structure , the method learns quickly , creates only task - relevant state distinctions , and handles noise well . u - tree uses a tree - structured representation , and is related to work on prediction suffix trees [ ron et al . , #NUM# ] , parti - game [ moore , #NUM# ] , g - algorithm [ chap - man and kaelbling , #NUM# ] , and variable resolution dynamic programming [ moore , #NUM# ] . it builds on utile suffix memory [ mccallum , #NUM# c ] , which only used short - term memory , not selective perception . the algorithm is demonstrated solving a highway driving task in which the agent weaves around slower and faster traffic . the agent uses active perception with simulated eye movements . the environment has hidden state , time pressure , stochasticity , over #NUM# , #NUM# world states and over #NUM# , #NUM# percepts . from this environment and sensory system , the agent uses a utile distinction test to build a tree that represents depth - three memory where necessary , and has just #NUM# internal statesfar fewer than the #NUM# #NUM# states that would have resulted from a fixed - sized history - window ap proach .
feature selection is a problem of choosing a subset of relevant features . in general , only exhaustive search can bring about the optimal subset . with a monotonic measure , exhaustive search can be avoided without sacrificing optimality . unfortunately , most error - or distance - based measures are not monotonic . a new measure is employed in this work that is monotonic and fast to compute . the search for relevant features according to this measure is guaranteed to be complete but not exhaustive . experiments are conducted for verification .
this paper describes a novel method by which a dialogue agent can learn to choose an optimal dialogue strategy . while it is widely agreed that dialogue strategies should be formulated in terms of communicative intentions , there has been little work on automatically optimizing an agent ' s choices when there are multiple ways to realize a communicative intention . our method is based on a combination of learning algorithms and empirical evaluation techniques . the learning component of our method is based on algorithms for reinforcement learning , such as dynamic programming and q - learning . the empirical component uses the paradise evaluation framework ( walker et al . , #NUM# ) to identify the important performance factors and to provide the performance function needed by the learning algorithm . we illustrate our method with a dialogue agent named elvis ( email voice interactive system ) , that supports access to email over the phone . we show how elvis can learn to choose among alternate strategies for agent initiative , for reading messages , and for summarizing email folders .
this paper outlines some problems that may occur with reduced error pruning in inductive logic programming , most notably efficiency . thereafter a new method , incremental reduced error pruning , is proposed that attempts to address all of these problems . experiments show that in many noisy domains this method is much more efficient than alternative algorithms , along with a slight gain in accuracy . however , the experiments show as well that the use of this algorithm cannot be recommended for domains with a very specific concept description .
eeg analysis has played a key role in the modeling of the brain ' s cortical dynamics , but relatively little effort has been devoted to developing eeg as a limited means of communication . if several mental states can be reliably distinguished by recognizing patterns in eeg , then a paralyzed person could communicate to a device like a wheelchair by composing sequences of these mental states . eeg pattern recognition is a difficult problem and hinges on the success of finding representations of the eeg signals in which the patterns can be distinguished . in this article , we report on a study comparing three eeg representations , the unprocessed signals , a reduced - dimensional representation using the karhunen - loeve transform , and a frequency - based representation . classification is performed with a two - layer neural network implemented on a cnaps server ( #NUM# processor , simd architecture ) by adaptive solutions , inc . . execution time comparisons show over a hundred - fold speed up over a sun sparc #NUM# . the best classification accuracy on untrained samples is #NUM# % using the frequency - based representation .
this paper surveys the field of reinforcement learning from a computer - science perspective . it is written to be accessible to researchers familiar with machine learning . both the historical basis of the field and a broad selection of current work are summarized . reinforcement learning is the problem faced by an agent that learns behavior through trial - and - error interactions with a dynamic environment . the work described here has a resemblance to work in psychology , but differs considerably in the details and in the use of the word " reinforcement . " the paper discusses central issues of reinforcement learning , including trading off exploration and exploitation , establishing the foundations of the field via markov decision theory , learning from delayed reinforcement , constructing empirical models to accelerate learning , making use of generalization and hierarchy , and coping with hidden state . it concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning .
we add internal memory to the xcs classifier system . we then test xcs with internal memory , named xcsm , in non - markovian environments with two and four aliasing states . experimental results show that xcsm can easily converge to optimal solutions in simple environments ; moreover , xcsm ' s performance is very stable with respect to the size of the internal memory involved in learning . however , the results we present evidence that in more complex non - markovian environments , xcsm may fail to evolve an optimal solution . our results suggest that this happens because , the exploration strategies currently employed with xcs , are not adequate to guarantee the convergence to an optimal policy with xcsm , in complex non - markovian environments .
simple modification of standard hill climbing optimization algorithm by taking into account learning features is discussed . basic concept of this approach is the socalled probability vector , its single entries determine probabilities of appearance of ' #NUM# ' entries in n - bit vectors . this vector is used for the random generation of n - bit vectors that form a neighborhood ( specified by the given probability vector ) . within the neighborhood a few best solutions ( with smallest functional values of a minimized function ) are recorded . the feature of learning is introduced here so that the probability vector is updated by a formal analogue of hebbian learning rule , well - known in the theory of artificial neural networks . the process is repeated until the probability vector entries are close either to zero or to one . the resulting probability vector unambiguously determines an n - bit vector which may be interpreted as an optimal solution of the given optimization task . resemblance with genetic algorithms is discussed . effectiveness of the proposed method is illustrated by an example of looking for global minima of a highly multimodal function .
a statistical approach to decision tree modeling is described . in this approach , each decision in the tree is modeled parametrically as is the process by which an output is generated from an input and a sequence of decisions . the resulting model yields a likelihood measure of goodness of fit , allowing ml and map estimation techniques to be utilized . an efficient algorithm is presented to estimate the parameters in the tree . the model selection problem is presented and several alternative proposals are considered . a hidden markov version of the tree is described for data sequences that have temporal dependencies .
where a is a binary matrix . our task is to infer s given z and a , and given assumptions about the statistical properties of s and n . this problem arises in the decoding of a noisy communication z which was transmitted using an error - correcting code based on parity checks of the original signal s , and in the inference of the sequence of a linear feedback shift register ( lfsr ) from a noisy observation of the sequence [ #NUM# ] . p ( zja ) i assume the decoder ' s aim is to find the most probable s . for large n an exhaustive search over the #NUM# n possible sequences s is not feasible . one way to attack such a combinatorial problem is to create a related continuous optimization problem in which the discrete variables are replaced by real variables [ #NUM# , #NUM# , #NUM# ] . here i derive a continuous representation in terms of a free energy approximation [ #NUM# ] to the awkward posterior distribution ( #NUM# ) .
an intelligent system has to be capable of adapting to a constantly changing environment . it therefore , ought to be capable of learning from its perceptual interactions with its surroundings . this requires a certain amount of plasticity in its structure . any attempt to model the perceptual capabilities of a living system or , for that matter , to construct a synthetic system of comparable abilities , must therefore , account for such plasticity through a variety of developmental and learning mechanisms . this paper examines some results from neuroanatomical , morphological , as well as behavioral studies of the development of visual perception ; integrates them into a computational framework ; and suggests several interesting experiments with computational models that can yield insights into the development of visual perception . in order to understand the development of information processing structures in the brain , one needs knowledge of changes it undergoes from birth to maturity in the context of a normal environment . however , knowledge of its development in aberrant settings is also extremely useful , because it reveals the extent to which the development is a function of environmental experience ( as opposed to genetically determined pre - wiring ) . accordingly , we consider development of the visual system under both normal and restricted rearing conditions . the role of experience in the early development of the sensory systems in general , and the visual system in particular , has been widely studied through a variety of experiments involving carefully controlled manipulation of the environment presented to an animal . extensive reviews of such results can be found in ( mitchell , #NUM# ; movshon , #NUM# ; hirsch , #NUM# ; boothe , #NUM# ; singer , #NUM# ) . some examples of manipulation of visual experience are total pattern deprivation ( e . g . , dark rearing ) , selective deprivation of a certain class of patterns ( e . g . , vertical lines ) , monocular deprivation in animals with binocular vision , etc . extensive studies involving behavioral deficits resulting from total visual pattern deprivation indicate that the deficits arise primarily as a result of impairment of visual information processing in the brain . the results of these experiments suggest specific developmental or learning mechanisms that may be operating at various stages of development , and at different levels in the system . we will discuss some of these hhhhhhhhhhhhhhh this is a working draft . all comments , especially constructive criticism and suggestions for improvement , will be appreciated . i am indebted to prof . james dannemiller for introducing me to some of the literature in infant development ; to prof . leonard uhr for his helpful comments on an initial draft of the paper ; and to numerous researchers whose experimental work has provided the basis for the model outlined in this paper . this research was partially supported by grants from the national science foundation and the university of wisconsin graduate school .
this paper studies the problem of ergodicity of transition probability matrices in marko - vian models , such as hidden markov models ( hmms ) , and how it makes very difficult the task of learning to represent long - term context for sequential data . this phenomenon hurts the forward propagation of long - term context information , as well as learning a hidden state representation to represent long - term context , which depends on propagating credit information backwards in time . using results from markov chain theory , we show that this problem of diffusion of context and credit is reduced when the transition probabilities approach #NUM# or #NUM# , i . e . , the transition probability matrices are sparse and the model essentially deterministic . the results found in this paper apply to learning approaches based on continuous optimization , such as gradient descent and the baum - welch algorithm .
modern industry of today needs flexible , adaptive and fault - tolerant methods for information processing . several applications have shown that neural networks fulfill these requirements . in this paper application areas , in which neural networks have been successfully used , are presented . then a kind of check list is described , which mentioned the different steps , when applying neural networks . the paper finished with a discussion of some neural networks projects done in the research group interactive planning at the research center for computer science ( fzi ) .
gas , oil and other pipelines need to be inspected for corrosion and other defects at regular intervals . for this application pipetronix gmbh ( ptx ) karlsruhe has developed a special ultrasonic based probe . based on the recorded wall thicknesses of this so called pipe pig the research center for computer science ( fzi ) has developed in cooperation with ptx an automatic inspection system called neuropipe . neuropipe has the task to detect defects like metal loss . the kernel of this inspection tool is a neural classifier which was trained using manually collected defect examples . the following paper focus on the aspects of successfull use of learning methods in an industrial application .
we construct a mixture of locally linear generative models of a collection of pixel - based images of digits , and use them for recognition . different models of a given digit are used to capture different styles of writing , and new images are classified by evaluating their log - likelihoods under each model . we use an em - based algorithm in which the m - step is computationally straightforward principal components analysis ( pca ) . incorporating tangent - plane information [ #NUM# ] about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the pca , and it demonstrably improves performance .
this article addresses these problems using gated experts , consisting of a ( nonlinear ) gating network , and several ( also nonlinear ) competing experts . each expert learns to predict the conditional mean , and each expert adapts its width to match the noise level in its regime . the gating network learns to predict the probability of each expert , given the input . this article focuses on the case where the gating network bases its decision on information from the inputs . this can be contrasted to hidden markov models where the decision is based on the previous state ( s ) ( i . e . , on the output of the gating network at the previous time step ) , as well as to averaging over several predictors . in contrast , gated experts soft - partition the input space . this article discusses the underlying statistical assumptions , derives the weight update rules , and compares the performance of gated experts to standard methods on three time series : ( #NUM# ) a computer - generated series , obtained by randomly switching between two nonlinear processes , ( #NUM# ) a time series from the santa fe time series competition ( the light intensity of a laser in chaotic state ) , and ( #NUM# ) the daily electricity demand of france , a real - world multivariate problem with structure on several time scales . the main results are ( #NUM# ) the gating network correctly discovers the different regimes of the process , ( #NUM# ) the widths associated with each expert are important for the segmentation task ( and they can be used to characterize the sub - processes ) , and ( #NUM# ) there is less overfitting compared to single networks ( homogeneous multi - layer perceptrons ) , since the experts learn to match their variances to the ( local ) noise levels . this can be viewed as matching the local complexity of the model to the local complexity of the data .
this paper describes an approach to modelling drug activity using machine learning tools . some experiments in modelling the quantitative structure - activity relationship ( qsar ) using a standard , hansch , method and a machine learning system golem were already reported in the literature . the paper describes the results of applying two other machine learning systems , magnus assistant and retis , on the same data . the results achieved by the machine learning systems , are better then the results of the hansch method ; therefore , machine learning tools can be considered as very promising for solving that kind of problems . the given results also illustrate the variations of performance of the different machine learning systems applied to this drug design problem .
in this paper we describe one aspect of our research in the project called hiped , which addressed the problem of performing design of engineering devices by accessing heterogeneous databases . the front end of the hiped system consisted of interactive kri - tik , a multimodal reasoning system that combined case based and model based reasoning to solve a design problem . this paper focuses on the backend processing where five types of queries received from the front end are evaluated by mapping them appropriately using the " facts " about the schemas of the underlying databases and " rules " that establish the correspondance among the data in these databases in terms of relationships such as equivalence , overlap and set containment . the uniqueness of our approach stems from the fact that the mapping process is very forgiving in that the query received from the front end is evaluated with respect to a large number of possibilities . these possibilities are encoded in the form of rules that consider various ways in which the tokens in the given query may match relation names , attrribute names , or values in the underlying tables . the approach has been implemented using coral deductive database system as the rule processing engine .
temporal difference methods solve the temporal credit assignment problem for reinforcement learning . an important subproblem of general reinforcement learning is learning to achieve dynamic goals . although existing temporal difference methods , such as q learning , can be applied to this problem , they do not take advantage of its special structure . this paper presents the dg - learning algorithm , which learns efficiently to achieve dynamically changing goals and exhibits good knowledge transfer between goals . in addition , this paper shows how traditional relaxation techniques can be applied to the problem . finally , experimental results are given that demonstrate the superiority of dg learning over q learning in a moderately large , synthetic , non - deterministic domain .
in this paper we prove the intractability of learning several classes of boolean functions in the distribution - free model ( also called the probably approximately correct or pac model ) of learning from examples . these results are representation independent , in that they hold regardless of the syntactic form in which the learner chooses to represent its hypotheses . our methods reduce the problems of cracking a number of well - known public - key cryptosys - tems to the learning problems . we prove that a polynomial - time learning algorithm for boolean formulae , deterministic finite automata or constant - depth threshold circuits would have dramatic consequences for cryptography and number theory in particular , such an algorithm could be used to break the rsa cryptosystem , factor blum integers ( composite numbers equivalent to #NUM# modulo #NUM# ) , and detect quadratic residues . the results hold even if the learning algorithm is only required to obtain a slight advantage in prediction over random guessing . the techniques used demonstrate an interesting duality between learning and cryptography . we also apply our results to obtain strong intractability results for approximating a gener - alization of graph coloring .
to have learned the morphology of a natural language is to have the capacity both to recognize and to produce words consisting of novel combinations of familiar morphemes . most recent work on the acquisition of morphology takes the perspective of production , but it is receptive morphology which comes first in the child . this paper presents a connectionist model of the acquisition of the capacity to recognize morphologically complex words . the model takes sequences of phonetic segments as inputs and maps them onto output units representing the meanings of lexical and grammatical morphemes . it consists of a simple recurrent network with separate hidden - layer modules for the tasks of recognizing the root and the grammatical morphemes of the input word . experiments with artificial language stimuli demonstrate that the model generalizes to novel words for morphological rules of all but one of the major types found in natural languages and that a version of the network with unassigned hidden - layer modules can learn to assign them to the output recognition tasks in an efficient manner . i also argue that for rules involving reduplication , that is , the copying of portions of a root , the network requires separate recurrent subnetworks for sequences of larger units such as syllables . the network can learn to develop its own syllable representations which not only support the recognition of reduplication but also provide the basis for learning to produce , as well as recognize , morphologically complex words . the model makes many detailed predictions about the learning difficulty of particular morphological rules .
this paper presents an algorithm that combines traditional ebl techniques and recent developments in inductive logic programming to learn effective clause selection rules for prolog programs . when these control rules are incorporated into the original program , significant speed - up may be achieved . the algorithm is shown to be an improvement over competing ebl approaches in several domains . additionally , the algorithm is capable of automatically transforming some intractable algorithms into ones that run in polynomial time .
neurons in the ventral stream of the primate visual system exhibit responses to the images of objects which are invariant with respect to natural transformations such as translation , size , and view . anatomical and neurophysiological evidence suggests that this is achieved through a series of hierarchical processing areas . in an attempt to elucidate the manner in which such representations are established , we have constructed a model of cortical visual processing which seeks to parallel many features of this system , specifically the multi - stage hierarchy with its topologically constrained convergent connectivity . each stage is constructed as a competitive network utilising a modified hebb - like learning rule , called the trace rule , which incorporates previous as well as current neuronal activity . the trace rule enables neurons to learn about whatever is invariant over short time periods ( e . g . #NUM# . #NUM# s ) in the representation of objects as the objects transform in the real world . the trace rule enables neurons to learn the statistical invariances about objects during their transformations , by associating together representations which occur close together in time . we show that by using the trace rule training algorithm the model can indeed learn to produce transformation invariant responses to natural stimuli such as faces .
in this paper we propose recurrent neural networks with feedback into the input units for handling two types of data analysis problems . on the one hand , this scheme can be used for static data when some of the input variables are missing . on the other hand , it can also be used for sequential data , when some of the input variables are missing or are available at different frequencies . unlike in the case of probabilistic models ( e . g . gaussian ) of the missing variables , the network does not attempt to model the distribution of the missing variables given the observed variables . instead it is a more " discriminant " approach that fills in the missing variables for the sole purpose of minimizing a learning criterion ( e . g . , to minimize an output error ) .
the use of case - based reasoning as a process model of design involves the subtasks of recalling previously known designs from memory and adapting these design cases or subcases to fit the current design context . the development of this process model for a particular design domain proceeds in parallel with the development of a representation for the cases , the case memory organisation , and the design knowledge needed in addition to specific designs . the selection of a particular representational paradigm for these types of information , and the details of its use for a particular problemsolving domain , depend on the intended use of the information to be represented and the project information available , as well as the nature of the domain . in this paper we describe the development and implementation of four case - based design systems : casecad , cadsyn , win , and demex . each system is described in terms of the content , organisation , and source of case memory , and the implementation of case recall and case adaptation . a comparison of these systems considers the relative advantages and disadvantages of the implementations .
we describe a biologically plausible model of dynamic recognition and learning in the visual cortex based on the statistical theory of kalman filtering from optimal control theory . the model utilizes a hierarchical network whose successive levels implement kalman filters operating over successively larger spatial and temporal scales . each hierarchical level in the network predicts the current visual recognition state at a lower level and adapts its own recognition state using the residual error between the prediction and the actual lower - level state . simultaneously , the network also learns an internal model of the spatiotemporal dynamics of the input stream by adapting the synaptic weights at each hierarchical level in order to minimize prediction errors . the kalman filter model respects key neuroanatomical data such as the reciprocity of connections between visual cortical areas , and assigns specific computational roles to the inter - laminar connections known to exist between neurons in the visual cortex . previous work elucidated the usefulness of this model in explaining neurophysiological phenomena such as endstopping and other related extra - classical receptive field effects . in this paper , in addition to providing a more detailed exposition of the model , we present a variety of experimental results demonstrating the ability of this model to perform robust spatiotemporal segmentation and recognition of objects and image sequences in the presence of varying amounts of occlusion , background clutter , and noise .
individual lifetime learning can ` guide ' an evolving population to areas of high fitness in genotype space through an evolutionary phenomenon known as the baldwin effect ( baldwin , #NUM# ; hin - ton & nowlan , #NUM# ) . it is the accepted wisdom that this guiding speeds up the rate of evolution . by highlighting another interaction between learning and evolution , that will be termed the hiding effect , it will be argued here that this depends on the measure of evolutionary speed one adopts . the hiding effect shows that learning can reduce the selection pressure between individuals by ` hiding ' their genetic differences . there is thus a trade - off between the baldwin effect and the hiding effect to determine learning ' s influence on evolution and two factors that contribute to this trade - off , the cost of learning and landscape epis tasis , are investigated experimentally .
this paper focuses on the optimization of hyper - parameters for function approximators . we describe a kind of racing algorithm for continuous optimization problems that spends less time evaluating poor parameter settings and more time honing its estimates in the most promising regions of the parameter space . the algorithm is able to automatically optimize the parameters of a function approximator with less computation time . we demonstrate the algorithm on the problem of finding good parameters for a memory based learner and show the tradeoffs involved in choosing the right amount of computation to spend on each evaluation .
based on our analysis and experiments using real - world datasets , we find that the greediness of forward feature selection algorithms does not severely corrupt the accuracy of function approximation using the selected input features , but improves the efficiency significantly . hence , we propose three greedier algorithms in order to further enhance the efficiency of the feature selection processing . we provide empirical results for linear regression , locally weighted regression and k - nearest - neighbor models . we also propose to use these algorithms to develop an offline chinese and japanese handwriting recognition system with auto matically configured , local models .
this paper considers an aspect of mixture modelling . significantly overlapping distributions require more data for their parameters to be accurately estimated than well separated distributions . for example , two gaussian distributions are considered to significantly overlap when their means are within three standard deviations of each other . if insufficient data is available , only a single component distribution will be estimated , although the data originates from two component distributions . we consider how much data is required to distinguish two component distributions from one distribution in mixture modelling using the minimum message length ( mml ) criterion . first , we perform experiments which show the mml criterion performs well relative to other bayesian criteria . second , we make two improvements to the existing mml estimates , that improve its performance with overlapping distributions .
in this paper , we present a performance prediction model for indicating the performance range of mimd parallel processor systems for neural network simulations . the model expresses the total execution time of a simulation as a function of the execution times of a small number of kernel functions , which have to be measured on only one processor and one physical communication link . the functions depend on the type of neural network , its geometry , decomposition and the connection structure of the mimd machine . using the model , the execution time , speedup , scalability and efficiency of large mimd systems can be predicted . the model is validated quantitatively by applying it to two popular neural networks , backpropagation and the kohonen self - organizing feature map , decomposed on a gcel - #NUM# #NUM# , a #NUM# transputer system . measurements are taken from network simulations decomposed via dataset and network decomposition techniques . agreement of the model with the measurements is within #NUM# % - #NUM# % . estimates are given for the performances that can be expected for the new t #NUM# transputer systems . the presented method can also be used for other application areas such as image processing .
with the goal of reducing computational costs without sacrificing accuracy , we describe two algorithms to find sets of prototypes for nearest neighbor classification . here , the term prototypes refers to the reference instances used in a nearest neighbor computation the instances with respect to which similarity is assessed in order to assign a class to a new data item . both algorithms rely on stochastic techniques to search the space of sets of prototypes and are simple to implement . the first is a monte carlo sampling algorithm ; the second applies random mutation hill climbing . on four datasets we show that only three or four prototypes sufficed to give predictive accuracy equal or superior to a basic nearest neighbor algorithm whose run - time storage costs were approximately #NUM# to #NUM# times greater . we briefly investigate how random mutation hill climbing may be applied to select features and prototypes simultaneously . finally , we explain the performance of the sampling algorithm on these datasets in terms of a statistical measure of the extent of clustering displayed by the target classes .
we present a new self - organizing neural network model having two variants . the first variant performs unsupervised learning and can be used for data visualization , clustering , and vector quantization . the main advantage over existing approaches , e . g . , the kohonen feature map , is the ability of the model to automatically find a suitable network structure and size . this is achieved through a controlled growth process which also includes occasional removal of units . the second variant of the model is a supervised learning method which results from the combination of the abovementioned self - organizing network with the radial basis function ( rbf ) approach . in this model it is possible in contrast to earlier approaches toperform the positioning of the rbf units and the supervised training of the weights in parallel . therefore , the current classification error can be used to determine where to insert new rbf units . this leads to small networks which generalize very well . results on the two - spirals benchmark and a vowel classification problem are presented which are better than any results previously published .
i present first results on columbus , an autonomous mobile robot . columbus operates in initially unknown , structured environments . its task is to explore and model the environment efficiently while avoiding collisions with obstacles . columbus uses an instance - based learning technique for modeling its environment . real - world experiences are generalized via two artificial neural networks that encode the characteristics of the robot ' s sensors , as well as the characteristics of typical environments the robot is assumed to face . once trained , these networks allow for knowledge transfer across different environments the robot will face over its lifetime . columbus ' models represent both the expected reward and the confidence in these expectations . exploration is achieved by navigating to low confidence regions . an efficient dynamic programming method is employed in background to find minimal - cost paths that , executed by the robot , maximize exploration . columbus operates in real - time . it has been operating successfully in an office building environment for periods up to hours .
we analyze the performance of a genetic algorithm ( ga ) we call culling and a variety of other algorithms on a problem we refer to as additive search problem ( asp ) . asp is closely related to several previously well studied problems , such as the game of mastermind and additive fitness functions . we show that the problem of learning the ising perceptron is reducible to a noisy version of asp . culling is efficient on asp , highly noise tolerant , and the best known approach in some regimes . noisy asp is the first problem we are aware of where a genetic type algorithm bests all known competitors . standard ga ' s , by contrast , perform much more poorly on asp than hillclimbing and other approaches even though the schema theorem holds for asp . we generalize asp to k - asp to study whether ga ' s will achieve ` implicit parallelism ' in a problem with many more schemata . ga ' s fail to achieve this implicit parallelism , but we describe an algorithm we call explicitly parallel search that succeeds . we also compute the optimal culling point for selective breeding , which turns out to be independent of the fitness function or the population distribution . we also analyze a mean field theoretic algorithm performing similarly to culling on many problems . these results provide insight into when and how ga ' s can beat competing methods .
many extensions have been proposed to help instance - based learning algorithms perform better on a wide variety of real - world applications . however , it is not trivial to decide what parameters or options to use when applying an instance - based learning algorithm to a particular problem . traditionally , cross - validation has been used to choose some parameters such as k in a k nearest neighbor classifier . this paper points out why cross validation often does not provide enough information to allow for fine - tuning the classifier , and how confidence levels can be used to break ties that are all too common when cross - validation is used . it proposes the fuzzy instance based learning ( fibl ) algorithm that uses distance - weighted voting with parameters set via a combination of cross - validation and confidence levels . in experiments on #NUM# datasets , fibl had higher average generalization accuracy than using majority voting or using cross - validation alone to determine parameters .
this paper describes a formulation of reinforcement learning that enables learning in noisy , dynamic environemnts such as in the complex concurrent multi - robot learning domain . the methodology involves minimizing the learning space through the use behaviors and conditions , and dealing with the credit assignment problem through shaped reinforcement in the form of heterogeneous reinforcement functions and progress estimators . we experimentally validate the ap proach on a group of four mobile robots learning a foraging task .
most existing decision tree systems use a greedy approach to induce trees | locally optimal splits are induced at every node of the tree . although the greedy approach is suboptimal , it is believed to produce reasonably good trees . in the current work , we attempt to verify this belief . we quantify the goodness of greedy tree induction empirically , using the popular decision tree algorithms , c #NUM# . #NUM# and cart . we induce decision trees on thousands of synthetic data sets and compare them to the corresponding optimal trees , which in turn are found using a novel map coloring idea . we measure the effect on greedy induction of variables such as the underlying concept complexity , training set size , noise and dimensionality . our experiments show , among other things , that the expected classification cost of a greedily induced tree is consistently very close to that of the optimal tree .
attractor networks , which map a continuous input space to a discrete output space , are useful for pattern completion , cleaning up noisy or missing features in an input . however , designing a net to have a given set of attractors is notoriously tricky ; training procedures are cpu intensive and often produce spurious attractors and ill - conditioned attractor basins . these difficulties occur because each connection in the network participates in the encoding of multiple attractors . we describe an alternative formulation of attractor networks in which the encoding of knowledge is local , not distributed . although localist attractor nets have similar dynamics to their distributed counterparts , they are much easier to work with and interpret . we propose a statistical formulation of localist attractor net dynamics , which yields a convergence proof and a mathematical interpretation of model parameters . we present simulation experiments that explore the behavior of lo - calist attractor nets , showing that they produce a gang effectthe presence of an attractor enhances the attractor basins of neighboring attractorsand that spurious attractors occur only at points of symmetry in state space .
according to wolpert ' s no - free - lunch ( nfl ) theorems [ #NUM# , #NUM# ] , gener - alisation in the absence of domain knowledge is necessarily a zero - sum enterprise . good generalisation performance in one situation is always offset by bad performance in another . wolpert notes that the theorems do not demonstrate that effective generalisation is a logical impossibility but merely that a learner ' s bias ( or assumption set ) is of key importance
learning when limited to modification of some parameters has a limited scope ; the capability to modify the system structure is also needed to get a wider range of the learnable . in the case of artificial neural networks , learning by iterative adjustment of synaptic weights can only succeed if the network designer predefines an appropriate network structure , i . e . , number of hidden layers , units , and the size and shape of their receptive and projective fields . this paper advocates the view that the network structure should not , as usually done , be determined by trial - and - error but should be computed by the learning algorithm . incremental learning algorithms can modify the network structure by addition and / or removal of units and / or links . a survey of current connectionist literature is given on this line of thought . " grow and learn " ( gal ) is a new algorithm that learns an association at one - shot due to being incremental and using a local representation . during the so - called " sleep " phase , units that were previously stored but which are no longer necessary due to recent modifications are removed to minimize network complexity . the incrementally constructed network can later be finetuned off - line to improve performance . another method proposed that greatly increases recognition accuracy is to train a number of networks and vote over their responses . the algorithm and its variants are tested on recognition of handwritten numerals and seem promising especially in terms of learning speed . this makes the algorithm attractive for on - line learning tasks , e . g . , in robotics . the biological plausibility of incremental learning is also discussed briefly . earlier part of this work was realized at the laboratoire de microinformatique of ecole polytechnique federale de lausanne and was supported by the fonds national suisse de la recherche scientifique . later part was realized at and supported by the international computer science institute . a number of people helped by guiding , stimulating discussions or questions : subutai ahmad , peter clarke , jerry feldman , christian jutten , pierre marchal , jean daniel nicoud , steve omohondro and leon personnaz .
this paper introduces a probability model , the mixture of trees that can account for sparse , dynamically changing dependence relationships . we present a family of efficient algorithms that use em and the minimum spanning tree algorithm to find the ml and map mixture of trees for a variety of priors , including the dirichlet and the mdl priors .
two fundamental problems in analyzing dna sequences are ( #NUM# ) locating the regions of a dna sequence that encode proteins , and ( #NUM# ) determining the reading frame for each region . we investigate using artificial neural networks ( anns ) to find coding regions , determine reading frames , and detect frameshift errors in e . coli dna sequences . we describe our adaptation of the approach used by uberbacher and mural to identify coding regions in human dna , and we compare the performance of anns to several conventional methods for predicting reading frames . our experiments demonstrate that anns can outperform these conventional approaches .
the paper describes a self - learning control system for a mobile robot . based on sensor information the control system has to provide a steering signal in such a way that collisions are avoided . since in our case no ` examples ' are available , the system learns on the basis of an external reinforcement signal which is negative in case of a collision and zero otherwise . rules from temporal difference learning are used to find the correct mapping between the ( discrete ) sensor input space and the steering signal . we describe the algorithm for learning the correct mapping from the input ( state ) vector to the output ( steering ) signal , and the algorithm which is used for a discrete coding of the input state space .
work is currently underway to devise learning methods which are better able to transfer knowledge from one task to another . the process of knowledge transfer is usually viewed as logically separate from the inductive procedures of ordinary learning . however , this paper argues that this ` seperatist ' view leads to a number of conceptual difficulties . it offers a task analysis which situates the transfer process inside a generalised inductive protocol . it argues that transfer should be viewed as a subprocess within induction and not as an independent procedure for transporting knowledge between learning trials .
this chapter describes three studies which address the question of how neural network learning can be improved via the incorporation of information extracted from other networks . this general problem , which we call network transfer , encompasses many types of relationships between source and target networks . our focus is on the utilization of weights from source networks which solve a subproblem of the target network task , with the goal of speeding up learning on the target task . we demonstrate how the approach described here can improve learning speed by up to ten times over learning starting with random weights .
the work discussed in this paper is motivated by the need of building decision support systems for real - world problem domains . our goal is to use these systems as a tool for supporting bayes optimal decision making , where the action maximizing the expected utility , with respect to predicted probabilities of the possible outcomes , should be selected . for this reason , the models used need to be probabilistic in nature | the output of a model has to be a probability distribution , not just a set of numbers . for the model family , we have chosen the set of simple discrete finite mixture models which have the advantage of being computationally very efficient . in this work , we describe a bayesian approach for constructing finite mixture models from sample data . our approach is based on a two - phase unsupervised learning process which can be used both for exploratory analysis and model construction . in the first phase , the selection of a model class , i . e . , the number of parameters , is performed by calculating the cheeseman - stutz approximation for the model class evidence . in the second phase , the map parameters in the selected class are estimated by the em algorithm . in this framework , the overfitting problem common to many traditional learning approaches can be avoided , as the learning process automatically regulates the complexity of the model . this paper focuses on the model class selection phase and the approach is validated by presenting empirical results with both natural and synthetic data .
we introduce and analyze a new algorithm for linear classification which combines rosenblatt ' s perceptron algorithm with helmbold and warmuth ' s leave - one - out method . like vapnik ' s maximal - margin classifier , our algorithm takes advantage of data that are linearly separable with large margins . compared to vapnik ' s algorithm , however , ours is much simpler to implement , and much more efficient in terms of computation time . we also show that our algorithm can be efficiently used in very high dimensional spaces using kernel functions . we performed some experiments using our algorithm , and some variants of it , for classifying images of handwritten digits . the performance of our algorithm is close to , but not as good as , the performance of maximal - margin classifiers on the same problem .
in this paper we present a framework for building probabilistic automata parameterized by context - dependent probabilities . gibbs distributions are used to model state transitions and output generation , and parameter estimation is carried out using an em algorithm where the m - step uses a generalized iterative scaling procedure . we discuss relations with certain classes of stochastic feedforward neural networks , a geometric interpretation for parameter estimation , and a simple example of a statistical language model constructed using this methodology .
in the regression context , boosting and bagging are techniques to build a committee of regressors that may be superior to a single regressor . we use regression trees as fundamental building blocks in bagging committee machines and boosting committee machines . performance is analyzed on three non - linear functions and the boston housing database . in all cases , boosting is at least equivalent , and in most cases better than bagging in terms of prediction error .
current expert systems cannot properly handle imprecise and incomplete information . on the other hand , neural networks can perform pattern recognition operations even in noisy environments . against this background , we have implemented a neural expert system shell neula , whose computational mechanism processes imprecisely or incompletely given information by means of approximate probabilistic reasoning .
co - evolution can give rise to the " red queen effect " , where interacting populations alter each other ' s fitness landscapes . the red queen effect significantly complicates any measurement of co - evolutionary progress , introducing fitness ambiguities where improvements in performance of co - evolved individuals can appear as a decline or stasis in the usual measures of evolutionary progress . unfortunately , no appropriate measures of fitness given the red queen effect have been developed in artificial life , theoretical biology , population dynamics , or evolutionary genetics . we propose a set of appropriate performance measures based on both genetic and behavioral data , and illustrate their use in a simulation of co - evolution between genetically specified continuous - time noisy recurrent neural networks which generate pursuit and evasion behaviors in autonomous agents .
inferences in measurement error models can be sensitive to modeling assumptions . specifically , if the model is incorrect then the estimates can be inconsistent . to reduce sensitivity to modeling assumptions and yet still retain the efficiency of parametric inference we propose to use flexible parametric models which can accommodate departures from standard parametric models . we use mixtures of normals for this purpose . we study two cases in detail : a linear errors - in - variables model and a change - point berkson model .
in this paper we investigate the phenomenon of multi - parent reproduction , i . e . we study recombination mechanisms where an arbitrary n & gt ; #NUM# number of parents participate in creating children . in particular , we discuss scanning crossover that generalizes the standard uniform crossover and diagonal crossover that generalizes #NUM# - point crossover , and study the effects of different number of parents on the ga behavior . we conduct experiments on tough function optimization problems and observe that by multi - parent operators the performance of gas can be enhanced significantly . we also give a theoretical foundation by showing how these operators work on distributions .
draft a brief introduction to neural networks richard d . de veaux lyle h . ungar williams college university of pennsylvania abstract artificial neural networks are being used with increasing frequency for high dimensional problems of regression or classification . this article provides a tutorial overview of neural networks , focusing on back propagation networks as a method for approximating nonlinear multivariable functions . we explain , from a statistician ' s vantage point , why neural networks might be attractive and how they compare to other modern regression techniques . keywords nonparametric regression ; function approximation ; backpropagation . #NUM# introduction networks that mimic the way the brain works ; computer programs that actually learn patterns ; forecasting without having to know statistics . these are just some of the many claims and attractions of artificial neural networks . neural networks ( we will henceforth drop the term artificial , unless we need to distinguish them from biological neural networks ) seem to be everywhere these days , and at least in their advertising , are able to do all that statistics can do without all the fuss and bother of having to do anything except buy a piece of software . neural networks have been successfully used for many different applications including robotics , chemical process control , speech recognition , optical character recognition , credit card fraud detection , interpretation of chemical spectra and vision for autonomous navigation of vehicles . ( pointers to the literature are given at the end of this article . ) in this article we will attempt to explain how one particular type of neural network , feedforward networks with sigmoidal activation functions ( " backpropagation networks " ) actually works , how it is " trained " , and how it compares with some more well known statistical techniques . as an example of why someone would want to use a neural network , consider the problem of recognizing hand written zip codes on letters . this is a classification problem , where the #NUM#
to apply the algorithm for classification we assign each class a separate set of codebook gaussians . each set is only trained with patterns from a single class . after having trained the codebook gaussians , each set provides an estimate of the probability function of one class ; just as with parzen window estimation , we take as the estimate of the pattern distribution the average of all gaussians in the set . classification of a pattern may now be done by calculating the probability of each class at the respective sample point , and assigning to the pattern the class with the highest probability . hence the whole codebook plays a role in the classification of patterns . this is not the case with regular classification schemes using codebooks . we have tested the classification scheme on several classification tasks including the two spiral problem . we compared our algorithm to various other classification algorithms and it came out second ; the best algorithm for the applications is the parzen window estimation . however , the computing time and memory for parzen window estimation are excessive when compared to our algorithm , and hence , in practical situations , our algorithm is to be preferred . we have developed a fast algorithm which combines attractive properties of both parzen window estimation and vector quantization . the scale parameter is tuned adaptively and , therefore , is not set in an ad hoc manner . it allows a classification strategy in which all the codebook vectors are taken into account . this yields better results than the standard vector quantization techniques . an interesting topic for further research is to use radially non - symmetric gaussians .
predictions of lifetimes of dynamically allocated objects can be used to improve time and space efficiency of dynamic memory management in computer programs . barrett and zorn [ #NUM# ] used a simple lifetime predictor and demonstrated this improvement on a variety of computer programs . in this paper , we use decision trees to do lifetime prediction on the same programs and show significantly better prediction . our method also has the advantage that during training we can use a large number of features and let the decision tree automatically choose the relevant subset .
markov decision processes ( mdps ) have recently been applied to the problem of modeling decision - theoretic planning . while traditional methods for solving mdps are often practical for small states spaces , their effectiveness for large ai planning problems is questionable . we present an algorithm , called structured policy iteration ( spi ) , that constructs optimal policies without explicit enumeration of the state space . the algorithm retains the fundamental computational steps of the commonly used modified policy iteration algorithm , but exploits the variable and propositional independencies reflected in a temporal bayesian network representation of mdps . the principles behind spi can be applied to any structured representation of stochastic actions , policies and value functions , and the algorithm itself can be used in conjunction with re cent approximation methods .
this paper reviews features of a new class of multilayer connectionist architectures known as asocs ( adaptive self - organizing concurrent systems ) . asocs is similar to most decision - making neural network models in that it attempts to learn an adaptive set of arbitrary vector mappings . however , it differs dramatically in its mechanisms . asocs is based on networks of adaptive digital elements which self - modify using local information . function specification is entered incrementally by use of rules , rather than complete input - output vectors , such that a processing network is able to extract critical features from a large environment and give output in a parallel fashion . learning also uses parallelism and self - organization such that a new rule is completely learned in time linear with the depth of the network . the model guarantees learning of any arbitrary mapping of boolean input - output vectors . the model is also stable in that learning does not erase any previously learned mappings except those explicitly contradicted .
maximum working likelihood ( mwl ) inference in the presence of missing data can be quite challenging because of the intractability of the associated marginal likelihood . this problem can be further exacerbated when the number of parameters involved is large . we propose using markov chain monte carlo ( mcmc ) to first obtain both the mwl estimator and the working fisher information matrix and , second , using monte carlo quadrature to obtain the remaining components of the correct asymptotic mwl variance . evaluation of the marginal likelihood is not needed . we demonstrate consistency and asymptotic normality when the number of independent and identically distributed data clusters is large but the likelihood may be incorrectly specified . an analysis of longitudinal ordinal data is given for an example . key words : convergence of posterior distributions , maximum likelihood , metropolis
natural images contain characteristic statistical regularities that set them apart from purely random images . understanding what these regularities are can enable natural images to be coded more efficiently . in this paper , we describe some of the forms of structure that are contained in natural images , and we show how these are related to the response properties of neurons at early stages of the visual system . many of the important forms of structure require higher - order ( i . e . , more than linear , pairwise ) statistics to characterize , which makes models based on linear hebbian learning , or principal components analysis , inappropriate for finding efficient codes for natural images . we suggest that a good objective for an efficient coding of natural scenes is to maximize the sparseness of the representation , and we show that a network that learns sparse codes of natural scenes succeeds in developing localized , oriented , bandpass receptive fields similar to those in the primate striate cortex .
in this paper we develop an empirical methodology for studying the behavior of evolutionary algorithms based on problem generators . we then describe three generators that can be used to study the effects of epistasis on the performance of eas . finally , we illustrate the use of these ideas in a preliminary exploration of the effects of epistasis on simple gas .
traditionally , genetic algorithms have relied upon #NUM# and #NUM# - point crossover operators . many recent empirical studies , however , have shown the benefits of higher numbers of crossover points . some of the most intriguing recent work has focused on uniform crossover , which involves on the average l / #NUM# crossover points for strings of length l . despite theoretical analysis , however , it appears difficult to predict when a particular crossover form will be optimal for a given problem . this paper describes an adaptive genetic algorithm that decides , as it runs , which form is optimal .
conditional logics , introduced by lewis and stalnaker , have been utilized in artificial intelligence to capture a broad range of phenomena . in this paper we examine the complexity of several variants discussed in the literature . we show that , in general , deciding satisfiability is pspace - complete for formulas with arbitrary conditional nesting and np - complete for formulas with bounded nesting of conditionals . however , we provide several exceptions to this rule . of particular note are results showing that ( a ) when assuming uniformity ( i . e . , that all worlds agree on what worlds are possible ) , the decision problem becomes exptime - complete even for formulas with bounded nesting , and ( b ) when assuming absoluteness ( i . e . , that all worlds agree on all conditional statements ) , the decision problem is np - complete for for mulas with arbitrary nesting .
an incremental , higher - order , non - recurrent network combines two properties found to be useful for learning sequential tasks : higher - order connections and incremental introduction of new units . the network adds higher orders when needed by adding new units that dynamically modify connection weights . since the new units modify the weights at the next time - step with information from the previous step , temporal tasks can be learned without the use of feedback , thereby greatly simplifying training . furthermore , a theoretically unlimited number of units can be added to reach into the arbitrarily distant past . experiments with the reber grammar have demonstrated speedups of two orders of magnitude over recurrent networks .
i propose a novel general principle for unsupervised learning of distributed non - redundant internal representations of input patterns . the principle is based on two opposing forces . for each representational unit there is an adaptive predictor which tries to predict the unit from the remaining units . in turn , each unit tries to react to the environment such that it minimizes its predictability . this encourages each unit to filter ` abstract concepts ' out of the environmental input such that these concepts are statistically independent of those upon which the other units focus . i discuss various simple yet potentially powerful implementations of the principle which aim at finding binary factorial codes ( bar - low et al . , #NUM# ) , i . e . codes where the probability of the occurrence of a particular input is simply the product of the probabilities of the corresponding code symbols . such codes are potentially relevant for ( #NUM# ) segmentation tasks , ( #NUM# ) speeding up supervised learning , ( #NUM# ) novelty detection . methods for finding factorial codes automatically implement occam ' s razor for finding codes using a minimal number of units . unlike previous methods the novel principle has a potential for removing not only linear but also non - linear output redundancy . illustrative experiments show that algorithms based on the principle of predictability minimization are practically feasible . the final part of this paper describes an entirely local algorithm that has a potential for learning unique representations of extended input sequences .
in this paper we study learning in the pac model of valiant [ #NUM# ] in which the example oracle used for learning may be faulty in one of two ways : either by misclassifying the example or by distorting the distribution of examples . we first consider models in which examples are misclassified . kearns [ #NUM# ] recently showed that efficient learning in a new model using statistical queries is a sufficient condition for pac learning with classification noise . we show that efficient learning with statistical queries is sufficient for learning in the pac model with malicious error rate proportional to the required statistical query accuracy . one application of this result is a new lower bound for tolerable malicious error in learning monomials of k literals . this is the first such bound which is independent of the number of irrelevant attributes n . we also use the statistical query model to give sufficient conditions for using distribution specific algorithms on distributions outside their prescribed domains . a corollary of this result expands the class of distributions on which we can weakly learn monotone boolean formulae . we also consider new models of learning in which examples are not chosen according to the distribution on which the learner will be tested . we examine three variations of distribution noise and give necessary and sufficient conditions for polynomial time learning with such noise . we show containments and separations between the various models of faulty oracles . finally , we examine hypothesis boosting algorithms in the context of learning with distribution noise , and show that schapire ' s result regarding the strength of weak learnabil - ity [ #NUM# ] is in some sense tight in requiring the weak learner to be nearly distribution free .
this paper describes the first stage of our study on evolution of learning abilities . we use a simple maze exploration problem designed by r . sut - ton as the task of each individual , and encode the inherent learning parameters on the genome . the learning architecture we use is a one step q - learning using look - up table , where the inherent parameters are initial q - values , learning rate , discount rate of rewards , and exploration rate . under the fitness measure proportioning to the number of times it achieves at the goal in the later half of life , learners evolve through a genetic algorithm . the results of computer simulation indicated that learning ability emerge when the environment changes every generation , and that the inherent map for the optimal path can be acquired when the environment doesn ' t change . these results suggest that emergence of learning ability needs environmental change faster than alternate generation .
we examine the problem of performing exact dynamic - programming updates in partially observable markov decision processes ( pomdps ) from a computational complexity viewpoint . dynamic - programming updates are a crucial operation in a wide range of pomdp solution methods and we find that it is intractable to perform these updates on piecewise - linear convex value functions for general pomdps . we offer a new algorithm , called the witness algorithm , which can compute updated value functions efficiently on a restricted class of pomdps in which the number of linear facets is not too great . we compare the witness algorithm to existing algorithms analytically and empirically and find that it is the fastest algorithm over a wide range of pomdp sizes .
in this paper we present a framework for building probabilistic automata parameterized by context - dependent probabilities . gibbs distributions are used to model state transitions and output generation , and parameter estimation is carried out using an em algorithm where the m - step uses a generalized iterative scaling procedure . we discuss relations with certain classes of stochastic feedforward neural networks , a geometric interpretation for parameter estimation , and a simple example of a statistical language model constructed using this methodology .
models of unsupervised correlation - based ( hebbian ) synaptic plasticity are typically unstable : either all synapses grow until each reaches the maximum allowed strength , or all synapses decay to zero strength . a common method of avoiding these outcomes is to use a constraint that conserves or limits the total synaptic strength over a cell . we study the dynamical effects of such constraints . two methods of enforcing a constraint are distinguished , multiplicative and subtractive . for otherwise linear learning rules , multiplicative enforcement of a constraint results in dynamics that converge to the principal eigenvector of the operator determining unconstrained synaptic development . subtractive enforcement , in contrast , typically leads to a final state in which almost all synaptic strengths reach either the maximum or minimum allowed value . this final state is often dominated by weight configurations other than the principal eigenvector of the unconstrained operator . multiplicative enforcement yields a " graded " receptive field in which most mutually correlated inputs are represented , whereas subtractive enforcement yields a receptive field that is " sharpened " to a subset of maximally - correlated inputs . if two equivalent input populations ( e . g . two eyes ) innervate a common target , multiplicative enforcement prevents their segregation ( ocular dominance segregation ) when the two populations are weakly correlated ; whereas subtractive enforcement allows segregation under these circumstances . these results may be used to understand constraints both over output cells and over input cells . a variety of rules that can implement constrained dynamics are discussed .
this project was supported in part by a grant from the mcdonnell - pew foundation , by a grant from atr human information processing research laboratories , by a grant from siemens corporation , and by grant n #NUM# - #NUM# - j - #NUM# from the office of naval research . the project was also supported by nsf grant asc - #NUM# in support of the center for biological and computational learning at mit , including funds provided by darpa under the hpcc program . michael i . jordan is a nsf presidential young investigator .
learning can be made more efficient if we can actively select particularly salient data points . within a bayesian learning framework , objective functions are discussed which measure the expected informativeness of candidate measurements . three alternative specifications of what we want to gain information about lead to three different criteria for data selection . all these criteria depend on the assumption that the hypothesis space is correct , which may prove to be their main weakness .
knowledge of clusters and their relations is important in understanding high - dimensional input data with unknown distribution . ordinary feature maps with fully connected , fixed grid topology cannot properly reflect the structure of clusters in the input space | there are no cluster boundaries on the map . incremental feature map algorithms , where nodes and connections are added to or deleted from the map according to the input distribution , can overcome this problem . however , so far such algorithms have been limited to maps that can be drawn in #NUM# - d only in the case of #NUM# - dimensional input space . in the approach proposed in this paper , nodes are added incrementally to a regular , #NUM# - dimensional grid , which is drawable at all times , irrespective of the dimensionality of the input space . the process results in a map that explicitly represents the cluster structure of the high - dimensional input .
lattice conditional independence ( lci ) models for multivariate normal data recently have been introduced for the analysis of non - monotone missing data patterns and of nonnested dependent linear regression models ( seemingly unrelated regressions ) . it is shown here that the class of lci models coincides with a subclass of the class of graphical markov models determined by acyclic digraphs ( adgs ) , namely , the subclass of transitive adg models . an explicit graph - theoretic characterization of those adgs that are markov equivalent to some transitive adg is obtained . this characterization allows one to determine whether a specific adg d is markov equivalent to some transitive adg , hence to some lci model , in polynomial time , without an exhaustive search of the ( exponentially large ) equivalence class [ d ] . these results do not require the existence or positivity of joint densities .
in this paper we describe a method for improving genetic - algorithm - based optimization using case - based learning . the idea is to utilize the sequence of points explored during a search to guide further exploration . the proposed method is particularly suitable for continuous spaces with expensive evaluation functions , such as arise in engineering design . empirical results in two engineering design domains and across different representations demonstrate that the proposed method can significantly improve the efficiency and reliability of the ga optimizer . moreover , the results suggest that the modification makes the genetic algorithm less sensitive to poor choices of tuning parameters such as muta tion rate .
genetic algorithms ( gas ) have been extensively used in different domains as a means of doing global optimization in a simple yet reliable manner . they have a much better chance of getting to global optima than gradient based methods which usually converge to local sub optima . however , gas have a tendency of getting only moderately close to the optima in a small number of iterations . to get very close to the optima , the ga needs a very large number of iterations . whereas gradient based optimizers usually get very close to local optima in a relatively small number of iterations . in this paper we describe a new crossover operator which is designed to endow the ga with gradient - like abilities without actually computing any gradients and without sacrificing global optimality . the operator works by using guidance from all members of the ga population to select a direction for exploration . empirical results in two engineering design domains and across both binary and floating point representations demonstrate that the operator can significantly improve the steady state error of the ga optimizer .
d . e . rumelhart , g . e . hinton and r . j . williams , " learning internal representations by error propagation " , in d . e . rumelhart and j . l . mcclelland ( eds . ) parallel distributed processing : explorations in the microstructure of cognition ( vol . #NUM# ) , mit press ( #NUM# ) .
evolutionary trees are frequently used as the underlying model in the design of algorithms , optimization criteria and software packages for multiple sequence alignment ( msa ) . in this paper , we reexamine the suitability of trees as a universal model for msa in light of the broad range of biological questions that msa ' s are used to address . a tree model consists of a tree topology and a model of accepted mutations along the branches . after surveying the major applications of msa , examples from the molecular biology literature are used to illustrate situations in which this tree model fails . this occurs when the relationship between residues in a column cannot be described by a tree ; for example , in some structural and functional applications of msa . it also occurs in situations , such as lateral gene transfer , where an entire gene cannot be modeled by a unique tree . in cases of nonparsimonous data or convergent evolution , it may be difficult to find a consistent mutational model . we hope that this survey will promote dialogue between biologists and computer scientists , leading to more biologically realistic research on msa .
selective suppression of transmission at feedback synapses during learning is proposed as a mechanism for combining associative feedback with self - organization of feedforward synapses . experimental data demonstrates cholinergic suppression of synaptic transmission in layer i ( feedback synapses ) , and a lack of suppression in layer iv ( feed - forward synapses ) . a network with this feature uses local rules to learn mappings which are not linearly separable . during learning , sensory stimuli and desired response are simultaneously presented as input . feedforward connections form self - organized representations of input , while suppressed feedback connections learn the transpose of feedfor - ward connectivity . during recall , suppression is removed , sensory input activates the self - organized representation , and activity generates the learned response .
one way to sample from a distribution is to sample uniformly from the region under the plot of its density function . a markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal ` slice ' defined by the current vertical position . variations on such ` slice sampling ' methods can easily be implemented for univariate distributions , and can be used to sample from a multivariate distribution by updating each variable in turn . this approach is often easier to implement than gibbs sampling , and may be more efficient than easily - constructed versions of the metropolis algorithm . slice sampling is therefore attractive in routine markov chain monte carlo applications , and for use by software that automatically generates a markov chain sampler from a model specification . one can also easily devise overrelaxed versions of slice sampling , which sometimes greatly improve sampling efficiency by suppressing random walk behaviour . random walks can also be avoided in some slice sampling schemes that simultaneously update all variables .
markov decision problems ( mdps ) provide the foundations for a number of problems of interest to ai researchers studying automated planning and reinforcement learning . in this paper , we summarize results regarding the complexity of solving mdps and the running time of mdp solution algorithms . we argue that , although mdps can be solved efficiently in theory , more study is needed to reveal practical algorithms for solving large problems quickly . to encourage future research , we sketch some alternative methods of analysis that rely on the struc ture of mdps .
learning from reinforcements is a promising approach for creating intelligent agents . however , reinforcement learning usually requires a large number of training episodes . we present an approach that addresses this shortcoming by allowing a connectionist q - learner to accept advice given , at any time and in a natural manner , by an external observer . in our approach , the advice - giver watches the learner and occasionally makes suggestions , expressed as instructions in a simple programming language . based on techniques from knowledge - based neural networks , these programs are inserted directly into the agent ' s utility function . subsequent reinforcement learning further integrates and refines the advice . we present empirical evidence that shows our approach leads to statistically - significant gains in expected reward . importantly , the advice improves the expected reward regardless of the stage of training at which it is given .
this paper presents the mathematical foundations of dirichlet mixtures , which have been used to improve database search results for homologous sequences , when a variable number of sequences from a protein family or domain are known . we present a method for condensing the information in a protein database into a mixture of dirichlet densities . these mixtures are designed to be combined with observed amino acid frequencies , to form estimates of expected amino acid probabilities at each position in a profile , hidden markov model , or other statistical model . these estimates give a statistical model greater generalization capacity , such that remotely related family members can be more reliably recognized by the model . dirichlet mixtures have been shown to outperform substitution matrices and other methods for computing these expected amino acid distributions in database search , resulting in fewer false positives and false negatives for the families tested . this paper corrects a previously published formula for estimating these expected probabilities , and contains complete derivations of the dirichlet mixture formulas , methods for optimizing the mixtures to match particular databases , and suggestions for efficient implementation .
derivational analogy is a technique for reusing problem solving experience to improve problem solving performance . this research addresses an issue common to all problem solvers that use derivational analogy : overcoming the mismatches between past experiences and new problems that impede reuse . first , this research describes the variety of mismatches that can arise and proposes a new approach to derivational analogy that uses appropriate adaptation strategies for each . second , it compares this approach with seven others in a common domain . this empirical study shows that derivational analogy is almost always more efficient than problem solving from scratch , but the amount it contributes depends on its ability to overcome mismatches
pollack ( #NUM# ) demonstrated that second - order recurrent neural networks can act as dynamical recognizers for formal languages when trained on positive and negative examples , and observed both phase transitions in learning and ifs - like fractal state sets . follow - on work focused mainly on the extraction and minimization of a finite state automaton ( fsa ) from the trained network . however , such networks are capable of inducing languages which are not regular , and therefore not equivalent to any fsa . indeed , it may be simpler for a small network to fit its training data by inducing such a non - regular language . but when is the network ' s language not regular ? in this paper , using a low dimensional network capable of learning all the tomita data sets , we present an empirical method for testing whether the language induced by the network is regular or not . we also provide a detailed " - machine analysis of trained networks for both regular and non - regular languages .
this article presents an algorithm for inducing multiclass decision trees with multivariate tests at internal decision nodes . each test is constructed by training a linear machine and eliminating variables in a controlled manner . empirical results demonstrate that the algorithm builds small accurate trees across a variety of tasks .
funes , p . and pollack , j . ( #NUM# ) computer evolution of buildable objects . fourth european conference on artificial life . p . husbands and i . harvey , eds . , mit press . pp #NUM# - #NUM# . knowledge into the program , which would result in familiar structures , we provided the algorithm with a model of the physical reality and a purely utilitarian fitness function , thus supplying measures of feasibility and functionality . in this way the evolutionary process runs in an environment that has not been unnecessarily constrained . we added , however , a requirement of computability to reject overly complex structures when they took too long for our simulations to evaluate . the results are encouraging . the evolved structures had a surprisingly alien look : they are not based in common knowledge on how to build with brick toys ; instead , the computer found ways of its own through the evolutionary search process . we were able to assemble the final designs manually and confirm that they accomplish the objectives introduced with our fitness functions . after some background on related problems , we describe our physical simulation model for two - dimensional lego structures , and the representation for encoding them and applying evolution . we demonstrate the feasibility of our work with photos of actual objects which were the result of particular optimizations . finally , we discuss future work and draw some conclusions . in order to evolve both the morphology and behavior of autonomous mechanical devices which can be manufactured , one must have a simulator which operates under several constraints , and a resultant controller which is adaptive enough to cover the gap between simulated and real world . eral space of mechanisms . conservative - because simulation is never perfect , it should preserve a margin of safety . efficient - it should be quicker to test in simulation than through physical production and test . buildable - results should be convertible from a simula tion to a real object computer evolution of buildable objects abstract the idea of co - evolution of bodies and brains is becoming popular , but little work has been done in evolution of physical structure because of the lack of a general framework for doing it . evolution of creatures in simulation has been constrained by the reality gap which implies that resultant objects are usually not buildable . the work we present takes a step in the problem of body evolution by applying evolutionary techniques to the design of structures assembled out of parts . evolution takes place in a simulator we designed , which computes forces and stresses and predicts failure for #NUM# - dimensional lego structures . the final printout of our program is a schematic assembly , which can then be built physically . we demonstrate its functionality in several different evolved entities .
in this paper we are concerned with the problem of acquiring knowledge by integration . our aim is to construct an integrated knowledge base from several separate sources . the need to merge knowledge bases can arise , for example , when knowledge bases are acquired independently from interactions with several domain experts . as opinions of different domain experts may differ , the knowledge bases constructed in this way will normally differ too . a similar problem can also arise whenever separate knowledge bases are generated by learning algorithms . the objective of integration is to construct one system that exploits all the knowledge that is available and has a good performance . the aim of this paper is to discuss the methodology of knowledge integration , describe the implemented system ( integ . #NUM# ) , and present some concrete results which demonstrate the advantages of this method .
many arthropods ( particularly insects ) exhibit sophisticated visually guided behaviours . yet in most cases the behaviours are guided by input from a few hundreds or thousands of " pixels " ( i . e . ommatidia in the compound eye ) . inspired by this observation , we have for several years been exploring the possibilities of visually guided robots with low - bandwidth vision . rather than design the robot controllers by hand , we use artificial evolution ( in the form of an extended genetic algorithm ) to automatically generate the architectures for artificial neural networks which generate effective sensory - motor coordination when controlling mobile robots . analytic techniques drawn from neuroethology and dynamical systems theory allow us to understand how the evolved robot controllers function , and to predict their behaviour in environments other than those used during the evolutionary process . initial experiments were performed in simulation , but the techniques have now been successfully transferred to work with a variety of real physical robot platforms . this chapter reviews our past work , concentrating on the analysis of evolved controllers , and gives an overview of our current research . we conclude with a discussion of the application of our evolutionary techniques to problems in biological vision .
the major implementational problem for reversible jump mcmc is that there is commonly no natural way to choose jump proposals since there is no euclidean structure to guide our choice . in this paper we will consider a mechanism for guiding the proposal choice by analysis of acceptance probabilities for jumps . essentially the method involves an approximation for the acceptance probability around certain canonical jumps . we will illustrate the procedure using an example of a reversible jump mcmc application , involving a bayesian analysis of graphical gaussian models .
instance - based learning methods explicitly remember all the data that they receive . they usually have no training phase , and only at prediction time do they perform computation . then , they take a query , search the database for similar datapoints and build an on - line local model ( such as a local average or local regression ) with which to predict an output value . in this paper we review the advantages of instance based methods for autonomous systems , but we also note the ensuing cost : hopelessly slow computation as the database grows large . we present and evaluate a new way of structuring a database and a new algorithm for accessing it that maintains the advantages of instance - based learning . earlier attempts to combat the cost of instance - based learning have sacrificed the explicit retention of all data , or been applicable only to instance - based predictions based on a small number of near neighbors or have had to re - introduce an explicit training phase in the form of an interpolative data structure . our approach builds a multiresolution data structure to summarize the database of experiences at all resolutions of interest simultaneously . this permits us to query the database with the same exibility as a conventional linear search , but at greatly reduced computational cost .
in the standard on - line model the learning algorithm tries to minimize the total number of mistakes made in a series of trials . on each trial the learner sees an instance , either accepts or rejects that instance , and then is told the appropriate response . we define a natural variant of this model ( " apple tasting " ) where the learner gets feedback only when the instance is accepted . we use two transformations to relate the apple tasting model to an enhanced standard model where false acceptances are counted separately from false rejections . we present a strategy for trading between false acceptances and false rejections in the standard model . from one perspective this strategy is exactly optimal , including constants . we apply our results to obtain a good general purpose apple tasting algorithm as well as nearly optimal apple tasting algorithms for a variety of standard classes , such as conjunctions and disjunctions of n boolean variables . we also present and analyze a simpler transformation useful when the instances are drawn at random rather than selected by an adversary .
in this paper we describe an algorithm which exploits the error distribution generated by a learning algorithm in order to break up the domain which is being approximated into piecewise learnable partitions . traditionally , the error distribution has been neglected in favor of a lump error measure such as rms . by doing this , however , we lose a lot of important information . the error distribution tells us where the algorithm is doing badly , and if there exists a " ridge " of errors , also tells us how to partition the space so that one part of the space will not interfere with the learning of another . the algorithm builds a variable arity k - d tree whose leaves contain the partitions . using this tree , new points can be predicted using the correct partition by traversing the tree . we instantiate this algorithm using memory based learners and cross - validation .
preens a parallel research execution environment for neural systems is a distributed neurosimulator , targeted on networks of workstations and transputer systems . as current applications of neural networks often contain large amounts of data and as the neural networks involved in tasks such as vision are very large , high requirements on memory and computational resources are imposed on the target execution platforms . preens can be executed in a distributed environment , i . e . tools and neural network simulation programs can be running on any machine connectable via tcp / ip . using this approach , larger tasks and more data can be examined using an efficient coarse grained parallelism . furthermore , the design of preens allows for neural networks to be running on any high performance mimd machine such as a trans - puter system . in this paper , the different features and design concepts of preens are discussed . these can also be used for other applications , like image processing .
it is well known that standard learning classifier systems , when applied to many different domains , exhibit a number of problems : payoff oscillation , difficult to regulate interplay between the reward system and the background genetic algorithm ( ga ) , rule chains instability , default hierarchies instability , are only a few . alecsys is a parallel version of a standard learning classifier system ( cs ) , and as such suffers of these same problems . in this paper we propose some innovative solutions to some of these problems . we introduce the following original features . mutespec , a new genetic operator used to specialize potentially useful classifiers . energy , a quantity introduced to measure global convergence in order to apply the genetic algorithm only when the system is close to a steady state . dynamical adjustment of the classifiers set cardinality , in order to speed up the performance phase of the algorithm . we present simulation results of experiments run in a simulated two - dimensional world in which a simple agent learns to follow a light source .
supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases . so during learning , it is important to keep the weights simple by penalizing the amount of information they contain . the amount of information in a weight can be controlled by adding gaussian noise and the noise level can be adapted during learning to optimize the trade - off between the expected squared error of the network and the amount of information in the weights . we describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non - linear hidden units . provided the output units are linear , the exact derivatives can be computed efficiently without time - consuming monte carlo simulations . the idea of minimizing the amount of information that is required to communicate the weights of a neural network leads to a number of interesting schemes for encoding the weights .
there are many applications in which it is desirable to order rather than classify instances . here we consider the problem of learning how to order , given feedback in the form of preference judgments , i . e . , statements to the effect that one instance should be ranked ahead of another . we outline a two - stage approach in which one first learns by conventional means a preference function , of the form pref ( u ; v ) , which indicates whether it is advisable to rank u before v . new instances are then ordered so as to maximize agreements with the learned preference function . we show that the problem of finding the ordering that agrees best with a preference function is np - complete , even under very restrictive assumptions . nevertheless , we describe a simple greedy algorithm that is guaranteed to find a good approximation . we then discuss an on - line learning algorithm , based on the hedge algorithm , for finding a good linear combination of ranking experts . we use the ordering algorithm combined with the on - line learning algorithm to find a combination of search experts , each of which is a domain - specific query expansion strategy for a www search engine , and present experimental results that demonstrate the merits of our approach .
evolutionary algorithms are powerful techniques for optimisation whose operation principles are inspired by natural selection and genetics . in this paper we discuss the relation between evolutionary techniques , numerical and classical search methods and we show that all these methods are instances of a single more general search strategy , which we call the ` evolutionary computation cookbook ' . by combining the features of classical and evolutionary methods in different ways new instances of this general strategy can be generated , i . e . new evolutionary ( or classical ) algorithms can be designed . one such algorithm , ga fl , is described .
we present a neural net architecture that can discover hierarchical and recursive structure in symbol strings . to detect structure at multiple levels , the architecture has the capability of reducing symbols substrings to single symbols , and makes use of an external stack memory . in terms of formal languages , the architecture can learn to parse strings in an lr ( #NUM# ) context - free grammar . given training sets of positive and negative exemplars , the architecture has been trained to recognize many different grammars . the architecture has only one layer of modifiable weights , allowing for a many cognitive domains involve complex sequences that contain hierarchical or recursive structure , e . g . , music , natural language parsing , event perception . to illustrate , " the spider that ate the hairy fly " is a noun phrase containing the embedded noun phrase " the hairy fly . " understanding such multilevel structures requires forming reduced descriptions ( hinton , #NUM# ) in which a string of symbols or states ( " the hairy fly " ) is reduced to a single symbolic entity ( a noun phrase ) . we present a neural net architecture that learns to encode the structure of symbol strings via such reduction transformations . the difficult problem of extracting multilevel structure from complex , extended sequences has been studied by mozer ( #NUM# ) , ring ( #NUM# ) , rohwer ( #NUM# ) , and schmidhuber ( #NUM# ) , among others . while these previous efforts have made some straightforward interpretation of its behavior .
self - organizing feature maps are usually implemented by abstracting the low - level neural and parallel distributed processes . an external supervisor finds the unit whose weight vector is closest in euclidian distance to the input vector and determines the neighborhood for weight adaptation . the weights are changed proportional to the euclidian distance . in a biologically more plausible implementation , similarity is measured by a scalar product , neighborhood is selected through lateral inhibition and weights are changed by redistributing synaptic resources . the resulting self - organizing process is quite similar to the abstract case . however , the process is somewhat hampered by boundary effects and the parameters need to be carefully evolved . it is also necessary to add a redundant dimension to the input vectors .
the application of decision making and learning algorithms to multi - agent systems presents many interestingresearch challenges and opportunities . among these is the ability for agents to learn how to act by observing or imitating other agents . we describe an algorithm , the iq - algorithm , that integrates imitation with q - learning . roughly , a q - learner uses the observations it has made of an expert agent to bias its exploration in promising directions . this algorithm goes beyond previous work in this direction by relaxing the oft - made assumptions that the learner ( observer ) and the expert ( observed agent ) share the same objectives and abilities . our preliminary experiments demonstrate significant transfer between agents using the iq - model and in many cases reductions in training time .
faces represent complex , multidimensional , meaningful visual stimuli and developing a computational model for face recognition is difficult [ #NUM# ] . we present a hybrid neural network solution which compares favorably with other methods . the system combines local image sampling , a self - organizing map neural network , and a convolutional neural network . the self - organizing map provides a quantization of the image samples into a topological space where inputs that are nearby in the original space are also nearby in the output space , thereby providing dimensionality reduction and invariance to minor changes in the image sample , and the convolutional neural network provides for partial invariance to translation , rotation , scale , and deformation . the convolutional network extracts successively larger features in a hierarchical set of layers . we present results using the karhunen - loeve transform in place of the self - organizing map , and a multi - layer perceptron in place of the convolutional network . the karhunen - loeve transform performs almost as well ( #NUM# . #NUM# % error versus #NUM# . #NUM# % ) . the multi - layer perceptron performs very poorly ( #NUM# % error versus #NUM# . #NUM# % ) . the method is capable of rapid classification , requires only fast , approximate normalization and preprocessing , and consistently exhibits better classification performance than the eigenfaces approach [ #NUM# ] on the database considered as the number of images per person in the training database is varied from #NUM# to #NUM# . with #NUM# images per person the proposed method and eigenfaces result in #NUM# . #NUM# % and #NUM# . #NUM# % error respectively . the recognizer provides a measure of confidence in its output and classification error approaches zero when rejecting as few as #NUM# % of the examples . we use a database of #NUM# images of #NUM# individuals which contains quite a high degree of variability in expression , pose , and facial details . we analyze computational complexity and discuss how new classes could be added to the trained recognizer .
we present a new algorithm for solving markov decision problems that extends the modified policy iteration algorithm of puterman and shin [ #NUM# ] in two important ways : #NUM# ) the new algorithm is asynchronous in that it allows the values of states to be updated in arbitrary order , and it does not need to consider all actions in each state while updating the policy . #NUM# ) the new algorithm converges under more general initial conditions than those required by modified policy iteration . specifically , the set of initial policy - value function pairs for which our algorithm guarantees convergence is a strict superset of the set for which modified policy iteration converges . this generalization was obtained by making a simple and easily implementable change to the policy evaluation operator used in updating the value function . both the asynchronous nature of our algorithm and its convergence under more general conditions expand the range of problems to which our algorithm can be applied .
recently , markov chain monte carlo ( mcmc ) sampling methods have become widely used for determining properties of a posterior distribution . alternative to the gibbs sampler , we elaborate on the hit - and - run sampler and its generalization , a black - box sampling scheme , to generate a time - reversible markov chain from a posterior distribution . the proof of convergence and its applications to bayesian computation with constrained parameter spaces are provided and comparisons with the other mcmc samplers are made . in addition , we propose an importance weighted marginal density estimation ( iwmde ) method . an iwmde is obtained by averaging many dependent observations of the ratio of the full joint posterior densities multiplied by a weighting conditional density w . the asymptotic properties for the iwmde and the guidelines for choosing a weighting conditional density w are also considered . the generalized version of iwmde for estimating marginal posterior densities when the full joint posterior density contains analytically intractable normalizing constants is developed . furthermore , we develop monte carlo methods based on kullback - leibler divergences for comparing marginal posterior density estimators . this article is a summary of the author ' s ph . d . thesis and it was presented in the savage award session .
in this paper , we further characterize the complexity of noise - tolerant learning in the pac model . specifically , we show a general lower bound of log ( #NUM# = ffi ) on the number of examples required for pac learning in the presence of classification noise . combined with a result of simon , we effectively show that the sample complexity of pac learning in the presence of classification noise is vc ( f ) " ( #NUM# ) #NUM# : furthermore , we demonstrate the optimality of the general lower bound by providing a noise - tolerant learning algorithm for the class of symmetric boolean functions which uses a sample size within a constant factor of this bound . finally , we note that our general lower bound compares favorably with various general upper bounds for pac learning in the presence of classification noise .
it has been recently realized that parasite virulence ( the harm caused by parasites to their hosts ) can be an adaptive trait . selection for a particular level of virulence can happen either at at the level of between - host tradeoffs or as a result of short - sighted within - host competition . this paper describes some simulations which study the effect that modifier genes for changes in mutation rate have on suppressing this short - sighted development of virulence , and investigates the interaction between this and a simplified model of im mune clearance .
much work in qualitative physics involves constructing models of physical systems using functional descriptions such as " flow monotonically increases with pressure . " semiquantitative methods improve model precision by adding numerical envelopes to these monotonic functions . ad hoc methods are normally used to determine these envelopes . this paper describes a systematic method for computing a bounding envelope of a multivariate monotonic function given a stream of data . the derived envelope is computed by determining a simultaneous confidence band for a special neural network which is guaranteed to produce only monotonic functions . by composing these envelopes , more complex systems can be simulated using semiquantitative methods .
in this paper we describe the application of memory - based learning to the problem of prepositional phrase attachment disambiguation . we compare memory - based learning , which stores examples in memory and generalizes by using intelligent similarity metrics , with a number of recently proposed statistical methods that are well suited to large numbers of features . we evaluate our methods on a common benchmark dataset and show that our method compares favorably to previous methods , and is well - suited to incorporating various unconventional representations of word patterns such as value difference metrics and lexical space .
hierarchically structured mixture models are studied in the context of data analysis and inference on neural synaptic transmission characteristics in mammalian , and other , central nervous systems . mixture structures arise due to uncertainties about the stochastic mechanisms governing the responses to electro - chemical stimulation of individual neuro - transmitter release sites at nerve junctions . models attempt to capture scientific features such as the sensitivity of individual synaptic transmission sites to electro - chemical stimuli , and the extent of their electro - chemical responses when stimulated . this is done via suitably structured classes of prior distributions for parameters describing these features . such priors may be structured to permit assessment of currently topical scientific hypotheses about fundamental neural function . posterior analysis is implemented via stochastic simulation . several data analyses are described to illustrate the approach , with resulting neurophysiological insights in some recently generated experimental contexts . further developments and open questions , both neurophysiological and statistical , are noted . research partially supported by the nsf under grants dms - #NUM# , dms - #NUM# and dms - #NUM# . this work represents part of a collaborative project with dr dennis a turner , of duke university medical center and durham va . data was provided by dr turner and by dr howard v wheal of southampton university . a slightly revised version of this paper is published in the journal of the american statistical association ( vol #NUM# , pp #NUM# - #NUM# ) , under the modified title hierarchical mixture models in neurological transmission analysis . the author is the recipient of the #NUM# mitchell prize for " the bayesian analysis of a substantive and concrete problem " based on the work reported in this paper .
the need for software modules performing natural language processing ( nlp ) tasks is growing . these modules should perform efficiently and accurately , while at the same time rapid development is often mandatory . recent work has indicated that machine learning techniques in general , and memory - based learning ( mbl ) in particular , offer the tools to meet both ends . we present examples of modules trained with mbl on three nlp tasks : ( i ) text - to - speech conversion , ( ii ) part - of - speech tagging , and ( iii ) phrase chunking . we demonstrate that the three modules display high generalization accuracy , and argue why mbl is applicable similarly well to a large class of other nlp tasks .
we present a membership query ( i . e . interpolation ) algorithm for exactly identifying the class of read - once formulas over the basis of boolean threshold functions . using a generic transformation from [ angluin , hellerstein , karpin - ski #NUM# ] , this gives an algorithm using membership and equivalence queries for exactly identifying the class of read - once formulas over the basis of boolean threshold functions and negation . we also present a a series of generic transformations that can be used to convert an algorithm in one learning model into an algorithm in a different model .
we study a time series model that can be viewed as a decision tree with markov temporal structure . the model is intractable for exact calculations , thus we utilize variational approximations . we consider three different distributions for the approximation : one in which the markov calculations are performed exactly and the layers of the decision tree are decoupled , one in which the decision tree calculations are performed exactly and the time steps of the markov chain are decoupled , and one in which a viterbi - like assumption is made to pick out a single most likely state sequence . we present simulation results for artificial data and the bach chorales . accepted for oral presentation at nips * #NUM# .
stochastic simulation algorithms such as likelihood weighting often give fast , accurate approximations to posterior probabilities in probabilistic networks , and are the methods of choice for very large networks . unfortunately , the special characteristics of dynamic probabilistic networks ( dpns ) , which are used to represent stochastic temporal processes , mean that standard simulation algorithms perform very poorly . in essence , the simulation trials diverge further and further from reality as the process is observed over time . in this paper , we present simulation algorithms that use the evidence observed at each time step to push the set of trials back towards reality . the first algorithm , " evidence reversal " ( er ) restructures each time slice of the dpn so that the evidence nodes for the slice become ancestors of the state variables . the second algorithm , called " survival of the fittest " sampling ( sof ) , " repopulates " the set of trials at each time step using a stochastic reproduction rate weighted by the likelihood of the evidence according to each trial . we compare the performance of each algorithm with likelihood weighting on the original network , and also investigate the benefits of combining the er and sof methods . the er / sof combination appears to maintain bounded error independent of the number of time steps in the simulation .
simulated annealing search technique where a single trial solution is modified at random . an energy is defined which represents how good the solution is . the goal is to find the best solution by minimising the energy . changes which lead to a lower energy are always accepted ; an increase is probabilistically accepted . the probability is given by exp ( e = k b t ) . where e is the change in energy , k b is a constant and t is the temperature . initially the temperature is high corresponding to a liquid or molten state where large changes are possible and it is progressively reduced using a cooling schedule so allowing smaller changes until the system solidifies at a low energy solution .
a number of efficient learning algorithms achieve exact identification of an unknown function from some class using membership and equivalence queries . using a standard transformation such algorithms can easily be converted to on - line learning algorithms that use membership queries . under such a transformation the number of equivalence queries made by the query algorithm directly corresponds to the number of mistakes made by the on - line algorithm . in this paper we consider several of the natural classes known to be learnable in this setting , and investigate the minimum number of equivalence queries with accompanying counterexamples ( or equivalently the minimum number of mistakes in the on - line model ) that can be made by a learning algorithm that makes a polynomial number of membership queries and uses polynomial computation time . we are able both to reduce the number of equivalence queries used by the previous algorithms and often to prove matching lower bounds . as an example , consider the class of dnf formulas over n variables with at most k = o ( log n ) terms . previously , the algorithm of blum and rudich [ br #NUM# ] provided the best known upper bound of #NUM# o ( k ) log n for the minimum number of equivalence queries needed for exact identification . we greatly improve on this upper bound showing that exactly k counterexamples are needed if the learner knows k a priori and exactly k + #NUM# counterexamples are needed if the learner does not know k a priori . this exactly matches known lower bounds [ bc #NUM# ] . for many of our results we obtain a complete characterization of the tradeoff between the number of membership and equivalence queries needed for exact identification . the classes we consider here are monotone dnf formulas , horn sentences , o ( log n ) - term dnf formulas , read - k sat - j dnf formulas , read - once formulas over various bases , and deterministic finite automata .
we present a learning algorithm for rule - based concept representations called ripple - down rule sets . ripple - down rule sets allow us to deal with the exceptions for each rule separately by introducing exception rules , exception rules for each exception rule etc . up to a constant depth . these local exception rules are in contrast to decision lists , in which the exception rules must be placed into a global ordering of the rules . the localization of exceptions makes it possible to represent concepts that have no decision list representation . on the other hand , decision lists with a constant number of alternations between rules for different classes can be represented by constant depth ripple - down rule sets with only a polynomial increase in size . our algorithm is an occam algorithm for constant depth ripple - down rule sets and , hence , a pac learning algorithm . it is based on repeatedly applying the greedy approximation method for the weighted set cover problem to find good exception rule sets .
we present an algorithm for learning sets of rules that are organized into up to k levels . each level can contain an arbitrary number of rules " if c then l " where l is the class associated to the level and c is a concept from a given class of basic concepts . the rules of higher levels have precedence over the rules of lower levels and can be used to represent exceptions . as basic concepts we can use boolean attributes in the infinite attribute space model , or certain concepts defined in terms of substrings . given a sample of m examples , the algorithm runs in polynomial time and produces a consistent concept representation of size o ( ( log m ) k n k ) , where n is the size of the smallest consistent representation with k levels of rules . this implies that the algorithm learns in the pac model . the algorithm repeatedly applies the greedy heuristics for weighted set cover . the weights are obtained from approximate solutions to previous set cover problems .
in this paper we investigate representational and methodological issues in a attractor network model of the mapping from orthography to semantics based on [ plaut , #NUM# ] . we find that , contrary to psycholinguistic studies , the response time to concrete words ( represented by more #NUM# bits in the output pattern ) is slower than for abstract words . this model also predicts that response times to words in a dense semantic neighborhood will be faster than words which have few semantically similar neighbors in the language . this is conceptually consistent with the neighborhood effect seen in the mapping from orthography to phonology [ seidenberg & mcclelland , #NUM# , plaut et al . , #NUM# ] in that patterns with many neighbors are faster in both pathways , but since there is no regularity in the random mapping used here , it is clear that the cause of this effect is different than that of previous experiments . we also report a rather distressing finding . reaction time in this model is measured by the time it takes the network to settle after being presented with a new input . when the criterion used to determine when the network is settled is changed to include testing of the hidden units , each of the results reported above change the direction of effect abstract words are now slower , as are words in dense semantic neighborhoods . since there are independent reasons to exclude hidden units from the stopping criterion , and this is what is done in common practice , we believe this phenomenon to be of interest mostly to neural network practitioners . however , it does provide some insight into the interaction between the hidden and output units during settling .
case - based reasoning ( cbr ) can be used as a form of " caching " solved problems to speedup later problem solving . using " cached " cases brings additional costs with it due to retrieval time , case adaptation time and also storage space . simply storing all cases will result in a situation in which retrieving and trying to adapt old cases will take more time ( on average ) than not caching at all . this means that caching must be applied selectively to build a case memory that is actually useful . this is a form of the utility problem [ #NUM# , #NUM# ] . the approach taken here is to construct a " cost model " of a system that can be used to predict the effect of changes to the system . in this paper we describe the utility problem associated with " caching " cases and the construction of a " cost model " . we present experimental results that demonstrate that the model can be used to predict the effect of certain changes to the case memory .
a genetic algorithmic ( ga ) approach to vector quantizer design that combines the conventional generalized lloyd algorithm ( gla ) [ #NUM# ] is presented . we refer to this hybrid as the genetic generalized lloyd algorithm ( ggla ) . it works briefly as follows : a finite number of codebooks , called chromosomes , are selected . each codebook undergoes iterative cycles of reproduction . we perform experiments with various alternative design choices using gaussian - markov processes , speech , and image as source data and signal - to - noise ratio ( snr ) as the performance measure . in most cases , the ggla showed performance improvements with respect to the gla . we also compare our results with the zador - gersho formula [ #NUM# , #NUM# ] .
in case - based planning ( cbp ) , previously generated plans are stored as cases in memory and can be reused to solve similar planning problems in the future . cbp can save considerable time over planning from scratch ( generative planning ) , thus offering a potential ( heuristic ) mechanism for handling intractable problems . one drawback of cbp systems has been the need for a highly structured memory that requires significant domain engineering and complex memory indexing schemes to enable efficient case retrieval . in contrast , our cbp system , caper , is based on a massively parallel frame - based ai language and can do extremely fast retrieval of complex cases from a large , unindexed memory . the ability to do fast , frequent retrievals has many advantages : indexing is unnecessary ; very large casebases can be used ; and memory can be probed in numerous alternate ways , allowing more specific retrieval of stored plans that better fit a target problem with less adaptation .
we present an efficient method for the partitioning of rectangular domains into equi - area sub - domains of minimum total perimeter . for a variety of applications in parallel computation , this corresponds to a load - balanced distribution of tasks that minimize interprocessor communication . our method is based on utilizing , to the maximum extent possible , a set of optimal shapes for sub - domains . we prove that for a large class of these problems , we can construct solutions whose relative distance from a computable lower bound converges to zero as the problem size tends to infinity . perix - ga , a genetic algorithm employing this approach , has successfully solved to optimality million - variable instances of the perimeter - minimization problem and for a one - billion - variable problem has generated a solution within #NUM# . #NUM# % of the lower bound . we report on the results of an implementation on a cm - #NUM# supercomputer and make comparisons with other existing codes .
finding the bayesian balance between exploration and exploitation in adaptive optimal control is in general intractable . this paper shows how to compute suboptimal estimates based on a certainty equivalence approximation arising from a form of dual control . this systematizes and extends existing uses of exploration bonuses in reinforcement learning ( sutton , #NUM# ) . the approach has two components : a statistical model of uncertainty in the world and a way of turning this into exploratory behaviour .
this paper deals with nonlinear least - squares problems involving the fitting to data of parameterized analytic functions . for generic regression data , a general result establishes the countability , and under stronger assumptions finiteness , of the set of functions giving rise to critical points of the quadratic loss function . in the special case of what are usually called " single - hidden layer neural networks , " which are built upon the standard sigmoidal activation tanh ( x ) ( or equivalently ( #NUM# + e x ) #NUM# ) , a rough upper bound for this cardinality is provided as well .
this research was funded in part by nsf grant no . iri - #NUM# - #NUM# and in part by onr grant no . n #NUM# - #NUM# - j - #NUM# . we thank john clement for the use of his protocol transcript , james greeno for his contribution to developing our constructive modeling interpretation of it , and ryan tweney for his helpful comments todd w . griffith , nancy j . nersessian , and ashok goel abstract we hypothesize generic models to be central in conceptual change in science . this hypothesis has its origins in two theoretical sources . the first source , constructive modeling , derives from a philosophical theory that synthesizes analyses of historical conceptual changes in science with investigations of reasoning and representation in cognitive psychology . the theory of constructive modeling posits generic mental models as productive in conceptual change . the second source , adaptive modeling , derives from a computational theory of creative design . both theories posit situation independent domain abstractions , i . e . generic models . using a constructive modeling interpretation of the reasoning exhibited in protocols collected by john clement ( #NUM# ) of a problem solving session involving conceptual change , we employ the resources of the theory of adaptive modeling to develop a new computational model , torque . here we describe a piece of our analysis of the protocol to illustrate how our synthesis of the two theories is being used to develop a system for articulating and testing torque . the results of our research show how generic modeling plays a central role in conceptual change . they also demonstrate how such an interdisciplinary synthesis can provide significant insights into scientific reasoning .
this paper discusses the design of neural networks to solve specific problems of adaptive control . in particular , it investigates the influence of typical problems arising in real - world control tasks as well as techniques for their solution that exist in the framework of neurocontrol . based on this investigation , a systematic design method is developed . the method is exemplified for the development of an adaptive force controller for a robot manipulator .
we present the information - theoretic derivation of a learning algorithm that clusters unlabelled data with linear discriminants . in contrast to methods that try to preserve information about the input patterns , we maximize the information gained from observing the output of robust binary discriminators implemented with sigmoid nodes . we derive a local weight adaptation rule via gradient ascent in this objective , demonstrate its dynamics on some simple data sets , relate our approach to previous work and suggest directions in which it may be extended .
this paper presents an asocs ( adaptive self - organizing concurrent system ) model for massively parallel processing of incrementally defined rule systems in such areas as adaptive logic , robotics , logical inference , and dynamic control . an asocs is an adaptive network composed of many simple computing elements operating asynchronously and in parallel . this paper focuses on adaptive algorithm #NUM# ( aa #NUM# ) and details its architecture and learning algorithm . it has advantages over previous asocs models in simplicity , implementability , and cost . an asocs can operate in either a data processing mode or a learning mode . during the data processing mode , an asocs acts as a parallel hardware circuit . in learning mode , rules expressed as boolean conjunctions are incrementally presented to the asocs . all asocs learning algorithms incorporate a new rule in a distributed fashion in a short , bounded time .
this paper presents a method for analyzing coupled time series using markov models in a domain where the state space is immense . to make the parameter estimation tractable , the large state space is represented as the cartesian product of smaller state spaces , a paradigm known as factorial markov models . the transition matrix for this model is represented as a mixture of the transition matrices of the underlying dynamical processes . this formulation is know as mixed memory markov models . using this framework , we analyze the daily exchange rates for five currencies - british pound , canadian dollar , deutsch mark , japanese yen , and swiss franc as measured against the u . s . dollar .
this work explores the use of machine learning methods for extracting knowledge from simulations of complex systems . in particular , we use genetic algorithms to learn rule - based strategies used by autonomous robots . the evaluation of a given strategy may require several executions of a simulation to produce a meaningful estimate of the quality of the strategy . as a consequence , the evaluation of a single individual in the genetic algorithm requires a fairly substantial amount of computation . such a system suggests the sort of large - grained parallelism that is available on a network of workstations . we describe an implementation of a parallel genetic algorithm , and present case studies of the resulting speedup on two robot learning tasks .
most artificial neural networks ( anns ) have a fixed topology during learning , and often suffer from a number of shortcomings as a result . anns that use dynamic topologies have shown ability to overcome many of these problems . adaptive self organizing concurrent systems ( asocs ) are a class of learning models with inherently dynamic topologies . this paper introduces location - independent transformations ( lits ) as a general strategy for implementing learning models that use dynamic topologies efficiently in parallel hardware . a lit creates a set of location - independent nodes , where each node computes its part of the network output independent of other nodes , using local information . this type of transformation allows efficient support for adding and deleting nodes dynamically during learning . in particular , this paper presents the location - independent asocs ( lia ) model as a lit for asocs adaptive algorithm #NUM# . the description of lia gives formal definitions for lia algorithms . because lia implements basic asocs mechanisms , these definitions provide a formal description of basic asocs mechanisms in general , in addition to lia .
hard combinatorial problems in sequencing and scheduling led recently into further research of genetic algorithms . canonical coding of the symmetric tsp can be modified into a coding of the n - job m - machine flowshop problem , which configurates the solution space in a different way . we show that well known genetic operators act intelligently on this coding scheme . they implecitely prefer a subset of solutions which contain the probably best solutions with respect to an objective . we conjecture that every new problem needs a determination of this necessary condition for a genetic algorithm to work , i . e . a proof by experiment . we implemented an asynchronous parallel genetic algorithm on a unix - based computer network . computational results of the new heuristic are discussed .
this paper presents a vlsi implementation of the priority adaptive self - organizing concurrent system ( pasocs ) learning model that is built using a multi - chip module ( mcm ) substrate . many current hardware implementations of neural network learning models are direct implementations of classical neural network structures | a large number of simple computing nodes connected by a dense number of weighted links . pasocs is one of a class of asocs ( adaptive self - organizing concurrent system ) connectionist models whose overall goal is the same as classical neural networks models , but whose functional mechanisms differ significantly . this model has potential application in areas such as pattern recognition , robotics , logical inference , and dynamic control .
the application of adaptive optimization strategies to scheduling in manufacturing systems has recently become a research topic of broad interest . population based approaches to scheduling predominantly treat static data models , whereas real - world scheduling tends to be a dynamic problem . this paper briefly outlines the application of a genetic algorithm to the dynamic job shop problem arising in production scheduling . first we sketch a genetic algorithm which can handle release times of jobs . in a second step a preceding simulation method is used to improve the performance of the algorithm . finally the job shop is regarded as a nondeterministic optimization problem arising from the occurrence of job releases . temporal decomposition leads to a scheduling control that interweaves both simulation in time and genetic search .
neural network pruning methods on the level of individual network parameters ( e . g . connection weights ) can improve generalization . an open problem in the pruning methods known today ( obd , obs , autoprune , epsiprune ) is the selection of the number of parameters to be removed in each pruning step ( pruning strength ) . this paper presents a pruning method lprune that automatically adapts the pruning strength to the evolution of weights and loss of generalization during training . the method requires no algorithm parameter adjustment by the user . the results of extensive experimentation indicate that lprune is often superior to autoprune ( which is superior to obd ) on diagnosis tasks unless severe pruning early in the training process is required . results of statistical significance tests comparing autoprune to the new method lprune as well as to backpropagation with early stopping are given for #NUM# different problems .
case - based problem - solving systems rely on similarity assessment to select stored cases whose solutions are easily adaptable to fit current problems . however , widely - used similarity assessment strategies , such as evaluation of semantic similarity , can be poor predictors of adaptability . as a result , systems may select cases that are difficult or impossible for them to adapt , even when easily adaptable cases are available in memory . this paper presents a new similarity assessment approach which couples similarity judgments directly to a case library containing the system ' s adaptation knowledge . it examines this approach in the context of a case - based planning system that learns both new plans and new adaptations . empirical tests of alternative similarity assessment strategies show that this approach enables better case selection and increases the benefits accrued from learned adaptations .
the case - based reasoning process depends on multiple overlapping knowledge sources , each of which provides an opportunity for learning . exploiting these opportunities requires not only determining the learning mechanisms to use for each individual knowledge source , but also how the different learning mechanisms interact and their combined utility . this paper presents a case study examining the relative contributions and costs involved in learning processes for three different knowledge sources | cases , case adaptation knowledge , and similarity information | in a case - based planner . it demonstrates the importance of interactions between different learning processes and identifies a promising method for integrating multiple learning methods to improve case - based reasoning .
case - based reasoning depends on multiple knowledge sources beyond the case library , including knowledge about case adaptation and criteria for similarity assessment . because hand coding this knowledge accounts for a large part of the knowledge acquisition burden for developing cbr systems , it is appealing to acquire it by learning , and cbr is a promising learning method to apply . this observation suggests developing case - based cbr systems , cbr systems whose components themselves use cbr . however , despite early interest in case - based approaches to cbr , this method has received comparatively little attention . open questions include how case - based components of a cbr system should be designed , the amount of knowledge acquisition effort they require , and their effectiveness . this paper investigates these questions through a case study of issues addressed , methods used , and results achieved by a case - based planning system that uses cbr to guide its case adaptation and similarity assessment . the paper discusses design considerations and presents empirical results that support the usefulness of case - based cbr , that point to potential problems and tradeoffs , and that directly demonstrate the overlapping roles of different cbr knowledge sources . the paper closes with general lessons about case - based cbr and areas for future research .
a linear support vector machine formulation is used to generate a fast , finitely - terminating linear - programming algorithm for discriminating between two massive sets in n - dimensional space , where the number of points can be orders of magnitude larger than n . the algorithm creates a succession of sufficiently small linear programs that separate chunks of the data at a time . the key idea is that a small number of support vectors , corresponding to linear programming constraints with positive dual variables , are carried over between the successive small linear programs , each of which containing a chunk of the data . we prove that this procedure is monotonic and terminates in a finite number of steps at an exact solution that leads to a globally optimal separating plane for the entire dataset . numerical results on fully dense publicly available datasets , numbering #NUM# , #NUM# to #NUM# million points in #NUM# - dimensional space , confirm the theoretical results and demonstrate the ability to handle very large problems .
in #NUM# , sejnowski and rosenberg developed their famous nettalk system for english text - to - speech . this chapter describes a machine learning approach to text - to - speech that builds upon and extends the initial nettalk work . among the many extensions to the nettalk system were the following : a different learning algorithm , a wider input " window " , error - correcting output coding , a right - to - left scan of the word to be pronounced ( with the results of each decision influencing subsequent decisions ) , and the addition of several useful input features . these changes yielded a system that performs much better than the original nettalk system . after training on #NUM# , #NUM# words , the system achieves #NUM# . #NUM# % correct pronunciation of individual phonemes and #NUM# . #NUM# % correct pronunciation of whole words ( where the pronunciation must exactly match the dictionary pronunciation to be correct ) . based on the judgements of three human participants in a blind assessment study , our system was estimated to have a serious error rate of #NUM# . #NUM# % ( on whole words ) compared to an error rate of #NUM# . #NUM# % for the dectalk #NUM# . #NUM# rulebase .
the problem of minimizing the number of misclassified points by a plane , attempting to separate two point sets with intersecting convex hulls in n - dimensional real space , is formulated as a linear program with equilibrium constraints ( lpec ) . this general lpec can be converted to an exact penalty problem with a quadratic objective and linear constraints . a frank - wolfe - type algorithm is proposed for the penalty problem that terminates at a stationary point or a global solution . novel aspects of the approach include : ( i ) a linear complementarity formulation of the step function that " counts " misclassifications , ( ii ) exact penalty formulation without boundedness , nondegeneracy or constraint qualification assumptions , ( iii ) an exact solution extraction from the sequence of minimizers of the penalty function for a finite value of the penalty parameter for the general lpec and an explicitly exact solution for the lpec with uncoupled constraints , and ( iv ) a parametric quadratic programming formulation of the lpec associated with the misclassification minimization problem .
planning by analogical reasoning is a learning method that consists of the storage , retrieval , and replay of planning episodes . planning performance improves with the accumulation and reuse of a library of planning cases . retrieval is driven by domain - dependent similarity metrics based on planning goals and scenarios . in complex situations with multiple goals , retrieval may find multiple past planning cases that are jointly similar to the new planning situation . this paper presents the issues and implications involved in the replay of multiple planning cases , as opposed to a single one . multiple case plan replay involves the adaptation and merging of the annotated derivations of the planning cases . several merge strategies for replay are introduced that can process with various forms of eagerness the differences between the past and new situations and the annotated justifications at the planning cases . in particular , we introduce an effective merging strategy that considers plan step choices especially appropriate for the interleaving of planning and plan execution . we illustrate and discuss the effectiveness of the merging strategies in specific domains .
mixed - initiative planning envisions a framework in which automated and human planners interact to jointly construct plans that satisfy specific objectives . in this paper , we report on our work engineering a robust mixed - initiative planning system . human planners rely strongly on past planning experience to generate new plans . format is a case - based system that supports human planning through the accumulation of user - built plans , query - driven browsing of past plans , and several plan functionality analysis primitives . prodigy / analogy is an automated ai planner that combines generative and case - based planning . stored plans are annotated with plan rationale and reuse involves adaptation driven by this rationale . our system , mi - cbp , integrates format and prodigy / analogy into a real - time message - passing mixed - initiative planning system . the main technical approach consists of allowing the user to specify and link objectives that enable the system to capture and reuse plan rationale . we present mi - cbp and its concrete application to the domain of military force deployment planning . this synergistic system increases the planning efficiency of human planners through automated suggestion of similar past plans and plausible plan modifications .
the primary goal of inductive learning is to generalize well that is , induce a function that accurately produces the correct output for future inputs . hansen and salamon showed that , under certain assumptions , combining the predictions of several separately trained neural networks will improve generalization . one of their key assumptions is that the individual networks should be independent in the errors they produce . in the standard way of performing backpropagation this assumption may be violated , because the standard procedure is to initialize network weights in the region of weight space near the origin . this means that backpropagation ' s gradient - descent search may only reach a small subset of the possible local minima . in this paper we present an approach to initializing neural networks that uses competitive learning to intelligently create networks that are originally located far from the origin of weight space , thereby potentially increasing the set of reachable local minima . we report experiments on two real - world datasets where combinations of networks initialized with our method generalize better than combina tions of networks initialized the traditional way .
we present two algorithms for inducing structural equation models from data . assuming no latent variables , these models have a causal interpretation and their parameters may be estimated by linear multiple regression . our algorithms are comparable with pc [ #NUM# ] and ic [ #NUM# , #NUM# ] , which rely on conditional independence . we present the algorithms and empirical comparisons with pc and ic .
we investigate neural network based approximation methods . these methods depend on the locality of the basis functions . after discussing local and global basis functions , we propose a a multi - resolution hierarchical method . the various resolutions are stored at various levels in a tree . at the root of the tree , a global approximation is kept ; the leafs store the learning samples themselves . intermediate nodes store intermediate representations . in order to find an optimal partitioning of the input space , self - organising maps ( som ' s ) are used . the proposed method has implementational problems reminiscent of those encountered in many - particle simulations . we will investigate the parallel implementation of this method , using parallel hierarchical meth ods for many - particle simulations as a starting point .
today ' s potential users of machine learning technology are faced with the non - trivial problem of choosing , from the large , ever - increasing number of available tools , the one most appropriate for their particular task . to assist the often non - initiated users , it is desirable that this model selection process be automated . using experience from base level learning , researchers have proposed meta - learning as a possible solution . historically , predictive accuracy has been the de facto criterion , with most work in meta - learning focusing on the discovery of rules that match applications to models based on accuracy only . although predictive accuracy is clearly an important criterion , it is also the case that there are a number of other criteria that could , and often ought to , be considered when learning about model selection . this paper presents a number of such criteria and discusses the impact they have on meta - level approaches to model selection .
one approach to invariant object recognition employs a recurrent neural network as an associative memory . in the standard depiction of the network ' s state space , memories of objects are stored as attractive fixed points of the dynamics . i argue for a modification of this picture : if an object has a continuous family of instantiations , it should be represented by a continuous attractor . this idea is illustrated with a network that learns to complete patterns . to perform the task of filling in missing information , the network develops a continuous attractor that models the manifold from which the patterns are drawn . from a statistical viewpoint , the pattern completion task allows a formulation of unsupervised a classic approach to invariant object recognition is to use a recurrent neural network as an associative memory [ #NUM# ] . in spite of the intuitive appeal and biological plausibility of this approach , it has largely been abandoned in practical applications . this paper introduces two new concepts that could help resurrect it : object representation by continuous attractors , and learning attractors by pattern completion . in most models of associative memory , memories are stored as attractive fixed points at discrete locations in state space [ #NUM# ] . discrete attractors may not be appropriate for patterns with continuous variability , like the images of a three - dimensional object from different viewpoints . when the instantiations of an object lie on a continuous pattern manifold , it is more appropriate to represent objects by attractive manifolds of fixed points , or continuous attractors . to make this idea practical , it is important to find methods for learning attractors from examples . a naive method is to train the network to retain examples in short - term memory . this method is deficient because it does not prevent the network from storing spurious fixed points that are unrelated to the examples . a superior method is to train the network to restore examples that have been corrupted , so that it learns to complete patterns by filling in missing information . learning in terms of regression rather than density estimation .
in this paper we consider a problem independent constraint handling mechanism , stepwise adaptation of weights ( saw ) and show its working on graph coloring problems . saw - ing technically belongs to the penalty function based approaches and amounts to modifying the penalty function during the search . we show that it has a twofold benefit . first , it proves to be rather insensitive to its technical parameters , thereby providing a general , problem independent way to handle constrained problems . second , it leads to superior ea performance . in an extensive series of comparative experiments we show that the saw - ing ea outperforms a powerful graph coloring heuristic algorithm , dsatur , on the hardest graph instances and has a linear scale - up behaviour .
recently , several neural algorithms have been introduced for independent component analysis . here we approach the problem from the point of view of a single neuron . first , simple hebbian - like learning rules are introduced for estimating one of the independent components from sphered data . some of the learning rules can be used to estimate an independent component which has a negative kurtosis , and the others estimate a component of positive kurtosis . next , a two - unit system is introduced to estimate an independent component of any kurtosis . the results are then generalized to estimate independent components from non - sphered ( raw ) mixtures . to separate several independent components , a system of several neurons with linear negative feedback is used . the convergence of the learning rules is rigorously proven without any unnecessary hypotheses on the distributions of the independent components .
in this paper we define the task of place learning and describe one approach to this problem . our framework represents distinct places as evidence grids , a probabilistic description of occupancy . place recognition relies on nearest neighbor classification , augmented by a registration process to correct for translational differences between the two grids . the learning mechanism is lazy in that it involves the simple storage of inferred evidence grids . experimental studies with physical and simulated robots suggest that this approach improves place recognition with experience , that it can handle significant sensor noise , that it benefits from improved quality in stored cases , and that it scales well to environments with many distinct places . additional studies suggest that using historical information about the robot ' s path through the environment can actually reduce recognition accuracy . previous researchers have studied evidence grids and place learning , but they have not combined these two powerful concepts , nor have they used systematic experimentation to evaluate their methods ' abilities .
in constructive induction ( ci ) , the learner ' s problem representation is modified as a normal part of the learning process . this may be necessary if the initial representation is inadequate or inappropriate . however , the distinction between constructive and non - constructive methods appears to be highly ambiguous . several conventional definitions of the process of constructive induction appear to include all conceivable learning processes . in this paper i argue that the process of constructive learning should be identified with that of relational learning ( i . e . , i suggest that
when designing a ( deductive ) database , the designer has to decide for each predicate ( or relation ) whether it should be defined extensionally or intensionally , and what the definition should look like . an intelligent system is presented to assist the designer in this task . it starts from an example database in which all predicates are defined extensionally . it then tries to compact the database by transforming extensionally defined predicates into intensionally defined ones . the intelligent system employs techniques from the area of inductive logic programming .
this paper investigates a technique for creating sparsely connected feed - forward neural networks which may be capable of producing networks that have very large input and output layers . the architecture appears to be particularly suited to tasks that involve sparse training data as it is able to take advantage of the sparseness to further reduce training time . some initial results are presented based on tests on the #NUM# bit compression problem .
in this paper we propose a method to calculate the posterior probability of a nondecomposable graphical gaussian model . our proposal is based on a new device to sample from wishart distributions , conditional on the graphical constraints . as a result , our methodology allows bayesian model selection within the whole class of graphical gaussian models , including nondecomposable ones .
for an absorbing markov chain with a reinforcement on each transition , bertsekas ( #NUM# a ) gives a simple example where the function learned by td ( ll ) depends on ll . bertsekas showed that for ll = #NUM# the approximation is optimal with respect to a least - squares error of the value function , and that for ll = #NUM# the approximation obtained by the td method is poor with respect to the same metric . with respect to the error in the values , td ( #NUM# ) approximates the function better than td ( #NUM# ) . however , with respect to the error in the differences in the values , td ( #NUM# ) approximates the function better than td ( #NUM# ) . td ( #NUM# ) is only better than td ( #NUM# ) with respect to the former metric rather than the latter . in addition , direct td ( ll ) weights the errors unequally , while residual gradient methods ( baird , #NUM# , harmon , baird , & klopf , #NUM# ) weight the errors equally . for the case of control , a simple markov decision process is presented for which direct td ( #NUM# ) and residual gradient td ( #NUM# ) both learn the optimal policy , while td ( #NUM# ) learns a suboptimal policy . these results suggest that , for this example , the differences in state values are more significant than the state values themselves , so td ( #NUM# ) is preferable to td ( #NUM# ) .
lazy learning methods provide useful representations and training algorithms for learning about complex phenomena during autonomous adaptive control of complex systems . this paper surveys ways in which locally weighted learning , a type of lazy learning , has been applied by us to control tasks . we explain various forms that control tasks can take , and how this affects the choice of learning paradigm . the discussion section explores the interesting impact that explicitly remembering all previous experiences has on the problem of learning to control .
genetic programming ( gp ) is a variant of genetic algorithms where the data structures handled are trees . this makes gp especially useful for evolving functional relationships or computer programs , as both can be represented as trees . symbolic regression is the determination of a function dependence y = g ( x ) that approximates a set of data points ( x i ; y i ) . in this paper the feasibility of symbolic regression with gp is demonstrated on two examples taken from different domains . furthermore several suggested methods from literature are compared that are intended to improve gp performance and the readability of solutions by taking into account introns or redundancy that occurs in the trees and keeping the size of the trees small . the experiments show that gp is an elegant and useful tool to derive complex functional dependencies on numerical data .
we report on a study of mixture modeling problems arising in the assessment of chemical structure - activity relationships in drug design and discovery . pharmaceutical research laboratories developing test compounds for screening synthesize many related candidate compounds by linking together collections of basic molecular building blocks , known as monomers . these compounds are tested for biological activity , feeding in to screening for further analysis and drug design . the tests also provide data relating compound activity to chemical properties and aspects of the structure of associated monomers , and our focus here is studying such relationships as an aid to future monomer selection . the level of chemical activity of compounds is based on the geometry of chemical binding of test compounds to target binding sites on receptor compounds , but the screening tests are unable to identify binding configurations . hence potentially critical covari - ate information is missing as a natural latent variable . resulting statistical models are then mixed with respect to such missing information , so complicating data analysis and inference . this paper reports on a study of a two - monomer , two - binding site framework and associated data . we build structured mixture models that mix linear regression models , predicting chemical effectiveness , with respect to site - binding selection mechanisms . we discuss aspects of modeling and analysis , including problems and pitfalls , and describe results of analyses of a simulated and real data set . in modeling real data , we are led into critical model extensions that introduce hierarchical random effects components to adequately capture heterogeneities in both the site binding mechanisms and in the resulting levels of effectiveness of compounds once bound . comments on current and potential future directions conclude the report .
we give an analysis of the generalization error of cross validation in terms of two natural measures of the difficulty of the problem under consideration : the approximation rate ( the accuracy to which the target function can be ideally approximated as a function of the number of hypothesis parameters ) , and the estimation rate ( the deviation between the training and generalization errors as a function of the number of hypothesis parameters ) . the approximation rate captures the complexity of the target function with respect to the hypothesis model , and the estimation rate captures the extent to which the hypothesis model suffers from overfitting . using these two measures , we give a rigorous and general bound on the error of cross validation . the bound clearly shows the tradeoffs involved with making fl the fraction of data saved for testing too large or too small . by optimizing the bound with respect to fl , we then argue ( through a combination of formal analysis , plotting , and controlled experimentation ) that the following qualitative properties of cross validation behavior should be quite robust to significant changes in the underlying model selection problem :
model selection [ e . g . , #NUM# ] is considered the problem of choosing a hypothesis language which provides an optimal balance between low empirical error and high structural complexity . in this abstract , we discuss the intuition of a new , very efficient approach to model selection . our approach is inherently bayesian , but instead of using priors on target functions or hypotheses , we talk about priors on error values which leads us to a new mathematical characterization of the expected true error . in the setting of classification learning , a learner is given a sample , drawn according to an unknown distribution of labeled instances , and returns the empirical minimizer ( the hypothesis with the least empirical error ) which has a certain ( unknown ) true error . if this process is carried out repeatedly , the true error of the empirical minimizer will vary from run to run as the empirical minimizer depends on the ( randomly drawn ) sample . this induces a distribution of true errors of empirical minimizers , over the possible samples drawn according to the unknown distribution . if this distribution would be known , one could easily derive the expected true error of the empirical minimizer of a model by integrating over this distribution . this would immediately lead to an optimal model selection algorithm : enumerate the models , calculate the expected error of each model by integrating over the error distribution , and select the model with the least expected error . pac theory and the vc framework provide worst - case bounds on the chance of drawing a sample such that the true error of the minimizer exceeds some " - " worst - case " meaning that they hold for any distribution of instances and any concept in a given class . by contrast , we focus on how to determine this distribution for a fixed , given learning problem ( under some specified assumptions ) . unlike the worst - case bound ( which depends only on the size , or vc - dimension of the hypothesis space ) the actual error distribution depends on the hypothesis space and the unknown distribution of labeled instances itself . however , we can prove that , under a certain assumption of independence of hypotheses , the distribution of true errors and hence the expected true error can be expressed as a function of the distribution of empirical errors of uniformly drawn hypotheses ( which can be thought of as a prior on error values ) . the latter distribution ( which is always one - dimensional ) can be estimated from a fixed - sized initial portion of the training data and a fixed - sized set of randomly drawn hypotheses . this estimate of the distribution now leads us to an estimate of the expected true error of the empirical minimizer of the model which , in turn , leads to a highly efficient model selection algorithm . we study the behavior of this approach in several controlled experiments . our results show that the accuracy of the error estimate is at least comparable to the accuracy of the estimate obtained by #NUM# - fold cross - validation - provided the prior on error values can be estimated using at least #NUM# examples . but while #NUM# - cv requires ten invocations of the learner per model , the time which our algorithm requires to assess each model is constant in the size of the model . we also study the robustness of our algorithm against violations of our independence assumptions . we can observe a bias in our predictions when the hypotheses space is of size four or less . when the hypothesis space is of size #NUM# or more , the dependencies are so diluted that the violations of our assumptions are negligible and do not incur a significant error .
in the area of inductive learning , generalization is a main operation , and the usual definition of induction is based on logical implication . recently there has been a rising interest in clausal representation of knowledge in machine learning . almost all inductive learning systems that perform generalization of clauses use the relation - subsumption instead of implication . the main reason is that there is a well - known and simple technique to compute least general generalizations under - subsumption , but not under implication . however generalization under - subsumption is inappropriate for learning recursive clauses , which is a crucial problem since recursion is the basic program structure of logic programs . we note that implication between clauses is undecidable , and we therefore introduce a stronger form of implication , called t - implication , which is decidable between clauses . we show that for every finite set of clauses there exists a least general generalization under t - implication . we describe a technique to reduce generalizations under implication of a clause to generalizations under - subsumption of what we call an expansion of the original clause . moreover we show that for every non - tautological clause there exists a t - complete expansion , which means that every generalization under t - implication of the clause is reduced to a generalization under - subsumption of the expansion .
this paper argues that bayesian probability theory is a general method for machine learning . from two well - founded axioms , the theory is capable of accomplishing learning tasks that are incremental or non - incremental , supervised or unsupervised . it can learn from different types of data , regardless of whether they are noisy or perfect , independent facts or behaviors of an unknown machine . these capabilities are ( partially ) demonstrated in the paper through the uniform application of the theory to two typical types of machine learning : incremental concept learning and unsupervised data classification . the generality of the theory suggests that the process of learning may not have so many different " types " as currently held , and the method that is the oldest may be the best after all .
this paper focuses on a bias variance decomposition analysis of a local learning algorithm , the nearest neighbor classifier , that has been extended with error correcting output codes . this extended algorithm often considerably reduces the #NUM# - #NUM# ( i . e . , classification ) error in comparison with nearest neighbor ( ricci & aha , #NUM# ) . the analysis presented here reveals that this performance improvement is obtained by drastically reducing bias at the cost of increasing variance . we also show that , even in classification problems with few classes ( m #NUM# ) , extending the codeword length beyond the limit that assures column separation yields an error reduction . this error reduction is not only in the variance , which is due to the voting mechanism used for error - correcting output codes , but also in the bias .
we have integrated the distributed search of genetic programming based systems with collective memory to form a collective adaptation search method . such a system significantly improves search as problem complexity is increased . however , there is still considerable scope for improvement . in collective adaptation , search agents gather knowledge of their environment and deposit it in a central information repository . process agents are then able to manipulate that focused knowledge , exploiting the exploration of the search agents . we examine the utility of increasing the capabilities of the centralized pro cess agents .
the document presents an approach to judging relevance of retrieved information based on a novel approach to similarity assessment . contrary to other systems , we define relevance measures ( context in similarity ) at query time . this is necessary if since without a context in similarity one cannot guarantee that similar items will also be relevant .
this paper presents a self - improving reactive control system for autonomous robotic navigation . the navigation module uses a schema - based reactive control system to perform the navigation task . the learning module combines case - based reasoning and reinforcement learning to continuously tune the navigation system through experience . the case - based reasoning component perceives and characterizes the system ' s environment , retrieves an appropriate case , and uses the recommendations of the case to tune the parameters of the reactive control system . the reinforcement learning component refines the content of the cases based on the current experience . together , the learning components perform on - line adaptation , resulting in improved performance as the reactive control system tunes itself to the environment , as well as on - line learning , resulting in an improved library of cases that capture environmental regularities necessary to perform on - line adaptation . the system is extensively evaluated through simulation studies using several performance metrics and system configurations .
a model for on - site learning is presented . the system learns by querying " hard " patterns while classifying " easy " ones . this model is related to query - based filtering methods , but takes into account that in addition to labelling , filtering through the data has a cost . a few simple policies are introduced and analyzed for a simple problem ( #NUM# d high low game ) . in addition the query - by - committee algorithm ( seung et . al ) is suggested as a good approximator of the model space for real - world domains . results using this algorithm on a synthesized problem and a real - world ocr task using both a backpropagation network and a nearest neighbor classifier show that an on - site learner can perform as well as a classifier trained off - site , while achieving significant cost reduction .
the standard method of obtaining a response in tree - based genetic programming is to take the value returned by the root node . in non - tree representations , alternate methods have been explored . one alternative is to treat a specific location in indexed memory as the response value when the program terminates . the purpose of this paper is to explore the applicability of this technique to tree - structured programs and to explore the intron effects that these studies bring to light . this paper ' s experimental results support the finding that this memory - based program response technique is an improvement for some , but not all , problems . in addition , this paper ' s experimental results support the finding that , contrary to past research and speculation , the addition or even facilitation of introns can seriously degrade the search performance of genetic programming .
we discuss the implications of holte ' s recently - published article , which demonstrated that on the most commonly used data very simple classification rules are almost as accurate as decision trees produced by quinlan ' s c #NUM# . #NUM# . we consider , in particular , what is the significance of holte ' s results for the future of top - down induction of decision trees . to an extent , holte questioned the sense of further research on multilevel decision tree learning . we go in detail through all the parts of holte ' s study . we try to put the results into perspective . we argue that the ( in absolute terms ) small difference in accuracy between #NUM# r and c #NUM# . #NUM# that was witnessed by holte is still significant . we claim that c #NUM# . #NUM# possesses additional accuracy - related advantages over #NUM# r . in addition we discuss the representativeness of the databases used by holte . we compare empirically the optimal accuracies of multilevel and one - level decision trees and observe some significant differences . we point out several deficien cies of limited - complexity classifiers .
we describe an approach to grapheme - to - phoneme conversion which is both language - independent and data - oriented . given a set of examples ( spelling words with their associated phonetic representation ) in a language , a grapheme - to - phoneme conversion system is automatically produced for that language which takes as its input the spelling of words , and produces as its output the phonetic transcription according to the rules implicit in the training data . we describe the design of the system , and compare its performance to knowledge - based and alternative data - oriented approaches .
no finite sample is sufficient to determine the density , and therefore the entropy , of a signal directly . some assumption about either the functional form of the density or about its smoothness is necessary . both amount to a prior over the space of possible density functions . by far the most common approach is to assume that the density has a parametric form . by contrast we derive a differential learning rule called emma that optimizes entropy by way of kernel density estimation . entropy and its derivative can then be calculated by sampling from this density estimate . the resulting parameter update rule is surprisingly simple and efficient . we will describe two real - world applications that can be solved efficiently and reliably using emma . in the first application emma is used to align #NUM# d models to complex natural images . in the second application emma is used to detect and correct corruption in magnetic resonance images ( mri ) . both applications are beyond the scope of existing parametric entropy models .
a satisficing search problem consists of a set of probabilistic experiments to be performed in some order , without repetitions , until a satisfying configuration of successes and failures has been reached . the cost of performing the experiments depends on the order chosen . earlier work has concentrated on finding optimal search strategies in special cases of this model , such as search trees and and - or graphs , when the cost function and the success probabilities for the experiments are given . in contrast , we study the complexity of " learning " an approximately optimal search strategy when some of the success probabilities are not known at the outset . working in the fully general model , we show that if n is the number of unknown probabilities , and c is the maximum cost of performing all the experiments , then
we present a method for calculating phase diagrams for the high - dimensional variant of the self - organizing map ( som ) . the method requires only an ansatz for the tesselation of the data space induced by the map , not for the explicit state of the map . using this method we analyze two recently proposed models for the development of orientation and ocular dominance column maps . the phase transition condition for the orientation map turns out to be of different form than of the corresponding low - dimensional map .
we study the process of multi - agent reinforcement learning in the context of load balancing in a distributed system , without use of either central coordination or explicit communication . we first define a precise framework in which to study adaptive load balancing , important features of which are its stochastic nature and the purely local information available to individual agents . given this framework , we show illuminating results on the interplay between basic adaptive behavior parameters and their effect on system efficiency . we then investigate the properties of adaptive load balancing in heterogeneous populations , and address the issue of exploration vs . exploitation in that context . finally , we show that naive use of communication may not improve , and might even harm system efficiency .
efficient stochastic source coding and an application to a bayesian network source model . the computer journal #NUM# , #NUM# - #NUM# . in this paper , we introduce a new algorithm called " bits - back coding " that makes stochastic source codes efficient . for a given one - to - many source code , we show that this algorithm can actually be more efficient than the algorithm that always picks the shortest codeword . optimal efficiency is achieved when codewords are chosen according to the boltzmann distribution based on the codeword lengths . it turns out that a commonly used technique for determining parameters | maximum likelihood estimation | actually minimizes the bits - back coding cost when codewords are chosen according to the boltzmann distribution . a tractable approximation to maximum likelihood estimation | the generalized expectation maximization algorithm | minimizes the bits - back coding cost . after presenting a binary bayesian network model that assigns exponentially many codewords to each symbol , we show how a tractable approximation to the boltzmann distribution can be used for bits - back coding . we illustrate the performance of bits - back coding using using nonsynthetic data with a binary bayesian network source model that produces #NUM# #NUM# possible codewords for each input symbol . the rate for bits - back coding is nearly one half of that obtained by picking the shortest codeword for each symbol .
agents that learn about other agents and can exploit this information possess a distinct advantage in competitive situations . games provide stylized adversarial environments to study agent learning strategies . researchers have developed game playing programs that learn to play better from experience . we have developed a learning program that does not learn to play better , but learns to identify and exploit the weaknesses of a particular opponent by repeatedly playing it over several games . we propose a scheme for learning opponent action probabilities and a utility maximization framework that exploits this learned opponent model . we show that the proposed expected utility maximization strategy generalizes the traditional maximin strategy , and allows players to benefit by taking calculated risks that are avoided by the max - imin strategy . experiments in the popular board game of connect - #NUM# show that a learning player consistently outperforms a non - learning player when pitted against another automated player using a weaker heuristic . though our proposed mechanism does not improve the skill level of a computer player , it does improve its ability to play more effectively against a weaker opponent .
many real world learning problems are best characterized by an interaction of multiple independent causes or factors . discovering such causal structure from the data is the focus of this paper . based on zemel and hinton ' s cooperative vector quantizer ( cvq ) architecture , an unsupervised learning algorithm is derived from the expectation - maximization ( em ) framework . due to the combinatorial nature of the data generation process , the exact e - step is computationally intractable . two alternative methods for computing the e - step are proposed gibbs sampling and mean - field approximation , and some promising empirical results are presented .
this paper deals with the problem of blind identification and source separation which consists of estimation of the mixing matrix and / or the separation of a mixture of stochastically independent sources without a priori knowledge on the mixing matrix . the method we propose here estimates the mixture matrix by a recurrent input - output ( io ) identification using as inputs a nonlinear transformation of the estimated sources . herein , the nonlinear transformation ( distortion ) consists in constraining the modulus of the inputs of the io - identification device to be a constant . in contrast to other existing approaches , the covariance of the additive noise do not need to be modeled and can be estimated as a regular parameter if needed . the proposed approach is implemented using multi - layer neural networks in order to improve performance of separation . new associated on - line un - supervised adaptive learning rules are also developed . the effectiveness of the proposed method is illustrated by some computer simulations .
source separation consists in recovering a set of n independent signals from m n observed instantaneous mixtures of these signals , possibly corrupted by additive noise . many source separation algorithms use second order information in a whitening operation which reduces the non trivial part of the separation to determining a unitary matrix . most of them further show a kind of invariance property which can be exploited to predict some general results about their performance . our first contribution is to exhibit a lower bound to the performance in terms of accuracy of the separation . this bound is independent of the algorithm and , in the i . i . d . case , of the distribution of the source signals . second , we show that the performance of invariant algorithms depends on the mixing matrix and on the noise level in a specific way . a consequence is that at low noise levels , the performance does not depend on the mixture but only on the distribution of the sources , via a function which is characteristic of the given source separation algorithm .
in this paper a neural network approach for reconstruction of natural highly correlated images from linear ( additive ) mixture of them is proposed . a multi - layer architecture with local on - line learning rules is developed to solve the problem of blind separation of sources . the main motivation for using a multi - layer network instead of a single - layer one is to improve the performance and robustness of separation , while applying a very simple local learning rule , which is biologically plausible . moreover such architecture with on - chip learning is relatively easy implementable using vlsi electronic circuits . furthermore it enables the extraction of source signals sequentially one after the other , starting from the strongest signal and finishing with the weakest one . the experimental part focuses on separating highly correlated human faces from mixture of them , with additive noise and under unknown number of sources .
we consider on - line algorithms for predicting binary or continuous - valued outcomes , when the algorithm has available the predictions made by n experts . for a sequence of trials , we compute total losses for both the algorithm and the experts under a loss function . at the end of the trial sequence , we compare the total loss of the algorithm to the total loss of the best expert , i . e . , the expert with the least loss on the particular trial sequence . vovk has introduced a simple algorithm for this prediction problem and proved that for a large class of loss functions , with binary outcomes the total loss of the algorithm exceeds the total loss of the best expert at most by the amount c ln n , where c is a constant determined by the loss function . this upper bound does not depend on any assumptions on how the experts ' predictions or the outcomes are generated , and the trial sequence can be arbitrarily long . we give a straightforward alternative method for finding the correct value c and show by a lower bound that for this value of c , the upper bound is asymptotically tight . the lower bound is based on a probabilistic adversary argument . the class of loss functions for which the c ln n upper bound holds includes the square loss , the logarithmic loss , and the hellinger loss . we also consider another class of loss functions , including the absolute loss , for which we have an lower bound , where ` is the number of trials . we show that for the square and logarithmic loss functions , vovk ' s algorithm achieves the same worst - case upper bounds with continuous - valued outcomes as with binary outcomes . for the absolute loss , we show how bounds earlier achieved for binary outcomes can be achieved with continuous - valued outcomes using a slightly more complicated algorithm .
when dealing with the classification problems , current ilp systems often lag behind state - of - the - art attributional learners . part of the blame can be ascribed to a much larger hypothesis space which , therefore , cannot be as thoroughly explored . however , sometimes it is due to the fact that ilp systems do not take into account the probabilistic aspects of hypotheses when classifying unseen examples . this paper proposes just that . we developed a naive bayesian classifier within our ilp - r first order learner . the learner itself uses a clever relief based heuristic which is able to detect strong dependencies within the literal space when such dependencies exist . we conducted a series of experiments on artificial and real - world data sets . the results show that the combination of ilp - r together with the naive bayesian classifier sometimes significantly improves the classification of unseen instances as measured by both classification accuracy and average information score .
a frequently observed difficulty in the application of genetic algorithms to the domain of optimization arises from premature convergence . in order to preserve genotype diversity we develop a new model of auto - adaptive behavior for individuals . in this model a population member is an active individual that assumes social - like behavior patterns . different individuals living in the same population can assume different patterns . by moving in a hierarchy of " social states " individuals change their behavior . changes of social state are controlled by arguments of plausibility . these arguments are implemented as a rule set for a massively - parallel genetic algorithm . computational experiments on #NUM# large - scale job shop benchmark problems show that the results of the new approach dominate the ordinary genetic algorithm significantly .
proben #NUM# is a collection of problems for neural network learning in the realm of pattern classification and function approximation plus a set of rules and conventions for carrying out benchmark tests with these or similar problems . proben #NUM# contains #NUM# data sets from #NUM# different domains . all datasets represent realistic problems which could be called diagnosis tasks and all but one consist of real world data . the datasets are all presented in the same simple format , using an attribute representation that can directly be used for neural network training . along with the datasets , proben #NUM# defines a set of rules for how to conduct and how to document neural network benchmarking . the purpose of the problem and rule collection is to give researchers easy access to data for the evaluation of their algorithms and networks and to make direct comparison of the published results feasible . this report describes the datasets and the benchmarking rules . it also gives some basic performance measures indicating the difficulty of the various problems . these measures can be used as baselines for comparison .
this paper presents neurochess , a program which learns to play chess from the final outcome of games . neurochess learns chess board evaluation functions , represented by artificial neural networks . it integrates inductive neural network learning , temporal differencing , and a variant of explanation - based learning . performance results illustrate some of the strengths and weaknesses of this approach .
an important factor that plays a major role in determining the performances of a cbr system is the complexity and the accuracy of the case retrieval phase . both flat memory and inductive approaches suffer from serious drawbacks . in the first approach , the search time becomes considerable when dealing with large scale memory base , while in the second one the modifications of the case memory becomes very complex because of its sophisticated architecture . in this paper , we show how we can construct a simple efficient indexing system structure . we construct a case hierarchy with two levels of memory : the lower level contains cases organised into groups of similar cases , while the upper level contains prototypes , each of which represents one group of cases . the construction of prototypes is made by using an incremental prototype - based network . this upper level parallel memory is used as an indexing system during the retrieval phase .
developing the ability to recognize a landmark from a visual image of a robot ' s current location is a fundamental problem in robotics . we consider the problem of pac - learning the concept class of geometric patterns where the target geometric pattern is a configuration of k points on the real line . each instance is a configuration of n points on the real line , where it is labeled according to whether or not it visually resembles the target pattern . to capture the notion of visual resemblance we use the hausdorff metric . informally , two geometric patterns p and q resemble each other under the hausdorff metric , if every point on one pattern is " close " to some point on the other pattern . we relate the concept class of geometric patterns to the landmark recognition problem and then present a polynomial - time algorithm that pac - learns the class of one - dimensional geometric patterns . we also present some experimental results on how our algorithm performs .
the concept of measure functions for generalization performance is suggested . this concept provides an alternative way of selecting and evaluating learned models ( classifiers ) . in addition , it makes it possible to state a learning problem as a computational problem . the the known prior ( meta - ) knowledge about the problem domain is captured in a measure function that , to each possible combination of a training set and a classifier , assigns a value describing how good the classifier is . the computational problem is then to find a classifier maximizing the measure function . we argue that measure functions are of great value for practical applications . besides of being a tool for model selection , they : ( i ) force us to make explicit the relevant prior knowledge about the learning problem at hand , ( ii ) provide a deeper understanding of existing algorithms , and ( iii ) help us in the construction of problem - specific algorithms . we illustrate the last point by suggesting a novel algorithm based on incremental search for a classifier that optimizes a given measure function .
induced decision trees are an extensively - researched solution to classification tasks . for many practical tasks , the trees produced by tree - generation algorithms are not comprehensible to users due to their size and complexity . although many tree induction algorithms have been shown to produce simpler , more comprehensible trees ( or data structures derived from trees ) with good classification accuracy , tree simplification has usually been of secondary concern relative to accuracy and no attempt has been made to survey the literature from the perspective of simplification . we present a framework that organizes the approaches to tree simplification and summarize and critique the approaches within this framework . the purpose of this survey is to provide researchers and practitioners with a concise overview of tree - simplification approaches and insight into their relative capabilities . in our final discussion , we briefly describe some empirical findings and discuss the application of tree induction algorithms to case retrieval in case - based reasoning systems .
in this paper , we propose to monitor a markov chain sampler using the cusum path plot of a chosen #NUM# - dimensional summary statistic . we argue that the cusum path plot can bring out , more effectively than the sequential plot , those aspects of a markov sampler which tell the user how quickly or slowly the sampler is moving around in its sample space , in the direction of the summary statistic . the proposal is then illustrated in four examples which represent situations where the cusum path plot works well and not well . moreover , a rigorous analysis is given for one of the examples . we conclude that the cusum path plot is an effective tool for convergence diagnostics of a markov sampler and for comparing different markov samplers .
this paper gives precise , easy to compute bounds on the convergence time of the gibbs sampler used in bayesian image reconstruction . for sampling from the gibbs distribution both with and without the presence of an external field , bounds that are n #NUM# in the number of pixels are obtained , with a proportionality constant that is easy to calculate . some key words : bayesian image restoration ; convergence ; gibbs sampler ; ising model ; markov chain monte carlo .
we present methods for coupling hidden markov models ( hmms ) to model systems of multiple interacting processes . the resulting models have multiple state variables that are temporally coupled via matrices of conditional probabilities . we introduce a deterministic o ( t ( cn ) #NUM# ) approximation for maximum a posterior ( map ) state estimation which enables fast classification and parameter estimation via expectation maximization . an " n - heads " dynamic programming algorithm samples from the highest probability paths through a compact state trellis , minimizing an upper bound on the cross entropy with the full ( combinatoric ) dynamic programming problem . the complexity is o ( t ( cn ) #NUM# ) for c chains of n states apiece observing t data points , compared with o ( t n #NUM# c ) for naive ( cartesian product ) , exact ( state clustering ) , and stochastic ( monte carlo ) methods applied to the same inference problem . in several experiments examining training time , model likelihoods , classification accuracy , and robustness to initial conditions , coupled hmms compared favorably with conventional hmms and with energy - based approaches to coupled inference chains . we demonstrate and compare these algorithms on synthetic and real data , including interpretation of video .
the paper describes bayesian analysis for agricultural field experiments , a topic that has received very little previous attention , despite a vast frequentist literature . adoption of the bayesian paradigm simplifies the interpretation of the results , especially in ranking and selection . also , complex formulations can be analyzed with comparative ease , using markov chain monte carlo methods . a key ingredient in the approach is the need for spatial representations of the unobserved fertility patterns . this is discussed in detail . problems caused by outliers and by jumps in fertility are tackled via hierarchical - t formulations that may find use in other contexts . the paper includes three analyses of variety trials for yield and one example involving binary data ; none is entirely straightforward . some comparisons with frequentist analyses are made . the datasets are available at url .
we show in this paper that continuous state space markov chains can be rigorously discretized into finite markov chains . the idea is to subsample the continuous chain at renewal times related to small sets which control the discretization . once a finite markov chain is derived from the mcmc output , general convergence properties on finite state spaces can be exploited for convergence assessment in several directions . our choice is based on a divergence criterion derived from kemeny and snell ( #NUM# ) , which is first evaluated on parallel chains with a stopping time , and then implemented , more efficiently , on two parallel chains only , using birkhoff ' s pointwise ergodic theorem for stopping rules . the performance of this criterion is illustrated on three standard examples .
markov chain monte carlo ( mcmc ) samplers have proved remarkably popular as tools for bayesian computation . however , problems can arise in their application when the density of interest is high dimensional and strongly correlated . in these circumstances the sampler may be slow to traverse the state space and mixing is poor . in this article we offer a partial solution to this problem . the state space of the markov chain is augmented to accommodate multiple chains in parallel . updates to individual chains are based around a genetic style crossover operator acting on ` parent ' states drawn from the population of chains . this process makes efficient use of gradient information implicitly encoded within the distribution of states across the population . empirical studies support the claim that the crossover operator acting on a parallel population of chains improves mixing . this is illustrated with an example of sampling a high dimensional posterior probability density from a complex predictive model . by adopting a latent variable approach the methodology is extended to deal with variable selection and model averaging in high dimensions . this is illustrated with an example of knot selection for a spline interpolant .
we describe variational approximation methods for efficient probabilistic reasoning , applying these methods to the problem of diagnostic inference in the qmr - dt database . the qmr - dt database is a large - scale belief network based on statistical and expert knowledge in internal medicine . the size and complexity of this network render exact probabilistic diagnosis infeasible for all but a small set of cases . this has hindered the development of the qmr - dt network as a practical diagnostic tool and has hindered researchers from exploring and critiquing the diagnostic behavior of qmr . in this paper we describe how variational approximation methods can be applied to the qmr network , resulting in fast diagnostic inference . we evaluate the accuracy of our methods on a set of standard diagnostic cases and compare to stochastic sampling methods .
the effects of a neural network ' s topology on its performance are well known , yet the question of finding optimal configurations automatically remains largely open . this paper proposes a solution to this problem for rbf networks . a self - optimising approach , driven by an evolutionary strategy , is taken . the algorithm uses output information and a computationally efficient approximation of rbf networks to optimise the k - means clustering process by co - evolving the two determinant parameters of the network ' s layout the number of centroids and the centroids ' positions . empirical results demonstrate promise .
this paper describes a hybrid methodology that integrates genetic algorithms and decision tree learning in order to evolve useful subsets of discriminatory features for recognizing complex visual concepts . a genetic algorithm ( ga ) is used to search the space of all possible subsets of a large set of candidate discrimination features . candidate feature subsets are evaluated by using c #NUM# . #NUM# , a decision - tree learning algorithm , to produce a decision tree based on the given features using a limited amount of training data . the classification performance of the resulting decision tree on unseen testing data is used as the fitness of the underlying feature subset . experimental results are presented to show how increasing the amount of learning significantly improves feature set evolution for difficult visual recognition problems involving satellite and facial image data . in addition , we also report on the extent to which other more subtle aspects of the baldwin effect are exhibited by the system .
in this paper we examine the behavior of a human - computer system for crisis response . as one instance of crisis management , we describe the task of responding to spills and fires involving hazardous materials . we then describe inca , an intelligent assistant for planning and scheduling in this domain , and its relation to human users . we focus on inca ' s strategy of retrieving a case from a case library , seeding the initial schedule , and then helping the user adapt this seed . we also present three hypotheses about the behavior of this mixed - initiative system and some experiments designed to test them . the results suggest that our approach leads to faster response development than user - generated or automatically - generated schedules but without sacrificing solution quality .
given an adequate simulation model of the task environment and payoff function that measures the quality of partially successful plans , competition - based heuristics such as genetic algorithms can develop high performance reactive rules for interesting sequential decision tasks . we have previously described an implemented system , called samuel , for learning reactive plans and have shown that the system can successfully learn rules for a laboratory scale tactical problem . in this paper , we describe a method for deriving explanations to justify the success of such empirically derived rule sets . the method consists of inferring plausible subgoals and then explaining how the reactive rules trigger a sequence of actions ( i . e . , a stra tegy ) to satisfy the subgoals .
machine learning can be a most valuable tool for improving the flexibility and efficiency of robot applications . many approaches to applying machine learning to robotics are known . some approaches enhance the robot ' s high - level processing , the planning capabilities . other approaches enhance the low - level processing , the control of basic actions . in contrast , the approach presented in this paper uses machine learning for enhancing the link between the low - level representations of sensing and action and the high - level representation of planning . the aim is to facilitate the communication between the robot and the human user . a hierarchy of concepts is learned from route records of a mobile robot . perception and action are combined at every level , i . e . , the concepts are perceptually anchored . the relational learning algorithm grdt has been developed which completely searches in a hypothesis space , that is restricted by rule schemata , which the user defines in terms of grammars .
we motivate the use of convergence diagnostic techniques for markov chain monte carlo algorithms and review various methods proposed in the mcmc literature . a common notation is established and each method is discussed with particular emphasis on implementational issues and possible extensions . the methods are compared in terms of their interpretability and applicability and recommendations are provided for particular classes of problems .
covariance information can help an algorithm search for predictive causal models and estimate the strengths of causal relationships . this information should not be discarded after conditional independence constraints are identified , as is usual in contemporary causal induction algorithms . our fbd algorithm combines covariance information with an effective heuristic to build predictive causal models . we demonstrate that fbd is accurate and efficient . in one experiment we assess fbd ' s ability to find the best predictors for variables ; in another we compare its performance , using many measures , with pearl and verma ' s ic algorithm . and although fbd is based on multiple linear regression , we cite evidence that it performs well on problems that are very difficult for regression algorithms .
the problem of learning decision rules for sequential tasks is addressed , focusing on the problem of learning tactical plans from a simple flight simulator where a plane must avoid a missile . the learning method relies on the notion of competition and employs genetic algorithms to search the space of decision policies . experiments are presented that address issues arising from differences between the simulation model on which learning occurs and the target environment on which the decision rules are ultimately tested . specifically , either the model or the target environment may contain noise . these experiments examine the effect of learning tactical plans without noise and then testing the plans in a noisy environment , and the effect of learning plans in a noisy simulator and then testing the plans in a noise - free environment . empirical results show that , while best result are obtained when the training model closely matches the target environment , using a training environment that is more noisy than the target environment is better than using using a training environment that has less noise than the target environment .
the inductive learning problem consists of learning a concept given examples and non - examples of the concept . to perform this learning task , inductive learning algorithms bias their learning method . here we discuss biasing the learning method to use previously learned concepts from the same domain . these learned concepts highlight useful information for other concepts in the domain . we describe a transference bias and present m - focl , a horn clause relational learning algorithm , that utilizes this bias to learn multiple concepts . we provide preliminary empirical evaluation to show the effects of biasing previous information on noise - free and noisy data .
choosing the architecture of a neural network is one of the most important problems in making neural networks practically useful , but accounts of applications usually sweep these details under the carpet . how many hidden units are needed ? should weight decay be used , and if so how much ? what type of output units should be chosen ? and so on . we address these issues within the framework of statistical theory for model this paper is principally concerned with architecture selection issues for feed - forward neural networks ( also known as multi - layer perceptrons ) . many of the same issues arise in selecting radial basis function networks , recurrent networks and more widely . these problems occur in a much wider context within statistics , and applied statisticians have been selecting and combining models for decades . two recent discussions are [ #NUM# , #NUM# ] . references [ #NUM# , #NUM# , #NUM# , #NUM# ] discuss neural networks from a statistical perspective . choice , which provides a number of workable approximate answers .
we propose a model - theoretic definition of causation , and show that , contrary to common folklore , genuine causal influences can be distinguished from spurious covari - ations following standard norms of inductive reasoning . we also establish a complete characterization of the conditions under which such a distinction is possible . finally , we provide a proof - theoretical procedure for inductive causation and show that , for a large class of data and structures , effective algorithms exist that uncover the direction of causal influences as defined above .
automated decision making is often complicated by the complexity of the knowledge involved . much of this complexity arises from the context - sensitive variations of the underlying phenomena . we propose a framework for representing descriptive , context - sensitive knowledge . our approach attempts to integrate categorical and uncertain knowledge in a network formalism . this paper outlines the basic representation constructs , examines their expressiveness and efficiency , and discusses the potential applications of the framework .
we discuss a number of methods for estimating the standard error of predicted values from a multi - layer perceptron . these methods include the delta method based on the hessian , bootstrap estimators , and the " sandwich " estimator . the methods are described and compared in a number of examples . we find that the bootstrap methods perform best , partly because they capture variability due to the choice of starting weights .
discrete mixtures of normal distributions are widely used in modeling amplitude fluctuations of electrical potentials at synapses of human , and other animal nervous systems . the usual framework has independent data values y j arising as y j = j + x n #NUM# + j where the means j come from some discrete prior g ( ) and the unknown x n #NUM# + j ' s and observed x j ; j = #NUM# ; : : : ; n #NUM# are gaussian noise terms . a practically important development of the associated statistical methods is the issue of non - normality of the noise terms , often the norm rather than the exception in the neurological context . we have recently developed models , based on convolutions of dirichlet process mixtures , for such problems . explicitly , we model the noise data values x j as arising from a dirich - let process mixture of normals , in addition to modeling the location prior g ( ) as a dirichlet process itself . this induces a dirichlet mixture of mixtures of normals , whose analysis may be developed using gibbs sampling techniques . we discuss these models and their analysis , and illustrate in the context of neurological response analysis .
this paper overviews the aa #NUM# ( adaptive algorithm #NUM# ) model of asocs the ( adaptive self - organizing concurrent systems ) approach . it also presents promising empirical generalization results of aa #NUM# with actual data . aa #NUM# is a topologically dynamic network which grows to fit the problem being learned . aa #NUM# generalizes in a self - organizing fashion to a network which seeks to find features which discriminate between concepts . convergence to a training set is both guaranteed and bounded linearly in time .
this communication deals with the source separation problem which consists in the separation of a noisy mixture of independent sources without a priori knowledge of the mixture coefficients . in this paper , we consider the maximum likelihood ( ml ) approach for discrete source signals with known probability distributions . an important feature of the ml approach in gaussian noise is that the covariance matrix of the additive noise can be treated as a parameter . hence , it is not necessary to know or to model the spatial structure of the noise . another striking feature offered in the case of discrete sources is that , under mild assumptions , it is possible to separate more sources than sensors . in this paper , we consider maximization of the likelihood via the expectation - maximization ( em ) algorithm .
if a robust statistical model has been developed to classify the ` ` health ' ' of a system , a well - known taylor series approximation technique forms the basis of a diagnostic / recovery procedure that can be initiated when the system ' s health degrades or fails altogether . this procedure determines a ranked set of probable causes for the degraded health state , which can be used as a prioritized checklist for isolating system anomalies and quantifying corrective action . the diagnostic / recovery procedure is applicable to any classifier known to be robust ; it can be applied to both neural network and traditional parametric pattern classifiers generated by a supervised learning procedure in which an empirical risk / benefit measure is optimized . we describe the procedure mathematically and demonstrate its ability to detect and diagnose the cause ( s ) of faults in nasa ' s deep space communications complex at goldstone , california .
case combination is a difficult problem in case based reasoning , as sub - cases often exhibit conflicts when merged together . in our previous work we formalized case combination by representing each case as a constraint satisfaction problem , and used the minimum conflicts algorithm to systematically synthesize the global solution . however , we also found instances of the problem in which the minimum conflicts algorithm does not perform case combination efficiently . in this paper we describe those situations in which initially retrieved cases are not easily adaptable , and propose a method by which to improve case adaptability with a genetic algorithm . we introduce a fitness function that maintains as much retrieved case information as possible , while also perturbing a sub - solution to allow subsequent case combination to proceed more efficiently .
the dynamic constraint satisfaction problem ( dcsp ) formalism has been gaining attention as a valuable and often necessary extension of the static csp framework . dynamic constraint satisfaction enables csp techniques to be applied more extensively , since it can be applied in domains where the set of constraints and variables involved in the problem evolves with time . at the same time , the case - based reasoning ( cbr ) community has been working on techniques by which to reuse existing solutions when solving new problems . we have observed that dynamic constraint satisfaction matches very closely the case - based reasoning process of case adaptation . these observations emerged from our previous work on combining cbr and csp to achieve a constraint - based adaptation . this paper summarizes our previous results , describes the similarity of the challenges facing both dcsp and case adaptation , and shows how csp and cbr can together begin to address these chal lenges .
prior knowledge , or bias , regarding a concept can speed up the task of learning it . probably approximately correct ( pac ) learning is a mathematical model of concept learning that can be used to quantify the speed up due to different forms of bias on learning . thus far , pac learning has mostly been used to analyze syntactic bias , such as limiting concepts to conjunctions of boolean prepositions . this paper demonstrates that pac learning can also be used to analyze semantic bias , such as a domain theory about the concept being learned . the key idea is to view the hypothesis space in pac learning as that consistent with all prior knowledge , syntactic and semantic . in particular , the paper presents a pac analysis of determinations , a type of relevance knowledge . the results of the analysis reveal crisp distinctions and relations among different determinations , and illustrate the usefulness of an analysis based on the pac model .
computational models of natural systems often contain free parameters that must be set to optimize the predictive accuracy of the models . this process | called calibration | can be viewed as a form of supervised learning in the presence of prior knowledge . in this view , the fixed aspects of the model constitute the prior knowledge , and the goal is to learn values for the free parameters . we report on a series of attempts to learn parameter values for a global vegetation model called mapss ( mapped atmosphere - plant - soil system ) developed by our collaborator , ron neilson . standard machine learning methods do not work with mapss , because the constraints introduced by the structure of the model create a very difficult nonlinear optimization problem . we developed a new divide - and - conquer approach in which subsets of the parameters are calibrated while others are held constant . this approach succeeds because it is possible to select training examples that exercise only portions of the model .
the paper considers the situation in which a learner ' s testing set contains close approximations of cases which appear in the training set . such cases can be considered ` virtual seens ' since they are approximately seen by the learner . generalisation measures which do not take account of the frequency of virtual seens may be misleading . the paper shows that the #NUM# - nn algorithm can be used to derive a normalising baseline for gen - eralisation statistics . the normalisation process is demonstrated though application to holte ' s [ #NUM# ] study in which the generalisation performance of the #NUM# r algorithm was tested against c #NUM# . #NUM# on #NUM# commonly used datasets .
conversational case - based reasoning ( cbr ) systems , which incrementally extract a query description through a user - directed conversation , are advertised for their ease of use . however , designing large case libraries that have good performance ( i . e . , precision and querying efficiency ) is difficult . cbr vendors provide guidelines for designing these libraries manually , but the guidelines are difficult to apply . we describe an automated inductive approach that revises conversational case libraries to increase their conformance with design guidelines . revision increased performance on three conversational case libraries .
diagnosis is the process of identifying the disorders of a machine or a patient by considering its history , symptoms and other signs . starting from possible initial information , new information is requested in a sequential manner and the diagnosis is made more precise . it is thus a missing data problem since not everything is known . we model the joint probability distribution of the data from a case database with mixture models . model parameters are estimated by the em algorithm which gives the additional benefit that missing data in the database itself can also be handled correctly . request of new information to refine the diagnosis is performed using the maximum utility principle from decision theory . since the system is based on machine learning it is domain independent . an example using a heart disease database is presented .
we give an example of a neural net without hidden layers and with a sigmoid transfer function , together with a training set of binary vectors , for which the sum of the squared errors , regarded as a function of the weights , has a local minimum which is not a global minimum . the example consists of a set of #NUM# training instances , with four weights and a threshold to be learnt . we do not know if substantially smaller binary examples exist .
the multiple extension problem arises because a default theory can use different subsets of its defaults to propose different , mutually incompatible , answers to some queries . this paper presents an algorithm that uses a set of observations to learn a credulous version of this default theory that is ( essentially ) " optimally accurate " . in more detail , we can associate a given default theory with a set of related credulous theories r = fr i g , where each r i uses its own total ordering of the defaults to determine which single answer to return for each query . our goal is to select the credulous theory that has the highest " expected accuracy " , where each r i ' s expected accuracy is the probability that the answer it produces to a query will correspond correctly to the world . unfortunately , a theory ' s expected accuracy depends on the distribution of queries , which is usually not known . moreover , the task of identifying the optimal r opt #NUM# r , even given that distribution information , is intractable . this paper presents a method , optacc , that sidesteps these problems by using a set of samples to estimate the unknown distribution , and by hill - climbing to a local optimum . in particular , given any parameters * ; ffi & gt ; #NUM# , optacc produces an r oa #NUM# r whose expected accuracy is , with probability at least #NUM# ffi , within * of a local optimum . appeared in ecai workshop on theoretical foundations of knowledge representation and reasoning ,
compression of information is an important concept in the theory of learning . we argue for the hypothesis that there is an inherent compression pressure towards short , elegant and general solutions in a genetic programming system and other variable length evolutionary algorithms . this pressure becomes visible if the size or complexity of solutions are measured without non - effective code segments called introns . the built in parsimony pressure effects complex fitness functions , crossover probability , generality , maximum depth or length of solutions , explicit parsimony , granularity of fitness function , initialization depth or length , and modulariz - ation . some of these effects are positive and some are negative . in this work we provide a basis for an analysis of these effects and suggestions to overcome the negative implications in order to obtain the balance needed for successful evolution . an empirical investigation that supports our hypothesis is also presented .
current rule induction systems ( e . g . cn #NUM# ) typically rely on a " separate and conquer " strategy , learning each rule only from still - uncovered examples . this results in a dwindling number of examples being available for learning successive rules , adversely affecting the system ' s accuracy . an alternative is to learn all rules simultaneously , using the entire training set for each . this approach is implemented in the rise #NUM# . #NUM# system . empirical comparison of rise with cn #NUM# suggests that " conquering without separating " performs similarly to its counterpart in simple domains , but achieves increasingly substantial gains in accuracy as the domain difficulty grows .
a genetic programming method is investigated for optimizing both the architecture and the connection weights of multilayer feedforward neural networks . the genotype of each network is represented as a tree whose depth and width are dynamically adapted to the particular application by specifically defined genetic operators . the weights are trained by a next - ascent hillclimb - ing search . a new fitness function is proposed that quantifies the principle of occam ' s razor . it makes an optimal trade - off between the error fitting ability and the parsimony of the network . we discuss the results for two problems of differing complexity and study the convergence and scaling properties of the algorithm .
the article at hand discusses a tool for automatic generation of structured models for complex dynamic processes by means of genetic programming . in contrast to other techniques which use genetic programming to find an appropriate arithmetic expression in order to describe the input - output behaviour of a process , this tool is based on a block oriented approach with a transparent description of signal paths . a short survey on other techniques for computer based system identification is given and the basic concept of smog ( structured model generator ) is described . furthermore latest extensions of the system are presented in detail , including automatically defined sub - models and quali tative fitness criteria .
we examine the role of hyperplane ranking during search performed by a simple genetic algorithm . we also develop a metric for measuring the degree of ranking that exists with respect to static measurements taken directly from the function , as well as the measurement of dynamic ranking of hyperplanes during genetic search . we show that the degree of dynamic ranking induced by a simple genetic algorithm is highly correlated with the degree of static ranking that is inherent in the function , especially during the initial genera tions of search .
genetic algorithms rely on two genetic operators crossover and mutation . although there exists a large body of conventional wisdom concerning the roles of crossover and mutation , these roles have not been captured in a theoretical fashion . for example , it has never been theoretically shown that mutation is in some sense " less powerful " than crossover or vice versa . this paper provides some answers to these questions by theoretically demonstrating that there are some important characteristics of each operator that are not captured by the other .
in a recent paper , friedman , geiger , and goldszmidt [ #NUM# ] introduced a classifier based on bayesian networks , called tree augmented naive bayes ( tan ) , that outperforms naive bayes and performs competitively with c #NUM# . #NUM# and other state - of - the - art methods . this classifier has several advantages including robustness and polynomial computational complexity . one limitation of the tan classifier is that it applies only to discrete attributes , and thus , continuous attributes must be prediscretized . in this paper , we extend tan to deal with continuous attributes directly via parametric ( e . g . , gaussians ) and semiparametric ( e . g . , mixture of gaussians ) conditional probabilities . the result is a classifier that can represent and combine both discrete and continuous attributes . in addition , we propose a new method that takes advantage of the modeling language of bayesian networks in order to represent attributes both in discrete and continuous form simultaneously , and use both versions in the classification . this automates the process of deciding which form of the attribute is most relevant to the classification task . it also avoids the commitment to either a discretized or a ( semi ) parametric form , since different attributes may correlate better with one version or the other . our empirical results show that this latter method usually achieves classification performance that is as good as or better than either the purely discrete or the purely continuous tan models .
this paper considers the problem of representing complex systems that evolve stochastically over time . dynamic bayesian networks provide a compact representation for stochastic processes . unfortunately , they are often unwieldy since they cannot explicitly model the complex organizational structure of many real life systems : the fact that processes are typically composed of several interacting subprocesses , each of which can , in turn , be further decomposed . we propose a hierarchically structured representation language which extends both dynamic bayesian networks and the object - oriented bayesian network framework of [ #NUM# ] , and show that our language allows us to describe such systems in a natural and modular way . our language supports a natural representation for certain system characteristics that are hard to capture using more traditional frameworks . for example , it allows us to represent systems where some processes evolve at a different rate than others , or systems where the processes interact only intermittently . we provide a simple inference mechanism for our representation via translation to bayesian networks , and suggest ways in which the inference algorithm can exploit the additional structure encoded in our representation .
it is often difficult to predict the optimal neural network size for a particular application . constructive or destructive methods that add or subtract neurons , layers , connections , etc . might offer a solution to this problem . we prove that one method , recurrent cascade correlation , due to its topology , has fundamental limitations in representation and thus in its learning capabilities . it cannot represent with monotone ( i . e . sigmoid ) and hard - threshold activation functions certain finite state automata . we give a " preliminary " approach on how to get around these limitations by devising a simple constructive training method that adds neurons during training while still preserving the powerful fully - recurrent structure . we illustrate this approach by simulations which learn many examples of regular grammars that the
indexing of cases is an important topic for memory - based reasoning ( mbr ) . one key problem is how to assign weights to attributes of cases . although several weighting methods have been proposed , some methods cannot handle numeric attributes directly , so it is necessary to discretize numeric values by classification . furthermore , existing methods have no theoretical background , so little can be said about optimality . we propose a new weighting method based on a statistical technique called quantification method ii . it can handle both numeric and symbolic attributes in the same framework . generated attribute weights are optimal in the sense that they maximize the ratio of variance between classes to variance of all cases . experiments on several benchmark tests show that in many cases , our method obtains higher accuracies than some other weighting methods . the results also indicate that it can distinguish relevant attributes from irrelevant ones , and can tolerate noisy data .
this paper proposes a classification scheme based on integration of multiple ensembles of anns . it is demonstrated on a classification problem , in which seismic signals of natural earthquakes must be distinguished from seismic signals of artificial explosions . a redundant classification environment consists of several ensembles of neural networks is created and trained on bootstrap sample sets , using various data representations and architectures . the anns within the ensembles are aggregated ( as in bagging ) while the ensembles are integrated non - linearly , in a signal adaptive manner , using a posterior confidence measure based on the agreement ( variance ) within the ensembles . the proposed integrated classification machine achieved #NUM# . #NUM# % correct classifications on the seismic test data . cross validation evaluations and comparisons indicate that such integration of a collection of ann ' s ensembles is a robust way for handling high dimensional problems with a complex non - stationary signal space as in the current seismic classification problem .
this is the first draft of a chapter for bayesian biostatistics , edited by donald a . berry and darlene k . strangl . adrian e . raftery is professor of statistics and sociology , department of statistics , gn - #NUM# , university of washington , seattle , wa #NUM# , usa . sylvia richardson is directeur de recherche , inserm unite #NUM# , #NUM# avenue paul vaillant couturier , #NUM# villejuif cedex , france . raftery ' s research was supported by onr contract no . n - #NUM# - #NUM# - j - #NUM# , by the ministere de la recherche et de l ' espace , paris , by the universite de paris vi , and by inria , rocquencourt , france . raftery thanks the latter two institutions , paul deheuvels and gilles celeux for hearty hospitality during his paris sabbatical in which part of this chapter was written . the authors are grateful to christine montfort for excellent research assistance and to mariette gerber , michel chavance and david madigan for helpful discussions .
production / manufacturing scheduling typically involves the acquisition of user optimization preferences . the ill - structuredness of both the problem space and the desired objectives make practical scheduling problems difficult to formalize and costly to solve , especially when problem configurations and user optimization preferences change over time . this paper advocates an incremental revision framework for improving schedule quality and incorporating user dynamically changing preferences through case - based reasoning . our implemented system , called cabins , records situation - dependent tradeoffs and consequences that result from schedule revision to guide schedule improvement . the preliminary experimental results show that cabins is able to effectively capture both user static and dynamic preferences which are not known to the system and only exist implicitly in a extensional manner in the case base .
realization of autonomous behavior in mobile robots , using fuzzy logic control , requires formulation of rules which are collectively responsible for necessary levels of intelligence . such a collection of rules can be conveniently decomposed and efficiently implemented as a hierarchy of fuzzy - behaviors . this article describes how this can be done using a behavior - based architecture . a behavior hierarchy and mechanisms of control decision - making are described . in addition , an approach to behavior coordination is described with emphasis on evolution of fuzzy coordination rules using the genetic programming ( gp ) paradigm . both conventional gp and steady - state gp are applied to evolve a fuzzy - behavior for sensor - based goal - seeking . the usefulness of the behavior hierarchy , and partial design by gp , is evident in performance results of simulated autonomous navigation .
we present a distribution model for binary vectors , called the influence combination model and show how this model can be used as the basis for unsupervised learning algorithms for feature selection . the model is closely related to the harmonium model defined by smolensky [ rm #NUM# ] [ ch . #NUM# ] . in the first part of the paper we analyze properties of this distribution representation scheme . we show that arbitrary distributions of binary vectors can be approximated by the combination model . we show how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits . we compare the combination model with the mixture model and with principle component analysis . in the second part of the paper we present two algorithms for learning the combination model from examples . the first algorithm is based on gradient ascent . here we give a closed form for this gradient that is significantly easier to compute than the corresponding gradient for the general boltzmann machine . the second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time . this method is a variant of projection pursuit density estimation . in the third part of the paper we give experimental results for these learning methods on synthetic data and on natural data of handwritten digit images .
complex group behavior arises in social insects colonies as the integration of the actions of simple and redundant individual insects [ adler and gordon , #NUM# , oster and wilson , #NUM# ] . furthermore , the colony can act as an information center to expedite foraging [ brown , #NUM# ] . we apply these lessons from natural systems to model collective action and memory in a computational agent society . collective action can expedite search in combinatorial optimization problems [ dorigo et al . , #NUM# ] . collective memory can improve learning in multi - agent systems [ garland and alterman , #NUM# ] . our collective adaptation integrates the simplicity of collective action with the pattern detection of collective memory to significantly improve both the gathering and processing of knowledge . as a test of the role of the society as an information center , we examine the ability of the society to distribute task allocation without any omnipotent centralized control .
we study annealed theories of learning boolean functions using a concept class of finite cardinality . the naive annealed theory can be used to derive a universal learning curve bound for zero temperature learning , similar to the inverse square root bound from the vapnik - chervonenkis theory . tighter , nonuniversal learning curve bounds are also derived . a more refined annealed theory leads to still tighter bounds , which in some cases are very similar to results previously obtained using one - step replica symmetry breaking .
this paper introduces a new type of intelligent agent called a constructive induction - based learning agent ( cila ) . this agent differs from other adaptive agents because it has the ability to not only learn how to assist a user in some task , but also to incrementally adapt its knowledge representation space to better fit the given learning task . the agents ability to autonomously make problem - oriented modifications to the originally given representation space is due to its constructive induction ( ci ) learning method . selective induction ( si ) learning methods , and agents based on these methods , rely on a good representation space . a good representation space has no misclassification noise , inter - correlated attributes or irrelevant attributes . our proposed cila has methods for overcoming all of these problems . in agent domains with poor representations , the ci - based learning agent will learn more accurate rules and be more useful than an si - based learning agent . this paper gives an architecture for a ci - based learning agent and gives an empirical comparison of a ci and si for a set of six abstract domains involving dnf - type ( disjunctive normal form ) descriptions .
we propose a method for decreasing the computational complexity of self - organising maps . the method uses a partitioning of the neurons into disjoint clusters . teaching of the neurons occurs on a cluster - basis instead of on a neuron - basis . for teaching an n - neuron network with n #NUM# samples , the computational complexity decreases from o ( n #NUM# n ) to o ( n #NUM# log n ) . furthermore , we introduce a measure for the amount of order in a self - organising map , and show that the introduced algorithm behaves as well as the original algorithm .
inductive learning in relational domains has been shown to be intractable in general . many approaches to this task have been suggested nevertheless ; all in some way restrict the hypothesis space searched . they can be roughly divided into two groups : data - driven , where the restriction is encoded into the algorithm , and model - based , where the restrictions are made more or less explicit with some form of declarative bias . this paper describes incy , an inductive learner that seeks to combine aspects of both approaches . incy is initially data - driven , using examples and background knowledge to put forth and specialize hypotheses based on the " connectivity " of the data at hand . it is model - driven in that hypotheses are abstracted into rule models , which are used both for control decisions in the data - driven phase and for model - guided induction . key words : inductive learning in relational domains , cooperation of data - driven and model - guided methods , implicit and declarative bias .
the problem of learning decision rules for sequential tasks is addressed , focusing on the problem of learning tactical plans from a simple flight simulator where a plane must avoid a missile . the learning method relies on the notion of competition and employs genetic algorithms to search the space of decision policies . experiments are presented that address issues arising from differences between the simulation model on which learning occurs and the target environment on which the decision rules are ultimately tested . specifically , either the model or the target environment may contain noise . these experiments examine the effect of learning tactical plans without noise and then testing the plans in a noisy environment , and the effect of learning plans in a noisy simulator and then testing the plans in a noise - free environment . empirical results show that , while best result are obtained when the training model closely matches the target environment , using a training environment that is more noisy than the target environment is better than using using a training environment that has less noise than the target environment .
navigation through obstacles such as mine fields is an important capability for autonomous underwater vehicles . one way to produce robust behavior is to perform projective planning . however , real - time performance is a critical requirement in navigation . what is needed for a truly autonomous vehicle are robust reactive rules that perform well in a wide variety of situations , and that also achieve real - time performance . in this work , samuel , a learning system based on genetic algorithms , is used to learn high - performance reactive strategies for navigation and collision avoidance .
in this paper we introduce and investigate a mathematically rigorous theory of learning curves that is based on ideas from statistical mechanics . the advantage of our theory over the well - established vapnik - chervonenkis theory is that our bounds can be considerably tighter in many cases , and are also more reflective of the true behavior ( functional form ) of learning curves . this behavior can often exhibit dramatic properties such as phase transitions , as well as power law asymptotics not explained by the vc theory . the disadvantages of our theory are that its application requires knowledge of the input distribution , and it is limited so far to finite cardinality function classes . we illustrate our results with many concrete examples of learning curve bounds derived from our theory .
although considerable interest has been shown in language inference and automata induction using recurrent neural networks , success of these models has mostly been limited to regular languages . we have previously demonstrated that neural network pushdown automaton ( nnpda ) model is capable of learning deterministic context - free languages ( e . g . , a n b n and parenthesis languages ) from examples . however , the learning task is computationally intensive . in this paper we discuss some ways in which a priori knowledge about the task and data could be used for efficient learning . we also observe that such knowledge is often an experimental prerequisite for learning nontrivial languages ( eg . a n b n cb m a m ) .
connectionist learning procedures are presented for " sigmoid " and " noisy - or " varieties of stochastic feedforward network . these networks are in the same class as the " belief networks " used in expert systems . they represent a probability distribution over a set of visible variables using hidden variables to express correlations . conditional probability distributions can be exhibited by stochastic simulation for use in tasks such as classification . learning from empirical data is done via a gradient - ascent method analogous to that used in boltzmann machines , but due to the feedforward nature of the connections , the negative phase of boltzmann machine learning is unnecessary . experimental results show that , as a result , learning in a sigmoid feedforward network can be faster than in a boltzmann machine . these networks have other advantages over boltzmann machines in pattern classification and decision making applications , and provide a link between work on connectionist learning and work on the representation of expert knowledge .
genetic programming ( gp ) uses variable size representations as programs . size becomes an important and interesting emergent property of the structures evolved by gp . the size of programs can be both a controlling and a controlled factor in gp search . size influences the efficiency of the search process and is related to the generality of solutions . this paper analyzes the size and generality issues in standard gp and gp using subroutines and addresses the question whether such an analysis can help control the search process . we relate the size , generalization and modularity issues for programs evolved to control an agent in a dynamic and non - deterministic environment , as exemplified by the pac - man game .
we present a definition of cause and effect in terms of decision - theoretic primitives and thereby provide a principled foundation for causal reasoning . our definition departs from the traditional view of causation in that causal assertions may vary with the set of decisions available . we argue that this approach provides added clarity to the notion of cause . also in this paper , we examine the encoding of causal relationships in directed acyclic graphs . we describe a special class of influence diagrams , those in canonical form , and show its relationship to pearl ' s representation of cause and effect . finally , we show how canonical form facilitates counterfactual reasoning .
fuzzy logic and evolutionary computation have proven to be convenient tools for handling real - world uncertainty and designing control systems , respectively . an approach is presented that combines attributes of these paradigms for the purpose of developing intelligent control systems . the potential of the genetic programming paradigm ( gp ) for learning rules for use in fuzzy logic controllers ( flcs ) is evaluated by focussing on the problem of discovering a controller for mobile robot path tracking . performance results of incomplete rule - bases compare favorably to those of a complete flc designed by the usual trial - and - error approach . a constrained syntactic representation supported by structure - preserving genetic operators is also introduced .
genetic programming is distinguished from other evolutionary algorithms in that it uses tree representations of variable size instead of linear strings of fixed length . the flexible representation scheme is very important because it allows the underlying structure of the data to be discovered automatically . one primary difficulty , however , is that the solutions may grow too big without any improvement of their generalization ability . in this paper we investigate the fundamental relationship between the performance and complexity of the evolved structures . the essence of the parsimony problem is demonstrated empirically by analyzing error landscapes of programs evolved for neural network synthesis . we consider genetic programming as a statistical inference problem and apply the bayesian model - comparison framework to introduce a class of fitness functions with error and complexity terms . an adaptive learning method is then presented that automatically balances the model - complexity factor to evolve parsimonious programs without losing the diversity of the population needed for achieving the desired training accuracy . the effectiveness of this approach is empirically shown on the induction of sigma - pi neural networks for solving a real - world medical diagnosis problem as well as benchmark tasks .
dynamic probabilistic networks ( dpns ) are a useful tool for modeling complex stochastic processes . the simplest inference task in dpns is monitoring | that is , computing a posterior distribution for the state variables at each time step given all observations up to that time . recursive , constant - space algorithms are well - known for monitoring in dpns and other models . this paper is concerned with hindsight | that is , computing a posterior distribution given both past and future observations . hindsight is an essential subtask of learning dpn models from data . existing algorithms for hindsight in dpns use o ( sn ) space and time , where n is the total length of the observation sequence and s is the state space size for each time step . they are therefore impractical for hindsight in complex models with long observation sequences . this paper presents an o ( s log n ) space , o ( sn log n ) time hindsight algorithm . we demonstrates the effectiveness of the algorithm in two real - world dpn learning problems . we also discuss the possibility of an o ( s ) - space , o ( sn ) - time algorithm .
we have already shown that extracting long - term dependencies from sequential data is difficult , both for deterministic dynamical systems such as recurrent networks , and probabilistic models such as hidden markov models ( hmms ) or input / output hidden markov models ( iohmms ) . in practice , to avoid this problem , researchers have used domain specific a - priori knowledge to give meaning to the hidden or state variables representing past context . in this paper , we propose to use a more general type of a - priori knowledge , namely that the temporal dependencies are structured hierarchically . this implies that long - term dependencies are represented by variables with a long time scale . this principle is applied to a recurrent network which includes delays and multiple time scales . experiments confirm the advantages of such structures . a similar approach is proposed for hmms and iohmms .
we present a new algorithm for finding low complexity neural networks with high generalization capability . the algorithm searches for a " flat " minimum of the error function . a flat minimum is a large connected region in weight - space where the error remains approximately constant . an mdl - based , bayesian argument suggests that flat minima correspond to " simple " networks and low expected overfitting . the argument is based on a gibbs algorithm variant and a novel way of splitting generalization error into underfitting and overfitting error . unlike many previous approaches , ours does not require gaussian assumptions and does not depend on a " good " weight prior instead we have a prior over input / output functions , thus taking into account net architecture and training set . although our algorithm requires the computation of second order derivatives , it has backprop ' s order of complexity . automatically , it effectively prunes units , weights , and input lines . various experiments with feedforward and recurrent nets are described . in an application to stock market prediction , flat minimum search outperforms ( #NUM# ) conventional backprop , ( #NUM# ) weight decay , ( #NUM# ) " optimal brain surgeon " / " optimal brain damage " . we also provide pseudo code of the algorithm ( omitted from the nc - version ) .
this paper describes our method for improving the comprehensibility , accuracy , and generality of reactive plans . a reactive plan is a set of reactive rules . our method involves two phases : ( #NUM# ) formulate explanations of execution traces , and then ( #NUM# ) generate new reactive rules from the explanations . since the explanation phase has been previously described , the primary focus of this paper is the rule generation phase . this latter phase consists of taking a subset of the explanations and using these explanations to generate a set of new reactive rules to add to the original set . the particular subset of the explanations that is chosen yields rules that provide new domain knowledge for handling knowledge gaps in the original rule set . the original rule set , in a complimentary manner , provides expertise to fill the gaps where the domain knowledge provided by the new rules is incomplete .
a new method for developing good value - ordering strategies in constraint satisfaction search is presented . using an evolutionary technique called sane , in which individual neurons evolve to cooperate and form a neural network , problem - specific knowledge can be discovered that results in better value - ordering decisions than those based on problem - general heuristics . a neural network was evolved in a chronological backtrack search to decide the ordering of cars in a resource - limited assembly line . the network required #NUM# / #NUM# of the backtracks of random ordering and #NUM# / #NUM# of the backtracks of the maximization of future options heuristic . the sane approach should extend well to other domains where heuristic information is either difficult to discover or problem - specific .
conversational case - based reasoning ( cbr ) shells ( e . g . , inference ' s cbr express ) are commercially successful tools for supporting the development of help desk and related applications . in contrast to rule - based expert systems , they capture knowledge as cases rather than more problematic rules , and they can be incrementally extended . however , rather than eliminate the knowledge engineering bottleneck , they refocus it on case engineering , the task of carefully authoring cases according to library design guidelines to ensure good performance . designing complex libraries according to these guidelines is difficult ; software is needed to assist users with case authoring . we describe an approach for revising case libraries according to design guidelines , its implementation in clire , and empirical results showing that , under some conditions , this approach can improve conversational cbr performance .
reinforcement learning is a class of problems in which an autonomous agent acting in a given environment improves its behavior by progressively maximizing a function calculated just on the basis of a succession of scalar responses received from the environment . q - learning and classifier systems ( cs ) are two methods among the most used to solve reinforcement learning problems . notwithstanding their popularity and their shared goal , they have been in the past often considered as two different models . in this paper we first show that the classifier system , when restricted to a sharp simplification called discounted max very simple classifier system ( d max - vscs ) , boils down to tabular q - learning . it follows that d max - vscs converges to the optimal policy as proved by watkins & dayan ( #NUM# ) , and that it can draw profit from the results of experimental and theoretical works dedicated to improve q - learning and to facilitate its use in concrete applications . in the second part of the paper , we show that three of the restrictions we need to impose to the cs for deriving its equivalence with q - learning , that is , no internal states , no don ' t care symbols , and no structural changes , turn out so essential as to be recently rediscovered and reprogrammed by q - learning adepts . eventually , we sketch further similarities among ongoing work within both research contexts . the main contribution of the paper is therefore to make explicit the strong similarities existing between q - learning and classifier systems , and to show that experience gained with research within one domain can be useful to direct future research in the other one .
some recent work has investigated the dichotomy between compact coding using dimensionality reduction and sparse distributed coding in the context of understanding biological information processing . we introduce an artificial neural network which self organises on the basis of simple hebbian learning and negative feedback of activation and show that it is capable of both forming compact codings of data distributions and also of identifying filters most sensitive to sparse distributed codes . the network is extremely simple and its biological relevance is investigated via its response to a set of images which are typical of everyday life . however , an analysis of the network ' s identification of the filter for sparse coding reveals that this coding may not be globally optimal and that there exists an innate limiting factor which cannot be transcended .
for classes of concepts defined by certain classes of analytic functions depending on n parameters , there are nonempty open sets of samples of length #NUM# n + #NUM# which cannot be shattered . a slighly weaker result is also proved for piecewise - analytic functions . the special case of neural networks is discussed .
two important goals in the evaluation of an ai theory or model are to assess the merit of the design decisions in the performance of an implemented computer system and to analyze the impact in the performance when the system faces problem domains with different characteristics . this is particularly difficult in case - based reasoning systems because such systems are typically very complex , as are the tasks and domains in which they operate . we present a methodology for the evaluation of case - based reasoning systems through systematic empirical experimentation over a range of system configurations and environmental conditions , coupled with rigorous statistical analysis of the results of the experiments . this methodology enables us to understand the behavior of the system in terms of the theory and design of the computational model , to select the best system configuration for a given domain , and to predict how the system will behave in response to changing domain and problem characteristics . a case study of a mul - tistrategy case - based and reinforcement learning system which performs autonomous robotic navigation is presented as an example .
for a case - based reasoner to use its knowledge flexibly , it must be equipped with a powerful case adapter . a case - based reasoner can only cope with variation in the form of the problems it is given to the extent that its cases in memory can be efficiently adapted to fit a wide range of new situations . in this paper , we address the task of adapting abstract knowledge about planning to fit specific planning situations . first we show that adapting abstract cases requires reconciling incommensurate representations of planning situations . next , we describe a representation system , a memory organization , and an adaptation process tailored to this requirement . our approach is implemented in brainstormer , a planner that takes abstract advice .
people often give advice by telling stories . stories both recommend a course of action and exemplify general conditions in which that recommendation is appropriate . a computational model of advice taking using stories must address two related problems : determining the story ' s recommendations and appropriateness conditions , and showing that these obtain in the new situation . in this paper , we present an efficient solution to the second problem based on caching the results of the first . our proposal has been implemented in brainstormer , a planner that takes abstract advice .
there is increasing need for efficient estimation of mixture distributions , especially following the explosion in the use of these as modelling tools in many applied fields . we propose in this paper a bayesian noninformative approach for the estimation of normal mixtures which relies on a reparameterisation of the secondary components of the mixture in terms of divergence from the main component . as well as providing an intuitively appealing representation at the modelling stage , this reparameterisation has important bearing on both the prior distribution and the performance of mcmc algorithms . we compare two possible reparameterisations extending mengersen and robert ( #NUM# ) and show that the reparameterisation which does not link the secondary components together is associated with poor convergence properties of mcmc algorithms .
a world - wide web ( www ) server was implemented in common lisp in order to facilitate exploratory programming in the global hypermedia domain and to provide access to complex research programs , particularly artificial intelligence systems . the server was initially used to provide interfaces for document retrieval and for email servers . more advanced applications include interfaces to systems for inductive rule learning and natural - language question answering . continuing research seeks to more fully generalize automatic form - processing techniques developed for email servers to operate seamlessly over the web . the conclusions argue that presentation - based interfaces and more sophisticated form processing should be moved into the clients in order to reduce the load on servers and provide more advanced interaction models for users .
survival analysis is concerned with finding models to predict the survival of patients or to assess the efficacy of a clinical treatment . a key part of the model - building process is the selection of the predictor variables . it is standard to use a stepwise procedure guided by a series of significance tests to select a single model , and then to make inference conditionally on the selected model . however , this ignores model uncertainty , which can be substantial . we review the standard bayesian model averaging solution to this problem and extend it to survival analysis , introducing partial bayes factors to do so for the cox proportional hazards model . in two examples , taking account of model uncertainty enhances predictive performance , to an extent that could be clinically useful .
we propose a bootstrap - based method for model averaging and selection that focuses on training points that are left out of individual bootstrap samples . this information can be used to estimate optimal weighting factors for combining estimates from different bootstrap samples , and also for finding the best subsets the linear model setting . these proposals provide alternatives to bayesian approaches to model averaging and selection , requiring less computation and fewer subjective choices .
the theory behind the success of adaptive reweighting and combining algorithms ( arcing ) such as adaboost ( freund and schapire [ #NUM# ] . [ #NUM# ] ) and others in reducing generalization error has not been well understood . by formulating prediction , both classification and regression , as a game where one player makes a selection from instances in the training set and the other a convex linear combination of predictors from a finite set , existing arcing algorithms are shown to be algorithms for finding good game strategies . an optimal game strategy finds a combined predictor that minimizes the maximum of the error over the training set . a bound on the generalization error for the combined predictors in terms of their maximum error is proven that is sharper than bounds to date . arcing algorithms are described that converge to the optimal strategy . schapire et . al . [ #NUM# ] offered an explanation of why adaboost works in terms of its ability to reduce the margin . comparing adaboost to our optimal arcing algorithm shows that their explanation is not valid and that the answer lies elsewhere . in this situation the vc - type bounds are misleading . some empirical results are given to explore the situation .
conversational case - based reasoning ( ccbr ) is a form of interactive case - based reasoning where users input a partial problem description ( in text ) . the ccbr system responds with a ranked solution display , which lists the solutions of stored cases whose problem descriptions best match the user ' s , and a ranked question display , which lists the unanswered questions in these cases . users interact with these displays , either refining their problem description by answering selected questions , or selecting a solution to apply . ccbr systems should support dialogue inferencing ; they should infer answers to questions that are implied by the problem description . otherwise , questions will be listed that the user believes they have already answered . the standard approach to dialogue inferencing allows case library designers to insert rules that define implications between the problem description and unanswered questions . however , this approach imposes substantial knowledge engineering requirements . we introduce an alternative approach whereby an intelligent assistant guides the designer in defining a model of their case library , from which implication rules are derived . we detail this approach , its benefits , and explain how it can be supported through an integration with parka - db , a fast relational database system . we will evaluate our approach in the context of our ccbr system , named nacodae . this paper appeared at the #NUM# aaai spring symposium on multimodal reasoning , and is ncarai tr aic - #NUM# - #NUM# . we introduce an integrated reasoning approach in which a model - based reasoning component performs an important inferencing role in a conversational case - based reasoning ( ccbr ) system named nacodae ( breslow & aha , #NUM# ) ( figure #NUM# ) . ccbr is a form of case - based reasoning where users enter text queries describing a problem and the system assists in eliciting refinements of it ( aha & breslow , #NUM# ) . cases have three components :
a read - once formula is a boolean formula in which each variable occurs at most once . such formulas are also called - formulas or boolean trees . this paper treats the problem of exactly identifying an unknown read - once formula using specific kinds of queries . the main results are a polynomial time algorithm for exact identification of monotone read - once formulas using only membership queries , and a polynomial time algorithm for exact identification of general read - once formulas using equivalence and membership queries ( a protocol based on the notion of a minimally adequate teacher [ #NUM# ] ) . our results improve on valiant ' s previous results for read - once formulas [ #NUM# ] . we also show that no polynomial time algorithm using only membership queries or only equivalence queries can exactly identify all read - once formulas .
we propose and analyze a distribution learning algorithm for a subclass of acyclic probabilistic finite automata ( apfa ) . this subclass is characterized by a certain distinguishability property of the automata ' s states . though hardness results are known for learning distributions generated by general apfas , we prove that our algorithm can indeed efficiently learn distributions generated by the subclass of apfas we consider . in particular , we show that the kl - divergence between the distribution generated by the target source and the distribution generated by our hypothesis can be made small with high confidence in polynomial time . we present two applications of our algorithm . in the first , we show how to model cursively written letters . the resulting models are part of a complete cursive handwriting recognition system . in the second application we demonstrate how apfas can be used to build multiple - pronunciation models for spoken words . we evaluate the apfa based pronunciation models on labeled speech data . the good performance ( in terms of the log - likelihood obtained on test data ) achieved by the apfas and the incredibly small amount of time needed for learning suggests that the learning algorithm of apfas might be a powerful alternative to commonly used probabilistic models .
the clausal discovery engine claudien is presented . claudien discovers regularities in data and is a representative of the inductive logic programming paradigm . as such , it represents data and regularities by means of first order clausal theories . because the search space of clausal theories is larger than that of attribute value representation , claudien also accepts as input a declarative specification of the language bias , which determines the set of syntactically well - formed regularities . whereas other papers on claudien focuss on the semantics or logical problem specification of claudien , on the discovery algorithm , or the pac - learning aspects , this paper wants to illustrate the power of the resulting technique . in order to achieve this aim , we show how claudien can be used to learn #NUM# ) integrity constraints in databases , #NUM# ) functional dependencies and determinations , #NUM# ) properties of sequences , #NUM# ) mixed quantitative and qualitative laws , #NUM# ) reverse engineering , and #NUM# ) classification rules .
in the context of machine learning from examples this paper deals with the problem of estimating the quality of attributes with and without dependencies between them . greedy search prevents current inductive machine learning algorithms to detect significant dependencies between the attributes . recently , kira and rendell developed the relief algorithm for estimating the quality of attributes that is able to detect dependencies between attributes . we show strong relation between relief ' s estimates and impurity functions , that are usually used for heuristic guidance of inductive learning algorithms . we propose to use relieff , an extended version of relief , instead of myopic impurity functions . we have reimplemented assistant , a system for top down induction of decision trees , using relieff as an estimator of attributes at each selection step . the algorithm is tested on several artificial and several real world problems . results show the advantage of the presented approach to inductive learning and open a wide rang of possibilities for using relieff .
an investigation into the dynamics of genetic programming applied to chaotic time series prediction is reported . an interesting characteristic of adaptive search techniques is their ability to perform well in many problem domains while failing in others . because of genetic programming ' s flexible tree structure , any particular problem can be represented in myriad forms . these representations have variegated effects on search performance . therefore , an aspect of fundamental engineering significance is to find a representation which , when acted upon by genetic programming operators , optimizes search performance . we discover , in the case of chaotic time series prediction , that the representation commonly used in this domain does not yield optimal solutions . instead , we find that the population converges onto one " accurately replicating " tree before other trees can be explored . to correct for this premature convergence we make a simple modification to the crossover operator . in this paper we review previous work with gp time series prediction , pointing out an anomalous result related to overlearning , and report the improvement effected by our modified crossover operator .
current ilp algorithms typically use variants and extensions of the greedy search . this prevents them to detect significant relationships between the training objects . instead of myopic impurity functions , we propose the use of the heuristic based on relief for guidance of ilp algorithms . at each step , in our ilp - r system , this heuristic is used to determine a beam of candidate literals . the beam is then used in an exhaustive search for a potentially good conjunction of literals . from the efficiency point of view we introduce interesting declarative bias which enables us to keep the growth of the training set , when introducing new variables , within linear bounds ( linear with respect to the clause length ) . this bias prohibits cross - referencing of variables in variable dependency tree . the resulting system has been tested on various artificial problems . the advantages and deficiencies of our approach are discussed .
instead of myopic impurity functions , we propose the use of reli - eff for heuristic guidance of inductive learning algorithms . the basic algoritm relief , developed by kira and rendell ( kira and rendell , #NUM# a ; b ) , is able to efficiently solve classification problems involving highly dependent attributes , such as parity problems . however , it is sensitive to noise and is unable to deal with incomplete data , multi - class , and regression problems ( continuous class ) . we have extended relief in several directions . the extended algorithm relieff is able to deal with noisy and incomplete data , can be used for multiclass problems , and its regressional variant rrelieff can deal with regression problems . another area of application is inductive logic programming ( ilp ) where , instead of myopic measures , relieff can be used to estimate the utility of literals during the theory construction .
in this paper we present tdleaf ( ) , a variation on the td ( ) algorithm that enables it to be used in conjunction with minimax search . we present some experiments in which our chess program , knightcap , used tdleaf ( ) to learn its evaluation function while playing on the free ineternet chess server ( fics , fics . onenet . net ) . it improved from a #NUM# rating to a #NUM# rating in just #NUM# games and #NUM# days of play . we discuss some of the reasons for this success and also the relationship between our results and tesauro ' s results in backgammon .
we have explored two approaches to recognizing faces across changes in pose . first , we developed a representation of face images based on independent component analysis ( ica ) and compared it to a principal component analysis ( pca ) representation for face recognition . the ica basis vectors for this data set were more spatially local than the pca basis vectors and the ica representation had greater invariance to changes in pose . second , we present a model for the development of viewpoint invariant responses to faces from visual experience in a biological system . the temporal continuity of natural visual experience was incorporated into an attractor network model by hebbian learning following a lowpass temporal filter on unit activities . when combined with the temporal filter , a basic hebbian update rule became a generalization of griniasty et al . ( #NUM# ) , which associates temporally proximal input patterns into basins of attraction . the system acquired rep resentations of faces that were largely independent of pose .
problems of regression smoothing and curve fitting are addressed via predictive inference in a flexible class of mixture models . multi - dimensional density estimation using dirichlet mixture models provides the theoretical basis for semi - parametric regression methods in which fitted regression functions may be deduced as means of conditional predictive distributions . these bayesian regression functions have features similar to generalised kernel regression estimates , but the formal analysis addresses problems of multivariate smoothing parameter estimation and the assessment of uncertainties about regression functions naturally . computations are based on multi - dimensional versions of existing markov chain simulation analysis of univariate dirichlet mixture models .
on the basis of early theoretical and empirical studies , genetic algorithms have typically used #NUM# and #NUM# - point crossover operators as the standard mechanisms for implementing recombination . however , there have been a number of recent studies , primarily empirical in nature , which have shown the benefits of crossover operators involving a higher number of crossover points . from a traditional theoretical point of view , the most surprising of these new results relate to uniform crossover , which involves on the average l / #NUM# crossover points for strings of length l . in this paper we extend the existing theoretical results in an attempt to provide a broader explanatory and predictive theory of the role of multi - point crossover in genetic algorithms . in particular , we extend the traditional disruption analysis to include two general forms of multi - point crossover : n - point crossover and uniform crossover . we also analyze two other aspects of multi - point crossover operators , namely , their recombination potential and exploratory power . the results of this analysis provide a much clearer view of the role of multi - point crossover in genetic algorithms . the implications of these results on implementation issues and performance are discussed , and several directions for further research are suggested .
in this paper we discuss the methodological issues of using a class of neural networks called mixture density networks ( mdn ) for discriminant analysis . mdn models have the advantage of having a rigorous probabilistic interpretation , and they have proven to be a viable alternative as a classification procedure in discrete domains . we will address both the classification and interpretive aspects of discriminant analysis , and compare the approach to the traditional method of linear discrimin - ants as implemented in standard statistical packages . we show that the mdn approach adopted performs well in both aspects . many of the observations made are not restricted to the particular case at hand , and are applicable to most applications of discriminant analysis in educational research .
satisfiability ( sat ) refers to the task of finding a truth assignment that makes an arbitrary boolean expression true . this paper compares a simulated annealing algorithm ( sasat ) with gsat ( selman et al . , #NUM# ) , a greedy algorithm for solving satisfiability problems . gsat can solve problem instances that are extremely difficult for traditional satisfiability algorithms . results suggest that sasat scales up better as the number of variables increases , solving at least as many hard sat problems with less effort . the paper then presents an ablation study that helps to explain the relative advantage of sasat over gsat . next , an improvement to the basic sasat algorithm is examined , based on a random walk implemented in gsat ( selman et al . , #NUM# ) . finally , we examine the performance of sasat on a test suite of satisfiability problems produced by the #NUM# dimacs challenge .
we present a comparison of error - based and entropy - based methods for discretization of continuous features . our study includes both an extensive empirical comparison as well as an analysis of scenarios where error minimization may be an inappropriate discretization criterion . we present a discretization method based on the c #NUM# . #NUM# decision tree algorithm and compare it to an existing entropy - based discretization algorithm , which employs the minimum description length principle , and a recently proposed error - based technique . we evaluate these discretization methods with respect to c #NUM# . #NUM# and naive - bayesian classifiers on datasets from the uci repository and analyze the computational complexity of each method . our results indicate that the entropy - based mdl heuristic outperforms error minimization on average . we then analyze the shortcomings of error - based approaches in comparison to entropy - based methods .
here we show a similar construction for multiple - output systems , with some modifications . let = ( a ; b ; c ) s be a discrete - time sign - linear system with state space ir n and p outputs . perform a change of ; where a #NUM# ( n #NUM# fi n #NUM# ) is invertible and a #NUM# ( n #NUM# fi n #NUM# ) is nilpotent . if ( a ; b ) is a reachable pair and ( a ; c ) is an observable pair , then is minimal in the sense that any other sign - linear system with the same input / output behavior has dimension at least n . but , if n #NUM# & lt ; n , then det a = #NUM# and is not observable and hence not canonical . let us find another system ~ ( necessarily not sign - linear ) which has the same input / output behavior as , but is canonical . let i be the relative degree of the ith row of the markov sequence a , and = minf i : i = #NUM# ; : : : ; pg . let the initial state be x . there is a difference between the case when the smallest relative degree is greater or equal to n #NUM# and the case when & lt ; n #NUM# . roughly speaking , when n #NUM# the outputs of the sign - linear system give us information about sign ( cx ) , sign ( cax ) , : : : , sign ( ca #NUM# x ) , which are the first outputs of the sys tem . after that , we can use the inputs and outputs to learn only about x #NUM# ( the first n #NUM# components of x ) . when & lt ; n #NUM# , we may be able to use some controls to learn more about x #NUM# ( the last n #NUM# components of x ) before time n #NUM# when the nilpotency of a #NUM# has finally lemma #NUM# . #NUM# two states x and z are indistinguishable for if and only if ( x ) = ( z ) . proof . in the case n #NUM# , we have only the equations x #NUM# = z #NUM# and the equality of the ' s . the first ` output terms for are exactly the terms of . so these equalities are satisfied if and only if the first ` output terms coincide for x and z , for any input . equality of everything but the first n #NUM# components is equivalent to the first n #NUM# output terms coinciding for x and z , since the jth row of the qth output , for initial state x , for example , is either sign ( c j a q x ) if j & gt ; q , or sign ( c j a q x + + a j j u q j + #NUM# + ) if j q in which case we may use the control u q j + #NUM# to identify c j a q x ( using remark #NUM# . #NUM# in [ #NUM# ] ) .
many significant real - world classification tasks involve a large number of categories which are arranged in a hierarchical structure ; for example , classifying documents into subject categories under the library of congress scheme , or classifying world - wide - web documents into topic hierarchies . we investigate the potential benefits of using a given hierarchy over base classes to learn accurate multi - category classifiers for these domains . first , we consider the possibility of exploiting a class hierarchy as prior knowledge that can help one learn a more accurate classifier . we explore the benefits of learning category - discriminants in a hard top - down fashion and compare this to a soft approach which shares training data among sibling categories . in doing so , we verify that hierarchies have the potential to improve prediction accuracy . but we argue that the reasons for this can be subtle . sometimes , the improvement is only because using a hierarchy happens to constrain the expressiveness of a hypothesis class in an appropriate manner . however , various controlled experiments show that in other cases the performance advantage associated with using a hierarchy really does seem to be due to the prior knowledge it encodes .
many algorithms for inferring a decision tree from data involve a two - phase process : first , a very large decision tree is grown which typically ends up " over - fitting " the data . to reduce over - fitting , in the second phase , the tree is pruned using one of a number of available methods . the final tree is then output and used for classification on test data . in this paper , we suggest an alternative approach to the pruning phase . using a given unpruned decision tree , we present a new method of making predictions on test data , and we prove that our algorithm ' s performance will not be " much worse " ( in a precise technical sense ) than the predictions made by the best reasonably small pruning of the given decision tree . thus , our procedure is guaranteed to be competitive ( in terms of the quality of its predictions ) with any pruning algorithm . we prove that our procedure is very efficient and highly robust . our method can be viewed as a synthesis of two previously studied techniques . first , we apply cesa - bianchi et al . ' s [ #NUM# ] results on predicting using " expert advice " ( where we view each pruning as an " expert " ) to obtain an algorithm that has provably low prediction loss , but that is com - putationally infeasible . next , we generalize and apply a method developed by buntine [ #NUM# ] , [ #NUM# ] and willems , shtarkov and tjalkens [ #NUM# ] , [ #NUM# ] to derive a very efficient implementation of this procedure .
we present an efficient algorithm for pac - learning a very general class of geometric concepts over & lt ; d for fixed d . more specifically , let t be any set of s halfspaces . let x = ( x #NUM# ; : : : ; x d ) be an arbitrary point in & lt ; d . with each t #NUM# t we associate a boolean indicator function i t ( x ) which is #NUM# if and only if x is in the halfspace t . the concept class , c d s , that we study consists of all concepts formed by any boolean function over i t #NUM# ; : : : ; i t s for t i #NUM# t . this concept class is much more general than any geometric concept class known to be pac - learnable . our results can be easily extended to efficiently learn any boolean combination of a polynomial number of concepts selected from any concept class c over & lt ; d given that the vc - dimension of c has dependence only on d ( and is thus constant for any constant d ) , and there is a polynomial time algorithm to determine if there is a concept from c consistent with a given set of labeled examples . we also present a statistical query version of our algorithm that can tolerate random classification noise for any noise rate strictly less than #NUM# / #NUM# . finally we present a generalization of the standard * - net result of haussler and welzl [ #NUM# ] and apply it to give an alternative noise - tolerant algorithm for d = #NUM# based on geometric subdivisions .
in this work we develop a new criteria to perform pessimistic decision tree pruning . our method is theoretically sound and is based on theoretical concepts such as uniform convergence and the vapnik - chervonenkis dimension . we show that our criteria is very well motivated , from the theory side , and performs very well in practice . the accuracy of the new criteria is comparable to that of the current method used in c #NUM# . #NUM# .
in this paper we study the performance of probabilistic networks in the context of protein sequence analysis in molecular biology . specifically , we report the results of our initial experiments applying this framework to the problem of protein secondary structure prediction . one of the main advantages of the probabilistic approach we describe here is our ability to perform detailed experiments where we can experiment with different models . we can easily perform local substitutions ( mutations ) and measure ( probabilistically ) their effect on the global structure . window - based methods do not support such experimentation as readily . our method is efficient both during training and during prediction , which is important in order to be able to perform many experiments with different networks . we believe that probabilistic methods are comparable to other methods in prediction quality . in addition , the predictions generated by our methods have precise quantitative semantics which is not shared by other classification methods . specifically , all the causal and statistical independence assumptions are made explicit in our networks thereby allowing biologists to study and experiment with different causal models in a convenient manner .
in this paper we prove sanity - check bounds for the error of the leave - one - out cross - validation estimate of the generalization error : that is , bounds showing that the worst - case error of this estimate is not much worse than that of the training error estimate . the name sanity - check refers to the fact that although we often expect the leave - one - out estimate to perform considerably better than the training error estimate , we are here only seeking assurance that its performance will not be considerably worse . perhaps surprisingly , such assurance has been given only for rather limited cases in the prior literature on cross - validation . any nontrivial bound on the error of leave - one - out must rely on some notion of algorithmic stability . previous bounds relied on the rather strong notion of hypothesis stability , whose application was primarily limited to nearest - neighbor and other local algorithms . here we introduce the new and weaker notion of error stability , and apply it to obtain sanity - check bounds for leave - one - out for other classes of learning algorithms , including training error minimization procedures and bayesian algorithms . we also provide lower bounds demonstrating the necessity of error stability for proving bounds on the error of the leave - one - out estimate , and the fact that for training error minimization algorithms , in the worst case such bounds must still depend on the vapnik - chervonenkis dimension of the hypothesis class .
this paper describes a program which observes the behaviour of actors in a simulated world and uses these observations as guides to conducting experiments . an experiment is a sequence of actions carried out by an actor in order to support or weaken the case for a generalisation of a concept . a generalisation is attempted when the program observes a state of the world which is similar to a some previous state . a partial matching algorithm is used to find substitutions which enable the two states to be unified . the generalisation of the two states is their unifier .
widespread adoption of genetic programming techniques as a domain - independent problem solving tool depends on a good underlying software structure . a system is presented that mirrors the conceptual makeup of a gp system . consisting of a loose collection of software components , each with strict interface definitions and roles , the system maximises flexibility and minimises effort when applied to a new problem domain .
co - evolution of competitive species provides an interesting testbed to study the role of adaptive behavior because it provides unpredictable and dynamic environments . in this paper we experimentally investigate some arguments for the co - evolution of different adaptive protean behaviors in competing species of predators and preys . both species are implemented as simulated mobile robots ( kheperas ) with infrared proximity sensors , but the predator has an additional vision module whereas the prey has a maximum speed set to twice that of the predator . different types of variability during life for neurocontrollers with the same architecture and genetic length are compared . it is shown that simple forms of pro - teanism affect co - evolutionary dynamics and that preys rather exploit noisy controllers to generate random trajectories , whereas predators benefit from directional - change controllers to improve pursuit behavior .
the calculation of second derivatives is required by recent training and analysis techniques of connectionist networks , such as the elimination of superfluous weights , and the estimation of confidence intervals both for weights and network outputs . we here review and develop exact and approximate algorithms for calculating second derivatives . for networks with jwj weights , simply writing the full matrix of second derivatives requires o ( jwj #NUM# ) operations . for networks of radial basis units or sigmoid units , exact calculation of the necessary intermediate terms requires of the order of #NUM# h + #NUM# backward / forward - propagation passes where h is the number of hidden units in the network . we also review and compare three approximations ( ignoring some components of the second derivative , numerical differentiation , and scoring ) . our algorithms apply to arbitrary activation functions , networks , and error functions ( for instance , with connections that skip layers , or radial basis functions , or cross - entropy error and softmax units , etc . ) .
in this paper we describe how the principles of problem solving by analogy can be applied to the domain of functional program synthesis . for this reason , we treat programs as syntactical structures . we discuss two different methods to handle these structures : ( a ) a graph metric for determining the distance between two program schemes , and ( b ) the structure mapping engine ( an existing system to examine analogical processing ) . furthermore we show experimental results and discuss them .
there are many ways for a learning system to generalize from training set data . this paper presents several generalization styles using prototypes in an attempt to provide accurate generalization on training set data for a wide variety of applications . these generalization styles are efficient in terms of time and space , and lend themselves well to massively parallel architectures . empirical results of generalizing on several real - world applications are given , and these results indicate that the prototype styles of generalization presented have potential to provide accurate generalization for many applications .
most artificial neural networks ( anns ) have a fixed topology during learning , and often suffer from a number of shortcomings as a result . anns that use dynamic topologies have shown ability to overcome many of these problems . adaptive self organizing concurrent systems ( asocs ) are a class of learning models with inherently dynamic topologies . this paper introduces location - independent transformations ( lits ) as a general strategy for implementing learning models that use dynamic topologies efficiently in parallel hardware . a lit creates a set of location - independent nodes , where each node computes its part of the network output independent of other nodes , using local information . this type of transformation allows efficient support for adding and deleting nodes dynamically during learning . in particular , this paper presents the location - independent asocs ( lia ) model as a lit for asocs adaptive algorithm #NUM# . the description of lia gives formal definitions for lia algorithms . because lia implements basic asocs mechanisms , these definitions provide a formal description of basic asocs mechanisms in general , in addition to lia .
reinforcement learning algorithms often work by finding functions that satisfy the bellman equation . this yields an optimal solution for prediction with markov chains and for controlling a markov decision process ( mdp ) with a finite number of states and actions . this approach is also frequently applied to markov chains and mdps with infinite states . we show that , in this case , the bellman equation may have multiple solutions , many of which lead to erroneous predictions and policies ( baird , #NUM# ) . algorithms and conditions are presented that guarantee a single , optimal solution to the bellman equation .
a major issue in case - basedsystems is retrieving the appropriate cases from memory to solve a given problem . this implies that a case should be indexed appropriately when stored in memory . a case - based system , being dynamic in that it stores cases for reuse , needs to learn indices for the new knowledge as the system designers cannot envision that knowledge . irrespective of the type of indexing ( structural or functional ) , a hierarchical organization of the case memory raises two distinct but related issues in index learning learning the indexing vocabulary and learning the right level of generalization . in this paper we show how structure - behavior - function ( sbf ) models help in learning structural indices to design cases in the domain of physical devices . the sbf model of a design provides the functional and causal explanation of how the structure of the design delivers its function . we describe how the sbf model of a design provides both the vocabulary for structural indexing of design cases and the inductive biases for index generalization . we further discuss how model - based learning can be integrated with similarity - based learning ( that uses prior design cases ) for learning the level of index generalization .
constructive induction divides the problem of learning an inductive hypothesis into two intertwined searches : onefor the best representation space , and twofor the best hypothesis in that space . in data - driven constructive induction ( dci ) , a learning system searches for a better representation space by analyzing the input examples ( data ) . the presented data - driven constructive induction method combines an aq - type learning algorithm with two classes of representation space improvement operators : constructors , and destructors . the implemented system , aq #NUM# - dci , has been experimentally applied to a gnp prediction problem using a world bank database . the results show that decision rules learned by aq #NUM# - dci outperformed the rules learned in the original representation space both in predictive accuracy and rule simplicity .
we report a novel possibility for extracting a small subset of a data base which contains all the information necessary to solve a given classification task : using the support vector algorithm to train three different types of handwritten digit classifiers , we observed that these types of classifiers construct their decision surface from strongly overlapping small ( #NUM# % ) subsets of the data base . this finding opens up the possibility of compressing data bases significantly by disposing of the data which is not important for the solution of a given task . in addition , we show that the theory allows us to predict the classifier that will have the best generalization ability , based solely on performance on the training set and characteristics of the learning machines . this finding is important for cases where the amount of available data is limited .
physical variables such as the orientation of a line in the visual field or the location of the body in space are coded as activity levels in populations of neurons . reconstruction or decoding is an inverse problem in which the physical variables are estimated from observed neural activity . reconstruction is useful first in quantifying how much information about the physical variables is present in the population , and second , in providing insight into how the brain might use distributed representations in solving related computational problems such as visual object recognition and spatial navigation . two classes of reconstruction methods , namely , probabilistic or bayesian methods and basis function methods , are discussed . they include important existing methods as special cases , such as population vector coding , optimal linear estimation and template matching . as a representative example for the reconstruction problem , different methods were applied to multi - electrode spike train data from hippocampal place cells in freely moving rats . the reconstruction accuracy of the trajectories of the rats was compared for the different methods . bayesian methods were especially accurate when a continuity constraint was enforced , and the best errors were within a factor of two of the the information - theoretic limit on how accurate any reconstruction can be , which were comparable with the intrinsic experimental errors in position tracking . in addition , the reconstruction analysis uncovered some interesting aspects of place cell activity , such as the tendency for erratic jumps of the reconstructed trajectory when the animal stopped running . in general , the theoretical values of the minimal achievable reconstruction errors quantify how accurately a physical variable is encoded in the neuronal population in the sense of mean square error , regardless of the method used for reading out the information . one related result is that the theoretical accuracy is independent of the width of the gaussian tuning function only in two dimensions . finally , all the reconstruction methods considered in this paper can be implemented by a unified neural network architecture , which the brain could feasibly use to solve related problems .
the head - direction ( hd ) cells found in the limbic system in freely moving rats represent the instantaneous head direction of the animal in the horizontal plane regardless of the location of the animal . the internal direction represented by these cells uses both self - motion information for inertially based updating and familiar visual landmarks for calibration . here , a model of the dynamics of the hd cell ensemble is presented . the stability of a localized static activity profile in the network and a dynamic shift mechanism are explained naturally by synaptic weight distribution components with even and odd symmetry , respectively . under symmetric weights or symmetric reciprocal connections , a stable activity profile close to the known directional tuning curves will emerge . by adding a slight asymmetry to the weights , the activity profile will shift continuously without disturbances to its shape , and the shift speed can be accurately controlled by the strength of the odd - weight component . the generic formulation of the shift mechanism is determined uniquely within the current theoretical framework . the attractor dynamics of the system ensures modality - independence of the internal representation and facilitates the correction for cumulative error by the putative local - view detectors . the model offers a specific one - dimensional example of a computational mechanism in which a truly world - centered representation can be derived from observer - centered sensory inputs by integrating self - motion information .
we present a bias - variance decomposition of expected misclassification rate , the most commonly used loss function in supervised classification learning . the bias - variance decomposition for quadratic loss functions is well known and serves as an important tool for analyzing learning algorithms , yet no decomposition was offered for the more commonly used zero - one ( misclassification ) loss functions until the recent work of kong & dietterich ( #NUM# ) and breiman ( #NUM# ) . their decomposition suffers from some major shortcomings though ( e . g . , potentially negative variance ) , which our decomposition avoids . we show that , in practice , the naive frequency - based estimation of the decomposition terms is by itself biased and show how to correct for this bias . we illustrate the decomposition on various algorithms and datasets from the uci repository .
the dominant component of the computational burden of solving nontrivial problems with evolutionary algorithms is the task of measuring the fitness of each individual in each generation of the evolving population . the advent of r a p i d l y r e c o n f i g u r a b l e f i e l d - programmable gate arrays ( fpgas ) and the idea of evolvable hardware opens the possiblity of e m b o d y i n g each individual of the evolving population into hardware for the purpose of accelerating the time - consuming fitness evaluation task this paper demonstrates how the massive parallelism of the rapidly r e c o n f i g u r a b l e x i l i n x x c #NUM# #NUM# #NUM# #NUM# fpga can be exploited to accelerate the computationally burdensome fitness evaluation task of genetic programming . the work was done on virtual computing corporation ' s low - cost hots expansion board for pc type computers . a #NUM# - step #NUM# - sorter was evolved that has two fewer steps than the sorting network described in the #NUM# o ' connor and nelson patent on sorting networks and that has the same number of steps as the minimal #NUM# - sorter that was devised by floyd and knuth subsequent to the patent .
a theoretically justifiable fast finite successive linear approximation algorithm is proposed for obtaining a parsimonious solution to a corrupted linear system ax = b + p , where the corruption p is due to noise or error in measurement . the proposed linear - programming - based algorithm finds a solution x by parametrically minimizing the number of nonzero elements in x and the error k ax b p k #NUM# . numerical tests on a signal - processing - based example indicate that the proposed method is comparable to a method that parametrically minimizes the #NUM# - norm of the solution x and the error k ax b p k #NUM# , and that both methods are superior , by orders of magnitude , to solutions obtained by least squares as well by combinatorially choosing an optimal solution with a specific number of nonzero elements .
in natural visual experience , different views of an object or face tend to appear in close temporal proximity . a set of simulations is presented which demonstrate how viewpoint invariant representations of faces can be developed from visual experience by capturing the temporal relationships among the input patterns . the simulations explored the interaction of temporal smoothing of activity signals with hebbian learning ( foldiak , #NUM# ) in both a feed - forward system and a recurrent system . the recurrent system was a generalization of a hopfield network with a lowpass temporal filter on all unit activities . following training on sequences of graylevel images of faces as they changed pose , multiple views of a given face fell into the same basin of attraction , and the system acquired representations of faces that were approximately viewpoint invariant .
neural networks have been successfully applied in a wide range of supervised and unsupervised learning applications . neural - network methods are not commonly used for data - mining tasks , however , because they often produce incomprehensible models and require long training times . in this article , we describe neural - network learning algorithms that are able to produce comprehensible models , and that do not require excessive training times . specifically , we discuss two classes of approaches for data mining with neural networks . the first type of approach , often called rule extraction , involves extracting symbolic models from trained neural networks . the second approach is to directly learn simple , easy - to - understand networks . we argue that , given the current state of the art , neural - network methods deserve a place in the tool boxes of data - mining specialists .
a statistical theory for overtraining is proposed . the analysis treats realizable stochastic neural networks , trained with kullback - leibler loss in the asymptotic case . it is shown that the asymptotic gain in the generalization error is small if we perform early stopping , even if we have access to the optimal stopping time . considering cross - validation stopping we answer the question : in what ratio the examples should be divided into training and testing sets in order to obtain the optimum performance . in the non - asymptotic region cross - validated early stopping always decreases the generalization error . our large scale simulations done on a cm #NUM# are in nice agreement with our analytical findings .
mathematical programming approaches to three fundamental problems will be described : feature selection , clustering and robust representation . the feature selection problem considered is that of discriminating between two sets while recognizing irrelevant and redundant features and suppressing them . this creates a lean model that often generalizes better to new unseen data . computational results on real data confirm improved generalization of leaner models . clustering is exemplified by the unsupervised learning of patterns and clusters that may exist in a given database and is a useful tool for knowledge discovery in databases ( kdd ) . a mathematical programming formulation of this problem is proposed that is theoretically justifiable and computationally implementable in a finite number of steps . a resulting k - median algorithm is utilized to discover very useful survival curves for breast cancer patients from a medical database . robust representation is concerned with minimizing trained model degradation when applied to new problems . a novel approach is proposed that purposely tolerates a small error in the training process in order to avoid overfitting data that may contain errors . examples of applications of these concepts are given .
concept learning can be viewed as search of the space of concept descriptions . the hypothesis language determines the search space . in standard inductive learning algorithms , the structure of the search space is determined by generalization / specialization operators . algorithms perform locally optimal search by using a hill - climbing and / or a beam - search strategy . to overcome this limitation , concept learning can be viewed as stochastic search of the space of concept descriptions . the proposed stochastic search method is based on simulated annealing which is known as a successful means for solving combinatorial optimization problems . the stochastic search method , implemented in a rule learning system atris , is based on a compact and efficient representation of the problem and the appropriate operators for structuring the search space . furthermore , by heuristic pruning of the search space , the method enables also handling of imperfect data . the paper introduces the stochastic search method , describes the atris learning algorithm and gives results of the experiments .
the increasing availability of finely - grained parallel architectures has resulted in a variety of evolutionary algorithms ( eas ) in which the population is spatially distributed and local selection algorithms operate in parallel on small , overlapping neighborhoods . the effects of design choices regarding the particular type of local selection algorithm as well as the size and shape of the neighborhood are not particularly well understood and are generally tested empirically . in this paper we extend the techniques used to more formally analyze selection methods for sequential eas and apply them to local neighborhood models , resulting in a much clearer understanding of the effects of neighborhood size and shape .
qualitative probabilistic reasoning in a bayesian network often reveals tradeoffs : relationships that are ambiguous due to competing qualitative influences . we present two techniques that combine qualitative and numeric probabilistic reasoning to resolve such tradeoffs , inferring the qualitative relationship between nodes in a bayesian network . the first approach incrementally marginalizes nodes that contribute to the ambiguous qualitative relationships . the second approach evaluates approximate bayesian networks for bounds of probability distributions , and uses these bounds to determinate qualitative relationships in question . this approach is also incremental in that the algorithm refines the state spaces of random variables for tighter bounds until the qualitative relationships are resolved . both approaches provide systematic methods for tradeoff resolution at potentially lower computational cost than application of purely numeric methods .
a simple but powerful modification of the standard gaussian distribution is studied . the variables of the rectified gaussian are constrained to be nonnegative , enabling the use of nonconvex energy functions . two multimodal examples , the competitive and cooperative distributions , illustrate the representational power of the rectified gaussian . since the cooperative distribution can represent the translations of a pattern , it demonstrates the potential of the rectified gaussian for modeling pattern manifolds .
we introduce a novel fast algorithm for independent component analysis , which can be used for blind source separation and feature extraction . it is shown how a neural network learning rule can be transformed into a txed - point iteration , which provides an algorithm that is very simple , does not depend on any user - detned parameters , and is fast to converge to the most accurate solution allowed by the data . the algorithm tnds , one at a time , all non - gaussian independent components , regardless of their probability distributions . the computations can be performed either in batch mode or in a semi - adaptive manner . the convergence of the algorithm is rigorously proven , and the convergence speed is shown to be cubic . some comparisons to gradient based algorithms are made , showing that the new algorithm is usually #NUM# to #NUM# times faster , sometimes giving the solution in just a few iterations .
barlow ' s seminal work on minimal entropy codes and unsupervised learning is reiterated . in particular , the need to transmit the probability of events is put in a practical neuronal framework for detecting suspicious events . a variant of the bcm learning rule [ #NUM# ] is presented together with some mathematical results suggesting optimal minimal entropy coding .
constructive induction divides the problem of learning an inductive hypothesis into two intertwined searches : onefor the best representation space , and twofor the best hypothesis in that space . in data - driven constructive induction ( dci ) , a learning system searches for a better representation space by analyzing the input examples ( data ) . the presented data - driven constructive induction method combines an aq - type learning algorithm with two classes of representation space improvement operators : constructors , and destructors . the implemented system , aq #NUM# - dci , has been experimentally applied to a gnp prediction problem using a world bank database . the results show that decision rules learned by aq #NUM# - dci outperformed the rules learned in the original representation space both in predictive accuracy and rule simplicity .
heuristic measures for estimating the quality of attributes mostly assume the independence of attributes so in domains with strong dependencies between attributes their performance is poor . relief and its extension relieff are capable of correctly estimating the quality of attributes in classification problems with strong dependencies between attributes . by exploiting local information provided by different contexts they provide a global view . we present the analysis of reli - eff which lead us to its adaptation to regression ( continuous class ) problems . the experiments on artificial and real - world data sets show that re - gressional relieff correctly estimates the quality of attributes in various conditions , and can be used for non - myopic learning of the regression trees . regressional relieff and relieff provide a unified view on estimating the attribute quality in regression and classification .
a new research area , inductive logic programming , is presently emerging . while inheriting various positive characteristics of the parent subjects of logic programming and machine learning , it is hoped that the new area will overcome many of the limitations of its forebears . the background to present developments within this area is discussed and various goals and aspirations for the increasing body of researchers are identified . inductive logic programming needs to be based on sound principles from both logic and statistics . on the side of statistical justification of hypotheses we discuss the possible relationship between algorithmic complexity theory and probably - approximately - correct ( pac ) learning . in terms of logic we provide a unifying framework for muggleton and buntine ' s inverse resolution ( ir ) and plotkin ' s relative least general generali - sation ( rlgg ) by rederiving rlgg in terms of ir . this leads to a discussion of the feasibility of extending the rlgg framework to allow for the invention of new predicates , previously discussed only within the context of ir .
bayesian methods are applicable to complex modeling tasks . in this review , the principles of bayesian inference are presented and applied to neural network models . several approximate implementations are discussed , and their advantages over conventional fre - quentist model training and selection are outlined . it is argued that bayesian methods are preferable to traditional approaches , although empirical evidence for this is still sparse .
this paper presents an efficient algorithm for learning bayesian belief networks from databases . the algorithm takes a database as input and constructs the belief network structure as output . the construction process is based on the computation of mutual information of attribute pairs . given a data set that is large enough , this algorithm can generate a belief network very close to the underlying model , and at the same time , enjoys the time when the data set has a normal dag - faithful ( see section #NUM# . #NUM# ) probability distribution , the algorithm guarantees that the structure of a perfect map [ pearl , #NUM# ] of the underlying dependency model is generated . to evaluate this algorithm , we present the experimental results on three versions of the well - known alarm network database , which has #NUM# attributes and #NUM# , #NUM# records . the results show that this algorithm is accurate and efficient . the proof of correctness and the analysis of complexity of o n ( ) #NUM# on conditional independence ( ci ) tests .
this paper presents an efficient algorithm for constructing bayesian belief networks from databases . the algorithm takes a database and an attributes ordering ( i . e . , the causal attributes of an attribute should appear earlier in the order ) as input and constructs a belief network structure as output . the construction process is based on the computation of mutual information of attribute pairs . given a data set which is large enough and has a dag - isomorphic probability distribution , this algorithm guarantees that the perfect map [ #NUM# ] of the underlying dependency tests . to evaluate this algorithm , we present the experimental results on three versions of the well - known alarm network database , which has #NUM# attributes and #NUM# , #NUM# records . the correctness proof and the analysis of computational complexity are also presented . we also discuss the features of our work and relate it to previous works . model is generated , and at the same time , enjoys the time complexity of o n ( ) #NUM# on conditional independence ( ci )
a novel method for regression has been recently proposed by v . vapnik et al . [ #NUM# , #NUM# ] . the technique , called support vector machine ( svm ) , is very well founded from the mathematical point of view and seems to provide a new insight in function approximation . we implemented the svm and tested it on the same data base of chaotic time series that was used in [ #NUM# ] to compare the performances of different approximation techniques , including polynomial and rational approximation , local polynomial techniques , radial basis functions , and neural networks . the svm performs better than the approaches presented in [ #NUM# ] . we also study , for a particular time series , the variability in performance with respect to the few free parameters of svm .
the requirement for dense interconnect in artificial neural network systems has led researchers to seek high - density interconnect technologies . this paper reports an implementation using multi - chip modules ( mcms ) as the interconnect medium . the specific system described is a self - organizing , parallel , and dynamic learning model which requires a dense interconnect technology for effective implementation ; this requirement is fulfilled by exploiting mcm technology . the ideas presented in this paper regarding an mcm implementation of artificial neural networks are versatile and can be adapted to apply to other neural network and connectionist models .
when specializing a recursive predicate in order to exclude a set of negative examples without excluding a set of positive examples , it may not be possible to specialize or remove any of the clauses in a refutation of a negative example without excluding any positive exam ples . a previously proposed solution to this problem is to apply program transformation in order to obtain non - recursive target predicates from recursive ones . however , the application of this method prevents recursive specializations from being found . in this work , we present the algorithm spectre ii which is not limited to specializing non - recursive predicates . the key idea upon which the algorithm is based is that it is not enough to specialize or remove clauses in refutations of negative examples in order to obtain correct specializations , but it is sometimes necessary to specialize clauses that appear only in refutations of positive examples . in contrast to its predecessor spectre , the new algorithm is not limited to specializing clauses defining one predicate only , but may specialize clauses defining multiple predicates . furthermore , the positive and negative examples are no longer required to be instances of the same predicate . it is proven that the algorithm produces a correct specialization when all positive examples are logical consequences of the original program , there is a finite number of derivations of positive and negative examples and when no positive and negative examples have the same sequence of input clauses in their refutations .
program w . r . t . positive and negative examples can be viewed as the problem of pruning an sld - tree such that all refutations of negative examples and no refutations of positive examples are excluded . it is shown that the actual pruning can be performed by applying unfolding and clause removal . the algorithm spectre is presented , which is based on this idea . the input to the algorithm is , besides a logic program and positive and negative examples , a computation rule , which determines the shape of the sld - tree that is to be pruned . it is shown that the generality of the resulting specialization is dependent on the computation rule , and experimental results are presented from using three different computation rules . the experiments indicate that the computation rule should be formulated so that the number of applications of unfolding is kept as low as possible . the algorithm , which uses a divide - and - conquer method , is also compared with a covering algorithm . the experiments show that a higher predictive accuracy can be achieved if the focus is on discriminating positive from negative examples rather than on achieving a high coverage of positive examples only .
case - based reasoning systems have traditionally been used to perform high - level reasoning in problem domains that can be adequately described using discrete , symbolic representations . however , many real - world problem domains , such as autonomous robotic navigation , are better characterized using continuous representations . such problem domains also require continuous performance , such as continuous sensori - motor interaction with the environment , and continuous adaptation and learning during the performance task . we introduce a new method for continuous case - based reasoning , and discuss how it can be applied to the dynamic selection , modification , and acquisition of robot behaviors in autonomous navigation systems . we conclude with a general discussion of case - based reasoning issues addressed by this work .
previous algorithms for the recovery of bayesianbelief network structures from data have been either highly dependent on conditional independence ( ci ) tests , or have required an ordering on the nodes to be supplied by the user . we present an algorithm that integrates these two approaches - ci tests are used to generate an ordering on the nodes from the database which is then used to recover the underlying bayesian network structure using a non ci test based method . results of the evaluation of the algorithm on a number of databases ( e . g . alarm , led and soybean ) are presented . we also discuss some algorithm performance issues and open problems .
we propose a new criterion for model selection in prediction problems . the covariance inflation criterion adjusts the training error by the average covariance of the predictions and responses , when the prediction rule is applied to permuted versions of the dataset . this criterion can be applied to general prediction problems ( for example regression or classification ) , and to general prediction rules ( for example stepwise regression , tree - based models and neural nets ) . as a byproduct we obtain a measure of the effective number of parameters used by an adaptive procedure . we relate the covariance inflation criterion to other model selection procedures and illustrate its use in some regression and classification problems . we also revisit the conditional bootstrap approach to model selection .
this paper introduces the magnetic neural gas ( mng ) algorithm , which extends unsupervised competitive learning with class information to improve the positioning of radial basis functions . the basic idea of mng is to discover heterogeneous clusters ( i . e . , clusters with data from different classes ) and to migrate additional neurons towards them . the discovery is effected by a heterogeneity coefficient associated with each neuron and the migration is guided by introducing a kind of magnetic effect . the performance of mng is tested on a number of data sets , including the thyroid data set . results demonstrate promise .
this research is supported by a national science foundation fellowship awarded to dario salvucci and office of naval research grant n #NUM# - #NUM# - #NUM# - #NUM# awarded to john anderson . the views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies , either expressed or implied , of the national science foundation , the office of naval research , or the united states government .
a novel architecture and set of learning rules for cortical self - organization is proposed . the model is based on the idea that multiple information channels can modulate one another ' s plasticity . features learned from bottom - up information sources can thus be influenced by those learned from contextual pathways , and vice versa . a maximum likelihood cost function allows this scheme to be implemented in a biologically feasible , hierarchical neural circuit . in simulations of the model , we first demonstrate the utility of temporal context in modulating plasticity . the model learns a representation that categorizes people ' s faces according to identity , independent of viewpoint , by taking advantage of the temporal continuity in image sequences . in a second set of simulations , we add plasticity to the contextual stream and explore variations in the architecture . in this case , the model learns a two - tiered representation , starting with a coarse view - based clustering and proceeding to a finer clustering of more specific stimulus features . this model provides a tenable account of how people may perform #NUM# d object recognition in a hierarchical , bottom - up fashion .
the boosting algorithm adaboost , developed by freund and schapire , has exhibited outstanding performance on several benchmark problems when using c #NUM# . #NUM# as the " weak " algorithm to be " boosted . " like other ensemble learning approaches , adaboost constructs a composite hypothesis by voting many individual hypotheses . in practice , the large amount of memory required to store these hypotheses can make ensemble methods hard to deploy in applications . this paper shows that by selecting a subset of the hypotheses , it is possible to obtain nearly the same levels of performance as the entire set . the results also provide some insight into the behavior of adaboost .
infusion of a gaba agonist ( reiter & stryker , #NUM# ) and infusion of an nmda receptor antagonist ( bear et al . , #NUM# ) , in the primary visual cortex of kittens during monocular deprivation , shifts ocular dominance toward the closed eye , in the cortical region near the infusion site . this reverse ocular dominance shift has been previously modeled by variants of a covariance synaptic plasticity rule ( bear et al . , #NUM# ; clothiaux et al . , #NUM# ; miller et al . , #NUM# ; reiter & stryker , #NUM# ) . kasamatsu et al . ( #NUM# , #NUM# ) showed that infusion of an nmda receptor antagonist in adult cat primary visual cortex changes ocular dominance distribution , reduces binocularity , and reduces orientation and direction selectivity . this paper presents a novel account of the effects of these pharmacological treatments , based on the exin synaptic plasticity rules ( marshall , #NUM# ) , which include both an instar afferent excitatory and an outstar lateral inhibitory rule . functionally , the exin plasticity rules enhance the efficiency , discrimination , and context - sensitivity of a neural network ' s representation of perceptual patterns ( marshall , #NUM# ; marshall & gupta , #NUM# ) . the exin model decreases lateral inhibition from neurons outside the infusion site ( control regions ) to neurons inside the infusion region , during monocular deprivation . in the model , plasticity in afferent pathways to neurons affected by the pharmacological treatments is assumed to be blocked , as opposed to previous models ( bear et al . , #NUM# ; miller et al . , #NUM# ; reiter & stryker , #NUM# ) , in which afferent pathways from the open eye to neurons in the infusion region are weakened . the proposed model is consistent with results suggesting that long - term plasticity can be blocked by nmda antagonists or by postsynaptic hyperpolarization ( bear et al . , #NUM# ; dudek & bear , #NUM# ; goda & stevens , #NUM# ; kirkwood et al . , #NUM# ) . since the role of plasticity in lateral inhibitory pathways in producing cortical plasticity has not received much attention , several predictions are made based on the exin lateral inhibitory plasticity rule .
we first present an algorithm that uses membership and equivalence queries to exactly identify a discretized geometric concept defined by the union of m axis - parallel boxes in d - dimensional discretized euclidean space where each coordinate can have n discrete values . this algorithm receives at most md counterexamples and uses time and membership queries polynomial in m and log n for d any constant . furthermore , all equivalence queries can be formulated as the union of o ( md log m ) axis - parallel boxes . next , we show how to extend our algorithm to efficiently learn , from only equivalence queries , any discretized geometric concept generated from any number of halfspaces with any number of known ( to the learner ) slopes in a constant dimensional space . in particular , our algorithm exactly learns ( from equivalence queries only ) unions of discretized axis - parallel boxes in constant dimensional space in polynomial time . furthermore , this algorithm can be modified to handle a polynomial number of lies in the counterexamples provided by the environment . finally , we introduce a new complexity measure that better captures the complexity of the union of m boxes than simply the number of boxes and the dimension . our new measure , , is the number of segments in the target where a segment is a maximum portion of one of the sides of the target that lies entirely inside or entirely outside each of the other halfspaces defining the target . we present a modification of our first algorithm that uses time and queries polynomial in and log n . in fact , the time and queries ( both membership and equivalence ) used by this single algorithm are polynomial for either m or d constant .
local belief propagation rules of the sort proposed by pearl ( #NUM# ) are guaranteed to converge to the optimal beliefs for singly connected networks . recently , a number of researchers have empirically demonstrated good performance of these same algorithms on networks with loops , but a theoretical understanding of this performance has yet to be achieved . here we lay a foundation for an understanding of belief propagation in networks with loops . for networks with a single loop , we derive an analytical relationship between the steady state beliefs in the loopy network and the true posterior probability . using this relationship we show a category of networks for which the map estimate obtained by belief update and by belief revision can be proven to be optimal ( although the beliefs will be incorrect ) . we show how nodes can use local information in the messages they receive in order to correct the steady state beliefs . furthermore we prove that for all networks with a single loop , the map estimate obtained by belief revision at convergence is guaranteed to give the globally optimal sequence of states . the result is independent of the length of the cycle and the size of the state space . for networks with multiple loops , we introduce the concept of a " balanced network " and show simulation results comparing belief revision and update in such networks . we show that the turbo code structure is balanced and present simulations on a toy turbo code problem indicating the decoding obtained by belief revision at convergence is significantly more likely to be correct . this report describes research done at the center for biological and computational learning and the department of brain and cognitive sciences of the massachusetts institute of technology . support for the center is provided in part by a grant from the national science foundation under contract asc - #NUM# . yw was also supported by nei r #NUM# ey #NUM# to e . h . adelson
previous work showed the combination of a genetic algorithm using an order or permutation chromosome combined with hand coded " greedy " optimizers can readily produce an optimal schedule for a four node test problem [ langdon , #NUM# ] . following this the same ga has been used to find low cost schedules for the south wales region of the uk high voltage power network . this paper describes the evolution of the best known schedule for the base south wales problem using genetic programming starting from the hand coded heuris tics used with the ga .
search mechanisms of artificial intelligence combine two elements : representation , which determines the search space , and a search mechanism , which actually explores the space . unfortunately , many searches may explore redundant and / or invalid solutions . genetic programming refers to a class of evolutionary algorithms based on genetic algorithms but utilizing a parameterized representation in the form of trees . these algorithms perform searches based on simulation of nature . they face the same problems of redundant / invalid subspaces . these problems have just recently been addressed in a systematic manner . this paper presents a methodology devised for the public domain genetic programming tool lil - gp . this methodology uses data typing and semantic information to constrain the representation space so that only valid , and possibly unique , solutions will be explored . the user enters problem - specific constraints , which are transformed into a normal set . this set is checked for feasibility , and subsequently it is used to limit the space being explored . the constraints can determine valid , possibly unique space . moreover , they can also be used to exclude subspaces the user considers uninteresting , using some problem - specific knowledge . a simple example is followed thoroughly to illustrate the constraint language , transformations , and the normal set . experiments with boolean #NUM# - multiplexer illustrate practical applications of the method to limit redundant space exploration by utilizing problem - specific knowledge .
knowledge acquisition is a difficult , error - prone , and time - consuming task . the task of automatically improving an existing knowledge base using learning methods is addressed by the class of systems performing theory refinement . this paper presents a system , forte ( first - order revision of theories from examples ) , which refines first - order horn - clause theories by integrating a variety of different revision techniques into a coherent whole . forte uses these techniques within a hill - climbing framework , guided by a global heuristic . it identifies possible errors in the theory and calls on a library of operators to develop possible revisions . the best revision is implemented , and the process repeats until no further revisions are possible . operators are drawn from a variety of sources , including propositional theory refinement , first - order induction , and inverse resolution . forte is demonstrated in several domains , including logic programming and qualitative modelling .
the problem of sequence categorization is to generalize from a corpus of labeled sequences procedures for accurately labeling future unlabeled sequences . the choice of representation of sequences can have a major impact on this task , and in the absence of background knowledge a good representation is often not known and straightforward representations are often far from optimal . we propose a feature generation method ( called fgen ) that creates boolean features that check for the presence or absence of heuristically selected collections of subsequences . we show empirically that the representation computed by fgen improves the accuracy of two commonly used learning systems ( c #NUM# . #NUM# and ripper ) when the new features are added to existing representations of sequence data . we show the superiority of fgen across a range of tasks selected from three domains : dna sequences , unix command sequences , and english text .
genetic algorithms are one example of the use of a random element within an algorithm for combinatorial optimization . we consider the application of the genetic algorithm to a particular problem , the assembly line balancing problem . a general description of genetic algorithms is given , and their specialized use on our test - bed problems is discussed . we carry out extensive computational testing to find appropriate values for the various parameters associated with this genetic algorithm . these experiments underscore the importance of the correct choice of a scaling parameter and mutation rate to ensure the good performance of a genetic algorithm . we also describe a parallel implementation of the genetic algorithm and give some comparisons between the parallel and serial implementations . both versions of the algorithm are shown to be effective in producing good solutions for problems of this type ( with appropriately chosen parameters ) .
this paper presents the application of case - based reasoning methods to the kosimo data base of international conflicts . a case - based reasoning tool - vie - cbr has been deveolped and used for the classification of various outcome variables , like political , military , and territorial outcome , solution modalities , and conflict intensity . in addition , the case retrieval algorithms are presented as an interactive , user - modifiable tool for intelli gently searching the conflict data base for precedent cases .
in order to learn more about the behaviour of case - based reasoners as learning systems , we form - alise a simple case - based learner as a pac learning algorithm , using the case - based representation hcb ; i . we first consider a ` naive ' case - based learning algorithm cb #NUM# ( h ) which learns by collecting all available cases into the case - base and which calculates similarity by counting the number of features on which two problem descriptions agree . we present results concerning the consistency of this learning algorithm and give some partial results regarding its sample complexity . we are able to characterise cb #NUM# ( h ) as a ` weak but general ' learning algorithm . we then consider how the sample complexity of case - based learning can be reduced for specific classes of target concept by the application of inductive bias , or prior knowledge of the class of target concepts . following recent work demonstrating how case - based learning can be improved by choosing a similarity measure appropriate to the concept being learnt , we define a second case - based learning ` algorithm ' cb #NUM# which learns using the best possible similarity measure that might be inferred for the chosen target concept . while cb #NUM# is not an executable learning strategy ( since the chosen similarity measure is defined in terms of a priori knowledge of the actual target concept ) it allows us to assess in the limit the maximum possible contribution of this approach to case - based learning . also , in addition to illustrating the role of inductive bias , the definition of cb #NUM# simplifies the general problem of establishing which functions might be represented in the form hcb ; i . reasoning about the case - based representation in this special case has therefore been a little more straight - forward than in the general case of cb #NUM# ( h ) , allowing more substantial results regarding representable functions and sample complexity to be presented for cb #NUM# . in assessing these results , we are forced to conclude that case - based learning is not the best approach to learning the chosen concept space ( the space of monomial functions ) . we discuss , however , how our study has demonstrated , in the context of case - based learning , the operation of concepts well known in machine learning such as inductive bias and the trade - off between computational complexity and sample complexity .
in the past few years the evolutionary computation landscape has been rapidly changing as a result of increased levels of interaction between various research groups and the injection of new ideas which challenge old tenets . the effect has been simultaneously exciting , invigorating , annoying , and bewildering to the old - timers as well as the new - comers to the field . emerging out of all of this activity are the beginnings of some structure , some common themes , and some agreement on important open issues . we attempt to summarize these emergent properties in this paper .
we quantify both experimentally and analytically the performance of memory - based reasoning ( mbr ) algorithms . to start gaining insight into the capabilities of mbr algorithms , we compare an mbr algorithm using a value difference metric to a popular bayesian classifier . these two approaches are similar in that they both make certain independence assumptions about the data . however , whereas mbr uses specific cases to perform classification , bayesian methods summarize the data probabilistically . we demonstrate that a particular mbr system called pebls works comparatively well on a wide range of domains using both real and artificial data . with respect to the artificial data , we consider distributions where the concept classes are separated by functional discriminants , as well as time - series data generated by markov models of varying complexity . finally , we show formally that pebls can learn ( in the limit ) natural concept classes that the bayesian classifier cannot learn , and that it will attain perfect accuracy whenever
the k - nearest - neighbor decision rule assigns an object of unknown class to the plurality class among the k labeled " training " objects that are closest to it . closeness is usually deflned in terms of a metric distance on the euclidean space with the input measurement variables as axes . the metric chosen to deflne this distance can strongly efiect performance . an optimal choice depends on the problem at hand as characterized by the respective class distributions on the input measurement space , and within a given problem , on the location of the unknown object in that space . in this paper new types of k - nearest - neighbor procedures are described that estimate the local relevance of each input variable , or their linear combinations , for each individual point to be classifled . this information is then used to separately customize the metric used to deflne distance from that object in flnding its nearest neighbors . these procedures are a hybrid between regular k - nearest - neighbor methods and treestructured recursive partitioning techniques popular in statistics and machine learning .
seismic data interpretation problems are typically solved using computationally intensive local search methods which often result in inferior solutions . here , a traditional hybrid genetic algorithm is compared with different staged hybrid genetic algorithms on the geophysical imaging static corrections problem . the traditional hybrid genetic algorithm used here applied local search to every offspring produced by genetic search . the staged hybrid genetic algorithms were designed to temporally separate the local and genetic search components into distinct phases so as to minimize interference between the two search methods . the results show that some staged hybrid genetic algorithms produce higher quality solutions while using significantly less computational time for this problem .
we describe an immune system model based on a universe of binary strings . the model is directed at understanding the pattern recognition processes and learning that take place at both the individual and species levels in the immune system . the genetic algorithm ( ga ) is a central component of our model . in the paper we study the behavior of the ga on two pattern recognition problems that are relevant to natural immune systems . finally , we compare our model with explicit fitness sharing techniques for genetic algorithms , and show that our model implements a form of implicit fitness sharing .
several authors have made a link between hidden markov models for time series and energy - based models ( luttrell #NUM# , williams #NUM# , saul and jordan #NUM# ) . saul and jordan ( #NUM# ) discuss a linear boltzmann chain model with state - state transition energies a ii #NUM# ( going from state i to state i #NUM# ) and symbol emission energies b ij , under which the probability of an entire state fi l ; j l g l whilst any hmm can be written as a linear boltzmann chain by setting exp ( a ii #NUM# ) = a ii #NUM# , exp ( b ij ) = b ij and exp ( i ) = i , not all linear boltzmann chains can be represented as hmms ( saul and jordan #NUM# ) . however , the difference between the two models is minimal . to be precise , if the final hidden
we present a coevolutionary approach to learning sequential decision rules which appears to have a number of advantages over non - coevolutionary approaches . the coevolutionary approach encourages the formation of stable niches representing simpler sub - behaviors . the evolutionary direction of each subbehavior can be controlled independently , providing an alternative to evolving complex behavior using intermediate training steps . results are presented showing a significant learning rate speedup over a non - coevolutionary approach in a simulated robot domain . in addition , the results suggest the coevolutionary approach may lead to emer gent problem decompositions .
appropriate bias is widely viewed as the key to efficient learning and generalization . i present a new algorithm , the incremental delta - bar - delta ( idbd ) algorithm , for the learning of appropriate biases based on previous learning experience . the idbd algorithm is developed for the case of a simple , linear learning system | the lms or delta rule with a separate learning - rate parameter for each input . the idbd algorithm adjusts the learning - rate parameters , which are an important form of bias for this system . because bias in this approach is adapted based on previous learning experience , the appropriate testbeds are drifting or non - stationary learning tasks . for particular tasks of this type , i show that the idbd algorithm performs better than ordinary lms and in fact finds the optimal learning rates . the idbd algorithm extends and improves over prior work by jacobs and by me in that it is fully incremental and has only a single free parameter . this paper also extends previous work by presenting a derivation of the idbd algorithm as gradient descent in the space of learning - rate parameters . finally , i offer a novel interpretation of the idbd algorithm as an incremental form of hold - one - out cross validation .
neural network pruning methods on the level of individual network parameters ( e . g . connection weights ) can improve generalization . an open problem in the pruning methods known today ( obd , obs , autoprune , epsiprune ) is the selection of the number of parameters to be removed in each pruning step ( pruning strength ) . this paper presents a pruning method lprune that automatically adapts the pruning strength to the evolution of weights and loss of generalization during training . the method requires no algorithm parameter adjustment by the user . the results of extensive experimentation indicate that lprune is often superior to autoprune ( which is superior to obd ) on diagnosis tasks unless severe pruning early in the training process is required . results of statistical significance tests comparing autoprune to the new method lprune as well as to backpropagation with early stopping are given for #NUM# different problems .
icsim is a connectionist net simulator being developed at icsi and written in sather . it is object - oriented to meet the requirements for flexibility and reuse of homogeneous and structured connectionist nets and to allow the user to encapsulate efficient customized implementations perhaps running on dedicated hardware . nets are composed by combining off - the - shelf library classes and if necessary by specializing some of their behaviour . general user interface classes allow a uniform or customized graphic presentation of the nets being modeled .
in experience - based ( or case - based ) reasoning , new problems are solved by retrieving and adapting the solutions to similar problems encountered in the past . an important issue in experience - based reasoning is to identify different types of knowledge and reasoning useful for different classes of case - adaptation tasks . in this paper , we examine a class of non - routine case - adaptation tasks that involve patterned insertions of new elements in old solutions . we describe a model - based method for solving this task in the context of the design of physical devices . the method uses knowledge of generic teleological mechanisms ( gtms ) such as cascading . old designs are adapted to meet new functional specifications by accessing and instantiating the appropriate gtm . the kritik #NUM# system evaluates the computational feasibility and sufficiency of this method for design adaptation .
the utility problem in learning systems occurs when knowledge learned in an attempt to improve a system ' s performance degrades performance instead . we present a methodology for the analysis of utility problems which uses computational models of problem solving systems to isolate the root causes of a utility problem , to detect the threshold conditions under which the problem will arise , and to design strategies to eliminate it . we present models of case - based reasoning and control - rule learning systems and compare their performance with respect to the swamping utility problem . our analysis suggests that case - based reasoning systems are more resistant to the utility problem than control - rule learning systems . #NUM#
we present a model of similarity - based retrieval which attempts to capture three psychological phenomena : ( #NUM# ) people are extremely good at judging similarity and analogy when given items to compare . ( #NUM# ) superficial remindings are much more frequent than structural remindings . ( #NUM# ) people sometimes experience and use purely structural analogical re - mindings . our model , called mac / fac ( for " many are called but few are chosen " ) consists of two stages . the first stage ( mac ) uses a computationally cheap , non - structural matcher to filter candidates from a pool of memory items . that is , we redundantly encode structured representations as content vectors , whose dot product yields an estimate of how well the corresponding structural representations will match . the second stage ( fac ) uses sme to compute a true structural match between the probe and output from the first stage . mac / fac has been fully implemented , and we show that it is capable of modeling patterns of access found in psychological data .
this paper presents an asocs ( adaptive self - organizing concurrent system ) model for massively parallel processing of incrementally defined rule systems in such areas as adaptive logic , robotics , logical inference , and dynamic control . an asocs is an adaptive network composed of many simple computing elements operating asynchronously and in parallel . this paper focuses on adaptive algorithm #NUM# ( aa #NUM# ) and details its architecture and learning algorithm . it has advantages over previous asocs models in simplicity , implementability , and cost . an asocs can operate in either a data processing mode or a learning mode . during the data processing mode , an asocs acts as a parallel hardware circuit . in learning mode , rules expressed as boolean conjunctions are incrementally presented to the asocs . all asocs learning algorithms incorporate a new rule in a distributed fashion in a short , bounded time .
a fundamental issue in case - based reasoning is similarity assessment : determining similarities and differences between new and retrieved cases . many methods have been developed for comparing input case descriptions to the cases already in memory . however , the success of such methods depends on the input case description being sufficiently complete to reflect the important features of the new situation , which is not assured . in case - based explanation of anomalous events during story understanding , the anomaly arises because the current situation is incompletely understood ; consequently , similarity assessment based on matches between known current features and old cases is likely to fail because of gaps in the current case ' s description . our solution to the problem of gaps in a new case ' s description is an approach that we call constructive similarity assessment . constructive similarity assessment treats similarity assessment not as a simple comparison between fixed new and old cases , but as a process for deciding which types of features should be investigated in the new situation and , if the features are borne out by other knowledge , added to the description of the current case . constructive similarity assessment does not merely compare new cases to old : using prior cases as its guide , it dynamically carves augmented descriptions of new cases out of memory .
much recent research on modeling memory processes has focused on identifying useful indices and retrieval strategies to support particular memory tasks . another important question concerning memory processes , however , is how retrieval criteria are learned . this paper examines the issues involved in modeling the learning of memory search strategies . it discusses the general requirements for appropriate strategy learning and presents a model of memory search strategy learning applied to the problem of retrieving relevant information for adapting cases in case - based reasoning . it discusses an implementation of that model , and , based on the lessons learned from that implementation , points towards issues and directions in refining the model .
the problem of approximating a probability distribution occurs frequently in many areas of applied mathematics , including statistics , communication theory , machine learning , and the theoretical analysis of complex systems such as neural networks . saul and jordan ( #NUM# ) have recently proposed a powerful method for efficiently approximating probability distributions known as structured variational approximations . in structured variational approximations , exact algorithms for probability computation on tractable substructures are combined with variational methods to handle the interactions between the substructures which make the system as a whole intractable . in this note , i present a mathematical result which can simplify the derivation of struc tured variational approximations in the exponential family of distributions .
this paper presents an asocs ( adaptive self - organizing concurrent system ) model for massively parallel processing of incrementally defined rule systems in such areas as adaptive logic , robotics , logical inference , and dynamic control . an asocs is an adaptive network composed of many simple computing elements operating asynchronously and in parallel . this paper focuses on adaptive algorithm #NUM# ( aa #NUM# ) and details its architecture and learning algorithm . it has advantages over previous asocs models in simplicity , implementability , and cost . an asocs can operate in either a data processing mode or a learning mode . during the data processing mode , an asocs acts as a parallel hardware circuit . in learning mode , rules expressed as boolean conjunctions are incrementally presented to the asocs . all asocs learning algorithms incorporate a new rule in a distributed fashion in a short , bounded time .
autonomous vehicles are likely to require sophisticated software controllers to maintain vehicle performance in the presence of vehicle faults . the test and evaluation of complex software controllers is expected to be a challenging task . the goal of this e ffort is to apply machine learning techniques from the field of arti ficial intelligence to the general problem of evaluating an intelligent controller for an autonomous vehicle . the approach involves subjecting a controller to an adaptively chosen set of fault scenarios within a vehicle simulator , and searching for combinations of faults that produce noteworthy performance by the vehicle controller . the search employs a genetic algorithm . we illustrate the approach by evaluating the performance of a subsumption - based controller for an autonomous vehicle . the preliminary evidence suggests that this approach is an e ffective alternative to manual testing of sophisticated software controllers .
speedup learning seeks to improve the efficiency of search - based problem solvers . in this paper , we propose a new theoretical model of speedup learning which captures systems that improve problem solving performance by solving a user - given set of problems . we also use this model to motivate the notion of " batch problem solving , " and argue that it is more congenial to learning than sequential problem solving . our theoretical results are applicable to all serially decomposable domains . we empirically validate our results in the domain of eight puzzle . #NUM#
non - parametric density estimation is the problem of approximating the values of a probability density function , given samples from the associated distribution . non - parametric estimation finds applications in discriminant analysis , cluster analysis , and flow calculations based on smoothed particle hydrodynamics . usual estimators make use of kernel functions , and require on the order of n #NUM# arithmetic operations to evaluate the density at n sample points . we describe a sequence of special weight functions which requires almost linear number of operations in n for the same computation .
in this paper , we consider learning first - order horn programs from entailment . in particular , we show that any subclass of first - order acyclic horn programs with constant arity is exactly learnable from equivalence and entailment membership queries provided it allows a polynomial - time subsumption procedure and satisfies some closure conditions . one consequence of this is that first - order acyclic determinate horn programs with constant arity are exactly learnable from equiv alence and entailment membership queries .
a strategy for using genetic algorithms ( gas ) to solve np - complete problems is presented . the key aspect of the approach taken is to exploit the observation that , although all np - complete problems are equally difficult in a general computational sense , some have much better ga representations than others , leading to much more successful use of gas on some np - complete problems than on others . since any np - complete problem can be mapped into any other one in polynomial time , the strategy described here consists of identifying a canonical np - complete problem on which gas work well , and solving other np - complete problems indirectly by mapping them onto the canonical problem . initial empirical results are presented which support the claim that the boolean satisfiability problem ( sat ) is a ga - effective canonical problem , and that other np - complete problems with poor ga representations can be solved efficiently by mapping them first onto sat problems .
fully cooperative multiagent systemsthose in which agents share a joint utility modelis of special interest in ai . a key problem is that of ensuring that the actions of individual agents are coordinated , especially in settings where the agents are autonomous decision makers . we investigate approaches to learning coordinated strategies in stochastic domains where an agent ' s actions are not directly observable by others . much recent work in game theory has adopted a bayesian learning perspective to the more general problem of equilibrium selection , but tends to assume that actions can be observed . we discuss the special problems that arise when actions are not observable , including effects on rates of convergence , and the effect of action failure probabilities and asymmetries . we also use likelihood estimates as a means of generalizing fictitious play learning models in our setting . finally , we propose the use of maximum likelihood as a means of removing strategies from consideration , with the aim of convergence to a conventional equilibrium , at which point learning and deliberation can cease .
humans appear to often solve problems in a new domain by transferring their expertise from a more familiar domain . however , making such cross - domain analogies is hard and often requires abstractions common to the source and target domains . recent work in case - based design suggests that generic mechanisms are one type of abstractions used by designers . however , one important yet unexplored issue is where these generic mechanisms come from . we hypothesize that they are acquired incrementally from problem - solving experiences in familiar domains by generalization over patterns of regularity . three important issues in generalization from experiences are what to generalize from an experience , how far to generalize , and what methods to use . in this paper , we show that mental models in a familiar domain provide the content , and together with the problem - solving context in which learning occurs , also provide the constraints for learning generic mechanisms from design experiences . in particular , we show how the model - based learning method integrated with similarity - based learning addresses the issues in generalization from experiences .
the optimization of a single bit string by means of iterated mutation and selection of the best ( a ( #NUM# + #NUM# ) - genetic algorithm ) is discussed with respect to three simple fitness functions : the counting ones problem , a standard binary encoded integer , and a gray coded integer optimization problem . a mutation rate schedule that is optimal with respect to the success probability of mutation is presented for each of the objective functions , and it turns out that the standard binary code can hamper the search process even in case of unimodal objective functions . while normally a mutation rate of #NUM# = l ( where l denotes the bit string length ) is recommendable , our results indicate that a variation of the mutation rate is useful in cases where the fitness function is a multimodal pseudo - boolean function , where multimodality may be caused by the objective function as well as the encoding mechanism .
genetic algorithms are used to learn navigation and collision avoidance behaviors for robots . the learning is performed under simulation , and the resulting behaviors are then used to control the the approach to learning behaviors for robots described here reflects a particular methodology for learning via a simulation model . the motivation is that making mistakes on real systems may be costly or dangerous . in addition , time constraints might limit the number of experiences during learning in the real world , while in many cases , the simulation model can be made to run faster than real time . since learning may require experimenting with behaviors that might occasionally produce unacceptable results if applied to the real world , or might require too much time in the real environment , we assume that hypothetical behaviors will be evaluated in a simulation model ( the off - line system ) . as illustrated in figure #NUM# , the current best behavior can be placed in the real , on - line system , while learning continues in the off - line system [ #NUM# ] . the learning algorithm was designed to learn useful behaviors from simulations of limited fidelity . the expectation is that behaviors learned in these simulations will be useful in real - world environments . previous studies have illustrated that knowledge learned under simulation is robust and might be applicable to the real world if the simulation is more general ( i . e . has more noise , more varied conditions , etc . ) than the real world environment [ #NUM# ] . where this is not possible , it is important to identify the differences between the simulation and the world and note the effect upon the learning process . the research reported here continues to examine this hypothesis . the next section very briefly explains the learning algorithm ( and gives pointers to where more extensive documentation can be found ) . after that , the actual robot is described . then we describe the simulation of the robot . the task _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ actual robot .
conventional intelligent tutoring systems ( its ) do not acknowledge uncertainty about the student ' s knowledge . yet , both the outcome of any teaching intervention and the exact state of the student ' s knowledge are uncertain . in recent years , researchers have made startling progress in the management of uncertainty in knowledge - based systems . building on these developments , we describe an its architecture that explicitly models uncertainty . this will facilitate more accurate student modeling and provide its ' s which can learn .
satisfiability ( sat ) refers to the task of finding a truth assignment that makes an arbitrary boolean expression true . this paper compares a simulated annealing algorithm ( sasat ) with gsat ( selman et al . , #NUM# ) , a greedy algorithm for solving satisfiability problems . gsat can solve problem instances that are extremely difficult for traditional satisfiability algorithms . results suggest that sasat scales up better as the number of variables increases , solving at least as many hard sat problems with less effort . the paper then presents an ablation study that helps to explain the relative advantage of sasat over gsat . next , an improvement to the basic sasat algorithm is examined , based on a random walk implemented in gsat ( selman et al . , #NUM# ) . finally , we examine the performance of sasat on a test suite of satisfiability problems produced by the #NUM# dimacs challenge .
in the last few years several researchers within the artificial life and mobile robotics community used artificial neural networks . explicitly viewing neural networks in an artificial life perspective has a number of consequences that make research on what we will call artificial life neural networks ( alnns ) rather different from traditional connectionist research . the aim of the paper is to make the differences between alnns and " classical " neural networks explicit .
recent interest has come about in deriving various neural network architectures for modelling time - dependent signals . a number of algorithms have been published for multilayer perceptrons with synapses described by finite impulse response ( fir ) and infinite impulse response ( iir ) filters ( the latter case is also known as locally recurrent globally feedforward networks ) . the derivations of these algorithms have used different approaches in calculating the gradients , and in this note , we present a short , but unifying account of how these different algorithms compare for the fir case , both in derivation , and performance . new algorithms are subsequently presented . simulation results have been performed to benchmark these algorithms . in this note , results are compared for the mackey - glass chaotic time series against a number of other methods including a standard multilayer perceptron , and a local approximation method .
we propose a methodology for bayesian model determination in decomposable graphical gaussian models . to achieve this aim we consider a hyper inverse wishart prior distribution on the concentration matrix for each given graph . to ensure compatibility across models , such prior distributions are obtained by marginalisation from the prior conditional on the complete graph . we explore alternative structures for the hyperparameters of the latter , and their consequences for the model . model determination is carried out by implementing a reversible jump mcmc sampler . in particular , the dimension - changing move we propose involves adding or dropping an edge from the graph . we characterise the set of moves which preserve the decomposability of the graph , giving a fast algorithm for maintaining the junction tree representation of the graph at each sweep . as state variable , we propose to use the incomplete variance - covariance matrix , containing only the elements for which the corresponding element of the inverse is nonzero . this allows all computations to be performed locally , at the clique level , which is a clear advantage for the analysis of large and complex data - sets . finally , the statistical and computational performance of the procedure is illustrated by means of both artificial and real multidimensional data - sets .
an essential component of opportunistic behavior is opportunity recognition , the recognition of those conditions that facilitate the pursuit of some suspended goal . opportunity recognition is a special case of situation assessment , the process of sizing up a novel situation . the ability to recognize opportunities for reinstating suspended problem contexts ( one way in which goals manifest themselves in design ) is crucial to creative design . in order to deal with real world opportunity recognition , we attribute limited inferential power to relevant suspended goals . we propose that goals suspended in the working memory monitor the internal ( hidden ) representations of the currently recognized objects . a suspended goal is satisfied when the current internal representation and a suspended goal match . we propose a computational model for working memory and we compare it with other relevant theories of opportunistic planning . this working memory model is implemented as part of our improviser system .
one of the most important aspects of any machine learning paradigm is how it scales according to problem size and complexity . using a task with known optimal training error , and a pre - specified maximum number of training updates , we investigate the convergence of the backpropagation algorithm with respect to a ) the complexity of the required function approximation , b ) the size of the network in relation to the size required for an optimal solution , and c ) the degree of noise in the training data . in general , for a ) the solution found is worse when the function to be approximated is more complex , for b ) oversized networks can result in lower training and generalization error in certain cases , and for c ) the use of committee or ensemble techniques can be more beneficial as the level of noise in the training data is increased . for the experiments we performed , we do not obtain the optimal solution in any case . we further support the observation that larger networks can produce better training and generalization error using a face recognition example where a network with many more parameters than training points generalizes better than smaller networks .
for many reasons , neural networks have become very popular ai machine learning models . two of the most important aspects of machine learning models are how well the model generalizes to unseen data , and how well the model scales with problem complexity . using a controlled task with known optimal training error , we investigate the convergence of the backpropagation ( bp ) algorithm . we find that the optimal solution is typically not found . furthermore , we observe that networks larger than might be expected can result in lower training and generalization error . this result is supported by another real world example . we further investigate the training behavior by analyzing the weights in trained networks ( excess degrees of freedom are seen to do little harm and to aid convergence ) , and contrasting the interpolation characteristics of multi - layer perceptron neural networks ( mlps ) and polynomial models ( overfitting behavior is very different the mlp is often biased towards smoother solutions ) . finally , we analyze relevant theory outlining the reasons for significant practical differences . these results bring into question common beliefs about neural network training regarding convergence and optimal network size , suggest alternate guidelines for practical use ( lower fear of excess degrees of freedom ) , and help to direct future work ( e . g . methods for creation of more parsimonious solutions , importance of the mlp / bp bias and possibly worse performance of improved training algorithms ) .
this paper deals with the retrieval of useful cases in case - based reasoning . it focuses on the questions of what " useful " could mean and how the search for useful cases can be organized . we present the new search algorithm fish and shrink that is able to search quickly through the case base , even if the aspects that deflne usefulness are spontaneously combined at query time . we compare fish and shrink to other algorithms and show that most of them make an implicit closed world assumption . we flnally refer to a realization of the presented idea in the context of the prototype of the fabel - project #NUM# . the scenery is as follows . previously collected cases are stored in a large scaled case base . an expert describes his problem and gives the aspects in which the requested case should be similar . the similarity measure thus given spontaneously shall now be used to explore the case base within a short time , shall present a required number of cases and make sure that none of the other cases is more similar . the question is now how to prepare the previously collected cases and how to deflne a retrieval algorithm which is able to deal with sponta neously user - deflned similarity measures .
the parallel genetic algorithm ( pga ) uses two major modifications compared to the genetic algorithm . firstly , selection for mating is distributed . individuals live in a #NUM# - d world . selection of a mate is done by each individual independently in its neighborhood . secondly , each individual may improve its fitness during its lifetime by e . g . local hill - climbing . the pga is totally asynchronous , running with maximal efficiency on mimd parallel computers . the search strategy of the pga is based on a small number of active and intelligent individuals , whereas a ga uses a large population of passive individuals . we will investigate the pga with deceptive problems and the traveling salesman problem . we outline why and when the pga is succesful . abstractly , a pga is a parallel search with information exchange between the individuals . if we represent the optimization problem as a fitness landscape in a certain configuration space , we see , that a pga tries to jump from two local minima to a third , still better local minima , by using the crossover operator . this jump is ( probabilistically ) successful , if the fitness landscape has a certain correlation . we show the correlation for the traveling salesman problem by a configuration space analysis . the pga explores implicitly the above correlation .
the dominant theme of case - based research at recent ml conferences has been on classifying cases represented by feature vectors . however , other useful tasks can be targeted , and other representations are often preferable . we review the recent literature on case - based learning , focusing on alternative performance tasks and more expressive case representations . we also highlight topics in need of additional research .
current approaches to computational lexicology in language technology are knowledge - based ( competence - oriented ) and try to abstract away from specific formalisms , domains , and applications . this results in severe complexity , acquisition and reusability bottlenecks . as an alternative , we propose a particular performance - oriented approach to natural language processing based on automatic memory - based learning of linguistic ( lexical ) tasks . the consequences of the approach for computational lexicology are discussed , and the application of the approach on a number of lexical acquisition and disambiguation tasks in phonology , morphology and syntax is described .
in this paper we study a new information - theoretically justified approach to missing data estimation for multivariate categorical data . the approach discussed is a model - based imputation procedure relative to a model class ( i . e . , a functional form for the probability distribution of the complete data matrix ) , which in our case is the set of multinomial models with some independence assumptions . based on the given model class assumption an information - theoretic criterion can be derived to select between the different complete data matrices . intuitively this general criterion , called stochastic complexity , represents the shortest code length needed for coding the complete data matrix relative to the model class chosen . using this information - theoretic criteria , the missing data problem is reduced to a search problem , i . e . , finding the data completion with minimal stochastic complexity . in the experimental part of the paper we present empirical results of the approach using two real data sets , and compare these results to those achived by commonly used techniques such as case deletion and imputating sample averages .
we use reversible jump markov chain monte carlo ( mcmc ) methods ( green #NUM# ) to address the problem of model order uncertainty in au - toregressive ( ar ) time series within a bayesian framework . efficient model jumping is achieved by proposing model space moves from the full conditional density for the ar parameters , which is obtained analytically . this is compared with an alternative method , for which the moves are cheaper to compute , in which proposals are made only for the new parameters in each move . results are presented for both synthetic and audio time series .
learning can be viewed as a problem of planning a series of modifications to memory . we adopt this view of learning and propose the applicability of the case - based planning methodology to the task of planning to learn . we argue that relatively simple , fine - grained primitive inferential operators are needed to support flexible planning . we show that it is possible to obtain the benefits of case - based reasoning within a planning to learn framework .
v s - cbr [ #NUM# ] is a simple instance - based learning algorithm that adjusts a weighted similarity measure as well as collecting cases . this paper presents a ` pac ' analysis of v s - cbr , motivated by the pac learning framework , which demonstrates two main ideas relevant to the study of instance - based learners . firstly , the hypothesis spaces of a learner on different target concepts can be compared to predict the difficulty of the target concepts for the learner . secondly , it is helpful to consider the ` constituent parts ' of an instance - based learner : to explore separately how many examples are needed to infer a good similarity measure and how many examples are needed for the case base . applying these approaches , we show that v s - cbr learns quickly if most of the variables in the representation are irrelevant to the target concept and more slowly if there are more relevant variables . the paper relates this overall behaviour to the behaviour of the constituent parts of v s - cbr .
partial determinations are an interesting form of dependency between attributes in a relation . they generalize functional dependencies by allowing exceptions . we modify a known mdl formula for evaluating such partial determinations to allow for its use in an admissible heuristic in exhaustive search . furthermore we describe an efficient preprocessing - based approach for handling numerical attributes . an empirical investigation tries to evaluate the viability of the presented ideas .
the prediction of survival time or recurrence time is an important learning problem in medical domains . the recurrence surface approximation ( rsa ) method is a natural , effective method for predicting recurrence times using censored input data . this paper introduces the survival curve rsa ( sc - rsa ) , an extension to the rsa approach which produces accurate predicted rates of recurrence , while maintaining accuracy on individual predicted recurrence times . the method is applied to the problem of breast cancer recurrence using two different datasets .
we analyze the " query by committee " algorithm , a method for filtering informative queries from a random stream of inputs . we show that if the two - member committee algorithm achieves information gain with positive lower bound , then the prediction error decreases exponentially with the number of queries . we show that , in particular , this exponential decrease holds for query learning of perceptrons .
a new method for performing a nonlinear form of principal component analysis is proposed . by the use of integral operator kernel functions , one can efficiently compute principal components in high - dimensional feature spaces , related to input space by some nonlinear map ; for instance the space of all possible d - pixel products in images . we give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition .
modeling techniques developed recently in the ai and uncertain reasoning communities permit significantly more flexible specifications of probabilistic knowledge . specifically , graphical decision - modeling formalisms | belief networks , influence diagrams , and their variants | provide compact representation of probabilistic relationships , and support inference algorithms that automatically exploit the dependence structure in such models [ #NUM# , #NUM# , #NUM# ] . these advances have brought on a resurgence of interest in computational decision systems based on normative theories of belief and preference . however , graphical decision - modeling languages are still quite limited for purposes of knowledge representation because , while they can describe the relationships among particular event instances , they cannot capture general knowledge about probabilistic relationships across classes of events . the inability to capture general knowledge is a serious impediment for those ai tasks in which the relevant factors of a decision problem cannot be enumerated in advance . a graphical decision model encodes a particular set of probabilistic dependencies , a predefined set of decision alternatives , and a specific mathematical form for a utility function . given a properly specified model , there exist relatively efficient algorithms for calculating posterior probabilities and optimal decision policies . a range of similar cases may be handled by parametric variations of the original model . however , if the structure of dependencies , the set of available alternatives , or the form of utility function changes from situation to situation , then a fixed network representation is no longer adequate . an ideal computational decision system would possess general , broad knowledge of a domain , but would have the ability to reason about the particular circumstances of any given decision problem within the domain . one obvious approach | which we call call knowledge - based model construction ( kbmc ) | is to generate a decision model dynamically at run - time , based on the problem description and information received thus far . model construction consists of selection , instantiation , and assembly of causal and associational relationships from a broad knowledge base of general relationships among domain concepts . for example , suppose we wish to develop a system to recommend appropriate actions for maintaining a computer network . the natural graphical decision model would include chance
determining the conditions for which a given learning algorithm is appropriate is an open problem in machine learning . methods for selecting a learning algorithm for a given domain have met with limited success . this paper proposes a new approach to predicting a given example ' s class by locating it in the " example space " and then choosing the best learner ( s ) in that region of the example space to make predictions . the regions of the example space are defined by the prediction patterns of the learners being used . the learner ( s ) chosen for prediction are selected according to their past performance in that region . this dynamic approach to learning algorithm selection is compared to other methods for selecting from multiple learning algorithms . the approach is then extended to weight rather than select the algorithms according to their past performance in a given region . both approaches are further evaluated on a set of determining the conditions for which a given learning algorithm is appropriate is an open problem in machine learning . methods for selecting a learning algorithm for a given domain ( e . g . [ aha #NUM# , breiman #NUM# ] ) or for a portion of the domain ( [ brodley #NUM# , brodley #NUM# ] ) have met with limited success . this paper proposes a new approach that dynamically selects a learning algorithm for each example by locating it in the " example space " and then choosing the best learner ( s ) for prediction in that part of the example space . the regions of the example space are formed by the observed prediction patterns of the learners being used . the learner ( s ) chosen for prediction are selected according to their past performance in that region which is defined by the " cross - validation history . " this paper introduces ds , a method for the dynamic selection of a learning algorithm ( s ) . we call it " dynamic " because the learning algorithm ( s ) used to classify a novel example depends on that example . preliminary experimentation motivated dw , an extension to ds that dynamically weights the learners predictions according to their regional accuracy . further experimentation compares ds and dw to a collection of other meta - learning strategies such as cross - validation ( [ breiman #NUM# ] ) and various forms of stacking ( [ wolpert #NUM# ] ) . in this phase of the experiementation , the meta - learners have six constituent learners which are heterogeneous in their search and representation methods ( e . g . a rule learner , cn #NUM# [ clark #NUM# ] ; a decision tree learner , c #NUM# . #NUM# [ quinlan #NUM# ] ; an oblique decision tree learner , oc #NUM# [ murthy #NUM# ] ; an instance - based learner , pebls [ cost #NUM# ] ; a k - nearest neighbor learner , ten domains and compared to several other meta - learning strategies .
two important issues in machine learning are explored : the role that memory plays in acquiring new concepts ; and the extent to which the learner can take an active part in acquiring these concepts . this chapter describes a program , called marvin , which uses concepts it has learned previously to learn new concepts . the program forms hypotheses about the concept being learned and tests the hypotheses by asking the trainer questions . learning begins when the trainer shows marvin an example of the concept to be learned . the program determines which objects in the example belong to concepts stored in the memory . a description of the new concept is formed by using the information obtained from the memory to generalize the description of the training example . the generalized description is tested when the program constructs new examples and shows these to the trainer , asking if they belong to the target concept .
adaptation of ecological systems to their environments is commonly viewed through some explicit fitness function defined a priori by the experimenter , or measured a posteriori by estimations based on population size and / or reproductive rates . these methods do not capture the role of environmental complexity in shaping the selective pressures that control the adaptive process . ecological simulations enabled by computational tools such as the latent energy environments ( lee ) model allow us to characterize more closely the effects of environmental complexity on the evolution of adaptive behaviors . lee is described in this paper . its motivation arises from the need to vary complexity in controlled and predictable ways , without assuming the relationship of these changes to the adaptive behaviors they engender . this goal is achieved through a careful characterization of environments in which different forms of " energy " are well - defined . a genetic algorithm using endogenous fitness and local selection is used to model the evolutionary process . individuals in the population are modeled by neural networks with simple sensory - motor systems , and variations in their behaviors are related to interactions with varying environments . we outline the results of three experiments that analyze different sources of environmental complexity and their effects on the collective behaviors of evolving populations .
in this paper we investigate the efficiency of - subsumption ( ` ) , the basic provability relation in ilp . as d ` c is np - complete even if we restrict ourselves to linked horn clauses and fix c to contain only a small constant number of literals , we investigate in several restrictions of d . we first adapt the notion of determinate clauses used in ilp and show that - subsumption is decidable in polynomial time if d is determinate with respect to c . secondly , we adapt the notion of k - local horn clauses and show that - subsumption is efficiently computable for some reasonably small k . we then show how these results can be combined , to give an efficient reasoning procedure for determinate k - local horn clauses , an ilp - problem recently suggested to be polynomial predictable by cohen ( #NUM# ) by a simple counting argument . we finally outline how the - reduction algorithm , an essential part of every lgg ilp - learning algorithm , can be im proved by these ideas .
genetic programming is a powerful method for automatically generating computer programs via the process of natural selection [ koza #NUM# ] . however , it has the limitation known as " closure " , i . e . that all the variables , constants , arguments for functions , and values returned from functions must be of the same data type . to correct this deficiency , we introduce a variation of genetic programming called " strongly typed " genetic programming ( stgp ) . in stgp , variables , constants , arguments , and returned values can be of any data type with the provision that the data type for each such value be specified beforehand . this allows the initialization process and the genetic operators to only generate parse trees such that the arguments of each function in each tree have the required types . an extension to stgp which makes it easier to use is the concept of generic functions , which are not true strongly typed functions but rather templates for classes of such functions . to illustrate stgp , we present three examples involving vector and matrix manipulation : ( #NUM# ) a basis representation problem ( which can be constructed to be deceptive by any reasonable definition of " deception " ) , ( #NUM# ) the n - dimensional least - squares regression problem , and ( #NUM# ) preliminary work on the kalman filter .
existing proofs demonstrating the computational limitations of the recurrent cascade correlation ( rcc ) network ( fahlman , #NUM# ) explicitly limit their results to units having sigmoidal or hard - threshold transfer functions ( giles et al . , #NUM# ; and kremer , #NUM# ) . the proof given here shows that , for any given finite , discrete , deterministic transfer function used by the units of an rcc network , there are finite - state automata ( fsa ) that the network cannot model , no matter how many units are used . the proof applies equally well to continuous transfer functions with a finite number of fixed - points , such as the sigmoid function .
subsumption is a decidable but incomplete approximation of logic implication , important to inductive logic programming and theorem proving . we show that by context based elimination of possible matches a certain superset of the determinate clauses can be tested for subsumption in polynomial time . we discuss the relation between subsumption and the clique problem , showing in particular that using additional prior knowledge about the substitution space only a small fraction of the search space can be identified as possibly containing globally consistent solutions , which leads to an effective pruning rule . we present empirical results , demonstrating that a combination of both of the above approaches provides an extreme reduction of computational effort .
we introduce a new algorithm designed to learn sparse perceptrons over input representations which include high - order features . our algorithm , which is based on a hypothesis - boosting method , is able to pac - learn a relatively natural class of target concepts . moreover , the algorithm appears to work well in practice : on a set of three problem domains , the algorithm produces classifiers that utilize small numbers of features yet exhibit good generalization performance . perhaps most importantly , our algorithm generates concept descriptions that are easy for humans to understand .
current inductive machine learning algorithms typically use greedy search with limited looka - head . this prevents them to detect significant conditional dependencies between the attributes that describe training objects . instead of myopic impurity functions and lookahead , we propose to use reli - eff , an extension of relief developed by kira and rendell [ #NUM# ] , [ #NUM# ] , for heuristic guidance of inductive learning algorithms . we have reimplemented assistant , a system for top down induction of decision trees , using relieff as an estimator of attributes at each selection step . the algorithm is tested on several artificial and several real world problems and the results are compared with some other well known machine learning algorithms . excellent results on artificial data sets and two real world problems show the advantage of the presented approach to inductive learning .
this paper presents a new approach to hierarchical reinforcement learning based on the maxq decomposition of the value function . the maxq decomposition has both a procedural semanticsas a subroutine hierarchyand a declarative semanticsas a representation of the value function of a hierarchical policy . maxq unifies and extends previous work on hierarchical reinforcement learning by singh , kaelbling , and dayan and hinton . conditions under which the maxq decomposition can represent the optimal value function are derived . the paper defines a hierarchical q learning algorithm , proves its convergence , and shows experimentally that it can learn much faster than ordinary flat q learning . finally , the paper discusses some interesting issues that arise in hierarchical reinforcement learning including the hierarchical credit assignment problem and non - hierarchical execution of the maxq hierarchy .
causality relates changes in the structure of an object with the effects of such changes , that is changes in the properties or behavior of the object . this paper analyzes the concept of causality in genetic programming ( gp ) and suggests how it can be used in adapting control parameters for speeding up gp search . we first analyze the effects of crossover to show the weak causality of the gp representation and operators . hierarchical gp approaches based on the discovery and evolution of functions amplify this phenomenon . however , selection gradually retains strongly causal changes . causality is correlated to search space exploitation and is discussed in the context of the exploration - exploitation tradeoff . the results described argue for a bottom - up gp evolutionary thesis . finally , new developments based on the idea of gp architecture evolution ( koza , #NUM# a ) are discussed from the causality perspective .
methods for voting classification algorithms , such as bagging and adaboost , have been shown to be very successful in improving the accuracy of certain classifiers for artificial and real - world datasets . we review these algorithms and describe a large empirical study comparing several variants in conjunction with a decision tree inducer ( three variants ) and a naive - bayes inducer . the purpose of the study is to improve our understanding of why and when these algorithms , which use perturbation , reweighting , and combination techniques , affect classification error . we provide a bias and variance decomposition of the error to show how different methods and variants influence these two terms . this allowed us to determine that bagging reduced variance of unstable methods , while boosting methods ( adaboost and arc - x #NUM# ) reduced both the bias and variance of unstable methods but increased the variance for naive - bayes , which was very stable . we observed that arc - x #NUM# behaves differently than adaboost if reweighting is used instead of resampling , indicating a fundamental difference . voting variants , some of which are introduced in this paper , include : pruning versus no pruning , use of probabilistic estimates , weight perturbations ( wagging ) , and backfitting of data . we found that bagging improves when probabilistic estimates in conjunction with no - pruning are used , as well as when the data was backfit . we measure tree sizes and show an interesting positive correlation between the increase in the average tree size in adaboost trials and its success in reducing the error . we compare the mean - squared error of voting methods to non - voting methods and show that the voting methods lead to large and significant reductions in the mean - squared errors . practical problems that arise in implementing boosting algorithms are explored , including numerical instabilities and underflows . we use scatterplots that graphically show how adaboost reweights instances , emphasizing not only " hard " areas but also outliers and noise .
the long - term goal of our field is the creation and understanding of intelligence . productive research in ai , both practical and theoretical , benefits from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results . the concept of rational agency has long been considered a leading candidate to fulfill this role . this paper outlines a gradual evolution in the formal conception of rationality that brings it closer to our informal conception of intelligence and simultaneously reduces the gap between theory and practice . some directions for future research are indicated .
a solution to the problem of representing compositional structure using distributed representations is described . the method uses circular convolution to associate items , which are represented by vectors . arbitrary variable bindings , short sequences of various lengths , frames , and reduced representations can be compressed into a fixed width vector . these representations are items in their own right , and can be used in constructing compositional structures . the noisy reconstructions given by convolution memories can be cleaned up by using a separate associative memory that has good reconstructive properties .
that is based on angluin ' s l fl algorithm . the algorithm maintains a model consistent with its past examples . when a new counterexample arrives it tries to extend the model in a minimal fashion . we conducted a set of experiments where random automata that represent different strategies were generated , and the algorithm tried to learn them based on prefix - closed samples of their behavior . the algorithm managed to learn very compact models that agree with the samples . the size of the sample had a small effect on the size of the model . the experimental results suggest that for random prefix - closed samples the algorithm behaves well . however , following angluin ' s result on the difficulty of learning almost uniform complete samples [ an - gluin , #NUM# ] , it is obvious that our algorithm does not solve the complexity issue of inferring a dfa from a general prefix - closed sample . we are currently looking for classes of prefix - closed samples in which us - l * behaves well . [ carmel and markovitch , #NUM# ] d . carmel and s . markovitch . the m * algorithm : incorporating opponent models into adversary search . technical report cis report #NUM# , technion , march #NUM# . [ carmel and markovitch , #NUM# ] d . carmel and s . markovitch . unsupervised learning of finite automata : a practical approach . technical report cis report #NUM# , technion , march #NUM# . [ shoham and tennenholtz , #NUM# ] y . shoham and m . tennenholtz . co - learning and the evolution of social activity . technical report stan - cs - tr - #NUM# - #NUM# , stanford univrsity , department of computer science , #NUM# .
aa #NUM# is an incremental learning algorithm for adaptive self - organizing concurrent systems ( asocs ) . asocs are self - organizing , dynamically growing networks of computing nodes . aa #NUM# learns by discrimination and implements knowledge in a distributed fashion over all the nodes . this paper reviews aa #NUM# from the perspective of convergence and generalization . a formal proof that aa #NUM# converges on any arbitrary boolean instance set is given . a discussion of generalization and other aspects of aa #NUM# , including the problem of handling inconsistency , follows . results of simulations with real - world data are presented . they show that aa #NUM# gives promising generalization .
the term " bias " is widely used | and with different meanings | in the fields of machine learning and statistics . this paper clarifies the uses of this term and shows how to measure and visualize the statistical bias and variance of learning algorithms . statistical bias and variance can be applied to diagnose problems with machine learning bias , and the paper shows four examples of this . finally , the paper discusses methods of reducing bias and variance . methods based on voting can reduce variance , and the paper compares breiman ' s bagging method and our own tree randomization method for voting decision trees . both methods uniformly improve performance on data sets from the irvine repository . tree randomization yields perfect performance on the letter recognition task . a weighted nearest neighbor algorithm based on the infinite bootstrap is also introduced . in general , decision tree algorithms have moderate - to - high variance , so an important implication of this work is that variance | rather than appropriate or inappropriate machine learning bias | is an important cause of poor performance for decision tree algorithms .
we analyze the use of built - in policies , or macro - actions , as a form of domain knowledge that can improve the speed and scaling of reinforcement learning algorithms . such macro - actions are often used in robotics , and macro - operators are also well - known as an aid to state - space search in ai systems . the macro - actions we consider are closed - loop policies with termination conditions . the macro - actions can be chosen at the same level as primitive actions . macro - actions commit the learning agent to act in a particular , purposeful way for a sustained period of time . overall , macro - actions may either accelerate or retard learning , depending on the appropriateness of the macro - actions to the particular task . we analyze their effect in a simple example , breaking the acceleration effect into two parts : #NUM# ) the effect of the macro - action in changing exploratory behavior , independent of learning , and #NUM# ) the effect of the macro - action on learning , independent of its effect on behavior . in our example , both effects are significant , but the latter appears to be larger . finally , we provide a more complex gridworld illustration of how appropriately chosen macro - actions can accelerate overall learning .
we present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines . this allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems . our approach can be seen as providing a link between reinforcement learning and behavior - based or teleo - reactive approaches to control . we present provably convergent algorithms for problem - solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states .
when a case - based planner is retrieving a previous case in preparation for solving a new similar problem , it is often not aware of all of the implicit features of the new problem situation which determine if a particular case may be successfully applied . this means that some cases may fail to improve the planner ' s performance . by detecting and explaining these case failures as they occur , retrieval may be improved incrementally . in this paper we provide a definition of case failure for the case - based planner , dersnlp ( derivation replay in snlp ) , which solves new problems by replaying its previous plan derivations . we provide explanation - based learning ( ebl ) techniques for detecting and constructing the reasons for the case failure . we also describe how the case library is organized so as to incorporate this failure information as it is produced . finally we present an empirical study which demonstrates the effectiveness of this approach in improving the performance of dersnlp .
this paper presents new methods for training large neural networks for phoneme probability estimation . an architecture combining timedelay windows and recurrent connections is used to capture the important dynamic information of the speech signal . because the number of connections in a fully connected recurrent network grows super - linear with the number of hidden units , schemes for sparse connection and connection pruning are explored . it is found that sparsely connected networks outperform their fully connected counterparts with an equal number of connections . the implementation of the combined architecture and training scheme is described in detail . the networks are evaluated in a hybrid hmm / ann system for phoneme recognition on the timit database , and for word recognition on the waxholm database . the achieved phone error - rate , #NUM# . #NUM# % , for the standard #NUM# phoneme set on the core testset of the timit database is in the range of the lowest reported . all training and simulation software used is made freely available by the author , and detailed information about the software and the training process is given in an appendix .
the error rate of decision - tree and other classification learners can often be much reduced by bagging : learning multiple models from bootstrap samples of the database , and combining them by uniform voting . in this paper we empirically test two alternative explanations for this , both based on bayesian learning theory : ( #NUM# ) bagging works because it is an approximation to the optimal procedure of bayesian model averaging , with an appropriate implicit prior ; ( #NUM# ) bagging works because it effectively shifts the prior to a more appropriate region of model space . all the experimental evidence contradicts the first hypothesis , and confirms the second . bagging ( breiman #NUM# a ) is a simple and effective way to reduce the error rate of many classification learning algorithms . for example , in the empirical study described below , it reduces the error of a decision - tree learner in #NUM# of #NUM# databases , by #NUM# % on average . in the bagging procedure , given a training set of size s , a " bootstrap " replicate of it is constructed by taking s samples with replacement from the training set . thus a new training set of the same size is produced , where each of the original examples may appear once , more than once , or not . on average , #NUM# % of the original examples will appear in the bootstrap sample . the learning algorithm is then applied to this training set . this procedure is repeated m times , and the resulting m models are aggregated by uniform voting . bagging is one of several " multiple model " approaches that have recently received much attention ( see , for example , ( chan , stolfo , & wolpert #NUM# ) ) . other procedures of this type include boosting ( freund & schapire #NUM# ) and stacking ( wolpert #NUM# ) .
we propose an algorithm called query by committee , in which a committee of students is trained on the same data set . the next query is chosen according to the principle of maximal disagreement . the algorithm is studied for two toy models the high - low game and perceptron learning of another perceptron . as the number of queries goes to infinity , the committee algorithm yields asymptotically finite information gain . this leads to generalization error that decreases exponentially with the number of examples . this in marked contrast to learning from randomly chosen inputs , for which the information gain approaches zero and the generalization error decreases with a relatively slow inverse power law . we suggest that asymptotically finite information gain may be an important characteristic of good query algorithms .
this paper examines the minimum encoding approaches to inference , minimum message length ( mml ) and minimum description length ( mdl ) . this paper was written with the objective of providing an introduction to this area for statisticians . we describe coding techniques for data , and examine how these techniques can be applied to perform inference and model selection .
field ( #NUM# ) has suggested that neurons with line and edge selectivities found in primary visual cortex of cats and monkeys form a sparse , distributed representation of natural scenes , and barlow ( #NUM# ) has reasoned that such responses should emerge from an unsupervised learning algorithm that attempts to find a factorial code of independent visual features . we show here that non - linear ` infomax ' , when applied to an ensemble of natural scenes , produces sets of visual filters that are localised and oriented . some of these filters are gabor - like and resemble those produced by the sparseness - maximisation network of olshausen & field ( #NUM# ) . in addition , the outputs of these filters are as independent as possible , since the info - max network is able to perform independent components analysis ( ica ) . we compare the resulting ica filters and their associated basis functions , with other decorrelating filters produced by principal components analysis ( pca ) and zero - phase whitening filters ( zca ) . the ica filters have more sparsely distributed ( kurtotic ) outputs on natural scenes . they also resemble the receptive fields of simple cells in visual cortex , which suggests that these neurons form an information - theoretic co - ordinate system for images .
loan applications at banks are often long , requiring the applicant to provide large amounts of data . is all of it necessary ? can we save the applicant some frustration and the bank some expense by using only a subset of the relevant variables ? to answer this question , i have attempted to model the current loan approval process at a particular bank . i have used several model selection techniques for logistic regression , including stepwise regression , occam ' s window , markov chain monte carlo model composition ( raftery , madigan , and hoeting , #NUM# ) , and bayesian random searching . the resulting models largely agree upon a subset of only one - third of the original variables .
learning , planning , and representing knowledge at multiple levels of temporal abstraction are key challenges for ai . in this paper we develop an approach to these problems based on the mathematical framework of reinforcement learning and markov decision processes ( mdps ) . we extend the usual notion of action to include options | whole courses of behavior that may be temporally extended , stochastic , and contingent on events . examples of options include picking up an object , going to lunch , and traveling to a distant city , as well as primitive actions such as muscle twitches and joint torques . options may be given a priori , learned by experience , or both . they may be used interchangeably with actions in a variety of planning and learning methods . the theory of semi - markov decision processes ( smdps ) can be applied to model the consequences of options and as a basis for planning and learning methods using them . in this paper we develop these connections , building on prior work by bradtke and duff ( #NUM# ) , parr ( in prep . ) and others . our main novel results concern the interface between the mdp and smdp levels of analysis . we show how a set of options can be altered by changing only their termination conditions to improve over smdp methods with no additional cost . we also introduce intra - option temporal - difference methods that are able to learn from fragments of an option ' s execution . finally , we propose a notion of subgoal which can be used to improve the options themselves . overall , we argue that options and their models provide hitherto missing aspects of a powerful , clear , and expressive framework for representing and organizing knowledge .
#NUM# articles about neural network learning algorithms published in #NUM# and #NUM# are examined for the amount of experimental evaluation they contain . #NUM# % of them employ not even a single realistic or real learning problem . only #NUM# % of the articles present results for more than one problem using real world data . furthermore , one third of all articles do not present any quantitative comparison with a previously known algorithm . these results suggest that we should strive for better assessment practices in neural network learning algorithm research . for the long - term benefit of the field , the publication standards should be raised in this respect and easily accessible collections of benchmark problems should be built .
the developmental mechanisms transforming genotypic to phenotypic forms are typically omitted in formulations of genetic algorithms ( gas ) in which these two representational spaces are identical . we argue that a careful analysis of developmental mechanisms is useful when understanding the success of several standard ga techniques , and can clarify the relationships between more recently proposed enhancements . we provide a framework which distinguishes between two developmental mechanisms | learning and maturation | while also showing several common effects on ga search . this framework is used to analyze how maturation and local search can change the dynamics of the ga . we observe that in some contexts , maturation and local search can be incorporated into the fitness evaluation , but illustrate reasons for considering them seperately . further , we identify contexts in which maturation and local search can be distinguished from the fitness evaluation .
finding optimal or at least good monitoring strategies is an important consideration when designing an agent . we have applied genetic programming to this task , with mixed results . since the agent control language was kept purposefully general , the set of monitoring strategies constitutes only a small part of the overall space of possible behaviors . because of this , it was often difficult for the genetic algorithm to evolve them , even though their performance was superior . these results raise questions as to how easy it will be for genetic programming to scale up as the areas it is applied to become more complex .
marketing decision making tasks require the acquisition of efficient decision rules from noisy questionnaire data . unlike popular learning - from - example methods , in such tasks , we must interpret the characteristics of the data without clear features of the data nor pre - determined evaluation criteria . the problem is how domain experts get simple , easy - to - understand , and accurate knowledge from noisy data . this paper describes a novel method to acquire efficient decision rules from questionnaire data using both simulated breeding and inductive learning techniques . the basic ideas of the method are that simulated breeding is used to get the effective features from the questionnaire data and that inductive learning is used to acquire simple decision rules from the data . the simulated breeding is one of the genetic algorithm based techniques to subjectively or interactively evaluate the qualities of offspring generated by genetic operations . the proposed method has been qualitatively and quantitatively validated by a case study on consumer product questionnaire data : the acquired rules are simpler than the results from the direct application of inductive learning ; a domain expert admits that they are easy to understand ; and they are at the same level on the accuracy compared with the other methods .
this paper experimentally compares three approaches to program induction : inductive logic programming ( ilp ) , genetic programming ( gp ) , and genetic logic programming ( glp ) ( a variant of gp for inducing pro - log programs ) . each of these methods was used to induce four simple , recursive , list - manipulation functions . the results indicate that ilp is the most likely to induce a correct program from small sets of random examples , while gp is generally less accurate . glp performs the worst , and is rarely able to induce a correct program . interpretations of these results in terms of differences in search methods and inductive biases are presented . keywords : genetic programming , inductive logic programming , empiri cal comparison this paper will also be submitted to the #NUM# th int . workshop on inductive logic programming , #NUM# .
two major problems in case - based reasoning are the efficient and justified retrieval of source cases and the adaptation of retrieved solutions to the conditions of the target . for analogical theorem proving by induction , we describe how a solution - relevant abstraction can restrict the retrieval of source cases and the mapping from the source problem to the target problem and how it can determine reformulations that further adapt the source solution .
most commonly , case - based reasoning is applied in domains where attribute value representations of cases are sufficient to represent the features relevant to support classification , diagnosis or design tasks . distance functions like the hamming - distance or their transformation into similarity functions are applied to retrieve past cases to be used to generate the solution of an actual problem . often , domain knowledge is available to adapt past solutions to new problems or to evaluate solutions . however , there are domains like architectural design or law in which structural case representations and corresponding structural similarity functions are needed . often , the acquisition of adaptation knowledge seems to be impossible or rather requires an effort that is not manageable for fielded applications . despite of this , humans use cases as the main source to generate adapted solutions . how to achieve this computationally ? this paper presents a general approach to structural similarity assessment and adaptation . the approach allows to explore structural case representations and limited domain knowledge to support design tasks . it is exemplarily instantiated in three modules of the design assistant fabel - idea that generates adapted design solutions on the basis of prior cad layouts .
the main difficulty in implementing the natural gradient learning rule is to compute the inverse of the fisher information matrix when the input dimension is large . we have found a new scheme to represent the fisher information matrix . based on this scheme , we have designed an algorithm to compute the inverse of the fisher information matrix . when the input dimension n is much larger than the number of hidden neurons , the complexity of this algorithm is of order o ( n #NUM# ) while the complexity of conventional algorithms for the same purpose is of order o ( n #NUM# ) . the simulation has confirmed the efficience and robustness of the natural gradient learning rule .
the ability of case - based reasoning ( cbr ) systems to apply cases to novel situations depends on their case adaptation knowledge . however , endowing cbr systems with adequate adaptation knowledge has proven to be a very difficult task . this paper describes a hybrid method for performing case adaptation , using a combination of rule - based and case - based reasoning . it shows how this approach provides a framework for acquiring flexible adaptation knowledge from experiences with autonomous adaptation and suggests its potential as a basis for acquisition of adaptation knowledge from interactive user guidance . it also presents initial experimental results examining the benefits of the approach and comparing the relative contributions of case learning and adaptation learning to reasoning performance .
this paper #NUM# discusses why traditional reinforcement learning methods , and algorithms applied to those models , result in poor performance in dynamic , situated multi - agent domains characterized by multiple goals , noisy perception and action , and inconsistent reinforcement . we propose a methodology for designing the representation and the forcement functions that take advantage of implicit domain knowledge in order to accelerate learning in such domains , and demonstrate it experimentally in two different mobile robot domains .
learning and problem solving are intimately related : problem solving determines the knowledge requirements of the reasoner which learning must fulfill , and learning enables improved problem - solving performance . different models of problem solving , however , recognize different knowledge needs , and , as a result , set up different learning tasks . some recent models analyze problem solving in terms of generic tasks , methods , and subtasks . these models require the learning of problem - solving concepts such as new tasks and new task decompositions . we view reflection as a core process for learning these problem - solving concepts . in this paper , we identify the learning issues raised by the task - structure framework of problem solving . we view the problem solver as an abstract device , and represent how it works in terms of a structure - behavior - function model which specifies how the knowledge and reasoning of the problem solver results in the accomplishment of its tasks . we describe how this model enables reflection , and how model - based reflection enables the reasoner to adapt its task structure to produce solutions of better quality . the autognostic system illustrates this reflection process .
realistic and complex planning situations require a mixed - initiative planning framework in which human and automated planners interact to mutually construct a desired plan . ideally , this joint cooperation has the potential of achieving better plans than either the human or the machine can create alone . human planners often take a case - based approach to planning , relying on their past experience and planning by retrieving and adapting past planning cases . planning by analogical reasoning in which generative and case - based planning are combined , as in prodigy / analogy , provides a suitable framework to study this mixed - initiative integration . however , having a human user engaged in this planning loop creates a variety of new research questions . the challenges we found creating a mixed - initiative planning system fall into three categories : planning paradigms differ in human and machine planning ; visualization of the plan and planning process is a complex , but necessary task ; and human users range across a spectrum of experience , both with respect to the planning domain and the underlying planning technology . this paper presents our approach to these three problems when designing an interface to incorporate a human into the process of planning by analogical reasoning with prodigy / analogy . the interface allows the user to follow both generative and case - based planning , it supports visualization of both plan and the planning rationale , and it addresses the variance in the experience of the user by allowing the user to control the presentation of information .
evolutionary programming and evolution strategies , rather similar representatives of a class of probabilistic optimization algorithms gleaned from the model of organic evolution , are discussed and compared to each other with respect to similarities and differences of their basic components as well as their performance in some experimental runs . theoretical results on global convergence , step size control for a strictly convex , quadratic function and an extension of the convergence rate theory for evolution strategies are presented and discussed with respect to their implications on evolutionary programming .
in this paper we investigate genetic algorithms where more than two parents are involved in the recombination operation . in particular , we introduce gene scanning as a reproduction mechanism that generalizes classical crossovers , such as n - point crossover or uniform crossover , and is applicable to an arbitrary number ( two or more ) of parents . we performed extensive tests for optimizing numerical functions , the tsp and graph coloring to observe the effect of different numbers of parents . the experiments show that #NUM# - parent recombination is outperformed when using more parents on the classical dejong functions . for the other problems the results are not conclusive , in some cases #NUM# parents are optimal , while in some others more parents are better .
a novel method is proposed for combining multiple probabilistic classifiers on different feature sets . in order to achieve the improved classification performance , a generalized finite mixture model is proposed as a linear combination scheme and implemented based on radial basis function networks . in the linear combination scheme , soft competition on different feature sets is adopted as an automatic feature rank mechanism so that different feature sets can be always simultaneously used in an optimal way to determine linear combination weights . for training the linear combination scheme , a learning algorithm is developed based on expectation - maximization ( em ) algorithm . the proposed method has been applied to a typical real world problem , viz . speaker identification , in which different feature sets often need consideration simultaneously for robustness . simulation results show that the proposed method yields good performance in speaker identification .
different learning models employ different styles of generalization on novel inputs . this paper proposes the need for multiple styles of generalization to support a broad application base . the priority asocs model ( priority adaptive self - organizing concurrent system ) is overviewed and presented as a potential platform which can support multiple generalization styles . pasocs is an adaptive network composed of many simple computing elements operating asynchronously and in parallel . the pasocs can operate in either a data processing mode or a learning mode . during data processing mode , the system acts as a parallel hardware circuit . during learning mode , the pasocs incorporates rules , with attached priorities , which represent the application being learned . learning is accomplished in a distributed fashion in time logarithmic in the number of rules . the new model has significant learning time and space complexity improvements over previous models . generalization in a learning system is at best always a guess . the proper style of generalization is application dependent . thus , one style of generalization may not be sufficient to allow a learning system to support a broad spectrum of applications [ #NUM# ] . current connectionist models use one specific style of generalization which is implicit in the learning algorithm . we suggest that the type of generalization used be a self - organizing parameter of the learning system which can be discovered as learning takes place . this requires a ) a model which allows flexible generalization styles , and b ) mechanisms to guide the system into the best style of generalization for the problem being learned . this paper overviews a learning model which seeks to efficiently support requirement a ) above . the model is called priority asocs ( pasocs ) [ #NUM# ] , which is a member of a class of models called asocs ( adaptive self - organizing concurrent systems ) [ #NUM# ] . section #NUM# of this paper gives an example of how different generalization techniques can approach a problem . section #NUM# presents an overview of pasocs . section #NUM# illustrates how flexible generalization can be supported . section #NUM# concludes the paper .
we introduce a new approach to model selection that performs better than the standard complexity - penalization and hold - out error estimation techniques in many cases . the basic idea is to exploit the intrinsic metric structure of a hypothesis space , as determined by the natural distribution of unlabeled training patterns , and use this metric as a reference to detect whether the empirical error estimates derived from a small ( labeled ) training sample can be trusted in the region around an empirically optimal hypothesis . using simple metric intuitions we develop new geometric strategies for detecting overfitting and performing robust yet responsive model selection in spaces of candidate functions . these new metric - based strategies dramatically outperform previous approaches in experimental studies of classical polynomial curve fitting . moreover , the technique is simple , efficient , and can be applied to most function learning tasks . the only requirement is access to an auxiliary collection of unlabeled training data .
in this paper , we use a genetic algorithm to evolve a set of classification rules with real - valued attributes . we show how real - valued attribute ranges can be encoded with real - valued genes and present a new uniform method for representing don ' t cares in the rules . we view supervised classification as an optimization problem , and evolve rule sets that maximize the number of correct classifications of input instances . we use a variant of the pitt approach to genetic - based machine learning system with a novel conflict resolution mechanism between competing rules within the same rule set . experimental results demonstrate the effectiveness of our proposed approach on a benchmark wine classifier system .
genetic algorithms have been proven to be a powerful tool within the area of machine learning . however , there are some classes of problems where they seem to be scarcely applicable , e . g . when the solution to a given problem consists of several parts that influence each other . in that case the classic genetic operators cross - over and mutation do not work very well thus preventing a good performance . this paper describes an approach to overcome this problem by using high - level genetic operators and integrating task specific but domain independent knowledge to guide the use of these operators . the advantages of this approach are shown for learning a rule base to adapt the parameters of an image processing operator path within the solution system .
in this paper , we present a novel multi - agent learning paradigm called team - partitioned , opaque - transition reinforcement learning ( tpot - rl ) . tpot - rl introduces the concept of using action - dependent features to generalize the state space . in our work , we use a learned action - dependent feature space . tpot - rl is an effective technique to allow a team of agents to learn to cooperate towards the achievement of a specific goal . it is an adaptation of traditional rl methods that is applicable in complex , non - markovian , multi - agent domains with large state spaces and limited training opportunities . multi - agent scenarios are opaque - transition , as team members are not always in full communication with one another and adversaries may affect the environment . hence , each learner cannot rely on having knowledge of future state transitions after acting in the world . tpot - rl enables teams of agents to learn effective policies with very few training examples even in the face of a large state space with large amounts of hidden state . the main responsible features are : dividing the learning task among team members , using a very coarse , action - dependent feature space , and allowing agents to gather reinforcement directly from observation of the environment . tpot - rl is fully implemented and has been tested in the robotic soccer domain , a complex , multi - agent framework . this paper presents the algorithmic details of tpot - rl as well as empirical results demonstrating the effectiveness of the developed multi - agent learning approach with learned features .
this paper discusses a method for training multilayer perceptron networks called dmp #NUM# ( dynamic multilayer perceptron #NUM# ) . the method is based upon a divide and conquer approach which builds networks in the form of binary trees , dynamically allocating nodes and layers as needed . the focus of this paper is on the effects of using multiple node types within the dmp framework . simulation results show that dmp #NUM# performs favorably in comparison with other learning algorithms , and that using multiple node types can be beneficial to network performance .
specification refinement is part of formal program derivation , a method by which software is directly constructed from a provably correct specification . because program derivation is an intensive manual exercise used for critical software systems , an automated approach would allow it to be viable for many other types of software systems . the goal of this research is to determine if genetic programming ( gp ) can be used to automate the specification refinement process . the initial steps toward this goal are to show that a well - known proof logic for program derivation can be encoded such that a gp - based system can infer sentences in the logic for proof of a particular sentence . the results are promising and indicate that gp can be useful in aiding pro gram derivation .
genetic programming ( gp ) is an automatic method for generating computer programs , which are stored as data structures and manipulated to evolve better programs . an extension restricting the search space is strongly typed genetic programming ( stgp ) , which has , as a basic premise , the removal of closure by typing both the arguments and return values of functions , and by also typing the terminal set . a restriction of stgp is that there are only two levels of typing . we extend stgp by allowing a type hierarchy , which allows more than two levels of typing .
we have integrated the distributed search of genetic programming based systems with collective memory to form a collective adaptation search method . such a system significantly improves search as problem complexity is increased . however , there is still considerable scope for improvement . in collective adaptation , search agents gather knowledge of their environment and deposit it in a central information repository . process agents are then able to manipulate that focused knowledge , exploiting the exploration of the search agents . we examine the utility of increasing the capabilities of the centralized pro cess agents .
our case - based reasoning ( cbr ) integration with the constraint satisfaction problem ( csp ) formalism has undergone several transformations on its journey from initial research idea to product - intent design . both unexpected research results as well as interesting insights into the real - world applicability of the integrated methodology emerged as the integration was explored from alternative viewpoints . in this paper , the alternative viewpoints and the results that were enabled by these viewpoints are described .
in order to rank the performance of machine learning algorithms , many researchers conduct experiments on benchmark data sets . since most learning algorithms have domain - specific parameters , it is a popular custom to adapt these parameters to obtain a minimal error rate on the test set . the same rate is then used to rank the algorithm , which causes an optimistic bias . we quantify this bias , showing , in particular , that an algorithm with more parameters will probably be ranked higher than an equally good algorithm with fewer parameters . we demonstrate this result , showing the number of parameters and trials required in order to pretend to outperform c #NUM# . #NUM# or foil , respectively , for various benchmark problems . we then describe out how unbiased ranking experiments should be conducted .
we report on a series of experiments in which all decision trees consistent with the training data are constructed . these experiments were run to gain an understanding of the properties of the set of consistent decision trees , and the factors that affect the error rate of individual trees . the experiments were performed on a massively parallel maspar #NUM# computer . the results of the experimentation on two artificial and two real world problems indicate that for three of the four problems investigated , the smallest consistent decision trees tend to be less accurate than the average accuracy of those slightly larger .
an ensemble consists of a set of independently trained classifiers ( such as neural networks or decision trees ) whose predictions are combined when classifying novel instances . previous research has shown that an ensemble as a whole is often more accurate than any of the single classifiers in the ensemble . bagging ( breiman #NUM# a ) and boosting ( freund & schapire #NUM# ) are two relatively new but popular methods for producing ensembles . in this paper we evaluate these methods using both neural networks and decision trees as our classification algorithms . our results clearly show two important facts . the first is that even though bagging almost always produces a better classifier than any of its individual component classifiers and is relatively impervious to overfitting , it does not generalize any better than a baseline neural - network ensemble method . the second is that boosting is a powerful technique that can usually produce better ensembles than bagging ; however , it is more susceptible to noise and can quickly overfit a data set .
pruning a decision tree is considered by some researchers to be the most important part of tree building in noisy domains . while , there are many approaches to pruning , an alternative approach of averaging over decision trees has not received as much attention . we perform an empirical comparison of pruning with the approach of averaging over decision trees . for this comparison we use a computa - tionally efficient method of averaging , namely averaging over the extended fanned set of a tree . since there are a wide range of approaches to pruning , we compare tree averaging with a traditional pruning approach , along with an optimal pruning approach .
we consider the problem of model selection and accounting for model uncertainty in high - dimensional contingency tables , motivated by expert system applications . the approach most used currently is a stepwise strategy guided by tests based on approximate asymptotic p - values leading to the selection of a single model ; inference is then conditional on the selected model . the sampling properties of such a strategy are complex , and the failure to take account of model uncertainty leads to underestimation of uncertainty about quantities of interest . in principle , a panacea is provided by the standard bayesian formalism which averages the posterior distributions of the quantity of interest under each of the models , weighted by their posterior model probabilities . furthermore , this approach is optimal in the sense of maximising predictive ability . however , this has not been used in practice because computing the posterior model probabilities is hard and the number of models is very large ( often greater than #NUM# #NUM# ) . we argue that the standard bayesian formalism is unsatisfactory and we propose an alternative bayesian approach that , we contend , takes full account of the true model uncertainty by averaging over a much smaller set of models . an efficient search algorithm is developed for finding these models . we consider two classes of graphical models that arise in expert systems : the recursive causal models and the decomposable
z york ' s research was supported by a nsf graduate fellowship . the authors are grateful to julian besag , david bradshaw , jeff bradshaw , james carlsen , david draper , ivar heuch , robert kass , augustine kong , steffen lauritzen , adrian raftery , and james zidek for helpful comments and discussions .
we discuss the advantages of using overdetermined mixtures to improve upon blind source separation algorithms that are designed to extract sound sources from acoustic mixtures . a study of the nature of room impulse responses helps us choose an adaptive filter architecture . we use ideal inverses of acquired room impulse responses to compare the effectiveness of different - sized separating filter configurations of various filter lengths . using a multi - channel blind least - mean - square algorithm ( mblms ) , we show that , by adding additional sensors , we can improve upon the separation of signals mixed with real world filters .
rissanen ' s minimum description length ( mdl ) principle is adapted to handle continuous attributes in the inductive logic programming setting . application of the developed coding as a mdl pruning mechanism is devised . the behavior of the mdl pruning is tested in a synthetic domain with artificially added noise of different levels and in two real life problems | modelling of the surface roughness of a grinding workpiece and modelling of the mutagenicity of nitroaromatic compounds . results indicate that mdl pruning is a successful parameter - free noise fighting tool in real - life domains since it acts as a safeguard against building too complex models while retaining the accuracy of the model .
we address the difficult problem of separating multiple speakers with multiple microphones in a real room . we combine the work of torkkola and amari , cichocki and yang , to give natural gradient information maximisation rules for recurrent ( iir ) networks , blindly adjusting delays , separating and deconvolving mixed signals . while they work well on simulated data , these rules fail in real rooms which usually involve non - minimum phase transfer functions , not - invertible using stable iir filters . an approach that sidesteps this problem is to perform infomax on a feedforward architecture in the frequency domain ( lambert #NUM# ) . we demonstrate real - room separation of two natural signals using this approach .
for blind source separation , when the fisher information matrix is used as the riemannian metric tensor for the parameter space , the steepest descent algorithm to maximize the likelihood function in this riemannian parameter space becomes the serial updating rule with equivariant property . this algorithm can be further simplified by using the asymptotic form of the fisher information matrix around the equilibrium .
we have discovered a new scheme to represent the fisher information matrix of a stochastic multi - layer perceptron . based on this scheme , we have designed an algorithm to compute the inverse of the fisher information matrix . when the input dimension n is much larger than the number of hidden neurons , the complexity of this algorithm is of order o ( n #NUM# ) while the complexity of conventional algorithms for the same purpose is of order o ( n #NUM# ) . the inverse of the fisher information matrix is used in the natural gradient descent algorithm to train single - layer or multi - layer perceptrons . it is confirmed by simulation that the natural gradient
in this paper we define the task of place learning and describe one approach to this problem . our framework represents distinct places as evidence grids , a probabilistic description of occupancy . place recognition relies on nearest neighbor classification , augmented by a registration process to correct for translational differences between the two grids . the learning mechanism is lazy in that it involves the simple storage of inferred evidence grids . experimental studies with physical and simulated robots suggest that this approach improves place recognition with experience , that it can handle significant sensor noise , that it benefits from improved quality in stored cases , and that it scales well to environments with many distinct places . additional studies suggest that using historical information about the robot ' s path through the environment can actually reduce recognition accuracy . previous researchers have studied evidence grids and place learning , but they have not combined these two powerful concepts , nor have they used systematic experimentation to evaluate their methods ' abilities .
let us call a non - deterministic incremental algorithm one that is able to construct any solution to a combinatorial problem by selecting incrementally an ordered sequence of choices that defines this solution , each choice being made non - deterministically . in that case , the state space can be represented as a tree , and a solution is a path from the root of that tree to a leaf . this paper describes how the simulated evolution of a population of such non - deterministic incremental algorithms offers a new approach for the exploration of a state space , compared to other techniques like genetic algorithms ( ga ) , evolutionary strategies ( es ) or hill climbing . in particular , the efficiency of this method , implemented as the evolving non - determinism ( end ) model , is presented for the sorting network problem , a reference problem that has challenged computer science . then , we shall show that the end model remedies some drawbacks of these optimization techniques and even outperforms them for this problem . indeed , some #NUM# - input sorting networks as good as the best known have been built from scratch , and even a #NUM# - year - old result for the #NUM# - input problem has been improved by one comparator .
visor is a neural network system for object recognition and scene analysis that learns visual schemas from examples . processing in visor is based on cooperation , competition , and parallel bottom - up and top - down activation of schema representations . similar principles appear to underlie much of human visual processing , and visor can therefore be used to model various perceptual phenomena . this paper focuses on analyzing three phenomena through simulation with visor ( #NUM# ) priming and mental imagery , ( #NUM# ) perceptual reversal , and ( #NUM# ) circular reaction . the results illustrate similarity and subtle differences between the mechanisms mediating priming and mental imagery , show how the two opposing accounts of perceptual reversal ( neural satiation and cognitive factors ) may both contribute to the phenomenon , and demonstrate how intentional actions can be gradually learned from reflex actions . successful simulation of such effects suggests that similar mechanisms may govern human visual perception and learning of visual schemas .
a novel approach to object recognition and scene analysis based on neural network representation of visual schemas is described . given an input scene , the visor system focuses attention successively at each component , and the schema representations cooperate and compete to match the inputs . the schema hierarchy is learned from examples through unsupervised adaptation and reinforcement learning . visor learns that some objects are more important than others in identifying the scene , and that the importance of spatial relations varies depending on the scene . as the inputs differ increasingly from the schemas , visor ' s recognition process is remarkably robust , and automatically generates a measure of confidence in the analysis .
truly autonomous vehicles will require both projec - tive planning and reactive components in order to perform robustly . projective components are needed for long - term planning and replanning where explicit reasoning about future states is required . reactive components allow the system to always have some action available in real - time , and themselves can exhibit robust behavior , but lack the ability to expli - citly reason about future states over a long time period . this work addresses the problem of creating reactive components for autonomous vehicles . creating reactive behaviors ( stimulus - response rules ) is generally difficult , requiring the acquisition of much knowledge from domain experts , a problem referred to as the knowledge acquisition bottleneck . samuel is a system that learns reactive behaviors for autonomous agents . samuel learns these behaviors under simulation , automating the process of creating stimulus - response rules and therefore reducing the bottleneck . the learning algorithm was designed to learn useful behaviors from simulations of limited fidelity . current work is investigating how well behaviors learned under simulation environments work in real world environments . in this paper , we describe samuel , and describe behaviors that have been learned for simulated autonomous aircraft , autonomous underwater vehicles , and robots . these behaviors include dog fighting , missile evasion , track - ing , navigation , and obstacle avoidance .
feedforward nets with sigmoidal activation functions are often designed by minimizing a cost criterion . it has been pointed out before that this technique may be outperformed by the classical perceptron learning rule , at least on some problems . in this paper , we show that no such pathologies can arise if the error criterion is of a threshold lms type , i . e . , is zero for values " beyond " the desired target values . more precisely , we show that if the data are linearly separable , and one considers nets with no hidden neurons , then an error function as above cannot have any local minima that are not global . simulations of networks with hidden units are consistent with these results , in that often data which can be classified when minimizing a threshold lms criterion may fail to be classified when using instead a simple lms cost . in addition , the proof gives the following stronger result , under the stated hypotheses the continuous gradient adjustment procedure is such that from any initial weight configuration a separating set of weights is obtained in finite time . this is a precise analogue of the perceptron learning theorem . the results are then compared with the more classical pattern recognition problem of threshold lms with linear activations , where no spurious local minima exist even for nonseparable data here it is shown that even if using the threshold criterion , such bad local minima may occur , if the data are not separable and sigmoids are used .
this paper combines existing models for longitudinal and spatial data in a hierarchical bayesian framework , with particular emphasis on the role of time - and space - varying covariate effects . data analysis is implemented via markov chain monte carlo methods . the methodology is illustrated by a tentative re - analysis of ohio lung cancer data #NUM# - #NUM# . two approaches that adjust for unmeasured spatial covariates , particularly tobacco consumption , are described . the first includes random effects in the model to account for unobserved heterogeneity ; the second adds a simple urbanization measure as a surrogate for smoking behaviour . the ohio dataset has been of particular interest because of the suggestion that a nuclear facility in the southwest of the state may have caused increased levels of lung cancer there . however , we contend here that the data are inadequate for a proper investigation of this issue .
although many algorithms for learning from examples have been developed and many comparisons have been reported , there is no generally accepted benchmark for classifier learning . the existence of a standard benchmark would greatly assist such comparisons . sixteen dimensions are proposed to describe classification tasks . based on these , thirteen real - world and synthetic datasets are chosen by a set covering method from the uci repository of machine learning databases to form such a benchmark .
holland ' s schema theorem is widely taken to be the foundation for explanations of the power of genetic algorithms ( gas ) . yet some dissent has been expressed as to its implications . here , dissenting arguments are reviewed and elaborated upon , explaining why the schema theorem has no implications for how well a ga is performing . interpretations of the schema theorem have implicitly assumed that a correlation exists between parent and offspring fitnesses , and this assumption is made explicit in results based on price ' s covariance and selection theorem . schemata do not play a part in the performance theorems derived for representations and operators in general . however , schemata re - emerge when recombination operators are used . using geiringer ' s recombination distribution representation of recombination operators , a " missing " schema theorem is derived which makes explicit the intuition for when a ga should perform well . finally , the method of " adaptive landscape " analysis is examined and counterexamples offered to the commonly used correlation statistic . instead , an alternative statistic | the transmission function in the fitness domain | is proposed as the optimal statistic for estimating ga performance from limited samples .
this report was supported in part by the navy medical research and development command and the office of naval research , department of the navy under work unit onr . reimb - #NUM# . the views expressed in this article are those of the authors and do not reflect the official policy or position of the department of the navy , department of defense , or the u . s . government . approved for public release , distribution unlimited .
an approach to analytic learning is described that searches for accurate entailments of a horn clause domain theory . a hill - climbing search , guided by an information based evaluation function , is performed by applying a set of operators that derive frontiers from domain theories . the analytic learning system is one component of a multi - strategy relational learning system . we compare the accuracy of concepts learned with this analytic strategy to concepts learned with an analytic strategy that operationalizes the domain theory .
any system that learns how to filter documents will suffer poor performance during an initial training phase . one way of addressing this problem is to exploit filters learned by other users in a collaborative fashion . we investigate " direct transfer " of learned filters in this setting | a limiting case for any collaborative learning system . we evaluate the stability of several different learning methods under direct transfer , and conclude that symbolic learning methods that use negatively correlated features of the data perform poorly in transfer , even when they perform well in more conventional evaluation settings . this effect is robust : it holds for several learning methods , when a diverse set of users is used in training the classifier , and even when the learned classifiers can be adapted to the new user ' s distribution . our experiments give rise to several concrete proposals for improving generalization performance in a collaborative setting , including a beneficial variation on a feature selection method that has been widely used in text categorization .
we present a coevolutionary architecture for solving decomposable problems and apply it to the evolution of artificial neural networks . although this work is preliminary in nature it has a number of advantages over non - coevolutionary approaches . the coevolutionary approach utilizes a divide - and - conquer technique in which species representing simpler subtasks are evolved in separate instances of a genetic algorithm executing in parallel . collaborations among the species are formed representing complete solutions . species are created dynamically as needed . results are presented in which the coevolutionary architecture produces higher quality solutions in fewer evolutionary trials when compared with an alternative non - coevolutionary approach on the problem of evolving cascade networks for parity computation .
we introduce an algorithm , lllama , which combines simple pattern recognizers into a general method for estimating the entropy of a sequence . each pattern recognizer exploits a partial match between subsequences to build a model of the sequence . since the primary features of interest in biological sequence domains are subsequences with small variations in exact composition , lllama is particularly suited to such domains . we describe two methods , lllama - length and lllama - alone , which use this entropy estimate to perform maximum a posteriori classification . we apply these methods to several problems in three - dimensional structure classification of short dna sequences .
rise ( domingos #NUM# ; in press ) is a rule induction algorithm that proceeds by gradually generalizing rules , starting with one rule per example . this has several advantages compared to the more common strategy of gradually specializing initially null rules , and has been shown to lead to significant accuracy gains over algorithms like c #NUM# . #NUM# rules and cn #NUM# in a large number of application domains . however , rise ' s running time ( like that of other rule induction algorithms ) is quadratic in the number of examples , making it unsuitable for processing very large databases . this paper studies the use of partitioning to speed up rise , and compares it with the well - known method of windowing . the use of partitioning in a specific - to - general induction setting creates synergies that would not be possible with a general - to - specific system . partitioning often reduces running time and improves accuracy at the same time . in noisy conditions , the performance of windowing deteriorates rapidly , while that of partitioning remains stable .
to investigate the issue of how modularity emerges in nature , we present an artificial life model that allow us to reproduce on the computer both the organisms ( i . e . , robots that have a genotype , a nervous system , and sensory and motor organs ) and the environment in which organisms live , behave and reproduce . in our simulations neural networks are evolutionarily trained to control a mobile robot designed to keep an arena clear by picking up trash objects and releasing them outside the arena . during the evolutionary process modular neural networks , which control the robot ' s behavior , emerge as a result of genetic duplications . preliminary simulation results show that duplication - based modular architecture outperforms the nonmod - ular architecture , which represents the starting architecture in our simulations . moreover , an interaction between mutation and duplication rate emerges from our results . our future goal is to use this model in order to explore the relationship between the evolutionary emergence of modularity and the phenomenon of gene duplication .
we describe a new theory of differential learning by which a broad family of pattern classifiers ( including many well - known neural network paradigms ) can learn stochastic concepts efficiently . we describe the relationship between a classifier ' s ability to generalize well to unseen test examples and the efficiency of the strategy by which it learns . we list a series of proofs that differential learning is efficient in its information and computational resource requirements , whereas traditional probabilistic learning strategies are not . the proofs are illustrated by a simple example that lends itself to closed - form analysis . we conclude with an optical character recognition task for which three different types of differentially generated classifiers generalize significantly better than their probabilistically generated counterparts .
with most machine learning methods , if the given knowledge representation space is inadequate then the learning process will fail . this is also true with methods using neural networks as the form of the representation space . to overcome this limitation , an automatic construction method for a neural network is proposed . this paper describes the bp - hci method for a hypothesis - driven constructive induction in a neural network trained by the backpropagation algorithm . the method searches for a better representation space by analyzing the hypotheses generated in each step of an iterative learning process . the method was applied to ten problems , which include , in particular , exclusive - or , monk #NUM# , parity - #NUM# bit and inverse parity - #NUM# bit problems . all problems were successfully solved with the same initial set of parameters ; the extension of representation space was no more than necessary extension for each problem .
this paper investigates alternative estimators of the accuracy of concepts learned from examples . in particular , the cross - validation and #NUM# bootstrap estimators are studied , using synthetic training data and the foil learning algorithm . our experimental results contradict previous papers in statistics , which advocate the #NUM# bootstrap method as superior to cross - validation . nevertheless , our results also suggest that conclusions based on cross - validation in previous machine learning papers are unreliable . specifically , our observations are that ( i ) the true error of the concept learned by foil from independently drawn sets of examples of the same concept varies widely , ( ii ) the estimate of true error provided by cross - validation has high variability but is approximately unbiased , and ( iii ) the #NUM# bootstrap estimator has lower variability than cross - validation , but is systematically biased .
the problem of driving an autonomous vehicle in normal traffic engages many areas of ai research and has substantial economic significance . we describe a new approach to this problem based on a decision - theoretic architecture using dynamic probabilistic networks . the architecture provides a sound solution to the problems of sensor noise , sensor failure , and uncertainty about the behavior of other vehicles and about the effects of one ' s own actions . we report on several advances in the theory and practice of inference and decision making in dynamic , partially observable domains . our approach has been implemented in a simulation system , and the autonomous vehicle successfully negotiates a variety of difficult situations . multiple submissions : this paper has not already been accepted by and is not currently under review for a journal or another conference . nor will it be submitted for such during ijcai ' s review period .
two recently implemented machine learning algorithms , ripper and sleeping experts for phrases , are evaluated on a number of large text categorization problems . these algorithms both construct classifiers that allow the " context " of a word w to affect how ( or even whether ) the presence or absence of w will contribute to a classification . however , ripper and sleeping experts differ radically in many other respects differences include different notions as to what constitutes a context , different ways of combining contexts to construct a classifier , different methods to search for a combination of contexts , and different criteria as to what contexts should be included in such a combination . in spite of these differences , both ripper and sleeping experts perform extremely well across a wide variety of categorization problems , generally outperforming previously applied learning methods . we view this result as a confirmation of the usefulness of classifiers that represent contextual information .
we address the problem of finding the parameter settings that will result in optimal performance of a given learning algorithm using a particular dataset as training data . we describe a " wrapper " method , considering determination of the best parameters as a discrete function optimization problem . the method uses best - first search and cross - validation to wrap around the basic induction algorithm : the search explores the space of parameter values , running the basic algorithm many times on training and holdout sets produced by cross - validation to get an estimate of the expected error of each parameter setting . thus , the final selected parameter settings are tuned for the specific induction algorithm and dataset being studied . we report experiments with this method on #NUM# datasets selected from the uci and statlog collections using c #NUM# . #NUM# as the basic induction algorithm . at a #NUM# % confidence level , our method improves the performance of c #NUM# . #NUM# on nine domains , degrades performance on one , and is statistically indistinguishable from c #NUM# . #NUM# on the rest . on the sample of datasets used for comparison , our method yields an average #NUM# % relative decrease in error rate . we expect to see similar performance improvements when using our method with other machine learning al gorithms .
the feature vector editor offers a user - extensible environment for exploratory data analysis . several empirical studies have applied this environment to the sher - facs international conflict management dataset . current analysis techniques include boolean analysis , temporal analysis , and automatic rule learning . implemented portably in ansi common lisp and the common lisp interface manager ( clim ) , the system features an advanced interface that makes it intuitive for people to manipulate data and discover significant relationships . the system encapsulates data within objects and defines generic protocols that mediate all interactions between data , users and analysis algorithms . generic data protocols make possible rapid integration of new datasets and new analytical algorithms with heterogeneous data formats . more sophisticated research reformulates sherfacs conflict codings as machine - parsable narratives suitable for processing into semantic representations by the relatus natural language system . experiments with #NUM# sherfacs cases demonstrated the feasibility of building knowledge bases from synthetic texts exceeding #NUM# pages .
we introduce two boosting algorithms that aim to increase the generalization accuracy of a given classifier by incorporating it as a level - #NUM# component in a stacked generalizer . both algorithms construct a complementary level - #NUM# classifier that can only generate coarse hypotheses for the training data . we show that the two algorithms boost generalization accuracy on a representative collection of data sets . the two algorithms are distinguished in that one of them modifies the class targets of selected training instances in order to train the complementary classifier . we show that the two algorithms achieve approximately equal generalization accuracy , but that they create complementary classifiers that display different degrees of accuracy and diversity . our study provides evidence that it may be useful to investigate families of boosting algorithms that incorporate varying levels of accuracy and diversity , so as to achieve an appropriate mix for a given task and domain .
object localization has applications in many areas of engineering and science . the goal is to spatially locate an arbitrarily - shaped object . in many applications , it is desirable to minimize the number of measurements collected for this purpose , while ensuring sufficient localization accuracy . in surgery , for example , collecting a large number of localization measurements may either extend the time required to perform a surgical procedure , or increase the radiation dosage to which a patient is exposed . localization accuracy is a function of the spatial distribution of discrete measurements over an object when measurement noise is present . in [ simon et al . , #NUM# a ] , metrics were presented to evaluate the information available from a set of discrete object measurements . in this study , new approaches to the discrete point data selection problem are described . these include hillclimbing , genetic algorithms ( gas ) , and population - based incremental learning ( pbil ) . extensions of the standard ga and pbil methods , which employ multiple parallel populations , are explored . the results of extensive empirical testing are provided . the results suggest that a combination of pbil and hillclimbing result in the best overall performance . a computer - assisted surgical system which incorporates some of the methods presented in this paper is currently being evaluated in cadaver trials . evolution - based methods for selecting point data shumeet baluja was supported by a national science foundation graduate student fellowship and a graduate student fellowship from the national aeronautics and space administration , administered by the lyndon b . johnson space center , houston , tx . david simon was partially supported by a national science foundation national challenge grant ( award iri - #NUM# ) . for object localization applications to
the research reported in this paper describes fossil , an ilp system that uses a search heuristic based on statistical correlation . this algorithm implements a new method for learning useful concepts in the presence of noise . in contrast to foil ' s stopping criterion , which allows theories to grow in complexity as the size of the training sets increases , we propose a new stopping criterion that is independent of the number of training examples . instead , fossil ' s stopping criterion depends on a search heuristic that estimates the utility of literals on a uniform scale . in addition we outline how this feature can be used for top - down pruning and present some preliminary results .
this paper describes an approach to using gp for image analysis based on the idea that image enhancement , feature detection and image segmentation can be re - framed as image filtering problems . gp can be used to discover efficient optimal filters which solve such problems . however , in order to make the search feasible and effective , terminal sets , function sets and fitness functions have to meet some requirements . in the paper these requirements are described and terminals , functions and fitness functions that satisfy them are proposed . some preliminary experiments are also reported in which gp ( with the mentioned characteristics ) is applied to the segmentation of the brain in magnetic resonance images ( an extremely difficult problem for which no simple solution is known ) and compared with artificial neural nets .
reading is an area of human cognition which has been studied for decades by psychologists , education researchers , and artificial intelligence researchers . yet , there still does not exist a theory which accurately describes the complete process . we believe that these past attempts fell short due to an incomplete understanding of the overall task of reading ; namely , the complete set of mental tasks a reasoner must perform to read and the mechanisms that carry out these tasks . we present a functional theory of the reading process and argue that it represents a coverage of the task . the theory combines experimental results from psychology , artificial intelligence , education , and linguistics , along with the insights we have gained from our own research . this greater understanding of the mental tasks necessary for reading will enable new natural language understanding systems to be more flexible and more capable than earlier ones . furthermore , we argue that creativity is a necessary component of the reading process and must be considered in any theory or system attempting to describe it . we present a functional theory of creative reading and a novel knowledge organization scheme that supports the creativity mechanisms . the reading theory is currently being implemented in the isaac ( integrated story analysis and creativity ) system , a computer system which reads science fiction stories .
the np - complete problem of determining whether two disjoint point sets in the n - dimensional real space r n can be separated by two planes is cast as a bilinear program , that is minimizing the scalar product of two linear functions on a polyhedral set . the bilinear program , which has a vertex solution , is processed by an iterative linear programming algorithm that terminates in a finite number of steps at a point satisfying a necessary optimality condition or at a global minimum . encouraging computational experience on a number of test problems is reported .
the problem of discriminating between two finite point sets in n - dimensional feature space by a separating plane that utilizes as few of the features as possible , is formulated as a mathematical program with a parametric objective function and linear constraints . the step function that appears in the objective function can be approximated by a sigmoid or by a concave exponential on the nonnegative real line , or it can be treated exactly by considering the equivalent linear program with equilibrium constraints ( lpec ) . computational tests of these three approaches on publicly available real - world databases have been carried out and compared with an adaptation of the optimal brain damage ( obd ) method for reducing neural network complexity . one feature selection algorithm via concave minimization ( fsv ) reduced cross - validation error on a cancer prognosis database by #NUM# . #NUM# % while reducing problem features from #NUM# to #NUM# . feature selection is an important problem in machine learning [ #NUM# , #NUM# , #NUM# , #NUM# , #NUM# ] . in its basic form the problem consists of eliminating as many of the features in a given problem as possible , while still carrying out a preassigned task with acceptable accuracy . having a minimal number of features often leads to better generalization and simpler models that can be more easily interpreted . in the present work , our task is to discriminate between two given sets in an n - dimensional feature space by using as few of the given features as possible . we shall formulate this problem as a mathematical program with a parametric objective function that will attempt to achieve this task by generating a separating plane in a feature space of as small a dimension as possible while minimizing the average distance of misclassified points to the plane . one of the computational experiments that we carried out on our feature selection procedure showed its effectiveness , not only in minimizing the number of features selected , but also in quickly recognizing and removing spurious random features that were introduced . thus , on the wisconsin prognosis breast cancer wpbc database [ #NUM# ] with a feature space of #NUM# dimensions and #NUM# random features added , one of our algorithms fsv ( #NUM# ) immediately removed the #NUM# random features as well as #NUM# of the original features resulting in a separating plane in a #NUM# - dimensional reduced feature space . by using tenfold cross - validation [ #NUM# ] , separation error in the #NUM# - dimensional space was reduced #NUM# . #NUM# % from the corresponding error in the original problem space . ( see section #NUM# for details . ) we note that mathematical programming approaches to the feature selection problem have been recently proposed in [ #NUM# , #NUM# ] . even though the approach of [ #NUM# ] is based on an lpec formulation , both the lpec and its method of solution are different from the ones used here . the polyhedral concave minimization approach of [ #NUM# ] is principally involved with theoretical considerations of one specific algorithm and no cross - validatory results are given . other effective computational applications of mathematical programming to neural networks are given in [ #NUM# , #NUM# ] .
this work describes an approach for inferring deterministic context - free ( dcf ) grammars in a connectionist paradigm using a recurrent neural network pushdown automaton ( nnpda ) . the nnpda consists of a recurrent neural network connected to an external stack memory through a common error function . we show that the nnpda is able to learn the dynamics of an underlying pushdown automaton from examples of grammatical and non - grammatical strings . not only does the network learn the state transitions in the automaton , it also learns the actions required to control the stack . in order to use continuous optimization methods , we develop an analog stack which reverts to a discrete stack by quantization of all activations , after the network has learned the transition rules and stack actions . we further show an enhancement of the network ' s learning capabilities by providing hints . in addition , an initial comparative study of simulations with first , second and third order recurrent networks has shown that the increased degree of freedom in a higher order networks improve generalization but not necessarily learning speed .
this paper presents the hga , a genetic algorithm written in vhdl and intended for a hardware implementation . due to pipelining , parallelization , and no function call overhead , a hardware ga yields a significant speedup over a software ga , which is especially useful when the ga is used for real - time applications , e . g . disk scheduling and image registration . since a general - purpose ga requires that the fitness function be easily changed , the hardware implementation must exploit the reprogrammability of certain types of field - programmable gate arrays ( fpgas ) , which are programmed via a bit pattern stored in a static ram and are thus easily reconfigured . after presenting some background on vhdl , this paper takes the reader through the hga ' s code . we then describe some applications of the hga that are feasible given the state - of - the - art in fpga technology and summarize some possible extensions of the design . finally , we review some other work in hardware - based gas .
one of the basic probabilistic tools used for time series modeling is the hidden markov model ( hmm ) . in an hmm , information about the past of the time series is conveyed through a single discrete variable | the hidden state . we present a generalization of hmms in which this state is factored into multiple state variables and is therefore represented in a distributed manner . both inference and learning in this model depend critically on computing the posterior probabilities of the hidden state variables given the observations . we present an exact algorithm for inference in this model , and relate it to the forward - backward algorithm for hmms and to algorithms for more general belief networks . due to the combinatorial nature of the hidden state representation , this exact algorithm is intractable . as in other intractable systems , approximate inference can be carried out using gibbs sampling or mean field theory . we also present a structured approximation in which the the state variables are decoupled , based on which we derive a tractable learning algorithm . empirical comparisons suggest that these approximations are efficient and accurate alternatives to the exact methods . finally , we use the structured approximation to model bach ' s chorales and show that it outperforms hmms in capturing the complex temporal patterns in this dataset .
we develop a refined mean field approximation for inference and learning in probabilistic neural networks . our mean field theory , unlike most , does not assume that the units behave as independent degrees of freedom ; instead , it exploits in a principled way the existence of large substructures that are computationally tractable . to illustrate the advantages of this framework , we show how to incorporate weak higher order interactions into a first - order hidden markov model , treating the corrections ( but not the first order structure ) within mean field theory .
this chapter takes a different standpoint to address the problem of learning . we will here reason only in terms of probability , and make extensive use of the chain rule known as " bayes ' rule " . a fast definition of the basics in probability is provided in appendix a for quick reference . most of this chapter is a review of the methods of bayesian learning applied to our modelling purposes . some original analyses and comments are also provided in section #NUM# . #NUM# , #NUM# . #NUM# and #NUM# . #NUM# . there is a latent rivalry between " bayesian " and " orthodox " statistics . it is by no means our intention to enter this kind of controversy . we are perfectly willing to accept orthodox as well as unorthodox methods , as long as they are scientifically sound and provide good results when applied to learning tasks . the same disclaimer applies to the two frameworks presented here . they have been the object of heated controversy in the past #NUM# years in the neural networks community . we will not take side , but only present both frameworks , with their strong points and their weaknesses . in the context of this work , the " bayesian frameworks " are especially interesting as the provide some continuous update rules that can be used during regularised cost minimisation to yield an automatic selection of the regularisation level . unlike the methods presented in chapter #NUM# , it is not necessary to try several regularisation levels and perform as many optimisations . the bayesian framework is the only one in which training is achieved through a one - pass optimisation procedure .
in this document , i first review the theory behind bits - back coding ( aka . free energy coding ) ( frey and hinton #NUM# ) and then describe the interface to c - language software that can be used for bits - back coding . this method is a new approach to the problem of optimal compression when a source code produces multiple codewords for a given symbol . it may seem that the most sensible codeword to use in this case is the shortest one . however , in the proposed bits - back approach , random codeword selection yields an effective codeword length that can be less than the shortest codeword length . if the random choices are boltzmann distributed , the effective length is optimal for the given source code . the software which i describe in this guide is easy to use and the source code is only a few pages long . i illustrate the bits - back coding software on a simple quantized gaussian mixture problem .
a modified recurrent neural network ( rnn ) is used to learn a self - routing interconnection network ( srin ) from a set of routing examples . the rnn is modified so that it has several distinct initial states . this is equivalent to a single rnn learning multiple different synchronous sequential machines . we define such a sequential machine structure as augmented and show that a srin is essentially an augmented synchronous sequential machine ( assm ) . as an example , we learn a small six - switch srin . after training we extract the net - work ' s internal representation of the assm and corresponding srin .
this paper outlines a methodology for analyzing the representational support for knowledge - based decision - modeling in a broad domain . a relevant set of inference patterns and knowledge types are identified . by comparing the analysis results to existing representations , some insights are gained into a design approach for integrating categorical and uncertain knowledge in a context sensitive manner .
we discuss two types of algorithms for selecting relevant examples that have been developed in the context of computation learning theory . the examples are selected out of a stream of examples that are generated independently at random . the first two algorithms are the so - called " boosting " algorithms of schapire [ schapire , #NUM# ] and fre - und [ freund , #NUM# ] , and the query - by - committee algorithm of seung [ seung et al . , #NUM# ] . we describe the algorithms and some of their proven properties , point to some of their commonalities , and suggest some possible future implications .
this paper traces the development of the main ideas that have led to the present state of knowledge in inductive logic programming . the story begins with research in psychology on the subject of human concept learning . results from this research influenced early efforts in artificial intelligence which combined with the formal methods of inductive inference to evolve into the present discipline of inductive logic programming . inductive logic programming is often considered to be a young discipline . however , it has its roots in research dating back nearly #NUM# years . this paper traces the development of ideas beginning in psychology and the effect they had on concept learning research in artificial intelligence . independent of any requirement for a psychological basis , formal methods of inductive inference were developed . these separate streams eventually gave rise to inductive logic programming . this account is not entirely unbiased . more attention is given to the work of those researchers who most influenced my own interest in machine learning . being a retrospective paper , i do not attempt to describe recent developments in ilp . this account only includes research prior to #NUM# the year in which the term inductive logic programming was first used ( muggleton , #NUM# ) . this is the reason for the subtitle a prehistoric tale . the major headings in the paper are taken from the names of periods in the evolution of life on earth .
recurrent neural networks readily process , recognize and generate temporal sequences . by encoding grammatical strings as temporal sequences , recurrent neural networks can be trained to behave like deterministic sequential finite - state automata . algorithms have been developed for extracting grammatical rules from trained networks . using a simple method for inserting prior knowledge ( or rules ) into recurrent neural networks , we show that recurrent neural networks are able to perform rule revision . rule revision is performed by comparing the inserted rules with the rules in the finite - state automata extracted from trained networks . the results from training a recurrent neural network to recognize a known non - trivial , randomly generated regular grammar show that not only do the networks preserve correct rules but that they are able to correct through training inserted rules which were initially incorrect . ( by incorrect , we mean that the rules were not the ones in the randomly generated grammar . )
in this section we survey recombination operators that can apply more than two parents to create offspring . some multi - parent recombination operators are defined for a fixed number of parents , e . g . have arity three , in some operators the number of parents is a random number that might be greater than two , and in yet other operators the arity is a parameter that can be set to an arbitrary integer number . we pay special attention to this latter type of operators and summarize results on the effect of operator arity on ea performance .
backpropagation learning algorithms typically collapse the network ' s structure into a single vector of weight parameters to be optimized . we suggest that their performance may be improved by utilizing the structural information instead of discarding it , and introduce a framework for tempering each weight accordingly . in the tempering model , activation and error signals are treated as approximately independent random variables . the characteristic scale of weight changes is then matched to that of the residuals , allowing structural properties such as a node ' s fan - in and fan - out to affect the local learning rate and backpropagated error . the model also permits calculation of an upper bound on the global learning rate for batch updates , which in turn leads to different update rules for bias vs . non - bias weights .
this paper addresses a class of learning problems that require a construction of descriptions that combine both m - of - n rules and traditional disjunctive normal form ( dnf ) rules . the presented method learns such descriptions , which we call conditional m - of - n rules , using the hypothesis - driven constructive induction approach . in this approach , the representation space is modified according to patterns discovered in the iteratively generated hypotheses . the need for the m - of - n rules is detected by observing " exclusive - or " or " equivalence " patterns in the hypotheses . these patterns indicate symmetry relations among pairs of attributes . symmetrical attributes are combined into maximal symmetry classes . for each symmetry class , the method constructs a " counting attribute " that adds a new dimension to the representation space . the search for hypothesis in iteratively modified representation spaces is done by the standard aq inductive rule learning algorithm . it is shown that the proposed method is capable of solving problems that would be very difficult to tackle by any of the traditional symbolic learning methods .
inference ' s conversational case - based reasoning ( ccbr ) approach , embedded in the cbr content navigator line of products , is susceptible to a bias in its case scoring algorithm . in particular , shorter cases tend to be given higher scores , assuming all other factors are held constant . this report summarizes our investigation for mediating this bias . we introduce an approach for eliminating this bias and evaluate how it affects retrieval performance for six case libraries . we also suggest explanations for these results , and note the limitations of our study .
we investigate the effectiveness of stochastic hillclimbing as a baseline for evaluating the performance of genetic algorithms ( gas ) as combinatorial function optimizers . in particular , we address four problems to which gas have been applied in the literature : the maximum cut problem , koza ' s #NUM# - multiplexer problem , mdap ( the multiprocessor document allocation problem ) , and the jobshop problem . we demonstrate that simple stochastic hillclimbing methods are able to achieve results comparable or superior to those obtained by the gas designed to address these four problems . we further illustrate , in the case of the jobshop problem , how insights obtained in the formulation of a stochastic hillclimbing algorithm can lead to improvements in the encoding used by a ga .
this paper describes several means for sharing between related concepts to improve learning in the same domain . the sharing comes in the form of substructures or possibly entire structures of previous concepts which may aid in learning other concepts . these substructures highlight useful information in the domain . using two domains , we evaluate the effectiveness of concept sharing with respect to accuracy , concept size , search complexity , and noise resistance .
genetic algorithms are stochastic search and optimization techniques which can be used for a wide range of applications . this paper addresses the application of genetic algorithms to the graph partitioning problem . standard genetic algorithms with large populations suffer from lack of efficiency ( quite high execution time ) . a massively parallel genetic algorithm is proposed , an implementation on a supernode of transputers and results of various benchmarks are given . a comparative analysis of our approach with hill - climbing algorithms and simulated annealing is also presented . the experimental measures show that our algorithm gives better results concerning both the quality of the solution and the time needed to reach it .
the support vector machine ( svm ) is a new and very promising classification technique developed by vapnik and his group at at & t bell laboratories [ #NUM# , #NUM# , #NUM# , #NUM# ] . this new learning algorithm can be seen as an alternative training technique for polynomial , radial basis function and multi - layer perceptron classifiers . the main idea behind the technique is to separate the classes with a surface that maximizes the margin between them . an interesting property of this approach is that it is an approximate implementation of the structural risk minimization ( srm ) induction principle [ #NUM# ] . the derivation of support vector machines , its relationship with srm , and its geometrical insight , are discussed in this paper . since structural risk minimization is an inductive principle that aims at minimizing a bound on the generalization error of a model , rather than minimizing the mean square error over the data set ( as empirical risk minimization methods do ) , training a svm to obtain the maximum margin classifier requires a different objective function . this objective function is then optimized by solving a large - scale quadratic programming problem with linear and box constraints . the problem is considered challenging , because the quadratic form is completely dense , so the memory needed to store the problem grows with the square of the number of data points . therefore , training problems arising in some real applications with large data sets are impossible to load into memory , and cannot be solved using standard non - linear constrained optimization algorithms . we present a decomposition algorithm that can be used to train svm ' s over large data sets . the main idea behind the decomposition is the iterative solution of sub - problems and the evaluation of , and also establish the stopping criteria for the algorithm . we present previous approaches , as well as results and important details of our implementation of the algorithm using a second - order variant of the reduced gradient method as the solver of the sub - problems . as an application of svm ' s , we present preliminary results in frontal human face detection in images . this application opens many interesting questions and future research opportunities , both in the context of faster and better optimization algorithms , and in the use of svm ' s in other pattern classification , recognition , and detection applications . this report describes research done within the center for biological and computational learning in the department of brain and cognitive sciences and the artificial intelligence laboratory at the massachusetts institute of technology . this research is sponsored by muri grant n #NUM# - #NUM# - #NUM# - #NUM# ; by a grant from onr / arpa under contract n #NUM# - #NUM# - j - #NUM# and by the national science foundation under contract asc - #NUM# ( this award includes funds from arpa provided under the hpcc program ) . edgar osuna was supported by fundacion gran mariscal de ayacucho and daimler benz . additional support is provided by daimler - benz , eastman kodak company , siemens corporate research , inc . and at & t .
a significant limitation of neural networks is that the representations they learn are usually incomprehensible to humans . we have developed an algorithm , called trepan , for extracting comprehensible , symbolic representations from trained neural networks . given a trained network , trepan produces a decision tree that approximates the concept represented by the network . in this article , we discuss the application of trepan to a neural network trained on a noisy time series task : predicting the dollar - mark exchange rate . we present experiments that show that trepan is able to extract a decision tree from this network that equals the network in terms of predictive accuracy , yet provides a comprehensible concept representation . moreover , our experiments indicate that decision trees induced directly from the training data using conventional algorithms do not match the accuracy nor the comprehensibility of the tree extracted by trepan .
in spite of the popularity of explanation - based learning ( ebl ) , its theoretical basis is not well - understood . using a generalization of probably approximately correct ( pac ) learning to problem solving domains , this paper formalizes two forms of explanation - based learning of macro - operators and proves the sufficient conditions for their success . these two forms of ebl , called " macro caching " and " serial parsing , " respectively exhibit two distinct sources of power or " bias " : the sparseness of the solution space and the decomposability of the problem - space . the analysis shows that exponential speedup can be achieved when either of these biases is suitable for a domain . somewhat surprisingly , it also shows that computing the preconditions of the macro - operators is not necessary to obtain these speedups . the theoretical results are confirmed by experiments in the domain of eight puzzle . our work suggests that the best way to address the utility problem in ebl is to implement a bias which exploits the problem - space structure of the set of domains that one is interested in learning .
developed only recently , support vector learning machines achieve high generalization ability by minimizing a bound on the expected test error ; however , so far there existed no way of adding knowledge about invariances of a classification problem at hand . we present a method of incorporating prior knowledge about transformation invari - ances by applying transformations to support vectors , the training ex amples most critical for determining the classification boundary .
this paper reports on recent results using genetic algorithms to learn decision rules for complex robot behaviors . the method involves evaluating hypothetical rule sets on a simulator and applying simulated evolution to evolve more effective rules . the main contributions of this paper are ( #NUM# ) the task learned is a complex behavior involving multiple mobile robots , and ( #NUM# ) the learned rules are verified through experiments on operational mobile robots . the case study involves a shepherding task in which one mobile robot attempts to guide another robot to a specified area .
in most learning systems examples are represented as fixed - length " feature vectors " , the components of which are either real numbers or nominal values . we propose an extension of the feature - vector representation that allows the value of a feature to be a set of strings ; for instance , to represent a small white and black dog with the nominal features size and species and the set - valued feature color , one might use a feature vector with size = small , species = canis - familiaris and color = fwhite , blackg . since we make no assumptions about the number of possible set elements , this extension of the traditional feature - vector representation is closely connected to blum ' s " infinite attribute " representation . we argue that many decision tree and rule learning algorithms can be easily extended to set - valued features . we also show by example that many real - world learning problems can be efficiently and naturally represented with set - valued features ; in particular , text categorization problems and problems that arise in propositionalizing first - order representations lend themselves to set - valued features .
the mobile robot domain challenges policy - iteration reinforcement learning algorithms with difficult problems of structural credit assignment and uncertainty . structural credit assignment is particularly acute in domains where " real - time " trial length is a limiting factor on the number of learning steps that physical hardware can perform . noisy sensors and effectors in complex dynamic environments further complicate the learning problem , leading to situations where speed of learning and policy flexibility may be more important than policy optimality . input generalization addresses these problems but is typically too time consuming for robot domains . we present two algorithms , yb - learning and yb , that perform simple and fast generalization of the input space based on bit - similarity . the algorithms trade off long - term optimality for immediate performance and flexibility . the algorithms were tested in simulation against non - generalized learning across different numbers of discounting steps , and yb was shown to perform better during the earlier stages of learning , particularly in the presence of noise . in trials performed on a sonar - based mobile robot subject to uncertainty of the " real world , " yb surpassed the simulation results by a wide margin , strongly supporting the role of such " quick and dirty " generalization strategies in noisy real - time mobile robot domains .
in time series problems , noise can be divided into two categories : dynamic noise which drives the process , and observational noise which is added in the measurement process , but does not influence future values of the system . in this framework , empirical volatilities ( the squared relative returns of prices ) exhibit a significant amount of observational noise . to model and predict their time evolution adequately , we estimate state space models that explicitly include observational noise . we obtain relaxation times for shocks in the logarithm of volatility ranging from three weeks ( for foreign exchange ) to three to five months ( for stock indices ) . in most cases , a two - dimensional hidden state is required to yield residuals that are consistent with white noise . we compare these results with ordinary autoregressive models ( without a hidden state ) and find that autoregressive models underestimate the relaxation times by about two orders of magnitude due to their ignoring the distinction between observational and dynamic noise . this new interpretation of the dynamics of volatility in terms of relaxators in a state space model carries over to stochastic volatility models and to garch models , and is useful for several problems in finance , including risk management and the pricing of derivative securities .
in this paper we present tdleaf ( ) , a variation on the td ( ) algorithm that enables it to be used in conjunction with minimax search . we present some experiments in which our chess program , knightcap , used tdleaf ( ) to learn its evaluation function while playing on the free ineternet chess server ( fics , fics . onenet . net ) . it improved from a #NUM# rating to a #NUM# rating in just #NUM# games and #NUM# days of play . we discuss some of the reasons for this success and also the relationship between our results and tesauro ' s results in backgammon .
the problem of minimizing the number of misclassified points by a plane , attempting to separate two point sets with intersecting convex hulls in n - dimensional real space , is formulated as a linear program with equilibrium constraints ( lpec ) . this general lpec can be converted to an exact penalty problem with a quadratic objective and linear constraints . a frank - wolfe - type algorithm is proposed for the penalty problem that terminates at a stationary point or a global solution . novel aspects of the approach include : ( i ) a linear complementarity formulation of the step function that " counts " misclassifications , ( ii ) exact penalty formulation without boundedness , nondegeneracy or constraint qualification assumptions , ( iii ) an exact solution extraction from the sequence of minimizers of the penalty function for a finite value of the penalty parameter for the general lpec and an explicitly exact solution for the lpec with uncoupled constraints , and ( iv ) a parametric quadratic programming formulation of the lpec associated with the misclassification minimization problem .
it has long been known that neural networks can learn faster when their input and hidden unit activities are centered about zero ; recently we have extended this approach to also encompass the centering of error signals ( schraudolph and sejnowski , #NUM# ) . here we generalize this notion to all factors involved in the weight update , leading us to propose centering the slope of hidden unit activation functions as well . slope centering removes the linear component of backpropagated error ; this improves credit assignment in networks with shortcut connections . benchmark results show that this can speed up learning significantly without adversely affecting the trained network ' s generalization ability .
this paper presents an asocs ( adaptive self - organizing concurrent system ) model for massively parallel processing of incrementally defined rule systems in such areas as adaptive logic , robotics , logical inference , and dynamic control . an asocs is an adaptive network composed of many simple computing elements operating asynchronously and in parallel . an asocs can operate in either a data processing mode or a learning mode . during data processing mode , an asocs acts as a parallel hardware circuit . during learning mode , an asocs incorporates a rule expressed as a boolean conjunction in a distributed fashion in time logarithmic in the number of rules . this paper proposes a learning algorithm and architecture for priority asocs . this new asocs model uses rules with priorities . the new model has significant learning time and space complexity improvements over previous models . non - von neumann architectures such as neural networks attack the word - at - a - time bottleneck of traditional computing systems [ #NUM# ] . neural networks learn input - output mappings using highly distributed processing and memory [ #NUM# , #NUM# , #NUM# ] . their numerous simple processing elements with modifiable weighted links permit a high degree of parallelism . a typical neural network has fixed topology . it learns by modifying weighted links between nodes . a new class of connectionist architectures has been proposed called asocs ( adaptive self - organizing concurrent systems ) [ #NUM# , #NUM# ] . asocs models support efficient computation through self - organized learning and parallel execution . learning is done through the incremental presentation of rules and / or examples . asocs models learn by modifying their topology . data types include boolean and multi - state variables ; recent models support analog variables . the model incorporates rules into an adaptive logic network in a parallel and self organizing fashion . in processing mode , asocs supports fully parallel execution on actual inputs according to the learned rules . the adaptive logic network acts as a parallel hardware circuit during execution , mapping n input boolean vectors into m output boolean vectors , in a combinatoric fashion . the overall philosophy of asocs follows the high level goals of current neural network models . however , the mechanisms of learning and execution vary significantly . the asocs logic network is topologically dynamic with the network growing to efficiently fit the specific application . current asocs models are based on digital nodes . asocs also supports use of symbolic and heuristic learning mechanisms , thus combining the parallelism and distributed nature of connectionist computing with the potential power of ai symbolic learning . a proof of concept asocs chip has been developed [ #NUM# ] .
we exhibit a theoretically founded algorithm t #NUM# for agnostic pac - learning of decision trees of at most #NUM# levels , whose computation time is almost linear in the size of the training set . we evaluate the performance of this learning algorithm t #NUM# on #NUM# common real - world datasets , and show that for most of these datasets t #NUM# provides simple decision trees with little or no loss in predictive power ( compared with c #NUM# . #NUM# ) . in fact , for datasets with continuous attributes its error rate tends to be lower than that of c #NUM# . #NUM# . to the best of our knowledge this is the first time that a pac - learning algorithm is shown to be applicable to real - world classification problems . since one can prove that t #NUM# is an agnostic pac - learning algorithm , t #NUM# is guaranteed to produce close to optimal #NUM# - level decision trees from sufficiently large training sets for any ( ! ) distribution of data . in this regard t #NUM# differs strongly from all other learning algorithms that are considered in applied machine learning , for which no guarantee can be given about their performance on new datasets . we also demonstrate that this algorithm t #NUM# can be used as a diagnostic tool for the investigation of the expressive limits of #NUM# - level decision trees . finally , t #NUM# , in combination with new bounds on the vc - dimension of decision trees of bounded depth that we derive , provides us now for the first time with the tools necessary for comparing learning curves of decision trees for real - world datasets with the theoretical estimates of pac learning theory .
andrew d . back was with the department of electrical and computer engineering , university of queensland . st . lucia , australia . he is now with the brain information processing group , frontier research program , riken , the institute of physical and chemical research , #NUM# - #NUM# hirosawa , wako - shi , saitama #NUM# - #NUM# , japan the performance of neural network simulations is often reported in terms of the mean and standard deviation of a number of simulations performed with different starting conditions . however , in many cases , the distribution of the individual results does not approximate a gaussian distribution , may not be symmetric , and may be multimodal . we present the distribution of results for practical problems and show that assuming gaussian distributions can significantly affect the interpretation of results , especially those of comparison studies . for a controlled task which we consider , we find that the distribution of performance is skewed towards better performance for smoother target functions and skewed towards worse performance
the structure of an environment affects the behaviors of the organisms that have evolved in it . how is that structure to be described , and how can its behavioral consequences be explained and predicted ? we aim to establish initial answers to these questions by simulating the evolution of very simple organisms in simple environments with different structures . our artificial creatures , called " minimats , " have neither sensors nor memory and behave solely by picking amongst the actions of moving , eating , reproducing , and sitting , according to an inherited probability distribution . our simulated environments contain only food ( and multiple minimats ) and are structured in terms of their spatial and temporal food density and the patchiness with which the food appears . changes in these environmental parameters affect the evolved behaviors of minimats in different ways , and all three parameters are of importance in describing the minimat world . one of the most useful behavioral strategies that evolves is " looping " movement , which allows minimats - despite their lack of internal state - to match their behavior to the temporal ( and spatial ) structure of their environment . ultimately we find that minimats construct their own environments through their individual behaviors , making the study of the impact of global environment structure on individual behavior much more complex .
the primary aim of this paper is to show how graphical models can be used as a mathematical language for integrating statistical and subject - matter information . in particular , the paper develops a principled , nonparametric framework for causal inference , in which diagrams are queried to determine if the assumptions available are sufficient for identifying causal effects from nonexperimental data . if so the diagrams can be queried to produce mathematical expressions for causal effects in terms of observed distributions ; otherwise , the diagrams can be queried to suggest additional observations or auxiliary experiments from which the desired inferences can be obtained . key words : causal inference , graph models , structural equations , treatment effect .
we analyse the biases of eleven measures for estimating the quality of the multi - valued attributes . the values of information gain , j - measure , gini - index , and relevance tend to linearly increase with the number of values of an attribute . the values of gain - ratio , distance measure , relief , and the weight of evidence decrease for informative attributes and increase for irrelevant attributes . the bias of the statistic tests based on the chi - square distribution is similar but these functions are not able to discriminate among the attributes of different quality . we also introduce a new function based on the mdl principle whose value slightly decreases with the increasing number of attribute ' s values .
in the past , nearest neighbor algorithms for learning from examples have worked best in domains in which all features had numeric values . in such domains , the examples can be treated as points and distance metrics can use standard definitions . in symbolic domains , a more sophisticated treatment of the feature space is required . we introduce a nearest neighbor algorithm for learning in domains with symbolic features . our algorithm calculates distance tables that allow it to produce real - valued distances between instances , and attaches weights to the instances to further modify the structure of feature space . we show that this technique produces excellent classification accuracy on three problems that have been studied by machine learning researchers : predicting protein secondary structure , identifying dna promoter sequences , and pronouncing english text . direct experimental comparisons with the other learning algorithms show that our nearest neighbor algorithm is comparable or superior in all three domains . in addition , our algorithm has advantages in training speed , simplicity , and perspicuity . we conclude that experimental evidence favors the use and continued development of nearest neighbor algorithms for domains such as the ones studied here .
many supervised machine learning algorithms require a discrete feature space . in this paper , we review previous work on continuous feature discretization , identify defining characteristics of the methods , and conduct an empirical evaluation of several methods . we compare binning , an unsupervised discretization method , to entropy - based and purity - based methods , which are supervised algorithms . we found that the performance of the naive - bayes algorithm significantly improved when features were discretized using an entropy - based method . in fact , over the #NUM# tested datasets , the discretized version of naive - bayes slightly outperformed c #NUM# . #NUM# on average . we also show that in some cases , the performance of the c #NUM# . #NUM# induction algorithm significantly improved if features were discretized in advance ; in our experiments , the performance never significantly degraded , an interesting phenomenon considering the fact that c #NUM# . #NUM# is capable of locally discretiz ing features .
in this paper we explore the use of an adaptive search technique ( genetic algorithms ) to construct a system gabil which continually learns and refines concept classification rules from its interaction with the environment . the performance of the system is measured on a set of concept learning problems and compared with the performance of two existing systems : id #NUM# r and c #NUM# . #NUM# . preliminary results support that , despite minimal system bias , gabil is an effective concept learner and is quite competitive with id #NUM# r and c #NUM# . #NUM# as the target concept increases in complexity .
we review accuracy estimation methods and compare the two most common methods : cross - validation and bootstrap . recent experimental results on artificial data and theoretical results in restricted settings have shown that for selecting a good classifier from a set of classifiers ( model selection ) , ten - fold cross - validation may be better than the more expensive leave - one - out cross - validation . we report on a large - scale experiment | over half a million runs of c #NUM# . #NUM# and a naive - bayes algorithm | to estimate the effects of different parameters on these algorithms on real - world datasets . for cross - validation , we vary the number of folds and whether the folds are stratified or not ; for bootstrap , we vary the number of bootstrap samples . our results indicate that for real - word datasets similar to ours , the best method to use for model selection is ten - fold stratified cross validation , even if computation power allows using more folds .
naive - bayes induction algorithms were previously shown to be surprisingly accurate on many classification tasks even when the conditional independence assumption on which they are based is violated . however , most studies were done on small databases . we show that in some larger databases , the accuracy of naive - bayes does not scale up as well as decision trees . we then propose a new algorithm , nbtree , which induces a hybrid of decision - tree classifiers and naive - bayes classifiers : the decision - tree nodes contain uni - variate splits as regular decision - trees , but the leaves contain naive - bayesian classifiers . the approach retains the interpretability of naive - bayes and decision trees , while resulting in classifiers that frequently outperform both constituents , especially in the larger databases tested .
we present mlc + + , a library of c + + classes and tools for supervised machine learning . while mlc + + provides general learning algorithms that can be used by end users , the main objective is to provide researchers and experts with a wide variety of tools that can accelerate algorithm development , increase software reliability , provide comparison tools , and display information visually . more than just a collection of existing algorithms , mlc + + is an attempt to extract commonalities of algorithms and decompose them for a unified view that is simple , coherent , and extensible . in this paper we discuss the problems mlc + + aims to solve , the design of mlc + + , and the current functionality .
bayesian models involving dirichlet process mixtures are at the heart of the modern nonparametric bayesian movement . much of the rapid development of these models in the last decade has been a direct result of advances in simulation - based computational methods . some of the very early work in this area , circa #NUM# - #NUM# , focused on the use of such nonparametric ideas and models in applications of otherwise standard hierarchical models . this chapter provides some historical review and perspective on these developments , with a prime focus on the use and integration of such nonparametric ideas in hierarchical models . we illustrate the ease with which the strict parametric assumptions common to most standard bayesian hierarchical models can be relaxed to incorporate uncertainties about functional forms using dirichlet process components , partly enabled by the approach to computation using mcmc methods . the resulting methology is illustrated with two examples taken from an unpublished #NUM# report on the topic .
in this paper we present an average - case analysis of the bayesian classifier , a simple induction algorithm that fares remarkably well on many learning tasks . our analysis assumes a monotone conjunctive target concept , and independent , noise - free boolean attributes . we calculate the probability that the algorithm will induce an arbitrary pair of concept descriptions and then use this to compute the probability of correct classification over the instance space . the analysis takes into account the number of training instances , the number of attributes , the distribution of these attributes , and the level of class noise . we also explore the behavioral implications of the analysis by presenting predicted learning curves for artificial domains , and give experimental results on these domains as a check on our reasoning . one goal of research in machine learning is to discover principles that relate algorithms and domain characteristics to behavior . to this end , many researchers have carried out systematic experimentation with natural and artificial domains in search of empirical regularities ( e . g . , kibler & langley , #NUM# ) . others have focused on theoretical analyses , often within the paradigm of probably approximately correct learning ( e . g . , haus - sler , #NUM# ) . however , most experimental studies are based only on informal analyses of the learning task , whereas most formal analyses address the worst case , and thus bear little relation to empirical results . ber of attributes , and the class and attribute frequencies , they obtain predictions about the behavior of induction algorithms and used experiments to check their analyses . #NUM# however , their research does not focus on algorithms typically used by the experimental and practical sides of machine learning , and it is important that average - case analyses be extended to such methods . recently , there has been growing interest in probabilistic approaches to inductive learning . for example , fisher ( #NUM# ) has described cobweb , an incremental algorithm for conceptual clustering that draws heavily on bayesian ideas , and the literature reports a number of systems that build on this work ( e . g . , allen & lang - ley , #NUM# ; iba & gennari , #NUM# ; thompson & langley , #NUM# ) . cheeseman et al . ( #NUM# ) have outlined auto - class , a nonincremental system that uses bayesian methods to cluster instances into groups , and other researchers have focused on the induction of bayesian inference networks ( e . g . , cooper & kerskovits , #NUM# ) . these recent bayesian learning algorithms are complex and not easily amenable to analysis , but they share a common ancestor that is simpler and more tractable . this supervised algorithm , which we refer to simply as a bayesian classifier , comes originally from work in pattern recognition ( duda & hart , #NUM# ) . the method stores a probabilistic summary for each class ; this summary contains the conditional probability of each attribute value given the class , as well as the probability ( or base rate ) of the class . this data structure approximates the representational power of a perceptron ; it describes a single decision boundary through the instance space . when the algorithm encounters a new instance , it updates the probabilities stored with the specified class . neither the order of training instances nor the occurrence of classification errors have any effect on this process . when given a test instance , the classifier uses an evaluation function ( which we describe in detail later ) to rank the alter
regularization , e . g . , in the form of weight decay , is important for training and optimization of neural network architectures . in this work we provide a tool based on asymptotic sampling theory , for iterative estimation of weight decay parameters . the basic idea is to do a gradient descent in the estimated generalization error with respect to the regularization parameters . the scheme is implemented in our designer net framework for network training and pruning , i . e . , is based on the diagonal hessian approximation . the scheme does not require essential computational overhead in addition to what is needed for training and pruning . the viability of the approach is demonstrated in an experiment concerning prediction of the chaotic mackey - glass series . we find that the optimized weight decays are relatively large for densely connected networks in the initial pruning phase , while they decrease as pruning proceeds .
vations of perceptrons ( #NUM# ) when the perceptron learning algorithm cycles among hyperplanes , the hyperplanes may be compared to select one that gives a best split of the examples , and ( #NUM# ) it is always possible for the perceptron to build a hyper - plane that separates at least one example from all the rest . we describe the extentron which grows multi - layer networks capable of distinguishing non - linearly - separable data using the simple perceptron rule for linear threshold units . the resulting algorithm is simple , very fast , scales well to large prob - lems , retains the convergence properties of the perceptron , and can be completely specified using only two parameters . results are presented comparing the extentron to other neural network paradigms and to symbolic learning systems .
it has long been known that neural networks can learn faster when their input and hidden unit activities are centered about zero ; recently we have extended this approach to also encompass the centering of error signals ( schraudolph and sejnowski , #NUM# ) . here we generalize this notion to all factors involved in the weight update , leading us to propose centering the slope of hidden unit activation functions as well . slope centering removes the linear component of backpropagated error ; this improves credit assignment in networks with shortcut connections . benchmark results show that this can speed up learning significantly without adversely affecting the trained network ' s generalization ability .
we introduce a new fault - tolerant model of algorithmic learning using an equivalence oracle and an incomplete membership oracle , in which the answers to a random subset of the learner ' s membership queries may be missing . we demonstrate that , with high probability , it is still possible to learn monotone dnf formulas in polynomial time , provided that the fraction of missing answers is bounded by some constant less than one . even when half the membership queries are expected to yield no information , our algorithm will exactly identify m - term , n - variable monotone dnf formulas with an expected o ( mn #NUM# ) queries . the same task has been shown to require exponential time using equivalence queries alone . we extend the algorithm to handle some one - sided errors , and discuss several other possible error models . it is hoped that this work may lead to a better understanding of the power of membership queries and the effects of faulty teachers on query models of concept learning .
one method for making analogies is to access and instantiate abstract domain principles , and one method for acquiring knowledge of abstract principles is to discover them from experience . we view generalization over experiences in the absence of any prior knowledge of the target principle as the task of hypothesis formation , a subtask of discovery . also , we view the use of the hypothesized principles for analogical design as the task of hypothesis testing , another subtask of discovery . in this paper , we focus on discovery of physical principles by generalization over design experiences in the domain of physical devices . some important issues in generalization from experiences are what to generalize from an experience , how far to generalize , and what methods to use . we represent a reasoner ' s comprehension of specific designs in the form of structure - behavior - function ( sbf ) models . an sbf model provides a functional and causal explanation of the working of a device . we represent domain principles as device - independent behavior - function ( bf ) models . we show that ( i ) the function of a device determines what to generalize from its sbf model , ( ii ) the sbf model itself suggests how far to generalize , and ( iii ) the typology of functions indicates what method to use .
the power of the case - based method comes from the ability to retrieve the " right " case when a new problem is specified . this implies that learning the " right " indices to a case before storing it for potential reuse is crucial for the success of the method . a hierarchical organization of the case memory raises two distinct but related issues in index learning learning the indexing vocabulary , and learning the right level of generalization . in this paper we show how the use of structure - behavior - function ( sbf ) models constrains index learning in the context of experience - based design of physical devices . the sbf model of a design provides the functional and causal explanation of how the structure of the design delivers its function . we describe how the sbf model of a design , together with a specification of the task for which the design case might be reused , provides the vocabulary for indexing the design case in memory . we also discuss how the prior design experiences stored in case - memory help to determine the level of index generalization . the kritik #NUM# system implements and evaluates the model - based method for learning indices to design cases .
the bayesian approach to comparing models involves calculating the posterior probability of each plausible model . for high - dimensional contingency tables , the set of plausible models is very large . we focus attention on reversible jump markov chain monte carlo ( green , #NUM# ) and develop strategies for calculating posterior probabilities of hierarchical , graphical or decomposable log - linear models . even for tables of moderate size , these sets of models may be very large . the choice of suitable prior distributions for model parameters is also discussed in detail , and two examples are presented . for the first example , a #NUM# fi #NUM# fi #NUM# table , the model probabilities calculated using our reversible jump approach are compared with model probabilities calculated exactly or by using an alternative approximation . the second example is a #NUM# #NUM# contingency table for which exact methods are infeasible , due to the large number of possible models .
in addition to learning new knowledge , a system must be able to learn when the knowledge is likely to be applicable . an index is a piece of information which , when identified in a given situation , triggers the relevant piece of knowledge ( or schema ) in the system ' s memory . we discuss the issue of how indices may be learned automatically in the context of a story understanding task , and present a program that can learn new indices for existing explanatory schemas . we discuss two methods using which the system can identify the relevant schema even if the input does not directly match an existing index , and learn a new index to allow it to retrieve this schema more efficiently in the future .
this paper introduces a hybrid learning methodology that integrates genetic algorithms ( gas ) and decision tree learning ( id #NUM# ) in order to evolve optimal subsets of discriminatory features for robust pattern classification . a ga is used to search the space of all possible subsets of a large set of candidate discrimination features . for a given feature subset , id #NUM# is invoked to produce a decision tree . the classification performance of the decision tree on unseen data is used as a measure of fitness for the given feature set , which , in turn , is used by the ga to evolve better feature sets . this ga - id #NUM# process iterates until a feature subset is found with satisfactory classification performance . experimental results are presented which illustrate the feasibility of our approach on difficult problems involving recognizing visual concepts in satellite and facial image data . the results also show improved classification performance and reduced description complexity when compared against standard methods for feature selection .
this paper presents a neural network architecture that can manage structured data and refine knowledge bases expressed in a first order logic language . the presented framework is well suited to classification problems in which concept de scriptions depend upon numerical features of the data . in fact , the main goal of the neural architecture is that of refining the numerical part of the knowledge base , without changing its structure . in particular , we discuss a method to translate a set of classification rules into neural computation units . here , we focus our attention on the translation method and on algorithms to refine network weights on struc tured data . the classification theory to be refined can be manually handcrafted or automatically acquired by a symbolic relational learning system able to deal with numerical features . as a matter of fact , the primary goal is to bring into a neural network architecture the capability of dealing with structured data of unrestricted size , by allowing to dynamically bind the classification rules to different items occur ring in the input data . an extensive experimentation on a challenging artificial case study shows that the network converges quite fastly and generalizes much better than propositional learners on an equivalent task definition .
the evolving population of neural nets contains information not only in terms of genes , but also in the collection of behaviors of the population members . such information can be thought of as a kind of culture of the population . two ways of exploiting that culture are explored in this paper ( #NUM# ) culling overlarge litters generate a large number of offspring with different crossovers , quickly evaluate them by comparing their performance to the population , and throw away those that appear poor . ( #NUM# ) teaching use backpropagation to train offspring toward the performance of the population . both techniques result in faster , more effective neuro - evolution , and they can be effectively combined , as is demonstrated on the inverted pendulum problem . additional methods of cultural exploitation are possible and will be studied in future work . these results suggest that cultural exploitation is a powerful idea that allows leveraging several aspects of the genetic algorithm .
this paper describes the structure - mapping engine ( sme ) , a program for studying analogical processing . sme has been built to explore gentner ' s structure - mapping theory of analogy , and provides a " tool kit " for constructing matching algorithms consistent with this theory . its flexibility enhances cognitive simulation studies by simplifying experimentation . furthermore , sme is very efficient , making it a useful component in machine learning systems as well . we review the structure - mapping theory and describe the design of the engine . we analyze the complexity of the algorithm , and demonstrate that most of the steps are polynomial , typically bounded by o ( n #NUM# ) . next we demonstrate some examples of its operation taken from our cognitive simulation studies and work in machine learning . finally , we compare sme to other analogy programs and discuss several areas for future work .
we investigate some aspects of cognition involved in invention , more precisely in the invention of the telephone by alexander graham bell . we propose the use of the structure - behavior - function ( sbf ) language for the representation of invention knowledge ; we claim that because sbf has been shown to support a wide range of reasoning about physical devices , it constitutes a plausible account of how an inventor might represent knowledge of an invention . we further propose the use of the act - r architecture for the implementation of this model . act - r has been shown to very precisely model a wide range of human cognition . we draw upon the architecture for execution of productions and matching of declarative knowledge through spreading activation . thus we present a model which combines the well - established cognitive validity of act - r with the powerful , specialized model - based reasoning methods facilitated by sbf .
in on - line character recognition we can observe two kinds of intra - class variations : small geometric deformations and completely different writing styles . we propose a new approach to deal with these problems by defining an extension of tangent distance [ #NUM# ] , well known in off - line character recognition . the system has been implemented with a k - nearest neighbor classifier and a so called diabolo classifier [ #NUM# ] respectively . both classifiers are invariant under transformations like rotation , scale or slope and can deal with variations in stroke order and writing direction . results are presented for our digit database with more than #NUM# writers .
exact boltzmann learning can be done in certain restricted networks by the technique of decimation . we have enlarged the set of dec - imatable boltzmann machines by introducing a new and more general decimation rule . we have compared solutions of a probability density estimation problem with decimatable boltzmann machines to the results obtained by gibbs sampling in unrestricted ( non - decimatable )
the majority of results in computational learning theory are concerned with concept learning , i . e . with the special case of function learning for classes of functions with range f #NUM# ; #NUM# g . much less is known about the theory of learning functions with a larger range such as in or ir . in particular relatively few results exist about the general structure of common models for function learning , and there are only very few nontrivial function classes for which positive learning results have been exhibited in any of these models . we introduce in this paper the notion of a binary branching adversary tree for function learning , which allows us to give a somewhat surprising equivalent characterization of the optimal learning cost for learning a class of real - valued functions ( in terms of a max - min definition which does not involve any " learning " model ) . another general structural result of this paper relates the cost for learning a union of function classes to the learning costs for the individual function classes . furthermore , we exhibit an efficient learning algorithm for learning convex piecewise linear functions from ir d into ir . previously , the class of linear functions from ir d into ir was the only class of functions with multi - dimensional domain that was known to be learnable within the rigorous framework of a formal model for on - line learning . finally we give a sufficient condition for an arbitrary class f of functions from ir into ir that allows us to learn the class of all functions that can be written as the pointwise maximum of k functions from f . this allows us to exhibit a number of further nontrivial classes of functions from ir into ir for which there exist efficient learning algorithms .
although they are applicable to a wide array of problems , and have demonstrated good performance on a number of difficult , real - world tasks , neural networks are not usually applied to problems in which comprehensibility of the acquired concepts is important . the concept representations formed by neural networks are hard to understand because they typically involve distributed , nonlinear relationships encoded by a large number of real - valued parameters . to address this limitation , we have been developing algorithms for extracting " symbolic " concept representations from trained neural networks . we first discuss why it is important to be able to understand the concept representations formed by neural networks . we then briefly describe our approach and discuss a number of issues pertaining to comprehensibility that have arisen in our work . finally , we discuss choices that we have made in our research to date , and open research issues that we have not yet addressed .
one view of computational learning theory is that of a learner acquiring the knowledge of a teacher . we introduce a formal model of learning capturing the idea that teachers may have gaps in their knowledge . in particular , we consider learning from a teacher who labels examples " + " ( a positive instance of the concept being learned ) , " " ( a negative instance of the concept being learned ) , and " ? " ( an instance with unknown classification ) , in such a way that knowledge of the concept class and all the positive and negative examples is not sufficient to determine the labelling of any of the examples labelled with " ? " . the goal of the learner is not to compensate for the ignorance of the teacher by attempting to infer " + " or " " labels for the examples labelled with " ? " , but is rather to learn ( an approximation to ) the ternary labelling presented by the teacher . thus , the goal of the learner is still to acquire the knowledge of the teacher , but now the learner must also identify the gaps . this is the notion of learning from a consistently ignorant teacher . we present general results describing when known learning algorithms can be used to obtain algorithms that learn from a consistently ignorant teacher . we investigate the learnability of a variety of concept classes in this model , including monomials , monotone dnf formulas , horn sentences , decision trees , dfas , and axis - parallel boxes in euclidean space , among others . both learnability and non - learnability results are presented .
in bagging [ bre #NUM# a ] one uses bootstrap replicates of the training set [ efr #NUM# , et #NUM# ] to try to improve a learning algorithm ' s performance . the computational requirements for estimating the resultant generalization error on a test set by means of cross - validation are often prohibitive ; for leave - one - out cross - validation one needs to train the underlying algorithm on the order of m - times , where m is the size of the training set and is the number of replicates . this paper presents several techniques for exploiting the bias - variance decomposition [ gbd #NUM# , wol #NUM# ] to estimate the generalization error of a bagged learning algorithm without invoking yet more training of the underlying learning algorithm . the best of our estimators exploits stacking [ wol #NUM# ] . in a set of experiments reported here , it was found to be more accurate than both the alternative cross - validation - based estimator of the bagged algorithm ' s error and the cross - validation - based estimator of the underlying algorithm ' s error . this improvement was particularly pronounced for small test sets . this suggests a novel justification for using bagging | im proved estimation of generalization error .
this paper presents an algorithm for the discovery of building blocks in genetic programming ( gp ) called adaptive representation through learning ( arl ) . the central idea of arl is the adaptation of the problem representation , by extending the set of terminals and functions with a set of evolvable subroutines . the set of subroutines extracts common knowledge emerging during the evolutionary process and acquires the necessary structure for solving the problem . arl supports subroutine creation and deletion . subroutine creation or discovery is performed automatically based on the differential parent - offspring fitness and block activation . subroutine deletion relies on a utility measure similar to schema fitness over a window of past generations . the technique described is tested on the problem of controlling an agent in a dynamic and non - deterministic environment . the automatic discovery of subroutines can help scale up the gp technique to complex problems .
in this paper we describe a new technique for exactly identifying certain classes of read - once boolean formulas . the method is based on sampling the input - output behavior of the target formula on a probability distribution which is determined by the fixed point of the formula ' s amplification function ( defined as the probability that a #NUM# is output by the formula when each input bit is #NUM# independently with probability p ) . by performing various statistical tests on easily sampled variants of the fixed - point distribution , we are able to efficiently infer all structural information about any logarithmic - depth formula ( with high probability ) . we apply our results to prove the existence of short universal identification sequences for large classes of formulas . we also describe extensions of our algorithms to handle high rates of noise , and to learn formulas of unbounded depth in valiant ' s model with respect to specific distributions .
we consider the problem of learning k - term dnf formulas using equivalence queries and incomplete membership queries as defined by angluin and slonim . we demonstrate that this model can be applied to non - monotone classes . namely , we describe a polynomial - time algorithm that exactly identifies a k - term dnf formula with a k - term dnf hypothesis using incomplete membership queries and equivalence queries from the class of dnf formulas .
most artificial neural networks ( anns ) have a fixed topology during learning , and often suffer from a number of shortcomings as a result . variations of anns that use dynamic topologies have shown ability to overcome many of these problems . this paper introduces location - independent transformations ( lits ) as a general strategy for implementing distributed feedforward networks that use dynamic topologies ( dynamic anns ) efficiently in parallel hardware . a lit creates a set of location - independent nodes , where each node computes its part of the network output independent of other nodes , using local information . this type of transformation allows efficient support for adding and deleting nodes dynamically during learning . in particular , this paper presents an lit for dynamic backpropagation networks with a single hidden layer . the complexity of both learning and execution algorithms is o ( n + p + logm ) for a single pattern , where nis the number of inputs , p is the number of outputs , and m is the number of hidden nodes in the original network .
we present a new method for obtaining local error bars for nonlinear regression , i . e . , estimates of the confidence in predicted values that depend on the input . we approach this problem by applying a maximum - likelihood framework to an assumed distribution of errors . we demonstrate our method first on computer - generated data with locally varying , normally distributed target noise . we then apply it to laser data from the santa fe time series competition where the underlying system noise is known quantization error and the error bars give local estimates of model misspecification . in both cases , the method also provides a weighted - regression effect that improves generalization performance .
introspective reasoning about a system ' s own reasoning processes can form the basis for learning to refine those reasoning processes . the robbie #NUM# system uses introspective reasoning to monitor the retrieval process of a case - based planner to detect retrieval of inappropriate cases . when retrieval problems are detected , the source of the problems is explained and the explanations are used to determine new indices to use during future case retrieval . the goal of robbie ' s learning is to increase its ability to focus retrieval on relevant cases , with the aim of simultaneously decreasing the number of candidates to consider and increasing the likelihood that the system will be able to successfully adapt the retrieved cases to fit the current situation . we evaluate the benefits of the approach in light of empirical results examining the effects of index learning in the
inductive learning algorithms try to obtain the knowledge of a system from a set of examples . one of the most difficult problems in machine learning consists in getting the structure of this knowledge . we propose an algorithm able to manage with fuzzy information and able to learn the structure of the rules that represent the system . the algorithm gives a reasonable small set of fuzzy rules that represent the original set of examples .
since we consider theory refinement ( tr ) as a possible key concept for a methodologically clear view of knowledge - base maintenance , we try to give a structured overview about the actual state - of - the - art in tr . this overview is arranged along the description of tr as a search problem . we explain the basic approach , show the variety of existing systems and try to give some hints about the direction future research should go .
the problem of protecting computer systems can be viewed generally as the problem of learning to distinguish self from other . we describe a method for change detection which is based on the generation of t cells in the immune system . mathematical analysis reveals computational costs of the system , and preliminary experiments illustrate how the method might be applied to the problem of computer viruses .
markov chain monte carlo ( mcmc ) methods , as introduced by gelfand and smith ( #NUM# ) , provide a simulation based strategy for statistical inference . the application fields related to these methods , as well as theoretical convergence properties , have been intensively studied in the recent literature . however , many improvements are still expected to provide workable and theoretically well - grounded solutions to the problem of monitoring the convergence of actual outputs from mcmc algorithms ( i . e . the convergence assessment problem ) . in this paper , we introduce and discuss a methodology based on the central limit theorem for markov chains to assess convergence of mcmc algorithms . instead of searching for approximate stationarity , we primarily intend to control the precision of estimates of the invariant probability measure , or of integrals of functions with respect to this measure , through confidence regions based on normal approximation . the first proposed control method tests the normality hypothesis for normalized averages of functions of the markov chain over independent parallel chains . this normality control provides good guarantees that the whole state space has been explored , even in multimodal situations . it can lead to automated stopping rules . a second tool connected with the normality control is based on graphical monitoring of the stabilization of the variance after n iterations near the limiting variance appearing in the clt . both methods require no knowledge of the sampler driving the chain . in this paper , we mainly focus on finite state markov chains , since this setting allows us to derive consistent estimates of both the limiting variance and the variance after n iterations . heuristic procedures based on berry - esseen bounds are investigated . an extension to the continuous case is also proposed . numerical simulations illustrating the performance of these methods are given for several examples : a finite chain with multimodal invariant probability , a finite state random walk for which the theoretical rate of convergence to stationarity is known , and a continuous state chain with multimodal invariant probability issued from a gibbs sampler .
this paper explores the application of temporal difference ( td ) learning ( sutton , #NUM# ) to forecasting the behavior of dynamical systems with real - valued outputs ( as opposed to game - like situations ) . the performance of td learning in comparison to standard supervised learning depends on the amount of noise present in the data . in this paper , we use a deterministic chaotic time series from a low - noise laser . for the task of direct five - step ahead predictions , our experiments show that standard supervised learning is better than td learning . the td algorithm can be viewed as linking adjacent predictions . a similar effect can be obtained by sharing the internal representation in the network . we thus compare two architectures for both paradigms : the first architecture ( separate hidden units ) consists of individual networks for each of the five direct multi - step prediction tasks , the second ( shared hidden units ) has a single ( larger ) hidden layer that finds a representation from which all five predictions for the next five steps are generated . for this data set we do not find any significant difference between the two architectures .
we describe the " wake - sleep " algorithm that allows a multilayer , unsupervised , neural network to build a hierarchy of representations of sensory input . the network has bottom - up " recognition " connections that are used to convert sensory input into underlying representations . unlike most artificial neural networks , it also has top - down " generative " connections that can be used to reconstruct the sensory input from the representations . in the " wake " phase of the learning algorithm , the network is driven by the bottom - up recognition connections and the top - down generative connections are trained to be better at reconstructing the sensory input from the representation chosen by the recognition process . in the " sleep " phase , the network is driven top - down by the generative connections to produce a fantasized representation and a fantasized sensory input . the recognition connections are then trained to be better at recovering the fantasized representation from the fantasized sensory input . in both phases , the synaptic learning rule is simple and local . the combined effect of the two phases is to create representations of the sensory input that are efficient in the following sense : on average , it takes more bits to describe each sensory input vector directly than to first describe the representation of the sensory input chosen by the recognition process and then describe the difference between the sensory input and its reconstruction from the chosen representation .
bayesian inference begins with a prior distribution for model parameters that is meant to capture prior beliefs about the relationship being modeled . for multilayer perceptron networks , where the parameters are the connection weights , the prior lacks any direct meaning | what matters is the prior over functions computed by the network that is implied by this prior over weights . in this paper , i show that priors over weights can be defined in such a way that the corresponding priors over functions reach reasonable limits as the number of hidden units in the network goes to infinity . when using such priors , there is thus no need to limit the size of the network in order to avoid " overfitting " . the infinite network limit also provides insight into the properties of different priors . a gaussian prior for hidden - to - output weights results in a gaussian process prior for functions , which can be smooth , brownian , or fractional brownian , depending on the hidden unit activation function and the prior for input - to - hidden weights . quite different effects can be obtained using priors based on non - gaussian stable distributions . in networks with more than one hidden layer , a combination of gaussian and non - gaussian priors appears most interesting .
we present new algorithms for reinforcement learning and prove that they have polynomial bounds on the resources required to achieve near - optimal return in general markov decision processes . after observing that the number of actions required to approach the optimal return is lower bounded by the mixing time t of the optimal policy ( in the undiscounted case ) or by the horizon time t ( in the discounted case ) , we then give algorithms requiring a number of actions and total computation time that are only polynomial in t and the number of states , for both the undiscounted and discounted cases . an interesting aspect of our algorithms is their explicit handling of the exploration - exploitation trade - off . these are the first results in the reinforcement learning literature giving algorithms that provably converge to near - optimal performance in polynomial time for general markov decision processes .
a basic premise of case - based reasoning ( cbr ) is that it involves reasoning from cases , which are representations of real episodes , rather than from rules , which are facts and if then structures with no stated connection to any real episodes . in fact , most cbr systems do not reason directly from cases | rather they reason from abstractions or simplifications of cases . in this paper , we argue for " pure " case - based reasoning , i . e . , reasoning from representations that are both concrete and reasonably complete . we claim that working from representations that satisfy these criteria we illustrate our argument with examples from three previous systems , chef , swale , and hypo , as well as from cookie , a cbr system being developed by the first author .
a straightforward approach to the curse of dimensionality in reinforcement learning and dynamic programming is to replace the lookup table with a generalizing function approximator such as a neural net . although this has been successful in the domain of backgammon , there is no guarantee of convergence . in this paper , we show that the combination of dynamic programming and function approximation is not robust , and in even very benign cases , may produce an entirely wrong policy . we then introduce grow - support , a new algorithm which is safe from divergence yet can still reap the benefits of successful generalization .
an exact model of a simple genetic algorithm is developed for permutation based representations . permutation based representations are used for scheduling problems and combinatorial problems such as the traveling salesman problem . a remapping function is developed to remap the model to all permutations in the search space . the mixing matrices for various permutation based operators are also developed .
test functions are commonly used to evaluate the effectiveness of different search algorithms . however , the results of evaluation are as dependent on the test problems as they are on the algorithms that are the subject of comparison . unfortunately , developing a test suite for evaluating competing search algorithms is difficult without clearly defined evaluation goals . in this paper we discuss some basic principles that can be used to develop test suites and we examine the role of test suites as they have been used to evaluate evolutionary search algorithms . current test suites include functions that are easily solved by simple search methods such as greedy hill - climbers . some test functions also have undesirable characteristics that are exaggerated as the dimensionality of the search space is increased . new methods are examined for constructing functions with different degrees of nonlinearity , where the interactions and the cost of evaluation scale with respect to the dimensionality of the search space .
source separation arises in a surprising number of signal processing applications , from speech recognition to eeg analysis . in the square linear blind source separation problem without time delays , one must find an unmixing matrix which can detangle the result of mixing n unknown independent sources through an unknown n fi n mixing matrix . the recently introduced ica blind source separation algorithm ( baram and roth #NUM# ; bell and sejnowski #NUM# ) is a powerful and surprisingly simple technique for solving this problem . ica is all the more remarkable for performing so well despite making absolutely no use of the temporal structure of its input ! this paper presents a new algorithm , contextual ica , which derives from a maximum likelihood density estimation formulation of the problem . cica can incorporate arbitrarily complex adaptive history - sensitive source models , and thereby make use of the temporal structure of its input . this allows it to separate in a number of situations where standard ica cannot , including sources with low kurtosis , colored gaussian sources , and sources which have gaussian histograms . since ica is a special case of cica , the mle derivation provides as a corollary a rigorous derivation of classic ica .
we inv estigate the applicability of an adaptive neural network to problems with time - dependent input by demonstrating that a deterministic parser for natural language inputs of significant syntactic complexity can be developed using recurrent connectionist architectures . the traditional stacking mechanism , known to be necessary for proper treatment of context - free languages in symbolic systems , is absent from the design , having been subsumed by recurrency in the network .
in this paper we address the problem of constructing reliable neural - net implementations , given the assumption that any particular implementation will not be totally correct . the approach taken in this paper is to organize the inevitable errors so as to minimize their impact in the context of a multiversion system . | i . e . the system functionality is reproduced in multiple versions which together will constitute the neural - net system . the unique characteristics of neural computing are exploited in order to engineer reliable systems in the form of diverse , multiversion systems which are used together with a ` decision strategy ' ( such as majority vote ) . theoretical notions of " methodological diversity " contributing to the improvement of system performance are implemented and tested . an important aspect of the engineering of an optimal system is to overproduce the components and then choose an optimal subset . three general techniques for choosing final system components are implemented and evaluated . several different approaches to the effective engineering of complex multiversion systems designs are realized and evaluated to determine overall reliability as well as reliability of the overall system in comparison to the lesser reliability of component substructures .
littlewood and miller [ #NUM# ] present a statistical framework for dealing with coincident failures in multiversion software systems . they develop a theoretical model that holds the promise of high system reliability through the use of multiple , diverse sets of alternative versions . in this paper we adapt their framework to investigate the feasibility of exploiting the diversity observable in multiple populations of neural networks developed using diverse methodologies . we evaluate the generalisation improvements achieved by a range of methodologically diverse network generation processes . we attempt to order the constituent methodological features with respect to their potential for use in the engineering of useful diversity . we also define and explore the use of relative measures of the diversity between version sets as a guide to the potential for exploiting inter - set diversity .
during the development and the life - cycle of knowledge - based systems the requirements on the system and the knowledge in the system will change . one of the types of knowledge affected by changing requirements is control - knowledge , which prescribes the ordering of problem - solving steps . machine - learning can aid developers of knowledge - based systems in adapting their systems to changing requirements . a number of machine - learning techniques for learning control - knowledge have been applied to problem - solvers ( prodigy - ebl , lex ) . in knowledge engineering , the focus has shifted to the construction of knowledge - level models of problem - solving instead of directly constructing a knowledge - based system in a problem - solver . in this paper we describe work in progress on how to apply machine learning techniques to the kads model of expertise .
results of the abbadingo one dfa learning abstract . this paper first describes the structure and results of the abbadingo one dfa learning competition . the competition was designed to encourage work on algorithms that scale wellboth to larger dfas and to sparser training data . we then describe and discuss the winning algorithm of rodney price , which orders state merges according to the amount of evidence in their favor . a second winning algorithm , of hugues juille , will be described in a separate paper .
in this work , we present a new bottom - up algorithm for decision tree pruning that is very efficient ( requiring only a single pass through the given tree ) , and prove a strong performance guarantee for the generalization error of the resulting pruned tree . we work in the typical setting in which the given tree t may have been derived from the given training sample s , and thus may badly overfit s . in this setting , we give bounds on the amount of additional generalization error that our pruning suffers compared to the optimal pruning of t . more generally , our results show that if there is a pruning of t with small error , and whose size is small compared to jsj , then our algorithm will find a pruning whose error is not much larger . this style of result has been called an index of resolvability result by barron and cover in the context of density estimation . a novel feature of our algorithm is its locality | the decision to prune a subtree is based entirely on properties of that subtree and the sample reaching it . to analyze our algorithm , we develop tools of local uniform convergence , a generalization of the standard notion that may prove useful in other settings .
we investigate the application of support vector machines ( svms ) in computer vision . svm is a learning technique developed by v . vapnik and his team ( at & t bell labs . ) that can be seen as a new method for training polynomial , neural network , or radial basis functions classifiers . the decision surfaces are found by solving a linearly constrained quadratic programming problem . this optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points . we present a decomposition algorithm that guarantees global optimality , and can be used to train svm ' s over very large data sets . the main idea behind the decomposition is the iterative solution of sub - problems and the evaluation of optimality conditions which are used both to generate improved iterative values , and also establish the stopping criteria for the algorithm . we present experimental results of our implementation of svm , and demonstrate the feasibility of our approach on a face detection problem that involves a data set of #NUM# , #NUM# data points .
one of the open problems listed in [ rivest and schapire , #NUM# ] is whether and how that the copies of l fl in their algorithm can be combined into one for better performance . this paper describes an algorithm called d fl that does that combination . the idea is to represent the states of the learned model using observable symbols as well as hidden symbols that are constructed during learning . these hidden symbols are created to reflect the distinct behaviors of the model states . the distinct behaviors are represented as local distinguishing experiments ( ldes ) ( not to be confused with global distinguishing sequences ) , and these ldes are created when the learner ' s prediction mismatches the actual observation from the unknown machine . to synchronize the model with the environment , these ldes can also be concatenated to form a homing sequence . it can be shown that d fl can learn , with probability #NUM# , a model that is an * - approximation of the unknown machine , in a number of actions polynomial in the size of the environment and
one of the issues in evolutionary algorithms ( eas ) is the relative importance of two search operators : mutation and crossover . genetic algorithms ( gas ) and genetic programming ( gp ) stress the role of crossover , while evolutionary programming ( ep ) and evolution strategies ( ess ) stress the role of mutation . the existence of many different forms of crossover further complicates the issue . despite theoretical analysis , it appears difficult to decide a priori which form of crossover to use , or even if crossover should be used at all . one possible solution to this difficulty is to have the ea be self - adaptive , i . e . , to have the ea dynamically modify which forms of crossover to use and how often to use them , as it solves a problem . this paper describes an adaptive mechanism for controlling the use of crossover in an ea and explores the behavior of this mechanism in a number of different situations . an improvement to the adaptive mechanism is then presented . surprisingly this improvement can also be used to enhance performance in a non - adaptive ea .
graphical techniques for modeling the dependencies of random variables have been explored in a variety of different areas including statistics , statistical physics , artificial intelligence , speech recognition , image processing , and genetics . formalisms for manipulating these models have been developed relatively independently in these research communities . in this paper we explore hidden markov models ( hmms ) and related structures within the general framework of probabilistic independence networks ( pins ) . the paper contains a self - contained review of the basic principles of pins . it is shown that the well - known forward - backward ( f - b ) and viterbi algorithms for hmms are special cases of more general inference algorithms for arbitrary pins . furthermore , the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for hmm practitioners who wish to explore a richer class of hmm structures . examples of relatively complex models to handle sensor fusion and coarticulation in speech recognition are introduced and treated within the graphical model framework to illustrate the advantages of the general approach .
perceptron learning of randomly labeled patterns is analyzed using a gibbs distribution on the set of realizable labelings of the patterns . the entropy of this distribution is an extension of the vapnik - chervonenkis ( vc ) entropy , reducing to it exactly in the limit of infinite temperature . the close relationship between the vc and gardner entropies can be seen within the replica formalism . there has been recent progress towards understanding the relationship between the statistical physics and vapnik - chervonenkis ( vc ) approaches to learning theory [ #NUM# , #NUM# , #NUM# , #NUM# ] . the two approaches can be unified in a statistical mechanics based on the vc entropy . this paper treats the case of learning randomly labeled patterns , or the capacity problem , and extends some of the results of previous work [ #NUM# , #NUM# ] to finite temperature . as will be explained in a companion paper , this extension is important for treating the generalization problem , which occurs in the context of learning patterns labeled by a target rule . our general framework is illustrated for the simple perceptron sgn ( w x ) , which maps an n - dimensional real - valued input x to a #NUM# - valued output . given a sample x = ( x #NUM# ; : : : ; x m ) of inputs , the weight vector w determines a labeling l = ( l #NUM# ; : : : ; l m ) of the sample via l i = sgn ( w x i ) . the weight vector w defines a normal hyperplane that separates the positive from the negative examples . the training error of a labeling l with respect to a reference labeling l #NUM# is defined by #NUM# m x #NUM# l i l #NUM# and is just the fraction of different labels in the two labelings . we consider the case in which the reference labeling is chosen at random , and address the issue of
this research aims to demonstrate that a solution for artificial ant problem [ #NUM# ] is very likely to be non - general and relying on the specific characteristics of the santa fe trail . it then presents a consistent method which promotes producing general solutions . using the concepts of training and testing from machine learning research , the method can be useful in producing general behaviours for simulation environments .
dynamic bayesian networks ( dbns ) are a useful tool for representing complex stochastic processes . recent developments in inference and learning in dbns allow their use in real - world applications . in this paper , we apply dbns to the problem of speech recognition . the factored state representation enabled by dbns allows us to explicitly represent long - term articulatory and acoustic context in addition to the phonetic - state information maintained by hidden markov models ( hmms ) . furthermore , it enables us to model the short - term correlations among multiple observation streams within single time - frames . given a dbn structure capable of representing these long - and short - term correlations , we applied the em algorithm to learn models with up to #NUM# , #NUM# parameters . the use of structured dbn models decreased the error rate by #NUM# to #NUM# % on a large - vocabulary isolated - word recognition task , compared to a discrete hmm ; it also improved significantly on other published results for the same task . this is the first successful application of dbns to a large - scale speech recognition problem . investigation of the learned models indicates that the hidden state variables are strongly correlated with acoustic properties of the speech signal .
we describe and evaluate multi - network connectionist systems composed of " expert " networks . by preprocessing training data with a competitive learning network , the system automatically organizes the process of decomposition into expert subtasks . using several different types of challenging problem , we assess this approach | the degree to which the automatically generated experts really are specialists on a predictable subset of the overall task , and a comparison of such decompositions with equivalent single - networks . in addition , we assess the utility of this approach alongside , and in competition to , non - expert multiversion systems . previously developed measures of ` diversity ' for such systems are also applied to provide a quantitative assessment of the degree of specialization obtained in an expert - net ensemble . we show that on both types of problem , abstract well - defined and data - defined , the automatic decomposition does produce an effective set of specialist networks which together can support a high level of performance . curiously , the study does not provide any support for a differential of effectiveness within the two classes of problem : continuous , homogeneous functions and discrete , discontinuous functions .
let us present briefly the learning problem we will address in this chapter and the following . the ultimate goal is the modelling of a mapping f : x #NUM# ! y from multidimensional input x to output y . the output can be multi - dimensional , but we will mostly address situations where it is a one dimensional real value . furthermore , we should take into account the fact that we scarcely ever observe the actual true mapping y = f ( x ) . this is due to perturbations such as e . g . observational noise . we will rather have a joint probability p ( x ; y ) . we expect this probability to be peaked for values of x and y corresponding to the mapping . we focus on automatic learning by example . a set d = of data sampled from the joint distribution p ( x ; y ) = p ( yjx ) p ( x ) is collected . with the help of this set , we try to identify a model of the data , parameterised by a set of #NUM# . #NUM# learning and optimisation the fit of the model to the system in a given point x is measured using a criterion representing the distance from the model prediction b y to the system , e ( y ; f w ( x ) ) . this is the local risk . the performance of the model is measured by the expected this quantity represents the ability to yield good performance for all the possible situations ( i . e . ( x ; y ) pairs ) and is thus called generalisation error . the optimal set #NUM# parameters w : f w : x #NUM# ! b y .
practical optimization problems such as job - shop scheduling often involve optimization criteria that change over time . repair - based frameworks have been identified as flexible computational paradigms for difficult combinatorial optimization problems . since the control problem of repair - based optimization is severe , reinforcement learning ( rl ) techniques can be potentially helpful . however , some of the fundamental assumptions made by traditional rl algorithms are not valid for repair - based optimization . case - based reasoning ( cbr ) compensates for some of the limitations of traditional rl approaches . in this paper , we present a case - based reasoning rl approach , implemented in the c a b i n s system , for repair - based optimization . we chose job - shop scheduling as the testbed for our approach . our experimental results show that c a b i n s is able to effectively solve problems with changing optimization criteria which are not known to the system and only exist implicitly in a extensional manner in the case base .
we have introduced earlier the use of regularisation in the learning procedure . it should now be understood that regularisation is most often a necessity to increase the quality of the results . even when the unregularised solution is acceptable , it is likely that some regularisation will produce an improvement in performance . there does not exist any method giving directly the best value for the regularisation parameter ~ , even in the linear case . the topic of this chapter is thus to propose some methods to estimate the best value . the best ~ being the one that leads to the smallest generalisation error , the methods presented and compared here propose estimators of the generalisation error . this estimation can then be used to approximate the best regularisation level . in sections #NUM# . #NUM# to #NUM# . #NUM# we present validation - based techniques . they estimate the generalisation error on the basis of some extra data . in sections #NUM# . #NUM# to #NUM# . #NUM# , we deal with algebraic estimates of this error , that do not use any extra data , but rely on a number of assumptions . the contribution of this chapter is to present all these techniques and analyse them on the same ground . we also present some short derivations clarifying the links between different estimators of generalisation error , as well as a comparison between them . during the course of this chapter , the error will be the quadratic difference . for the validation - based methods , it is possible to consider any kind of error without modification of the method . on the other hand , the algebraic estimates are specific to the quadratic cost . adapting them to another cost function would require to derive new expressions for the estimators .
evolutionary approaches have been advocated to automate robot design . some research work has shown the success of evolving controllers for the robots by genetic approaches . as we can observe , however , not only the controller but also the robot body itself can affect the behavior of the robot in a robot system . in this paper , we develop a hybrid gp / ga approach to evolve both controllers and robot bodies to achieve behavior - specified tasks . in order to assess the performance of the developed approach , it is used to evolve a simulated agent , with its own controller and body , to do obstacle avoidance in the simulated environment . experimental results show the promise of this work . in addition , the importance of co - evolving controllers and robot bodies is analyzed and discussed in this paper .
when m = #NUM# ( no delays ) , we set a #NUM# ( ffi ) = f ( j ; k ) ; j #NUM# = kg , such that p m ( * jffi ) depends only on * . the estimated probabilities above become quite noisy when the number of elements in set a m and b m are small . for this reason , we estimate the standard deviation of p m ( * jffi ) . notice that this estimate is the empirical average of a binomial variable ( either a given couple satisfied the conditions on ffi and * , or it does not ) . the standard deviation is then estimated easily by : generally speaking , p m ( * jffi ) increases with * ( laxer output test ) , and when ffi approaches #NUM# ( stricter input condition ) . let us now define by p m ( * ) the maximum over ffi of p m ( * jffi ) : p m ( * ) = max ffi & gt ; #NUM# p m ( * jffi ) . the dependability index is defined as : p #NUM# ( * ) represents how much data passes the continuity test when no input information is available . this dependability index measures how much of the remaining continuity information is associated with involving input i m . this index is then averaged over * with respect to the probability ( #NUM# p #NUM# ( * ) ) : m ( * ) ( #NUM# p #NUM# ( * ) ) d * ( #NUM# . #NUM# ) it is clear that m ( * ) , and therefore its average , should be positive quantities . furthermore , if the system is deterministic , the dependability is zero after a certain number of inputs , so the sum of averages saturates . if the system is also noise - free , they sum up to #NUM# . for any m greater than the embedding dimension : refers to results obtained using this method .
we used genetic programming to evolve b o t h the topology and the sizing ( numerical values ) for each component of an analog electrical circuit that can correctly classify an incoming analog electrical signal into three categories . then , the r e p e r t o i r e o f s o u r c e s w a s dynamically changed by adding a new source during the run . the p a p e r d e s c r i b e s h o w t h e
an essential component of an intelligent agent is the ability to observe , encode , and use information about its environment . traditional approaches to genetic programming have focused on evolving functional or reactive programs with only a minimal use of state . this paper presents an approach for investigating the evolution of learning , planning , and memory using genetic programming . the approach uses a multi - phasic fitness environment that enforces the use of memory and allows fairly straightforward comprehension of the evolved representations . an illustrative problem of ' gold ' collection is used to demonstrate the usefulness of the approach . the results indicate that the approach can evolve programs that store simple representations of their environments and use these representations to produce simple plans .
interest in genetic algorithms is expanding rapidly . this paper reviews software environments for programming genetic algorithms ( ga s ) . as background , we initially preview genetic algorithms ' models and their programming . next we classify ga software environments into three main categories : application - oriented , algorithm - oriented and toolkits . for each category of ga programming environment we review their common features and present a case study of a leading environment .
neural network pruning methods on the level of individual network parameters ( e . g . connection weights ) can improve generalization . an open problem in the pruning methods known today ( obd , obs , autoprune , epsiprune ) is the selection of the number of parameters to be removed in each pruning step ( pruning strength ) . this paper presents a pruning method lprune that automatically adapts the pruning strength to the evolution of weights and loss of generalization during training . the method requires no algorithm parameter adjustment by the user . the results of extensive experimentation indicate that lprune is often superior to autoprune ( which is superior to obd ) on diagnosis tasks unless severe pruning early in the training process is required . results of statistical significance tests comparing autoprune to the new method lprune as well as to backpropagation with early stopping are given for #NUM# different problems .
the relative performance of different methods for classifier learning varies across domains . some recent instance based learning ( ibl ) methods , such as ib #NUM# - mvdm * #NUM# , use similarity measures based on conditional class probabilities . these probabilities are a key component of naive bayes methods . given this commonality of approach , it is of interest to consider how the differences between the two methods are linked to their relative performance in different domains . here we interpret naive bayes in an ibl like framework , identifying differences between naive bayes and ib #NUM# - mvdm * in this framework . experiments on variants of ib #NUM# - mvdm * that lie between it and naive bayes in the framework are conducted on sixteen domains . the results strongly suggest that the relative performance of naive bayes and ib #NUM# - mvdm * is linked to the extent to which each class can be satisfactorily represented by a single instance in the ibl framework . however , this is not the only factor that appears significant .
assert demonstrates how theory refinement techniques developed in machine learning can be used to ef fec - tively build student models for intelligent tutoring systems . this application is unique since it inverts the normal goal of theory refinement from correcting errors in a knowledge base to introducing them . a comprehensive experiment involving a lar ge number of students interacting with an automated tutor for teaching concepts in c + + programming was used to evaluate the approach . this experiment demonstrated the ability of theory refinement to generate more accurate student models than raw induction , as well as the ability of the resulting models to support individualized feedback that actually improves students subsequent performance . carr , b . and goldstein , i . ( #NUM# ) . overlays : a theory of modeling for computer aided instruction . t echnical report a . i . memo #NUM# , cambridge , ma : mit . sandberg , j . and barnard , y . ( #NUM# ) . education and technology : what do we know ? and where is ai ? artificial intelligence communications , #NUM# ( #NUM# ) : #NUM# - #NUM# .
the monitoring and control of any dynamic system depends crucially on the ability to reason about its current status and its future trajectory . in the case of a stochastic system , these tasks typically involve the use of a belief state | a probability distribution over the state of the process at a given point in time . unfortunately , the state spaces of complex processes are very large , making an explicit representation of a belief state intractable . even in dynamic bayesian networks ( dbns ) , where the process itself can be represented compactly , the representation of the belief state is intractable . we investigate the idea of utilizing a compact approximation to the true belief state , and analyze the conditions under which the errors due to the approximations taken over the lifetime of the process do not accumulate to make our answers completely irrelevant . we show that the error in a belief state contracts exponentially as the process evolves . thus , even with multiple approximations , the error in our process remains bounded indefinitely . we show how the additional structure of a dbn can be used to design our approximation scheme , improving its performance significantly . we demonstrate the applicability of our ideas in the context of a monitoring task , showing that orders of magnitude faster inference can be achieved with only a small degradation in accuracy .
although case - based reasoning ( cbr ) is a natural formulation for many problems , our previous work on cbr as applied to design made it apparent that there were elements of the cbr paradigm that prevented it from being more widely applied . at the same time , we were evaluating constraint satisfaction techniques for design , and found a commonality in motivation between repair - based constraint satisfaction problems ( csp ) and case adaptation . this led us to combine the two methodologies in order to gain the advantages of csp for case - based reasoning , allowing cbr to be more widely and flexibly applied . in combining the two methodologies , we found some unexpected synergy and commonality between the approaches . this paper describes the synergy and commonality that emerged as we combined case - based and constraint - based reasoning , and gives a brief overview of our continuing and future work on exploiting the emergent synergy when combining these reasoning modes .
a two - eye visual environment is used in training a network of bcm neurons . we study the effect of misalignment between the synaptic density functions from the two eyes , on the formation of orientation selectivity and ocular dominance in a lateral inhibition network . the visual environment we use is composed of natural images . we show that for the bcm rule a natural image environment with binocular cortical misalignment is sufficient for producing networks with orientation selective cells and ocular dominance columns . this work is an extension of our previous single cell misalignment model ( shouval et al . , #NUM# ) .
the von mises distribution is a maximum entropy distribution . it corresponds to the distribution of an angle of a compass needle in a uniform magnetic field of direction , , with concentration parameter , . the concentration parameter , , is the ratio of the field strength to the temperature of thermal fluctuations . previously , we obtained a bayesian estimator for the von mises distribution parameters using the information - theoretic minimum message length ( mml ) principle . here , we examine a variety of bayesian estimation techniques by examining the posterior distribution in both polar and cartesian co - ordinates . we compare the mml estimator with these fellow bayesian techniques , and a range of classical estimators . we find that the bayesian estimators outperform the classical estimators .
a neural - network ensemble is a very successful technique where the outputs of a set of separately trained neural network are combined to form one unified prediction . an effective ensemble should consist of a set of networks that are not only highly correct , but ones that make their errors on different parts of the input space as well ; however , most existing techniques only indirectly address the problem of creating such a set . we present an algorithm called addemup that uses genetic algorithms to explicitly search for a highly diverse set of accurate trained networks . addemup works by first creating an initial population , then uses genetic operators to continually create new networks , keeping the set of networks that are highly accurate while disagreeing with each other as much as possible . experiments on four real - world domains show that addemup is able to generate a set of trained networks that is more accurate than several existing ensemble approaches . experiments also show that addemup is able to effectively incorporate prior knowledge , if available , to improve the quality of its ensemble .
classifier induction algorithms differ on what inductive hypotheses they can represent , and on how they search their space of hypotheses . no classifier is better than another for all problems : they have selective superiority . this paper empirically compares six classifier induction algorithms on the diagnosis of equine colic and the prediction of its mortality . the classification is based on simultaneously analyzing sixteen features measured from a patient . the relative merits of the algorithms ( linear regression , decision trees , nearest neighbor classifiers , the model class selection system , logistic regression ( with and without feature selection ) , and neural nets ) are qualitatively discussed , and the generalization accuracies quantitatively analyzed .
using the multi - parent diagonal and scanning crossover in gas reproduction operators obtain an adjustable arity . hereby sexuality becomes a graded feature instead of a boolean one . our main objective is to relate the performance of gas to the extent of sexuality used for reproduction on less arbitrary functions then those reported in the current literature . we investigate ga behaviour on kauffman ' s nk - landscapes that allow for systematic characterization and user control of ruggedness of the fitness landscape . we test gas with a varying extent of sexuality , ranging from asexual to ' very sexual ' . our tests were performed on two types of nk - landscapes : landscapes with random and landscapes with nearest neighbour epistasis . for both landscape types we selected landscapes from a range of ruggednesses . the results confirm the superiority of ( very ) sexual recombination on mildly epistatic problems .
this paper discusses the unsupervised learning problem . an important part of the unsupervised learning problem is determining the number of constituent groups ( components or classes ) which best describes some data . we apply the minimum message length ( mml ) criterion to the unsupervised learning problem , modifying an earlier such mml application . we give an empirical comparison of criteria prominent in the literature for estimating the number of components in a data set . we conclude that the minimum message length criterion performs better than the alternatives on the data considered here for unsupervised learning tasks .
the minimum message length ( mml ) technique is applied to the problem of estimating the parameters of a multivariate gaussian model in which the correlation structure is modelled by a single common factor . implicit estimator equations are derived and compared with those obtained from a maximum likelihood ( ml ) analysis . unlike ml , the mml estimators remain consistent when used to estimate both the factor loadings and factor scores . tests on simulated data show the mml estimates to be on av erage more accurate than the ml estimates when the former exist . if the data show little evidence for a factor , the mml estimate collapses . it is shown that the condition for the existence of an mml estimate is essentially that the log likelihood ratio in favour of the factor model exceed the value expected under the null ( no - factor ) hypotheses .
this paper firstly provides a re - appraisal of the development of techniques for inverting deduction , secondly introduces mode - directed inverse entailment ( mdie ) as a generalisation and enhancement of previous approaches and thirdly describes an implementation of mdie in the progol system . progol is implemented in c and available by anonymous ftp . the re - assessment of previous techniques in terms of inverse entailment leads to new results for learning from positive data and inverting implication between pairs of clauses .
first - order learning involves finding a clause - form definition of a relation from examples of the relation and relevant background information . in this paper , a particular first - order learning system is modified to customize it for finding definitions of functional relations . this restriction leads to faster learning times and , in some cases , to definitions that have higher predictive accuracy . other first - order learning systems might benefit from similar specialization .
boosting is a general method for improving the performance of any learning algorithm that consistently generates classifiers which need to perform only slightly better than random guessing . a recently proposed and very promising boosting algorithm is adaboost [ #NUM# ] . it has been applied with great success to several benchmark machine learning problems using rather simple learning algorithms [ #NUM# ] , in particular decision trees [ #NUM# , #NUM# , #NUM# ] . in this paper we use adaboost to improve the performances of neural networks applied to character recognition tasks . we compare training methods based on sampling the training set and weighting the cost function . our system achieves about #NUM# . #NUM# % error on a data base of online handwritten digits from more than #NUM# writers . adaptive boosting of a multi - layer network achieved #NUM# % error on the uci letters offline characters data set .
the ability of an inductive learning system to find a good solution to a given problem is dependent upon the representation used for the features of the problem . systems that perform constructive induction are able to change their representation by constructing new features . we describe an important , real - world problem finding genes in dna that we believe offers an interesting challenge to constructive - induction researchers . we report experiments that demonstrate that : ( #NUM# ) two different input representations for this task result in significantly different generalization performance for both neural networks and decision trees ; and ( #NUM# ) both neural and symbolic methods for constructive induction fail to bridge the gap between these two representations . we believe that this real - world domain provides an interesting challenge problem for constructive induction because the relationship between the two representations is well known , and because the representational shift involved in construct ing the better representation is not imposing .
a preliminary version of this paper appeared in the proceedings of the eurocolt ' #NUM# conference , published in volume #NUM# of lecture notes in artificial intelligence , pages #NUM# - #NUM# . springer - verlag , #NUM# . the journal version will appear in algoritmica . part of this research was done while the author was a ph . d . student at the technion . this research was supported by technion v . p . r . fund #NUM# - #NUM# and by japan technion society research fund .
this paper demonstrates the capabilities of foidl , an inductive logic programming ( ilp ) system whose distinguishing characteristics are the ability to produce first - order decision lists , the use of an output completeness assumption to provide implicit negative examples , and the use of intensional background knowledge . the development of foidl was originally motivated by the problem of learning to generate the past tense of english verbs ; however , this paper demonstrates its superior performance on two different sets of benchmark ilp problems . tests on the finite element mesh design problem show that foidl ' s decision lists enable it to produce better results than all other ilp systems whose results on this problem have been reported . tests with a selection of list - processing problems from bratko ' s introductory prolog text demonstrate that the combination of implicit negatives and intensionality allow foidl to learn correct programs from far fewer examples than foil .
we present algorithms for coupling and training hidden markov models ( hmms ) to model interacting processes , and demonstrate their superiority to conventional hmms in a vision task classifying two - handed actions . hmms are perhaps the most successful framework in perceptual computing for modeling and classifying dynamic behaviors , popular because they offer dynamic time warping , a training algorithm , and a clear bayesian semantics . however , the markovian framework makes strong restrictive assumptions about the system generating the signalthat it is a single process having a small number of states and an extremely limited state memory . the single - process model is often inappropriate for vision ( and speech ) applications , resulting in low ceilings on model performance . coupled hmms provide an efficient way to resolve many of these problems , and offer superior training speeds , model likelihoods , and robustness to initial conditions .
the general framework of reinforcement learning has been proposed by several researchers for both the solution of optimization problems and the realization of adaptive control schemes . to allow for an efficient application of reinforcement learning in either of these areas , it is necessary to solve both the structural and the temporal credit assignment problem . in this paper , we concentrate on the latter which is usually tackled through the use of learning algorithms that employ discounted rewards . we argue that for realistic problems this kind of solution is not satisfactory , since it does not address the effect of noise originating from different experiences and does not allow for an easy explanation of the parameters involved in the learning process . as a possible solution , we propose to keep the delayed reward undiscounted , but to discount the actual adaptation rate . empirical results show that dependent on the kind of discount used amore stable convergence and even an increase in performance can be obtained .
for certain classes of problems defined over two - dimensional regions with grid structure , minimum - perimeter domain decomposition provides tools for partitioning the problem tasks among processors so as to minimize interprocessor communication . minimizing interprocessor communication is shown to be equivalent to tiling the domain so as to minimize total tile perimeter , where each tile corresponds to the tasks assigned to some processor . the concepts of " slice - convexity " and " semi - perimeter " are introduced to characterize minimum - perimeter tiles . a tight lower bound on the perimeter of a tile as a function of its area is developed . we then show how to generate all possible minimum - perimeter tiles . certain classes of domains are shown to be optimally tilable .
we report a successful application of td ( ) with value function approximation to the task of job - shop scheduling . our scheduling problems are based on the problem of scheduling payload processing steps for the nasa space shuttle program . the value function is approximated by a #NUM# - layer feedforward network of sigmoid units . a one - step lookahead greedy algorithm using the learned evaluation function outperforms the best existing algorithm for this task , which is an iterative repair method incorporating simulated annealing . to understand the reasons for this performance improvement , this paper introduces several measurements of the learning process and discusses several hypotheses suggested by these measurements . we conclude that the use of value function approximation is not a source of difficulty for our method , and in fact , it may explain the success of the method independent of the use of value iteration . additional experiments are required to discriminate among our hypotheses .
several metrics are used in empirical studies to explore the mechanisms of convergence of genetic algorithms . the metric is designed to measure the consistency of an arbitrary ranking of hyperplanes in a partition with respect to a target string . walsh coefficients can be calculated for small functions in order to characterize sources of linear and nonlinear interactions . a simple deception measure is also developed to look closely at the effects of increasing nonlinearity of functions . correlations between the metric and deception measure are discussed and relationships between and convergence behavior of a simple genetic algorithm are studied over large sets of functions with varying degrees of nonlinearity .
exercises are problems ordered in increasing order of difficulty . teaching problem - solving through exercises is a widely used pedagogic technique . a computational reason for this is that the knowledge gained by solving simple problems is useful in efficiently solving more difficult problems . we adopt this approach of learning from exercises to acquire search - control knowledge in the form of goal - decomposition rules ( d - rules ) . d - rules are first order , and are learned using a new " generalize - and - test " algorithm which is based on inductive logic programming techniques . we demonstrate the feasibility of the approach by applying it in two planning do mains .
foveal vision features imagers with graded acuity coupled with context sensitive sensor gaze control , analogous to that prevalent throughout vertebrate vision . foveal vision operates more efficiently than uniform acuity vision because resolution is treated as a dynamically allocatable resource , but requires a more refined visual attention mechanism . we demonstrate that reinforcement learning ( rl ) significantly improves the performance of foveal visual attention , and of the overall vision system , for the task of model based target recognition . a simulated foveal vision system is shown to classify targets with fewer fixations by learning strategies for the acquisition of visual information relevant to the task , and learning how to generalize these strategies in ambiguous and unexpected scenario conditions .
a horn definition is a set of horn clauses with the same head literal . in this paper , we consider learning non - recursive , function - free first - order horn definitions . we show that this class is exactly learnable from equivalence and membership queries . it follows then that this class is pac learnable using examples and membership queries . our results have been shown to be applicable to learning efficient goal - decomposition rules in planning domains .
speedup learning is the study of improving the problem - solving performance with experience and from outside guidance . we describe here a system that successfully combines the best features of explanation - based learning and empirical learning to learn goal decomposition rules from examples of successful problem solving and membership queries . we demonstrate that our system can efficiently learn effective decomposition rules in three different domains . our results suggest that theory - guided empirical learning can overcome the problems of purely explanation - based learning and purely empirical learning , and be an effective speedup learning method .
when a case - based planner is retrieving a previous case in preparation for solving a new similar problem , it is often not aware of the implicit features of the new problem situation which determine if a particular case may be successfully applied . this means that some cases may be retrieved in error in that the case may fail to improve the planner ' s performance . retrieval may be incrementally improved by detecting and explaining these failures as they occur . in this paper we provide a definition of case failure for the planner , dersnlp ( derivation replay in snlp ) , which solves new problems by replaying its previous plan derivations . we provide ebl ( explanation - based learning ) techniques for detecting and constructing the reasons for the failure . we also describe how to organize a case library so as to incorporate this failure information as it is produced . finally we present an empirical study which demonstrates the effectiveness of this approach in improving the performance of dersnlp .
we describe and analyze a mixture model for supervised learning of probabilistic transducers . we devise an on - line learning algorithm that efficiently infers the structure and estimates the parameters of each probabilistic transducer in the mixture . theoretical analysis and comparative simulations indicate that the learning algorithm tracks the best transducer from an arbitrarily large ( possibly infinite ) pool of models . we also present an application of the model for inducing a noun phrase recognizer .
differentiation between the nodes of a competitive learning network is conventionally achieved through competition on the basis of neural activity . simple inhibitory mechanisms are limited to sparse representations , while decorrelation and factorization schemes that support distributed representations are computation - ally unattractive . by letting neural plasticity mediate the competitive interaction instead , we obtain diffuse , nonadaptive alternatives for fully distributed representations . we use this technique to simplify and improve our binary information gain optimization algorithm for feature extraction ( schraudolph and sejnowski , #NUM# ) ; the same approach could be used to improve other learning algorithms .
it is shown that bayesian training of backpropagation neural networks can feasibly be performed by the " hybrid monte carlo " method . this approach allows the true predictive distribution for a test case given a set of training cases to be approximated arbitrarily closely , in contrast to previous approaches which approximate the posterior weight distribution by a gaussian . in this work , the hybrid monte carlo method is implemented in conjunction with simulated annealing , in order to speed relaxation to a good region of parameter space . the method has been applied to a test problem , demonstrating that it can produce good predictions , as well as an indication of the uncertainty of these predictions . appropriate weight scaling factors are found automatically . by applying known techniques for calculation of " free energy " differences , it should also be possible to compare the merits of different network architectures . the work described here should also be applicable to a wide variety of statistical models other than neural networks .
former layouts contain much of the know - how of architects . a generic and automatic way to formalize this know - how in order to use it by a computer would save a lot of effort and money . however , there seems to be no such way . the only access to the know - how are the layouts themselves . developing a generic software tool to reuse former layouts you cannot consider every part of the architectual domain or things like personal style . tools used today only consider small parts of the architectual domain . any personal style is ignored . isn ' t it possible to build a basic tool which is adjusted by the content of the former layouts , but may be extended incremently by modeling as much of the domain as desirable ? this paper will describe a reuse tool to perform this task focusing on topological and geometrical binary relations .
an important and difficult prediction task in many domains , particularly medical decision making , is that of prognosis . prognosis presents a unique set of problems to a learning system when some of the outputs are unknown . this paper presents a new approach to prognostic prediction , using ideas from nonparametric statistics to fully utilize all of the available information in a neural architecture . the technique is applied to breast cancer prognosis , resulting in flexible , accurate models that may play a role in prevent ing unnecessary surgeries .
in this paper a new approach is presented , which transfers a basic idea from evolution strategies ( ess ) to gas . mutation rates are changed into endogeneous items which are adapting during the search process . first experimental results are presented , which indicate that environment - dependent self - adaptation of appropriate settings for the mutation rate is possible even for gas .
previous teaching models in the learning theory community have been batch models . that is , in these models the teacher has generated a single set of helpful examples to present to the learner . in this paper we present an interactive model in which the learner has the ability to ask queries as in the query learning model of angluin [ #NUM# ] . we show that this model is at least as powerful as previous teaching models . we also show that anything learnable with queries , even by a randomized learner , is teachable in our model . in all previous teaching models , all classes shown to be teachable are known to be efficiently learnable . an important concept class that is not known to be learnable is dnf formulas . we demonstrate the power of our approach by providing a deterministic teacher and learner for the class of dnf formulas . the learner makes only equivalence queries and all hypotheses are also dnf formulas .
a neural - network ensemble is a very successful technique where the outputs of a set of separately trained neural network are combined to form one unified prediction . an effective ensemble should consist of a set of networks that are not only highly correct , but ones that make their errors on different parts of the input space as well ; however , most existing techniques only indirectly address the problem of creating such a set . we present an algorithm called addemup that uses genetic algorithms to explicitly search for a highly diverse set of accurate trained networks . addemup works by first creating an initial population , then uses genetic operators to continually create new networks , keeping the set of networks that are highly accurate while disagreeing with each other as much as possible . experiments on four real - world domains show that addemup is able to generate a set of trained networks that is more accurate than several existing ensemble approaches . experiments also show that addemup is able to effectively incorporate prior knowledge , if available , to improve the quality of its ensemble .
the problem of maximizing the expected total discounted reward in a completely observable markovian environment , i . e . , a markov decision process ( mdp ) , models a particular class of sequential decision problems . algorithms have been developed for making optimal decisions in mdps given either an mdp specification or the opportunity to interact with the mdp over time . recently , other sequential decision - making problems have been studied prompting the development of new algorithms and analyses . we describe a new generalized model that subsumes mdps as well as many of the recent variations . we prove some basic results concerning this model and develop generalizations of value iteration , policy iteration , model - based reinforcement - learning , and q - learning that can be used to make optimal decisions in the generalized model under various assumptions . applications of the theory to particular models are described , including risk - averse mdps , exploration - sensitive mdps , sarsa , q - learning with spreading , two - player games , and approximate max picking via sampling . central to the results are the contraction property of the value operator and a stochastic - approximation theorem that reduces asynchronous convergence to synchronous convergence .
incorporating declarative bias or prior knowledge into learning is an active research topic in machine learning . tree - structured bias specifies the prior knowledge as a tree of " relevance " relationships between attributes . this paper presents a learning algorithm that implements tree - structured bias , i . e . , learns any target function probably approximately correctly from random examples and membership queries if it obeys a given tree - structured bias . the theoretical predictions of the paper are em pirically validated .
we introduce a large family of boltzmann machines that can be trained using standard gradient descent . the networks can have one or more layers of hidden units , with tree - like connectivity . we show how to implement the supervised learning algorithm for these boltzmann machines exactly , without resort to simulated or mean - field annealing . the stochastic averages that yield the gradients in weight space are computed by the technique of decimation . we present results on the problems of n - bit parity and the detection of hidden symmetries .
the data describing resolutions to telephone network local loop " troubles , " from which we wish to learn rules for dispatching technicians , are notoriously unreliable . anecdotes abound detailing reasons why a resolution entered by a technician would not be valid , ranging from sympathy to fear to ignorance to negligence to management pressure . in this paper , we describe four different approaches to dealing with the problem of " bad " data in order first to determine whether machine learning has promise in this domain , and then to determine how well machine learning might perform . we then offer evidence that machine learning can help to build a dispatching method that will perform better than the system currently in place .
we study the notions of bias and variance for classification rules . following efron ( #NUM# ) we develop a decomposition of prediction error into its natural components . then we derive bootstrap estimates of these components and illustrate how they can be used to describe the error behaviour of a classifier in practice . in the process we also obtain a bootstrap estimate of the error of a " bagged " classifier .
this paper deals with systems that are obtained from linear time - invariant continuous - or discrete - time devices followed by a function that just provides the sign of each output . such systems appear naturally in the study of quantized observations as well as in signal processing and neural network theory . results are given on observability , minimal realizations , and other system - theoretic concepts . certain major differences exist with the linear case , and other results generalize in a surprisingly straightforward manner .
case - based reasoning ( cbr ) paradigm is very close to the designer behavior during the conceptual design , and seems to be a fruitable computer aided - design approach if a library of design cases is available . the goal of this paper is to presents the general framework of a case - based retrieval system : repro , that supports chemical process design . the crucial problems like the case representation and structural similarity measure are widely described . the presented experimental results and the expert evaluation shows usefulness of the described system in real world problems . the papers ends with discussion concerning research problems and future work .
holland ' s analysis of the sources of power of genetic algorithms has served as guidance for the applications of genetic algorithms for more than #NUM# years . the technique of applying a recombination operator ( crossover ) to a population of individuals is a key to that power . neverless , there have been a number of contradictory results concerning crossover operators with respect to overall performance . recently , for example , genetic algorithms were used to design neural network modules and their control circuits . in these studies , a genetic algorithm without crossover outperformed a genetic algorithm with crossover . this report re - examines these studies , and concludes that the results were caused by a small population size . new results are presented that illustrate the effectiveness of crossover when the population size is larger . from a performance view , the results indicate that better neural networks can be evolved in a shorter time if the genetic algorithm uses crossover .
in this paper , we explore the use of genetic algorithms ( gas ) as a key element in the design and implementation of robust concept learning systems . we describe and evaluate a ga - based system called gabil that continually learns and refines concept classification rules from its interaction with the environment . the use of gas is motivated by recent studies showing the effects of various forms of bias built into different concept learning systems , resulting in systems that perform well on certain concept classes ( generally , those well matched to the biases ) and poorly on others . by incorporating a ga as the underlying adaptive search mechanism , we are able to construct a concept learning system that has a simple , unified architecture with several important features . first , the system is surprisingly robust even with minimal bias . second , the system can be easily extended to incorporate traditional forms of bias found in other concept learning systems . finally , the architecture of the system encourages explicit representation of such biases and , as a result , provides for an important additional feature : the ability to dynamically adjust system bias . the viability of this approach is illustrated by comparing the performance of gabil with that of four other more traditional concept learners ( aq #NUM# , c #NUM# . #NUM# , id #NUM# r , and iacl ) on a variety of target concepts . we conclude with some observations about the merits of this approach and about possible extensions .
suppose that , for a learning task , we have to select one hypothesis out of a set of hypotheses ( that may , for example , have been generated by multiple applications of a randomized learning algorithm ) . a common approach is to evaluate each hypothesis in the set on some previously unseen cross - validation data , and then to select the hypothesis that had the lowest cross - validation error . but when the cross - validation data is partially corrupted such as by noise , and if the set of hypotheses we are selecting from is large , then " folklore " also warns about " overfitting " the cross - validation data [ klockars and sax , #NUM# , tukey , #NUM# , tukey , #NUM# ] . in this paper , we explain how this " overfitting " really occurs , and show the surprising result that it can be overcome by selecting a hypothesis with a higher cross - validation error , over others with lower cross - validation errors . we give reasons for not selecting the hypothesis with the lowest cross - validation error , and propose a new algorithm , loocvcv , that uses a computa - tionally efficient form of leave - one - out cross - validation to select such a hypothesis . finally , we present experimental results for one domain , that show loocvcv consistently beating picking the hypothesis with the lowest cross - validation error , even when using reasonably large cross - validation sets .
we investigate learning with membership and equivalence queries assuming that the information provided to the learner is incomplete . by incomplete we mean that some of the membership queries may be answered by i don ' t know . this model is a worst - case version of the incomplete membership query model of angluin and slonim . it attempts to model practical learning situations , including an experiment of lang and baum that we describe , where the teacher may be unable to answer reliably some queries that are critical for the learning algorithm . we present algorithms to learn monotone k - term dnf with membership queries only , and to learn monotone dnf with membership and equivalence queries . compared to the complete information case , the query complexity increases by an additive term linear in the number of i don ' t know answers received . we also observe that the blowup in the number of queries can in general be exponential for both our new model and the incomplete membership model .
in the field of optimization and machine learning techniques , some very efficient and promising tools like genetic algorithms ( gas ) and hill - climbing have been designed . in this same field , the evolving non - determinism ( end ) model proposes an inventive way to explore the space of states that , combined with the use of simulated co - evolution , remedies some drawbacks of these previous techniques and even allow this model to outperform them on some difficult problems . this new model has been applied to the sorting network problem , a reference problem that challenged many computer scientists , and an original one - player game named solitaire . for the first problem , the end model has been able to build from scratch some sorting networks as good as the best known for the #NUM# - input problem . it even improved by one comparator a #NUM# years old result for the #NUM# - input problem ! for the solitaire game , end evolved a strategy comparable to a human designed strategy .
in the field of optimization and machine learning techniques , some very efficient and promising tools like genetic algorithms ( gas ) and hill - climbing have been designed . in this same field , the evolving non - determinism ( end ) model proposes an inventive way to explore the space of states that , combined with the use of simulated co - evolution , remedies some drawbacks of these previous techniques and even allow this model to outperform them on some difficult problems . this new model has been applied to the sorting network problem , a reference problem that challenged many computer scientists , and an original one - player game named solitaire . for the first problem , the end model has been able to build from scratch some sorting networks as good as the best known for the #NUM# - input problem . it even improved by one comparator a #NUM# years old result for the #NUM# - input problem ! for the solitaire game , end evolved a strategy comparable to a human designed strategy .
most case - based reasoning systems have used a single " best " or " most similar " case as the basis for a solution . for many problems , however , there is no single exact solution . rather , there is a range of acceptable answers . we use cases not only as a basis for a solution , but also to indicate the boundaries within which a solution can be found . we solve problems by choosing some point within those boundaries . in this paper , i discuss this use of cases with illustrations from chiron , a system i have implemented in the domain of personal income tax planning .
we have used genetic programming to develop efficient image processing software . the ultimate goal of our work is to detect certain signs of breast cancer that cannot be detected with current segmentation and classification methods . traditional techniques do a relatively good job of segmenting and classifying small - scale features of mammo - grams , such as micro - calcification clusters . our strongly - typed genetic programs work on a multi - resolution representation of the mammogram , and they are aimed at handling features at medium and large scales , such as stel - lated lesions and architectural distortions . the main problem is efficiency . we employ program optimizations that speed up the evolution process by more than a factor of ten . in this paper we present our genetic programming system , and we describe our optimization techniques .
we propose two algorithms for constructing and training compact feedforward networks of linear threshold units . the shift procedure constructs networks with a single hidden layer while the pti constructs multilayered networks . the resulting networks are guaranteed to perform any given task with binary or real - valued inputs . the various experimental results reported for tasks with binary and real inputs indicate that our methods compare favorably with alternative procedures deriving from similar strategies , both in terms of size of the resulting networks and of their generalization properties .
the naive bayesian classifier is simple and effective , but its attribute independence assumption is often violated in the real world . a number of approaches have been developed that seek to alleviate this problem . a bayesian tree learning algorithm builds a decision tree , and generates a local bayesian classifier at each leaf instead of predicting a single class . however , bayesian tree learning still suffers from the replication and fragmentation problems of tree learning . while inferred bayesian trees demonstrate low average prediction error rates , there is reason to believe that error rates will be higher for those leaves with few training examples . this paper proposes a novel lazy bayesian tree learning algorithm . for each test example , it conceptually builds a most appropriate bayesian tree . in practice , only one path with a local bayesian classifier at its leaf is created . experiments with a wide variety of real - world and artificial domains show that this new algorithm has significantly lower overall prediction error rates than a naive bayesian classifier , c #NUM# . #NUM# , and a bayesian tree learning algorithm .
the problem of learning bayesian networks with hidden variables is known to be a hard problem . even the simpler task of learning just the conditional probabilities on a bayesian network with hidden variables is hard . in this paper , we present an approach that learns the conditional probabilities on a bayesian network with hidden variables by transforming it into a multi - layer feedforward neural network ( ann ) . the conditional probabilities are mapped onto weights in the ann , which are then learned using standard backpropagation techniques . to avoid the problem of exponentially large anns , we focus on bayesian networks with noisy - or and noisy - and nodes . experiments on real world classification problems demonstrate the effectiveness of our technique .
computational models of natural systems often contain free parameters that must be set to optimize the predictive accuracy of the models . this process | called calibration | can be viewed as a form of supervised learning in the presence of prior knowledge . in this view , the fixed aspects of the model constitute the prior knowledge , and the goal is to learn values for the free parameters . we report on a series of attempts to learn parameter values for a global vegetation model called mapss ( mapped atmosphere - plant - soil system ) developed by our collaborator , ron neilson . standard machine learning methods do not work with mapss , because the constraints introduced by the structure of the model create a very difficult nonlinear optimization problem . we developed a new divide - and - conquer approach in which subsets of the parameters are calibrated while others are held constant . this approach succeeds because it is possible to select training examples that exercise only portions of the model .
evolutionary learning methods have been found to be useful in several areas in the development of intelligent robots . in the approach described here , evolutionary algorithms are used to explore alternative robot behaviors within a simulation model as a way of reducing the overall knowledge engineering effort . this paper presents some initial results of applying the samuel genetic learning system to a collision avoidance and navigation task for mobile robots .
the bias and variance of a real valued random variable , using squared error loss , are well understood . however because of recent developments in classification techniques it has become desirable to extend these concepts to general random variables and loss functions . the #NUM# - #NUM# ( misclassification ) loss function with categorical random variables has been of particular interest . we explore the concepts of variance and bias and develop a decomposition of the prediction error into functions of the systematic and variable parts of our predictor . after providing some examples we conclude with a discussion of the various definitions that have been proposed .
retrieving relevant cases is a crucial component of case - based reasoning systems . the task is to use user - defined query to retrieve useful information , i . e . , exact matches or partial matches which are close to query - defined request according to certain measures . the difficulty stems from the fact that it may not be easy ( or it may be even impossible ) to specify query requests precisely and completely resulting in a situation known as a fuzzy - querying . it is usually not a problem for small domains , but for a large repositories which store various information ( multifunctional information bases or a federated databases ) , a request specification becomes a bottleneck . thus , a flexible retrieval algorithm is required , allowing for imprecise query specification and for changing the viewpoint . efficient database techniques exists for locating exact matches . finding relevant partial matches might be a problem . this document proposes a context - based similarity as a basis for flexible retrieval . historical bacground on research in similarity assessment is presented and is used as a motivation for formal definition of context - based similarity . we also describe a similarity - based retrieval system for multifunctinal information bases .
in an earlier paper , we introduced a new boosting algorithm called adaboost which , theoretically , can be used to significantly reduce the error of any learning algorithm that consistently generates classifiers whose performance is a little better than random guessing . we also introduced the related notion of a pseudo - loss which is a method for forcing a learning algorithm of multi - label concepts to concentrate on the labels that are hardest to discriminate . in this paper , we describe experiments we carried out to assess how well adaboost with and without pseudo - loss , performs on real learning problems . we performed two sets of experiments . the first set compared boosting to breiman ' s bagging method when used to aggregate various classifiers ( including decision trees and single attribute - value tests ) . we compared the performance of the two methods on a collection of machine - learning benchmarks . in the second set of experiments , we studied in more detail the performance of boosting using a nearest - neighbor classifier on an ocr problem .
machine learning techniques can be used to extract knowledge from data stored in medical databases . in our application , various machine learning algorithms were used to extract diagnostic knowledge to support the diagnosis of sport injuries . the applied methods include variants of the assistant algorithm for top - down induction of decision trees , and variants of the bayesian classifier . the available dataset was insufficent for reliable diagnosis of all sport injuries considered by the system . consequently , expert - defined diagnostic rules were added and used as pre - classifiers or as generators of additional training instances for injuries with few training examples . experimental results show that the classification accuracy and the explanation capability of the naive bayesian classifier with the fuzzy discretization of numerical attributes was superior to other methods and was estimated as the most appro priate for practical use .
this paper introduces a new machine learning task | model calibration | and presents a method for solving a particularly difficult model calibration task that arose as part of a global climate change research project . the model calibration task is the problem of training the free parameters of a scientific model in order to optimize the accuracy of the model for making future predictions . it is a form of supervised learning from examples in the presence of prior knowledge . an obvious approach to solving calibration problems is to formulate them as global optimization problems in which the goal is to find values for the free parameters that minimize the error of the model on training data . unfortunately , this global optimization approach becomes computationally infeasible when the model is highly nonlinear . this paper presents a new divide - and - conquer method that analyzes the model to identify a series of smaller optimization problems whose sequential solution solves the global calibration problem . this paper argues that methods of this kind | rather than global optimization techniques | will be required in order for agents with large amounts of prior knowledge to learn efficiently .
we describe the principles and functionalities of dlab ( declarative language bias ) . dlab can be used in inductive learning systems to define syntactically and traverse efficiently finite subspaces of first order clausal logic , be it a set of propositional formulae , association rules , horn clauses , or full clauses . a prolog implementation of dlab is available by ftp access . keywords : declarative language bias , concept learning , knowledge dis covery
discovery involves collaboration among many intelligent activities . however , little is known about how and in what form such collaboration occurs . in this paper , a framework is proposed for autonomous systems that learn and discover from their environment . within this framework , many intelligent activities such as perception , action , exploration , experimentation , learning , problem solving , and new term construction can be integrated in a coherent way . the framework is presented in detail through an implemented system called live , and is evaluated through the performance of live on several discovery tasks . the conclusion is that autonomous learning from the environment is a feasible approach for integrating the activities involved in a discovery process .
support vector machines are used for time series prediction and compared to radial basis function networks . we make use of two different cost functions for support vectors : training with ( i ) an * insensitive loss and ( ii ) huber ' s robust loss function and discuss how to choose the regularization parameters in these models . two applications are considered : data from ( a ) a noisy ( normal and uniform noise ) mackey glass equation and ( b ) the santa fe competition ( set d ) . in both cases support vector machines show an excellent performance . in case ( b ) the support vector approach improves the best known result on the benchmark by a factor of #NUM# % .
in this paper we evaluate the classification accuracy of four statistical and three neural network classifiers for two image based pattern classification problems . these are fingerprint classification and optical character recognition ( ocr ) for isolated handprinted digits . the evaluation results reported here should be useful for designers of practical systems for these two important commercial applications . for the ocr problem , the karhunen - loeve ( k - l ) transform of the images is used to generate the input feature set . similarly for the fingerprint problem , the k - l transform of the ridge directions is used to generate the input feature set . the statistical classifiers used were euclidean minimum distance , quadratic minimum distance , normal , and k - nearest neighbor . the neural network classifiers used were multilayer perceptron , radial basis function , and probabilistic . the ocr data consisted of #NUM# , #NUM# digit images for training and #NUM# , #NUM# digit images for testing . the fingerprint data consisted of #NUM# , #NUM# training and #NUM# , #NUM# testing images . in addition to evaluation for accuracy , the multilayer perceptron and radial basis function networks were evaluated for size and generalization capability . for the evaluated datasets the best accuracy obtained for either problem was provided by the probabilistic neural network , where the minimum classification error was #NUM# . #NUM# % for ocr and #NUM# . #NUM# % for fingerprints .
genetic programming is applied to the task of finding all of the cliques in a graph . nodes in the graph are represented as tree structures , which are then manipulated to form candidate cliques . the intrinsic properties of clique detection complicates the design of a good fitness evaluation . we analyze those properties , and show the clique detector is found to be better at finding the maximum clique in the graph , not the set of all cliques .
computer models of case - based reasoning ( cbr ) generally guide case adaptation using a fixed set of adaptation rules . a difficult practical problem is how to identify the knowledge required to guide adaptation for particular tasks . likewise , an open issue for cbr as a cognitive model is how case adaptation knowledge is learned . we describe a new approach to acquiring case adaptation knowledge . in this approach , adaptation problems are initially solved by reasoning from scratch , using abstract rules about structural transformations and general memory search heuristics . traces of the processing used for successful rule - based adaptation are stored as cases to enable future adaptation to be done by case - based reasoning . when similar adaptation problems are encountered in the future , these adaptation cases provide task - and domain - specific guidance for the case adaptation process . we present the tenets of the approach concerning the relationship between memory search and case adaptation , the memory search process , and the storage and reuse of cases representing adaptation episodes . these points are discussed in the context of ongoing research on dial , a computer model that learns case adaptation knowledge for case - based disaster response planning .
the development of multistrategy learning systems should be based on a clear understanding of the roles and the applicability conditions of different learning strategies . to this end , this chapter introduces the inferential theory of learning that provides a conceptual framework for explaining logical capabilities of learning strategies , i . e . , their competence . viewing learning as a process of modifying the learners knowledge by exploring the learners experience , the theory postulates that any such process can be described as a search in a knowledge space , triggered by the learners experience and guided by learning goals . the search operators are instantiations of knowledge transmutations , which are generic patterns of knowledge change . transmutations may employ any basic type of inferencededuction , induction or analogy . several fundamental knowledge transmutations are described in a novel and general way , such as generalization , abstraction , explanation and similization , and their counterparts , specialization , concretion , prediction and dissimilization , respectively . generalization enlarges the reference set of a description ( the set of entities that are being described ) . abstraction reduces the amount of the detail about the reference set . explanation generates premises that explain ( or imply ) the given properties of the reference set . similization transfers knowledge from one reference set to a similar reference set . using concepts of the theory , a multistrategy task - adaptive learning ( mtl ) methodology is outlined , and illustrated b y an example . mtl dynamically adapts strategies to the learning task , defined by the input information , learners background knowledge , and the learning goal . it aims at synergistically integrating a whole range of inferential learning strategies , such as empirical generalization , constructive induction , deductive generalization , explanation , prediction , abstraction , and similization .
the support vector ( sv ) machine is a novel type of learning machine , based on statistical learning theory , which contains polynomial classifiers , neural networks , and radial basis function ( rbf ) networks as special cases . in the rbf case , the sv algorithm automatically determines centers , weights and threshold such as to minimize an upper bound on the expected test error . the present study is devoted to an experimental comparison of these machines with a classical approach , where the centers are determined by k - means clustering and the weights are found using error backpropagation . we consider three machines , namely a classical rbf machine , an sv machine with gaussian kernel , and a hybrid system with the centers determined by the sv method and the weights trained by error backpropagation . our results show that on the us postal service database of handwritten digits , the sv machine achieves the highest test accuracy , followed by the hybrid approach . the sv approach is thus not only theoretically well - founded , but also superior in a practical application .
ensembles of classifiers , e . g . decision trees , often exhibit greater predictive accuracy than single classifiers alone . bagging and boosting are two standard ways of generating and combining multiple classifiers . unfortunately , the increase in predictive performance is usually linked to a dramatic decrease in intelligibility : ensembles are more or less black boxes comparable to neural networks . so far attempts at pruning of ensembles have not been very successful , approximately reducing ensembles into half . this paper describes a different approach which both tries to keep ensemble - sizes small during induction already and also limits the complexity of single classifiers rigorously . single classifiers are decision - stumps of a prespecified maximal depth . they are combined by majority voting . ensembles are induced and pruned by a simple hill - climbing procedure . these ensembles can reasonably be transformed into equivalent decision trees . we conduct some empirical evaluation to investigate both predictive accuracies and classifier complexities .
the use of graphs to represent independence structure in multivariate probability models has been pursued in a relatively independent fashion across a wide variety of research disciplines since the beginning of this century . this paper provides a brief overview of the current status of such research with particular attention to recent developments which have served to unify such seemingly disparate topics as probabilistic expert systems , statistical physics , image analysis , genetics , decoding of error - correcting codes , kalman filters , and speech recognition with markov models .
an inference graph can have many " derivation strategies " , each a particular ordering of the steps involved in reducing a given query to a sequence of database retrievals . an " optimal strategy " for a given distribution of queries is a complete strategy whose " expected cost " is minimal , where the expected cost depends on the conditional probabilities that each requested retrieval succeeds , given that a member of this class of queries is posed . this paper describes the pao algorithm that first uses a set of training examples to approximate these probability values , and then uses these estimates to produce a " probably approximately optimal " strategy | i . e . , given any * ; ffi & gt ; #NUM# , pao produces a strategy whose cost is within * of the cost of the optimal strategy , with probability greater than #NUM# ffi . this paper also shows how to obtain these strategies in time polynomial in #NUM# = * , #NUM# = ffi and the size of the inference graph , for many important classes of graphs , including all and - or trees .
many real - world time series are multi - stationary , where the underlying data generating process ( dgp ) switches between different stationary subprocesses , or modes of operation . an important problem in modeling such systems is to discover the underlying switching process , which entails identifying the number of subprocesses and the dynamics of each subprocess . for many time series , this problem is ill - defined , since there are often no obvious means to distinguish the different subprocesses . we discuss the use of nonlinear gated experts to perform the segmentation and system identification of the time series . unlike standard gated experts methods , however , we use concepts from statistical physics to enhance the segmentation for high - noise problems where only a few experts are required .
a training set of data has been used to construct a rule for predicting future responses . what is the error rate of this rule ? the traditional answer to this question is given by cross - validation . the cross - validation estimate of prediction error is nearly unbiased , but can be highly variable . this article discusses bootstrap estimates of prediction error , which can be thought of as smoothed versions of cross - validation . a particular bootstrap method , the #NUM# + rule , is shown to substantially outperform cross - validation in a catalog of #NUM# simulation experiments . besides providing point estimates , we also consider estimating the variability of an error rate estimate . all of the results here are nonparametric , and apply to any possible prediction rule : however we only study classification problems with #NUM# - #NUM# loss in detail . our simulations include " smooth " prediction rules like fisher ' s linear discriminant function , and unsmooth ones like nearest neighbors .
in this paper we discuss the application of memory - based learning ( mbl ) to fast np chunking . we first discuss the application of a fast decision tree variant of mbl ( igtree ) on the dataset described in ( ramshaw and marcus , #NUM# ) , which consists of roughly #NUM# , #NUM# test and #NUM# , #NUM# train items . in a second series of experiments we used an architecture of two cascaded igtrees . in the second level of this cascaded classifier we added context predictions as extra features so that incorrect predictions from the first level can be corrected , yielding a #NUM# . #NUM# % generalisation accuracy with training and testing times in the order of seconds to minutes .
we examine the issue of consistency from a new perspective . to avoid overfitting the training data , a considerable number of current systems have sacrificed the goal of learning hypotheses that are perfectly consistent with the training instances by setting a new goal of hypothesis simplicity ( occam ' s razor ) . instead of using simplicity as a goal , we have developed a novel approach that addresses consistency directly . in other words , our concept learner has the explicit goal of selecting the most appropriate degree of consistency with the training data . we begin this paper by exploring concept learning with less than perfect consistency . next , we describe a system that can adapt its degree of consistency in response to feedback about predictive accuracy on test data . finally , we present the results of initial experiments that begin to address the question of how tightly hypotheses should fit the training data for different problems .
handling np complete problems with gas is a great challenge . in particular the presence of constraints makes finding solutions hard for a ga . in this paper we present a problem independent constraint handling mechanism , stepwise adaptation of weights ( saw ) , and apply it for solving the #NUM# - sat problem . our experiments prove that the saw mechanism substantially increases ga performance . furthermore , we compare our saw - ing ga with the best heuristic technique we could trace , wgsat , and conclude that the ga is superior to the heuristic method .
artificial neural network seem very promising for regression and classification , especially for large covariate spaces . these methods represent a non - linear function as a composition of low dimensional ridge functions and therefore appear to be less sensitive to the dimensionality of the covariate space . however , due to non uniqueness of a global minimum and the existence of ( possibly ) many local minima , the model revealed by the network is non stable . we introduce a method to interpret neural network results which uses novel robustification techniques . this results in a robust interpretation of the model employed by the network . simulated data from known models is used to demonstrate the interpretability results and to demonstrate the effects of different regularization methods on the robustness of the model . graphical methods are introduced to present the interpretation results . we further demonstrate how interaction between covariates can be revealed . from this study we conclude that the interpretation method works well , but that nn models may sometimes be misinterpreted , especially if the approximations to the true model are less robust .
in this paper we describe different ways to select and transform features using evolutionary computation . the features are intended to serve as inputs to a feedforward network . the first way is the selection of features using a standard genetic algorithm , and the solution found specifies whether a certain feature should be present or not . we show that for the prediction of unemployment rates in various european countries , this is a succesfull approach . in fact , this kind of selection of features is a special case of so - called functional links . functional links transform the input pattern space to a new pattern space . as functional links one can use polynomials , or more general functions . both can be found using evolutionary computation . polynomial functional links are found by evolving a coding of the powers of the polynomial . for symbolic functions we can use genetic programming . genetic programming finds the symbolic functions that are to be applied to the inputs . we compare the workings of the latter two methods on two artificial datasets , and on a real - world medical image dataset .
this paper reports on the development of a realistic knowledge - based application using the mobal system . some problems and requirements resulting from industrial - caliber tasks are formulated . a step - by - step account of the construction of a knowledge base for such a task demonstrates how the interleaved use of several learning algorithms in concert with an inference engine and a graphical interface can fulfill those requirements . design , analysis , revision , refinement and extension of a working model are combined in one incremental process . this illustrates the balanced cooperative modeling approach . the case study is taken from the telecommunications domain and more precisely deals with security management in telecommunications networks . mobal would be used as part of a security management tool for acquiring , validating and refining a security policy . the modeling approach is compared with other approaches , such as kads and stand - alone machine learning .
source separation consists in recovering a set of independent signals when only mixtures with unknown coefficients are observed . this paper introduces a class of adaptive algorithms for source separation which implements an adaptive version of equivariant estimation and is henceforth called easi ( equivariant adaptive separation via independence ) . the easi algorithms are based on the idea of serial updating : this specific form of matrix updates systematically yields algorithms with a simple , parallelizable structure , for both real and complex mixtures . most importantly , the performance of an easi algorithm does not depend on the mixing matrix . in particular , convergence rates , stability conditions and interference rejection levels depend only on the ( normalized ) distributions of the source signals . close form expressions of these quantities are given via an asymptotic performance analysis . this is completed by some numerical experiments illustrating the effectiveness of the proposed approach .
ensembles of classifiers , e . g . decision trees , often exhibit greater predictive accuracy than single classifiers alone . bagging and boosting are two standard ways of generating and combining multiple classifiers . unfortunately , the increase in predictive performance is usually linked to a dramatic decrease in intelligibility : ensembles are more or less black boxes comparable to neural networks . so far attempts at pruning of ensembles have not been very successful , approximately reducing ensembles into half . this paper describes a different approach which both tries to keep ensemble - sizes small during induction already and also limits the complexity of single classifiers rigorously . single classifiers are decision - stumps of a prespecified maximal depth . they are combined by majority voting . ensembles are induced and pruned by a simple hill - climbing procedure . these ensembles can reasonably be transformed into equivalent decision trees . we conduct some empirical evaluation to investigate both predictive accuracies and classifier complexities .
arcing the edge leo breiman technical report #NUM# , statistics department university of california , berkeley ca . #NUM# abstract recent work has shown that adaptively reweighting the training set , growing a classifier using the new weights , and combining the classifiers constructed to date can significantly decrease generalization error . procedures of this type were called arcing by breiman [ #NUM# ] . the first successful arcing procedure was introduced by freund and schapire [ #NUM# , #NUM# ] and called adaboost . in an effort to explain why adaboost works , schapire et . al . [ #NUM# ] derived a bound on the generalization error of a convex combination of classifiers in terms of the margin . we introduce a function called the edge , which differs from the margin only if there are more than two classes . a framework for understanding arcing algorithms is defined . in this framework , we see that the arcing algorithms currently in the literature are optimization algorithms which minimize some function of the edge . a relation is derived between the optimal reduction in the maximum value of the edge and the pac concept of weak learner . two algorithms are described which achieve the optimal reduction . tests on both synthetic and real data cast doubt on the schapire et . al . there is recent empirical evidence that significant reductions in generalization error can be gotten by growing a number of different classifiers on the same training set and letting these vote for the best class . freund and schapire ( [ #NUM# ] , [ #NUM# ] ) proposed an algorithm called adaboost which adaptively reweights the training set in a way based on the past history of misclassifications , constructs a new classifier using the current weights , and uses the misclassification rate of this classifier to determine the size of its vote . in a number of empirical studies on many data sets using trees ( cart or c #NUM# . #NUM# ) as the base classifier ( drucker and cortes [ #NUM# ] , quinlan [ #NUM# ] , freud and schapire [ #NUM# ] , breiman [ #NUM# ] ) adaboost produced dramatic decreases in generalization error compared to using a single tree . error rates were reduced to the point where tests on some well - known data sets gave the result that cart plus adaboost did significantly better than any other of the commonly used classification methods ( breiman [ #NUM# ] ) . meanwhile , empirical results showed that other methods of adaptive resampling ( or reweighting ) and combining ( called " arcing " by breiman [ #NUM# ] ) also led to low test set error rates . an algorithm called arc - x #NUM# ( breiman [ #NUM# ] ) gave error rates almost identical to adaboost . ji and ma [ #NUM# ] worked with classifiers consisting of randomly selected hyperplanes and using a different method of adaptive resampling and unweighted voting , also got low error rates . thus , there are a least three arcing algorithms extant , all of which give excellent classification accuracy . explanation .
in order to sequence the tasks of a job shop problem ( jsp ) on a number of machines related to the technological machine order of jobs , a new representation technique mathematically known as " permutation with repetition " is presented . the main advantage of this single chromosome representation is in analogy to the permutation scheme of the traveling salesman problem ( tsp ) that it cannot produce illegal sets of operation sequences ( infeasible symbolic solutions ) . as a consequence of the representation scheme a new crossover operator preserving the initial scheme structure of permutations with repetition will be sketched . its behavior is similar to the well known order - crossover for simple permutation schemes . actually the gox operator for permutations with repetition arises from a generalisation of ox . computational experiments show , that gox passes the information from a couple of parent solutions efficiently to offspring solutions . together , the new representation and gox support the cooperative aspect of the genetic search for scheduling problems strongly .
recently , bell and sejnowski have presented an approach to blind source separation based on the information maximization principle . we extend this approach into more general cases where the sources may have been delayed with respect to each other . we present a network architecture capable of coping with such sources , and we derive the adaptation equations for the delays and the weights in the network by maximizing the information transferred through the network . examples using wideband sources such as speech are presented to illustrate the algorithm .
this paper discusses the application of a modern signal processing technique known as independent component analysis ( ica ) or blind source separation to multivariate financial time series such as a portfolio of stocks . the key idea of ica is to linearly map the observed multivariate time series into a new space of statistically independent components ( ics ) . this can be viewed as a factorization of the portfolio since joint probabilities become simple products in the coordinate system of the ics . we apply ica to three years of daily returns of the #NUM# largest japanese stocks and compare the results with those obtained using principal component analysis . the results indicate that the estimated ics fall into two categories , ( i ) infrequent but large shocks ( responsible for the major changes in the stock prices ) , and ( ii ) frequent smaller fluctuations ( contributing little to the overall level of the stocks ) . we show that the overall stock price can be reconstructed surprisingly well by using a small number of thresholded weighted ics . in contrast , when using shocks derived from principal components instead of independent components , the reconstructed price is less similar to the original one . independent component analysis is a potentially powerful method of analyzing and understanding driving mechanisms in financial markets . there are further promising applications to risk management since ica focuses on higher order statistics .
this paper concerns the empirical basis of causation , and addresses the following issues : we propose a minimal - model semantics of causation , and show that , contrary to common folklore , genuine causal influences can be distinguished from spurious covariations following standard norms of inductive reasoning . we also establish a sound characterization of the conditions under which such a distinction is possible . we provide an effective algorithm for inferred causation and show that , for a large class of data the algorithm can uncover the direction of causal influences as defined above . finally , we ad dress the issue of non - temporal causation .
this paper presents a method for using qualitative models to guide inductive learning . our objectives are to induce rules which are not only accurate but also explainable with respect to the qualitative model , and to reduce learning time by exploiting domain knowledge in the learning process . such ex - plainability is essential both for practical application of inductive technology , and for integrating the results of learning back into an existing knowledge - base . we apply this method to two process control problems , a water tank network and an ore grinding process used in the mining industry . surprisingly , in addition to achieving explainability the classificational accuracy of the induced rules is also increased . we show how the value of the qualitative models can be quantified in terms of their equivalence to additional training examples , and finally discuss possible extensions .
explanation based learning has typically been considered a symbolic learning method . an explanation based learning method that utilizes purely neural network representations ( called ebnn ) has recently been developed , and has been shown to have several desirable properties , including robustness to errors in the domain theory . this paper briefly summarizes the ebnn algorithm , then explores the correspondence between this neural network based ebl method and ebl methods based on symbolic representations .
the multi - parent scanning crossover , generalizing the traditional uniform crossover , and diagonal crossover , generalizing #NUM# - point ( n - point ) crossovers , were introduced in [ #NUM# ] . in subsequent publications , see [ #NUM# , #NUM# , #NUM# ] , several aspects of multi - parent recombination are discussed . due to space limitations , however , a full overview of experimental results showing the performance of multi - parent gas on numerical optimization problems has never been published . this technical report is meant to fill this gap and make results available .
this report documents nacodae , the navy conversational decision aids environment being developed at the navy center for applied research in artificial intelligence ( ncarai ) , which is a branch of the naval research laboratory . na - codae is a software prototype that is being developed under the practical advances in case - based reasoning project , which is funded by the office for naval research , for the purpose of assisting navy and other dod personnel in decision aids tasks such as system maintenance , operational training , crisis response planning , logistics , fault diagnosis , target classification , and meteorological nowcasting . implemented in java , nacodae can be used on any machine containing a java virtual machine ( e . g . , pcs , unix ) . this document describes and exemplifies nacodae ' s capabilities . our goal is to transition this tool to operational personnel , and to continue its enhancement through user feedback and by testing recent research advances in case - based reasoning and related areas .
a new generation of sensor rich , massively distributed , autonomous systems are being developed that have the potential for profound social , environmental , and economic change . these include networked building energy systems , autonomous space probes , chemical plant control systems , satellite constellations for remote ecosystem monitoring , power grids , biosphere - like life support systems , and reconfigurable traffic systems , to highlight but a few . to achieve high performance , these immobile robots ( or immobots ) will need to develop sophisticated regulatory and immune systems that accurately and robustly control their complex internal functions . to accomplish this , im - mobots will exploit a vast nervous system of sensors to model themselves and their environment on a grand scale . they will use these models to dramatically reconfigure themselves in order to survive decades of autonomous operations . achieving these large scale modeling and configuration tasks will require a tight coupling between the higher level coordination function provided by symbolic reasoning , and the lower level autonomic processes of adaptive estimation and control . to be economically viable they will need to be programmable purely through high level compositional models . self modeling and self configuration , coordinating autonomic functions through symbolic reasoning , and compositional , model - based programming are the three key elements of a model - based autonomous systems architecture that is taking us into the new millennium .
traditional machine vision assumes that the vision system recovers a a complete , labeled description of the world [ marr , #NUM# ] . recently , several researchers have criticized this model and proposed an alternative model which considers perception as a distributed collection of task - specific , task - driven visual routines [ aloimonos , #NUM# , ullman , #NUM# ] . some of these researchers have argued that in natural living systems these visual routines are the product of natural selection [ ramachandran , #NUM# ] . so far , researchers have hand - coded task - specific visual routines for actual implementations ( e . g . [ chapman , #NUM# ] ) . in this paper we propose an alternative approach in which visual routines for simple tasks are evolved using an artificial evolution approach . we present results from a series of runs on actual camera images , in which simple routines were evolved using genetic programming techniques [ koza , #NUM# ] . the results obtained are promising : the evolved routines are able to correctly classify up to #NUM# % of the images , which is better than the best algorithm we were able to write by hand .
combinatorial explosion of inferences has always been a central problem in artificial intelligence . although the inferences that can be drawn from a reasoner ' s knowledge and from available inputs is very large ( potentially infinite ) , the inferential resources available to any reasoning system are limited . with limited inferential capacity and very many potential inferences , reasoners must somehow control the process of inference . not all inferences are equally useful to a given reasoning system . any reasoning system that has goals ( or any form of a utility function ) and acts based on its beliefs indirectly assigns utility to its beliefs . given limits on the process of inference , and variation in the utility of inferences , it is clear that a reasoner ought to draw the inferences that will be most valuable to it . this paper presents an approach to this problem that makes the utility of a ( potential ) belief an explicit part of the inference process . the method is to generate explicit desires for knowledge . the question of focus of attention is thereby transformed into two related problems : how can explicit desires for knowledge be used to control inference and facilitate resource - constrained goal pursuit in general ? and , where do these desires for knowledge come from ? we present a theory of knowledge goals , or desires for knowledge , and their use in the processes of understanding and learning . the theory is illustrated using two case studies , a natural language understanding program that learns by reading novel or unusual newspaper stories , and a differential diagnosis program that improves its accuracy with experience .
this paper presents a theory of motivational analysis , the construction of volitional explanations to describe the planning behavior of agents . we discuss both the content of such explanations , as well as the process by which an understander builds the explanations . explanations are constructed from decision models , which describe the planning process that an agent goes through when considering whether to perform an action . decision models are represented as explanation patterns , which are standard patterns of causality based on previous experiences of the understander . we discuss the nature of explanation patterns , their use in representing decision models , and the process by which they are retrieved , used and evaluated .
an evolutionary approach for developing improved neural network architectures is presented . it is shown that it is possible to use genetic algorithms for the construction of backpropagation networks for real world tasks . therefore a network representation is developed with certain properties . results with various application are presented .
this paper describes how a reasoner can improve its understanding of an incompletely understood domain through the application of what it already knows to novel problems in that domain . recent work in ai has dealt with the issue of using past explanations stored in the reasoner ' s memory to understand novel situations . however , this process assumes that past explanations are well understood and provide good " lessons " to be used for future situations . this assumption is usually false when one is learning about a novel domain , since situations encountered previously in this domain might not have been understood completely . instead , it is reasonable to assume that the reasoner would have gaps in its knowledge base . by reasoning about a new situation , the reasoner should be able to fill in these gaps as new information came in , reorganize its explanations in memory , and gradually evolve a better understanding of its domain . we present a story understanding program that retrieves past explanations from situations already in memory , and uses them to build explanations to understand novel stories about terrorism . in doing so , the system refines its understanding of the domain by filling in gaps in these explanations , by elaborating the explanations , or by learning new indices for the explanations . this is a type of incremental learning since the system improves its explanatory knowledge of the domain in an incremental fashion rather than by learning new xps as a whole .
we present a method for the analysis of nonstationary time series with multiple operating modes . in particular , it is possible to detect and to model both a switching of the dynamics and a less abrupt , time consuming drift from one mode to another . this is achieved in two steps . first , an unsupervised training method provides prediction experts for the inherent dynamical modes . then , the trained experts are used in a hidden markov model that allows to model drifts . an application to physiological wake / sleep data demonstrates that analysis and modeling of real - world time series can be improved when the drift paradigm is taken into account .
addressed in kbann ( which translates a theory into a neural - net , refines it using backpropagation , and then retranslates the result back into rules ) by adding extra hidden units and connections to the initial network ; however , this would require predetermining the num in this paper , we have presented constructive induction techniques recently added to the either theory refinement system . intermediate concept utilization employs existing rules in the theory to derive higher - level features for use in induction . intermediate concept creation employs inverse resolution to introduce new intermediate concepts in order to fill gaps in a theory than span multiple levels . these revisions allow either to make use of imperfect domain theories in the ways typical of previous work in both constructive induction and theory refinement . as a result , either is able to handle a wider range of theory imperfections than any other existing theory refinement system .
a number of reinforcement learning algorithms have been developed that are guaranteed to converge to the optimal solution when used with lookup tables . it is shown , however , that these algorithms can easily become unstable when implemented directly with a general function - approximation system , such as a sigmoidal multilayer perceptron , a radial - basis - function system , a memory - based learning system , or even a linear function - approximation system . a new class of algorithms , residual gradient algorithms , is proposed , which perform gradient descent on the mean squared bellman residual , guaranteeing convergence . it is shown , however , that they may learn very slowly in some cases . a larger class of algorithms , residual algorithms , is proposed that has the guaranteed convergence of the residual gradient algorithms , yet can retain the fast learning speed of direct algorithms . in fact , both direct and residual gradient algorithms are shown to be special cases of residual algorithms , and it is shown that residual algorithms can combine the advantages of each approach . the direct , residual gradient , and residual forms of value iteration , q - learning , and advantage learning are all presented . theoretical analysis is given explaining the properties these algorithms have , and simulation results are given that demonstrate these properties .
experiment design and execution is a central activity in the natural sciences . the seqer system provides a general architecture for the integration of automated planning techniques with a variety of domain knowledge in order to plan scientific experiments . these planning techniques include rule - based methods and , especially , the use of derivational analogy . derivational analogy allows planning experience , captured as cases , to be reused . analogy also allows the system to function in the absence of strong domain knowledge . cases are efficiently and flexibly retrieved from a large casebase using massively parallel methods .
finding good monitoring strategies is an important process in the design of any embedded agent . we describe the nature of the monitoring problem , point out what makes it difficult , and show that while periodic monitoring strategies are often the easiest to derive , they are not always the most appropriate . we demonstrate mathematically and empirically that for a wide class of problems , the so - called " cupcake problems " , there exists a simple strategy , interval reduction , that outperforms periodic monitoring . we also show how features of the environment may influence the choice of the optimal strategy . the paper concludes with some thoughts about a monitoring strategy taxonomy , and what its defining features might be .
this paper discusses issues related to bayesian network model learning for unbalanced binary classification tasks . in general , the primary focus of current research on bayesian network learning systems ( e . g . , k #NUM# and its variants ) is on the creation of the bayesian network structure that fits the database best . it turns out that when applied with a specific purpose in mind , such as classification , the performance of these network models may be very poor . we demonstrate that bayesian network models should be created to meet the specific goal or purpose intended for the model . we first present a goal - oriented algorithm for constructing bayesian networks for predicting uncollectibles in telecommunications risk - management datasets . second , we argue and demonstrate that current bayesian network learning methods may fail to perform satisfactorily in real life applications since they do not learn models tailored to a specific goal or purpose . third , we discuss the performance of goal oriented k #NUM# and its variant .
we have calculated analytical expressions for how the bias and variance of the estimators provided by various temporal difference value estimation algorithms change with o * ine updates over trials in absorbing markov chains using lookup table representations . we illustrate classes of learning curve behavior in various chains , and show the manner in which td is sensitive to the choice of its step size and eligibility trace parameters .
the problem of minimizing the number of misclassified points by a plane , attempting to separate two point sets with intersecting convex hulls in n - dimensional real space , is formulated as a linear program with equilibrium constraints ( lpec ) . this general lpec can be converted to an exact penalty problem with a quadratic objective and linear constraints . a frank - wolfe - type algorithm is proposed for the penalty problem that terminates at a stationary point or a global solution . novel aspects of the approach include : ( i ) a linear complementarity formulation of the step function that " counts " misclassifications , ( ii ) exact penalty formulation without boundedness , nondegeneracy or constraint qualification assumptions , ( iii ) an exact solution extraction from the sequence of minimizers of the penalty function for a finite value of the penalty parameter for the general lpec and an explicitly exact solution for the lpec with uncoupled constraints , and ( iv ) a parametric quadratic programming formulation of the lpec associated with the misclassification minimization problem .
in this paper , we introduce a new approach to the problem of optimal compression when a source code produces multiple codewords for a given symbol . it may seem that the most sensible codeword to use in this case is the shortest one . however , in the proposed free energy approach , random codeword selection yields an effective codeword length that can be less than the shortest codeword length . if the random choices are boltzmann distributed , the effective length is optimal for the given source code . the expectation - maximization parameter estimation algorithms minimize this effective codeword length . we illustrate the performance of free energy coding on a simple problem where a compression factor of two is gained by using the new method .
as bayesian networks and influence diagrams are being used more and more widely , the importance of an efficient explanation mechanism becomes more apparent . we focus on predictive explanations , the ones designed to explain predictions and recommendations of probabilistic systems . we analyze the issues involved in defining , computing and evaluating such explanations and present an algorithm to compute them .
this paper continues the introduction to minimum encoding inductive inference given by oliver and hand . this series of papers was written with the objective of providing an introduction to this area for statisticians . we describe the message length estimates used in wallace ' s minimum message length ( mml ) inference and rissanen ' s minimum description length ( mdl ) inference . the differences in the message length estimates of the two approaches are explained . the implications of these differences for applications are discussed .
in this paper , we present an interactive , case - based approach to crisis response that provides users with the ability to rapidly develop good responses while allowing them to retain ultimate control over the decision - making process . we have implemented this approach in inca , an interactive crisis assistant for planning and scheduling in crisis domains . inca relies on case - based methods to seed the response development process with initial candidate solutions drawn from previous cases . the human user then interacts with inca to adapt these solutions to the current situation . we will discuss this interactive approach to crisis response using an artificial hazardous materials domain , haz - mat , that we developed for the purpose of evaluating candidate assistant mechanisms for crisis response .
mixed - initiative systems present the challenge of finding an effective level of interaction between humans and computers . machine learning presents a promising approach to this problem in the form of systems that automatically adapt their behavior to accommodate different users . in this paper , we present an empirical study of learning user models in an adaptive assistant for crisis scheduling . we describe the problem domain and the scheduling assistant , then present an initial formulation of the adaptive assistant ' s learning task and the results of a baseline study . after this , we report the results of three subsequent experiments that investigate the effects of problem reformulation and representation augmentation . the results suggest that problem reformulation leads to significantly better accuracy without sacrificing the usefulness of the learned behavior . the studies also raise several interesting issues in adaptive assistance for scheduling .
we consider bayesian and information - theoretic approaches for determining non - informative prior distributions in a parametric model family . the information - theoretic approaches are based on the recently modified definition of stochastic complexity by rissanen , and on the minimum message length ( mml ) approach by wallace . the bayesian alternatives include the uniform prior , and the equivalent sample size priors . in order to be able to empirically compare the different approaches in practice , the methods are instantiated for a model family of practical importance , the family of bayesian networks .
intelligent information retrieval ( iir ) requires inference . the number of inferences that can be drawn by even a simple reasoner is very large , and the inferential resources available to any practical computer system are limited . this problem is one long faced by ai researchers . in this paper , we present a method used by two recent machine learning programs for control of inference that is relevant to the design of iir systems . the key feature of the approach is the use of explicit representations of desired knowledge , which we call knowledge goals . our theory addresses the representation of knowledge goals , methods for generating and transforming these goals , and heuristics for selecting among potential inferences in order to feasibly satisfy such goals . in this view , iir becomes a kind of planning decisions about what to infer , how to infer and when to infer are based on representations of desired knowledge , as well as internal representations of the system ' s inferential abilities and current state . the theory is illustrated using two case studies , a natural language understanding program that learns by reading novel newspaper stories , and a differential diagnosis program that improves its accuracy with experience . we conclude by making several suggestions on how this machine learning framework can be integrated with existing information retrieval methods .
this paper attempts to bridge the fields of machine learning , robotics , and distributed ai . it discusses the use of communication in reducing the undesirable effects of locality in fully distributed multi - agent systems with multiple agents / robots learning in parallel while interacting with each other . two key problems , hidden state and credit assignment , are addressed by applying local undirected broadcast communication in a dual role : as sensing and as reinforcement . the methodology is demonstrated on two multi - robot learning experiments . the first describes learning a tightly - coupled coordination task with two robots , the second a loosely - coupled task with four robots learning social rules . communication is used to share sensory data to overcome hidden state and reinforcement to overcome the credit assignment problem between the agents and to bridge the gap between local and global payoff . #NUM#
this paper investigates the power of genetic algorithms at solving the max - clique problem . we measure the performance of a standard genetic algorithm on an elementary set of problem instances consisting of embedded cliques in random graphs . we indicate the need for improvement , and introduce a new genetic algorithm , the multi - phase annealed ga , which exhibits superior performance on the same problem set . as we scale up the problem size and test on " hard " benchmark instances , we notice a degraded performance in the algorithm caused by premature convergence to local minima . to alleviate this problem , a sequence of modifications are implemented ranging from changes in input representation to systematic local search . the most recent version , called union ga , incorporates the features of union cross - over , greedy replacement , and diversity enhancement . it shows a marked speed - up in the number of iterations required to find a given solution , as well as some improvement in the clique size found . we discuss issues related to the simd implementation of the genetic algorithms on a thinking machines cm - #NUM# , which was necessitated by the intrinsically high time complexity ( o ( n #NUM# ) ) of the serial algorithm for computing one iteration . our preliminary conclusions are : ( #NUM# ) a genetic algorithm needs to be heavily customized to work " well " for the clique problem ; ( #NUM# ) a ga is computationally very expensive , and its use is only recommended if it is known to find larger cliques than other algorithms ; ( #NUM# ) although our customization effort is bringing forth continued improvements , there is no clear evidence , at this time , that a ga will have better success in circumventing local minima .
for many types of learners one can compute the statistically " optimal " way to select data . we review how these techniques have been used with feedforward neural networks [ mackay , #NUM# ; cohn , #NUM# ] . we then show how the same principles may be used to select data for two alternative , statistically - based learning architectures : mixtures of gaussians and locally weighted regression . while the techniques for neural networks are expensive and approximate , the techniques for mixtures of gaussians and locally weighted regres sion are both efficient and accurate .
we consider the standard problem of learning a concept from random examples . here a learning curve can be defined to be the expected error of a learner ' s hypotheses as a function of training sample size . haussler , littlestone and warmuth have shown that , in the distribution free setting , the smallest expected error a learner can achieve in the worst case over a concept class c converges rationally to zero error ( i . e . , fi ( #NUM# = t ) for training sample size t ) . however , recently cohn and tesauro have demonstrated how exponential convergence can often be observed in experimental settings ( i . e . , average error decreasing as e fi ( t ) ) . by addressing a simple non - uniformity in the original analysis , this paper shows how the dichotomy between rational and exponential worst case learning curves can be recovered in the distribution free theory . these results support the experimental findings of cohn and tesauro : for finite concept classes , any consistent learner achieves exponential convergence , even in the worst case ; but for continuous concept classes , no learner can exhibit sub - rational convergence for every target concept and domain distribution . a precise boundary between rational and exponential convergence is drawn for simple concept chains . here we show that somewhere dense chains always force rational convergence in the worst case , but exponential convergence can always be achieved for nowhere dense chains .
concepts learned by neural networks are difficult to understand because they are represented using large assemblages of real - valued parameters . one approach to understanding trained neural networks is to extract symbolic rules that describe their classification behavior . there are several existing rule - extraction approaches that operate by searching for such rules . we present a novel method that casts rule extraction not as a search problem , but instead as a learning problem . in addition to learning from training examples , our method exploits the property that networks can be efficiently queried . we describe algorithms for extracting both conjunctive and m - of - n rules , and present experiments that show that our method is more efficient than conventional search - based approaches .
this paper presents a fast algorithm that provides optimal or near optimal solutions to the minimum perimeter problem on a rectangular grid . the minimum perimeter problem is to partition a grid of size m n into p equal area regions while minimizing the total perimeter of the regions . the approach taken here is to divide the grid into stripes that can be filled completely with an integer number of regions . this striping method gives rise to a knapsack integer program that can be efficiently solved by existing codes . the solution of the knapsack problem is then used to generate the grid region assignments . an implementation of the algorithm partitioned a #NUM# #NUM# grid into #NUM# regions to a provably optimal solution in less than one second . with sufficient memory to hold the m n grid array , extremely large minimum perimeter problems can be solved easily .
this paper presents and evaluates two algorithms for incrementally constructing radial basis function networks , a class of neural networks which looks more suitable for adtaptive control applications than the more popular backpropagation networks . the first algorithm has been derived by a previous method developed by fritzke , while the second one has been inspired by the cart algorithm developed by breiman for generation regression trees . both algorithms proved to work well on a number of tests and exhibit comparable performances . an evaluation on the standard case study of the mackey - glass temporal series is reported .
number of prototypes used to represent each class , the position of each prototype within its class and the membership function associated with each prototype . this paper proposes a novel , evolutionary approach to data clustering and classification which overcomes many of the limitations of traditional systems . the approach rests on the optimisation of both the number and positions of fuzzy prototypes using a real - valued genetic algorithm ( ga ) . because the ga acts on all of the classes at once , the system benefits naturally from global information about possible class interactions . in addition , the concept of a receptive field for each prototype is used to replace the classical distance - based membership function by an infinite fuzzy support , multidimensional , gaussian function centred over the prototype and with unique variance in each dimension , reflecting the tightness of the cluster . hence , the notion of nearest - neighbour is replaced by that of nearest attracting prototype ( nap ) . the proposed model is a completely self - optimising , fuzzy system called ga - nap . most data clustering algorithms , including the popular k - means algorithm , require a priori knowledge about the problem domain to fix the number and starting positions of the prototypes . although such knowledge may be assumed for domains whose dimensionality is fairly small or whose underlying structure is relatively intuitive , it is clearly much less accessible in hyper - dimensional settings , where the number of input parameters may be very large . classical systems also suffer from the fact that they can only define clusters for one class at a time . hence , no account is made of potential interactions among classes . these drawbacks are further compounded by the fact that the ensuing classification is typically based on a fixed , distance - based membership function for all prototypes . this paper proposes a novel approach to data clustering and classification which overcomes the aforementioned limitations of traditional systems . the model is based on the genetic evolution of fuzzy prototypes . a real - valued genetic algorithm ( ga ) is used to optimise both the number and positions of prototypes . because the ga acts on all of the classes at once and measures fitness as classification accuracy , the system naturally profits from global information about class interaction . the concept of a receptive field for each prototype is also presented and used to replace the classical , fixed distance - based function by an infinite fuzzy support membership function . the new membership function is inspired by that used in the hidden layer of rbf networks . it consists of a multidimensional gaussian function centred over the prototype and with a unique variance in each dimension that reflects the tightness of the cluster . during classification , the notion of nearest - neighbour is replaced by that of nearest attracting prototype ( nap ) . the proposed model is a completely self - optimising , fuzzy system called ga - nap .
in this paper we study the performance of gradient descent when applied to the problem of on - line linear prediction in arbitrary inner product spaces . we show worst - case bounds on the sum of the squared prediction errors under various assumptions concerning the amount of a priori information about the sequence to predict . the algorithms we use are variants and extensions of on - line gradient descent . whereas our algorithms always predict using linear functions as hypotheses , none of our results requires the data to be linearly related . in fact , the bounds proved on the total prediction loss are typically expressed as a function of the total loss of the best fixed linear predictor with bounded norm . all the upper bounds are tight to within constants . matching lower bounds are provided in some cases . finally , we apply our results to the problem of on - line prediction for classes of smooth functions .
we study the on - line learning of classes of functions of a single real variable formed through bounds on various norms of functions ' derivatives . we determine the best bounds obtainable on the worst - case sum of squared errors ( also " absolute " errors ) for several such classes . we prove upper bounds for these classes of smooth functions for other loss functions , and prove upper and lower bounds in terms of the number of trials .
nearest - neighbor algorithms are known to depend heavily on their distance metric . in this paper , we investigate the use of a weighted euclidean metric in which the weight for each feature comes from a small set of options . we describe diet , an algorithm that directs search through a space of discrete weights using cross - validation error as its evaluation function . although a large set of possible weights can reduce the learner ' s bias , it can also lead to increased variance and overfitting . our empirical study shows that , for many data sets , there is an advantage to weighting features , but that increasing the number of possible weights beyond two ( zero and one ) has very little benefit and sometimes degrades performance .
in the context of machine learning from examples this paper deals with the problem of estimating the quality of attributes with and without dependencies among them . kira and rendell ( #NUM# a , b ) developed an algorithm called relief , which was shown to be very efficient in estimating attributes . original relief can deal with discrete and continuous attributes and is limited to only two - class problems . in this paper relief is analysed and extended to deal with noisy , incomplete , and multi - class data sets . the extensions are verified on various artificial and one well known real - world problem .
in this paper we present an average - case analysis of the nearest neighbor algorithm , a simple induction method that has been studied by many researchers . our analysis assumes a conjunctive target concept , noise - free boolean attributes , and a uniform distribution over the instance space . we calculate the probability that the algorithm will encounter a test instance that is distance d from the prototype of the concept , along with the probability that the nearest stored training case is distance e from this test instance . from this we compute the probability of correct classification as a function of the number of observed training cases , the number of relevant attributes , and the number of irrelevant attributes . we also explore the behavioral implications of the analysis by presenting predicted learning curves for artificial domains , and give experimental results on these domains as a check on our reasoning .
in order to better understand life , it is helpful to look beyond the envelop of life as we know it . a simple model of coevolution was implemented with the addition of genes for longevity and mutation rate in the individuals . this made it possible for a lineage to evolve to be immortal . it also allowed the evolution of no mutation or extremely high mutation rates . the model shows that when the individuals interact in a sort of zero - sum game , the lineages maintain relatively high mutation rates . however , when individuals engage in interactions that have greater consequences for one individual in the interaction than the other , lineages tend to evolve relatively low mutation rates . this model suggests that different genes may have evolved different mutation rates as adaptations to the varying pressures of interactions with other genes .
difficult . we face this problem using an architecture based on learning classifier systems and on the description of the learning technique used and of the organizational structure proposed , we present experiments that show how behaviour acquisition can be achieved . our simulated robot learns to structural properties of animal behavioural organization , as proposed by ethologists . after a
in this paper we present a probabilistic formalization of the instance - based learning approach . in our bayesian framework , moving from the construction of an explicit hypothesis to a data - driven instance - based learning approach , is equivalent to averaging over all the ( possibly infinitely many ) individual models . the general bayesian instance - based learning framework described in this paper can be applied with any set of assumptions defining a parametric model family , and to any discrete prediction task where the number of simultaneously predicted attributes is small , which includes for example all classification tasks prevalent in the machine learning literature . to illustrate the use of the suggested general framework in practice , we show how the approach can be implemented in the special case with the strong independence assumptions underlying the so called naive bayes classifier . the resulting bayesian instance - based classifier is validated empirically with public domain data sets and the results are compared to the performance of the traditional naive bayes classifier . the results suggest that the bayesian instance - based learning approach yields better results than the traditional naive bayes classifier , especially in cases where the amount of the training data is small .
we present a comparative study of genetic algorithms and their search properties when treated as a combinatorial optimization technique . this is done in the context of the np - hard problem max - sat , the comparison being relative to the metropolis process , and by extension , simulated annealing . our contribution is two - fold . first , we show that for large and difficult max - sat instances , the contribution of cross - over to the search process is marginal . little is lost if it is dispensed altogether , running mutation and selection as an enlarged metropolis process . second , we show that for these problem instances , genetic search consistently performs worse than simulated annealing when subject to similar resource bounds . the correspondence between the two algorithms is made more precise via a decomposition argument , and provides a framework for interpreting our results .
in constructive induction ( ci ) , the learner ' s problem representation is modified as a normal part of the learning process . this may be necessary if the initial representation is inadequate or inappropriate . however , the distinction between constructive and non - constructive methods appears to be highly ambiguous . several conventional definitions of the process of constructive induction appear to include all conceivable learning processes . in this paper i argue that the process of constructive learning should be identified with that of relational learning .
probabilistic models have recently been utilized for the optimization of large combinatorial search problems . however , complex probabilistic models that attempt to capture inter - parameter dependencies can have prohibitive computational costs . the algorithm presented in this paper , termed comit , provides a method for using probabilistic models in conjunction with fast search techniques . we show how comit can be used with two very different fast search algorithms : hillclimbing and population - based incremental learning ( pbil ) . the resulting algorithms maintain many of the benefits of probabilistic modeling , with far less computational expense . extensive empirical results are provided ; comit has been successfully applied to jobshop scheduling , traveling salesman , and knapsack problems . this paper also presents a review of probabilistic modeling for combi natorial optimization .
current systems in the field of inductive logic programming ( ilp ) use , primarily for the sake of efficiency , heuristically guided search techniques . such greedy algorithms suffer from local optimization problem . present paper describes a system named sfoil , that tries to alleviate this problem by using a stochastic search method , based on a generalization of simulated annealing , called markovian neural network . various tests were performed on benchmark , and real - world domains . the results show both , advantages and weaknesses of stochastic approach .
in many optimization problems , the structure of solutions reflects complex relationships between the different input parameters . for example , experience may tell us that certain parameters are closely related and should not be explored independently . similarly , experience may establish that a subset of parameters must take on particular values . any search of the cost landscape should take advantage of these relationships . we present mimic , a framework in which we analyze the global structure of the optimization landscape . a novel and efficient algorithm for the estimation of this structure is derived . we use knowledge of this structure to guide a randomized search through the solution space and , in turn , to refine our estimate of the structure . our technique obtains significant speed gains over other randomized optimization procedures .
we analyze the generalization behavior of the xcs classifier system in environments in which only a few generalizations can be done . experimental results presented in the paper evidence that the generalization mechanism of xcs can prevent it from learning even simple tasks in such environments . we present a new operator , named specify , which contributes to the solution of this problem . xcs with the specify operator , named xcss , is compared to xcs in terms of performance and generalization capabilities in different types of environments . experimental results show that xcss can deal with a greater variety of environments and that it is more robust than xcs with respect to population size .
in this paper , we present a computationally efficient method for inducing selective bayesian network classifiers . our approach is to use information - theoretic metrics to efficiently select a subset of attributes from which to learn the classifier . we explore three conditional , information - theoretic met - rics that are extensions of metrics used extensively in decision tree learning , namely quin - lan ' s gain and gain ratio metrics and man - taras ' s distance metric . we experimentally show that the algorithms based on gain ratio and distance metric learn selective bayesian networks that have predictive accuracies as good as or better than those learned by existing selective bayesian network induction approaches ( k #NUM# - as ) , but at a significantly lower computational cost . we prove that the subset - selection phase of these information - based algorithms has polynomial complexity , as compared to the worst - case exponential time complexity of the corresponding phase in k #NUM# - as .
the effectiveness of a case - based reasoning system is known to depend critically on its similarity measure . however , it is not clear whether there are elusive and esoteric similarity measures which might improve the performance of a case - based reasoner if substituted for the more commonly used measures . this paper therefore deals with the problem of choosing the best similarity measure , in the limited context of instance - based learning of classifications of a discrete example space . we consider both ` fixed ' similarity measures and ` learnt ' ones . in the former case , we give a definition of a similarity measure which we believe to be ` optimal ' w . r . t . the current prior distribution of target concepts and prove its optimality within a restricted class of similarity measures . we then show how this ` optimal ' similarity measure is instantiated by some specific prior distributions , and conclude that a very simple similarity measure is as good as any other in these cases . in a further section , we then show how our definition leads naturally to a conjecture about the
multi - armed bandits may be viewed as decompositionally - structured markov decision processes ( mdp ' s ) with potentially very large state sets . a particularly elegant methodology for computing optimal policies was developed over twenty ago by gittins [ gittins & jones , #NUM# ] . gittins ' approach reduces the problem of finding optimal policies for the original mdp to a sequence of low - dimensional stopping problems whose solutions determine the optimal policy through the so - called " gittins indices . " katehakis and veinott [ katehakis & veinott , #NUM# ] have shown that the gittins index for a task in state i may be interpreted as a particular component of the maximum - value function associated with the " restart - in - i " process , a simple mdp to which standard solution methods for computing optimal policies , such as successive approximation , apply . this paper explores the problem of learning the gittins indices on - line without the aid of a process model ; it suggests utilizing task - state - specific q - learning agents to solve their respective restart - in - state - i subproblems , and includes an example in which the online reinforcement learning approach is applied to a simple problem of stochastic scheduling | one instance drawn from a wide class of problems that may be formulated as bandit problems .
we analyze the performance of top - down algorithms for decision tree learning , such as those employed by the widely used c #NUM# . #NUM# and cart software packages . our main result is a proof that such algorithms are boosting algorithms . by this we mean that if the functions that label the internal nodes of the decision tree can weakly approximate the unknown target function , then the top - down algorithms we study will amplify this weak advantage to build a tree achieving any desired level of accuracy . the bounds we obtain for this amplification show an interesting dependence on the splitting criterion used by the top - down algorithm . more precisely , if the functions used to label the internal nodes have error #NUM# = #NUM# fl as approximations to the target function , then for the splitting criteria used by cart and c #NUM# . #NUM# , trees of size ( #NUM# = * ) o ( #NUM# = fl #NUM# * #NUM# ) and ( #NUM# = * ) o ( log ( #NUM# = * ) = fl #NUM# ) ( respectively ) suffice to drive the error below * . thus ( for example ) , a small constant advantage over random guessing is amplified to any larger constant advantage with trees of constant size . for a new splitting criterion suggested by our analysis , the much stronger
the paper describes a counter example to the hypothesis which states that a greedy decision tree generation algorithm that constructs binary decision trees and branches on a single attribute - value pair rather than on all values of the selected attribute will always lead to a tree with fewer leaves for any given training set . we show also that relieff is less myopic than other impurity functions and that it enables the induction algorithm that generates binary decision trees to reconstruct optimal ( the smallest ) decision trees in more cases .
real - world problems are often too difficult to be solved by a single monolithic system . there are many examples of natural and artificial systems which show that a modular approach can reduce the total complexity of the system while solving a difficult problem satisfactorily . the success of modular artificial neural networks in speech and image processing is a typical example . however , designing a modular system is a difficult task . it relies heavily on human experts and prior knowledge about the problem . there is no systematic and automatic way to form a modular system for a problem . this paper proposes a novel evolutionary learning approach to designing a modular system automatically , without human intervention . our starting point is speciation , using a technique based on fitness sharing . while speciation in genetic algorithms is not new , no effort has been made towards using a speciated population as a complete modular system . we harness the specialized expertise in the species of an entire population , rather than a single individual , by introducing a gating algorithm . we demonstrate our approach to automatic modularization by improving co - evolutionary game learning . following earlier researchers , we learn to play iterated prisoner ' s dilemma . we review some problems of earlier co - evolutionary learning , and explain their poor generalization ability and sudden mass extinctions . the generalization ability of our approach is significantly better than past efforts . using the specialized expertise of the entire speciated population though a gating algorithm , instead of the best individual , is the main contributor to this improvement .
in this paper we describe an approach to representing , using , and improving sensory skills for physical domains . we present icarus , an architecture that represents control knowledge in terms of durative states and sequences of such states . the system operates in cycles , activating a state that matches the environmental situation and letting that state control behavior until its conditions fail or until finding another matching state with higher priority . information about the probability that conditions will remain satisfied minimizes demands on sensing , as does knowledge about the durations of states and their likely successors . three statistical learning methods let the system gradually reduce sensory load as it gains experience in a domain . we report experimental evaluations of this ability on three simulated physical tasks : flying an aircraft , steering a truck , and balancing a pole . our experiments include lesion studies that identify the reduction in sensing due to each of the learning mechanisms and others that examine the effect of domain characteristics .
we follow axelrod [ #NUM# ] in using the genetic algorithm to play iterated prisoner ' s dilemma . each member of the population ( i . e . , each strategy ) is evaluated by how it performs against the other members of the current population . this creates a dynamic environment in which the algorithm is optimising to a moving target instead of the usual evaluation against some fixed set of strategies , causing an " arms race " of innovation [ #NUM# ] . we conduct two sets of experiments . the first set investigates what conditions evolve the best strategies . the second set studies the robustness of the strategies thus evolved , that is , are the strategies useful only in the round robin of its population or are they effective against a wide variety of opponents ? our results indicate that the population has nearly always converged by about #NUM# generations , by which time the bias in the population has almost always stabilised at #NUM# % . our results confirm that cooperation almost always becomes the dominant strategy [ #NUM# , #NUM# ] . we can also confirm that seeding the population with expert strategies is best done in small amounts so as to leave the initial population with plenty of genetic diversity [ #NUM# ] . the lack of robustness in strategies produced in the round robin evaluation is demonstrated by some examples of a population of nave cooperators being exploited by a defect - first strategy . this causes a sudden but ephemeral decline in the population ' s average score , but it recovers when less nave cooperators emerge and do well against the exploiting strategies . this example of runaway evolution is brought back to reality by a suitable mutation , reminiscent of punctuated equilibria [ #NUM# ] . we find that a way to reduce such navity is to make the ga population play against an extra ,
an unsupervised learning algorithm for a multilayer network of stochastic neurons is described . bottom - up recognition connections convert the input into representations in successive hidden layers and top - down generative connections reconstruct the representation in one layer from the representation in the layer above . in the wake phase , neurons are driven by recognition connections , and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below . in the sleep phase , neurons are driven by generative connections and recognition connections are adapted to increase the probability that they would produce supervised learning algorithms for multilayer neural networks face two problems : they require a teacher to specify the desired output of the network and they require some method of communicating error information to all of the connections . the wake - sleep algorithm avoids both these problems . when there is no external teaching signal to be matched , some other goal is required to force the hidden units to extract underlying structure . in the wake - sleep algorithm the goal is to learn representations that are economical to describe but allow the input to be reconstructed accurately . we can quantify this goal by imagining a communication game in which each vector of raw sensory inputs is communicated to a receiver by first sending its hidden representation and then sending the difference between the input vector and its top - down reconstruction from the hidden representation . the aim of learning is to minimize the description length which is the total number of bits that would be required to communicate the input vectors in this way [ #NUM# ] . no communication actually takes place , but minimizing the description length that would be required forces the network to learn economical representations that capture the underlying regularities in the data [ #NUM# ] . the correct activity vector in the layer above .
although recurrent neural nets have been moderately successful in learning to emulate finite - state machines ( fsms ) , the continuous internal state dynamics of a neural net are not well matched to the discrete behavior of an fsm . we describe an architecture , called dolce , that allows discrete states to evolve in a net as learning progresses . dolce consists of a standard recurrent neural net trained by gradient descent and an adaptive clustering technique that quantizes the state space . dolce is based on the assumption that a finite set of discrete internal states is required for the task , and that the actual network state belongs to this set but has been corrupted by noise due to inaccuracy in the weights . dolce learns to recover the discrete state with maximum a posteriori probability from the noisy state . simulations show that dolce leads to a significant improvement in generalization performance over earlier neural net approaches to fsm induction .
we propose a statistical mechanical framework for the modeling of discrete time series . maximum likelihood estimation is done via boltzmann learning in one - dimensional networks with tied weights . we call these networks boltzmann chains and show that they contain hidden markov models ( hmms ) as a special case . our framework also motivates new architectures that address particular shortcomings of hmms . we look at two such architectures : parallel chains that model feature sets with disparate time scales , and looped networks that model long - term dependencies between hidden states . for these networks , we show how to implement the boltzmann learning rule exactly , in polynomial time , without resort to simulated or mean - field annealing . the necessary computations are done by exact decimation procedures from statistical mechanics .
vector quantization is a lossy coding technique for encoding a set of vectors from different sources such as image and speech . the design of vector quantizers that yields the lowest distortion is one of the most challenging problems in the field of source coding . however , this problem is known to be difficult [ #NUM# ] . the conventional solution technique works through a process of iterative refinements which yield only locally optimal results . in this paper , we design and evaluate three versions of genetic algorithms for computing vector quantizers . our preliminary study with gaussian - markov sources showed that the genetic approach outperforms the conventional technique in most cases .
we discuss an approach to constructing composite features during the induction of decision trees . the composite features correspond to m - of - n concepts . there are three goals of this research . first , we explore a family of greedy methods for building m - of - n concepts ( one of which , gs , is described in this paper ) . second , we show how these concepts can be formed as internal nodes of decision trees , serving as a bias to the learner . finally , we evaluate the method on several artificially generated and naturally occurring data sets to determine the effects of this bias .
we present a new approach , called first order regression ( for ) , to handling numerical information in inductive logic programming ( ilp ) . for is a combination of ilp and numerical regression . first - order logic descriptions are induced to carve out those subspaces that are amenable to numerical regression among real - valued variables . the program fors is an implementation of this idea , where numerical regression is focused on a distinguished continuous argument of the target predicate . we show that this can be viewed as a generalisation of the usual ilp problem . applications of fors on several real - world data sets are described : the prediction of mutagenicity of chemicals , the modelling of liquid dynamics in a surge tank , predicting the roughness in steel grinding , finite element mesh design , and operator ' s skill reconstruction in electric discharge machining . a comparison of fors ' performance with previous results in these domains indicates that fors is an effective tool for ilp applications that involve numerical data .
this paper identifies goal handling processes that begin to account for the kind of processes involved in invention . we identify new kinds of goals with special properties and mechanisms for processing such goals , as well as means of integrating opportunism , deliberation , and social interaction into goal / plan processes . we focus on invention goals , which address significant enterprises associated with an inventor . invention goals represent seed goals of an expert , around which the whole knowledge of an expert gets reorganized and grows more or less opportunistically . invention goals reflect the idiosyncrasy of thematic goals among experts . they constantly increase the sensitivity of individuals for particular events that might contribute to their satisfaction . our exploration is based on a well - documented example : the invention of the telephone by alexander graham bell . we propose mechanisms to explain : ( #NUM# ) how bell ' s early thematic goals gave rise to the new goals to invent the multiple telegraph and the telephone , and ( #NUM# ) how the new goals interacted opportunistically . finally , we describe our computational model , alec , that accounts for the role of goals in invention .
in order to better understand life , it is helpful to look beyond the envelop of life as we know it . a simple model of coevolution was implemented with the addition of genes for longevity and mutation rate in the individuals . this made it possible for a lineage to evolve to be immortal . it also allowed the evolution of no mutation or extremely high mutation rates . the model shows that when the individuals interact in a sort of zero - sum game , the lineages maintain relatively high mutation rates . however , when individuals engage in interactions that have greater consequences for one individual in the interaction than the other , lineages tend to evolve relatively low mutation rates . this model suggests that different genes may have evolved different mutation rates as adaptations to the varying pressures of interactions with other genes .
in many learning tasks , data - query is neither free nor of constant cost . often the cost of a query depends on the distance from the current location in state space to the desired query point . this is easiest to visualize in robotics environments where a robot must physically move to a location in order to learn something there . the cost of this learning is the time and effort it takes to reach the new location . furthermore , this cost is characterized by a distance relationship : when the robot moves as directly as possible from a source state to a destination state , the states through which it passes are closer ( i . e . , cheaper to reach ) than is the destination state . distance relationships hold in many real - world non - robotics tasks also | any environment where states are not immediately accessible . optimizing the performance of a chemical plant , for example , requires the adjustment of analog controls which have a continuum of intermediate states . querying possibly optimal regions of state space in these environments is inadvisable if the path to the query point intersects a region of known volatility . in discrete environments with small numbers of states , it ' s possible to keep track of precisely where and to what degree learning has already been done sufficiently and where it still needs to be done . it is also possible to keep best known estimates of the distances from each state to each other ( see kaelbling , #NUM# ) . kael - bling ' s dg - learning algorithm is based on floyd ' s all - pairs shortest - path algorithm ( aho , hopcroft , & ull - man #NUM# ) and is just slightly different from that used here . these " all - goals " algorithms ( after kaelbling ) can provide a highly satisfying representation of the distance / benefit tradeoff . where e x is the exploration value of state x ( the potential benefit of exploring state x ) , d xy is the distance to state y , and a xy is the action to take in state x to move most cheaply to state y . this information can be learned incrementally and completely : that is , it can be guaranteed that if a path from any state x to any state y is deducible from the state transitions seen so far , then ( #NUM# ) the algorithm will have a non - null entry for s xy ( i . e . , the algorithm will know a path from x to y ) , and ( #NUM# ) the current value for d xy will be the best deducible value from all data seen so far . with this information , decisions about which areas to explore next can be based on not just the amount to be gained from such exploration but also on the cost of reaching each area together with the benefit of incidental exploration done on the way . though optimal exploration is np - hard ( i . e . , it ' s at least as difficult as tsp ) good approximations are easily computable . one such good approximation is to take the action at each state that leads in the direction of greatest accumulated exploration benefit :
we examine the representational capabilities of first - order and second - order single layer recurrent neural networks ( slrnns ) with hard - limiting neurons . we show that a second - order slrnn is strictly more powerful than a first - order slrnn . however , if the first - order slrnn is augmented with output layers of feedforward neurons , it can implement any finite - state recognizer , but only if state - splitting is employed . when a state is split , it is divided into two equivalent states . the judicious use of state - splitting allows for efficient implementation of finite - state recognizers using augmented first - order slrnns .
learning the past tense of english verbs | a seemingly minor aspect of language acquisition | has generated heated debates since #NUM# , and has become a landmark task for testing the adequacy of cognitive modeling . several artificial neural networks ( anns ) have been implemented , and a challenge for better symbolic models has been posed . in this paper , we present a general - purpose symbolic pattern associator ( spa ) based upon the decision - tree learning algorithm id #NUM# . we conduct extensive head - to - head comparisons on the generalization ability between ann models and the spa under different representations . we conclude that the spa generalizes the past tense of unseen verbs better than ann models by a wide margin , and we offer insights as to why this should be the case . we also discuss a new default strategy for decision - tree learning algorithms .
as probabilistic systems gain popularity and are coming into wider use , the need for a mechanism that explains the system ' s findings and recommendations becomes more critical . the system will also need a mechanism for ordering competing explanations . we examine two representative approaches to explanation in the literature one due to g ardenfors and one due to pearland show that both suffer from significant problems . we propose an approach to defining a notion of better explanation that combines some of the features of both together with more recent work by pearl and others on causality .
a cooperative coevolutionary approach to learning complex structures is presented which , although preliminary in nature , appears to have a number of advantages over non - coevolutionary approaches . the cooperative coevolutionary approach encourages the parallel evolution of substructures which interact in useful ways to form more complex higher level structures . the architecture is designed to be general enough to permit the inclusion , if appropriate , of a priori knowledge in the form of initial biases towards particular kinds of decompositions . a brief summary of initial results obtained from testing this architecture in several problem domains is presented which shows a significant speedup over more traditional non - coevolutionary approaches .
an intelligent system must be able to adapt and learn to correct and update its model of the environment incrementally and deliberately . in complex environments that have many parameters and where interactions have a cost , sampling the possible range of states to test the results of action executions is not a practical approach . we present a practical approach based on continuous and selective interaction with the environment that pinpoints the type of fault in the domain knowledge that causes any unexpected behavior of the environment , and resorts to experimentation when additional information is needed to correct the system ' s knowledge .
determining the architecture of a neural network is an important issue for any learning task . for recurrent neural networks no general methods exist that permit the estimation of the number of layers of hidden neurons , the size of layers or the number of weights . we present a simple pruning heuristic which significantly improves the generalization performance of trained recurrent networks . we illustrate this heuristic by training a fully recurrent neural network on positive and negative strings of a regular grammar . we also show that if rules are extracted from networks trained to recognize these strings , that rules extracted after pruning are more consistent with the rules to be learned . this performance improvement is obtained by pruning and retraining the networks . simulations are shown for training and pruning a recurrent neural net on strings generated by two regular grammars , a randomly - generated #NUM# - state grammar and an #NUM# - state triple parity grammar . further simulations indicate that this pruning method can gives generalization performance superior to that obtained by training with weight decay .
we investigate the structure of model selection problems via the bias / variance decomposition . in particular , we characterize the essential aspects of a model selection task by the bias and variance profiles it generates over the sequence of hypothesis classes . with this view , we develop a new understanding of complexity - penalization methods : first , the penalty terms can be interpreted as postulating a particular profile for the variances as a function of model complexityif the postulated and true profiles do not match , then systematic under - fitting or over - fitting results , depending on whether the penalty terms are too large or too small . second , we observe that it is generally best to penalize according to the true variances of the task , and therefore no fixed penalization strategy is optimal across all problems . we then use this characterization to introduce the notion of easy versus hard model selection problems . here we show that if the variance profile grows too rapidly in relation to the biases , then standard model selection techniques become prone to significant errors . this can happen , for example , in regression problems where the independent variables are drawn from wide - tailed distributions . to counter this , we discuss a new model selection strategy that dramatically outperforms standard complexity - penalization and hold - out meth ods on these hard tasks .
we consider the problem of how to combine a collection of general regression fit vectors in order to obtain a better predictive model . the individual fits may be from subset linear regression , ridge regression , or something more complex like a neural network . we develop a general framework for this problem and examine a recent cross - validation - based proposal called " stacking " in this context . combination methods based on the bootstrap and analytic methods are also derived and compared in a number of examples , including best subsets regression and regression trees . finally , we apply these ideas to classification problems where the estimated combination weights can yield insight into the structure of the problem .
we compare the performance of several machine learning algorithms in the problem of prognos - tics of the femoral neck fracture recovery : the k - nearest neighbours algorithm , the semi - naive bayesian classifier , backpropagation with weight elimination learning of the multilayered neural networks , the lfc ( lookahead feature construction ) algorithm , and the assistant - i and assistant - r algorithms for top down induction of decision trees using information gain and relieff as search heuristics , respectively . we compare the prognostic accuracy and the explanation ability of different classifiers . among the different algorithms the semi - naive bayesian classifier and assistant - r seem to be the most appropriate . we analyze the combination of decisions of several classifiers for solving prediction problems and show that the combined classifier improves both performance and the explanation ability .
parallel genetic algorithms have often been reported to yield better performance than genetic algorithms which use a single large panmictic population . in the case of the island model genetic algorithm , it has been informally argued that having multiple subpopulations helps to preserve genetic diversity , since each island can potentially follow a different search trajectory through the search space . on the other hand , linearly separable functions have often been used to test island model genetic algorithms ; it is possible that island models are particular well suited to separable problems . we look at how island models can track multiple search trajectories using the infinite population models of the simple genetic algorithm . we also introduce a simple model for better understanding when island model genetic algorithms may have an advantage when processing linearly separable problems .
bootstrap samples with noise are shown to be an effective smoothness and capacity control technique for training feed - forward networks and for other statistical methods such as generalized additive models . it is shown that noisy bootstrap performs best in conjunction with weight decay regularization and ensemble averaging . the two - spiral problem , a highly non - linear noise - free data , is used to demonstrate these findings . the combination of noisy bootstrap and ensemble averaging is also shown useful for generalized additive modeling , and is also demonstrated on the well known cleveland heart data [ #NUM# ] .
new approaches to prior specification and structuring in autoregressive time series models are introduced and developed . we focus on defining classes of prior distributions for parameters and latent variables related to latent components of an autoregressive model for an observed time series . these new priors naturally permit the incorporation of both qualitative and quantitative prior information about the number and relative importance of physically meaningful components that represent low frequency trends , quasi - periodic sub - processes , and high frequency residual noise components of observed series . the class of priors also naturally incorporates uncertainty about model order , and hence leads in posterior analysis to model order assessment and resulting posterior and predictive inferences that incorporate full uncertainties about model order as well as model parameters . analysis also formally incorporates uncertainty , and leads to inferences about , unknown initial values of the time series , as it does for predictions of future values . posterior analysis involves easily implemented iterative simulation methods , developed and described here . one motivating applied field is climatology , where the evaluation of latent structure , especially quasi - periodic structure , is of critical importance in connection with issues of global climatic variability . we explore analysis of data from the southern oscillation index ( soi ) , one of several series that has been central in recent high - profile debates in the atmospheric sciences about recent apparent trends in climatic indicators .
summary we detail and illustrate time series analysis and spectral inference in autoregressive models with a focus on the underlying latent structure and time series decompositions . a novel class of priors on parameters of latent components leads to a new class of smoothness priors on autoregressive coefficients , provides for formal inference on model order , including very high order models , and leads to the incorporation of uncertainty about model order into summary inferences . the class of prior models also allows for subsets of unit roots , and hence leads to inference on sustained though stochastically time - varying periodicities in time series . applications to analysis of the frequency composition of time series , in both time and spectral domains , is illustrated in a study of a time series from astronomy . this analyses demonstrates the impact and utility of the new class of priors in addressing model order uncertainty and in allowing for unit root structure . time domain decomposition of a time series into estimated latent components provides an important alternative view of the component spectral characteristics of a series . in addition , our data analysis illustrates the utility of the smoothness prior and allowance for unit root structure in inference about spectral densities . in particular , the framework overcomes supposed problems in spectral estimation with autoregressive models using more traditional model fitting methods .
this paper discusses a method for training multilayer perceptron networks called dmp #NUM# ( dynamic multilayer perceptron #NUM# ) . the method is based upon a divide and conquer approach which builds networks in the form of binary trees , dynamically allocating nodes and layers as needed . the focus of this paper is on the effects of using multiple node types within the dmp framework . simulation results show that dmp #NUM# performs favorably in comparison with other learning algorithms , and that using multiple node types can be beneficial to network performance .
neurodraughts is a draughts playing program similar in approach to neurogammon and neurochess [ tesauro , #NUM# , thrun , #NUM# ] . it uses an artificial neural network trained by the method of temporal difference learning to learn by self - play how to play the game of draughts . this paper discusses the relative contribution of board representation , search depth , training regime , architecture and run time parameters to the strength of the tdplayer produced by the system . keywords : temporal difference learning , input representation , search , draughts .
in the last decade research in machine learning has developed a variety of powerful tools for inductive learning and data analysis . on the other hand , research in international relations has developed a variety of different conflict databases that are mostly analyzed with classical statistical methods . as these databases are in general of a symbolic nature , they provide an interesting domain for application of machine learning algorithms . this paper gives a short overview of available conflict databases and subsequently concentrates on the application of machine learning methods for the analysis and interpretation of such databases .
in this survey , we review work in machine learning on methods for handling data sets containing large amounts of irrelevant information . we focus on two key issues : the problem of selecting relevant features , and the problem of selecting relevant examples . we describe the advances that have been made on these topics in both empirical and theoretical work in machine learning , and we present a general framework that we use to compare different methods . we close with some challenges for future work in this area .
we describe and illustrate bayesian approaches to modelling and analysis of multiple non - stationary time series . this begins with uni - variate models for collections of related time series assumedly driven by underlying but unobservable processes , referred to as dynamic latent factor processes . we focus on models in which the factor processes , and hence the observed time series , are modelled by time - varying autoregressions capable of flexibly representing ranges of observed non - stationary characteristics . we highlight concepts and new methods of time series decomposition to infer characteristics of latent components in time series , and relate uni - variate decomposition analyses to underlying multivariate dynamic factor structure . our motivating application is in analysis of multiple eeg traces from an ongoing eeg study at duke . in this study , individuals undergoing ect therapy generate multiple eeg traces at various scalp locations , and physiological interest lies in identifying dependencies and dissimilarities across series . in addition to the multivariate and non - stationary aspects of the series , this area provides illustration of the new results about decomposition of time series into latent , physically interpretable components ; this is illustrated in data analysis of one eeg data set . the paper also discusses current and future research directions .
the - subsumption problem is crucial to the efficiency of ilp learning systems . we discuss two - subsumption algorithms based on strategies for preselecting suitable matching literals . the class of clauses , for which subsumption becomes polynomial , is a superset of the deterministic clauses . we further map the general problem of - subsumption to a certain problem of finding a clique of fixed size in a graph , and in return show that a specialization of the pruning strategy of the car - raghan and pardalos clique algorithm provides a dramatic reduction of the subsumption search space . we also present empirical results for the mesh design data set .
case - based planning involves storing individual instances of problem - solving episodes and using them to tackle new planning problems . this paper is concerned with derivation replay , which is the main component of a form of case - based planning called derivational analogy ( da ) . prior to this study , implementations of derivation replay have been based within state - space planning . we are motivated by the acknowledged superiority of partial - order ( po ) planners in plan generation . here we demonstrate that plan - space planning also has an advantage in replay . we will argue that the decoupling of planning ( derivation ) order and the execution order of plan steps , provided by partial - order planners , enables them to exploit the guidance of previous cases in a more efficient and straightforward fashion . we validate our hypothesis through a focused empirical comparison .
it is a well - known fact that propositional learning algorithms require " good " features to perform well in practice . so a major step in data engineering for inductive learning is the construction of good features by domain experts . these features often represent properties of structured objects , where a property typically is the occurrence of a certain substructure having certain properties . to partly automate the process of " feature engineering " , we devised an algorithm that searches for features which are defined by such substructures . the algorithm stochastically conducts a top - down search for first - order clauses , where each clause represents a binary feature . it differs from existing algorithms in that its search is not class - blind , and that it is capable of considering clauses ( " context " ) of almost arbitrary length ( size ) . preliminary experiments are favorable , and support the view that this approach is promising .
automating the learning of causal models from sample data is a key step toward incorporating machine learning in the automation of decision - making and reasoning under uncertainty . this paper presents a bayesian approach to the discovery of causal models , using a minimum message length ( mml ) method . we have developed encoding and search methods for discovering linear causal models . the initial experimental results presented in this paper show that the mml induction approach can recover causal models from generated data which are quite accurate reflections of the original models ; our results compare favorably with those of the tetrad ii program of spirtes et al . [ #NUM# ] even when their algorithm is supplied with prior temporal information and mml is not .
we have previously used grammars as a formalism to structure a ga ' s search for program called sorting networks ( snets ) [ kbw #NUM# ] . in this paper we restrict ourselves to stochastic context - free grammars which , while more analytically tracxtable than our snet grammars , are more difficult than others previously considered by the ga community . in our approach : production rules of a grammar are encoded as genes of a genome ; this grammar is used as a recognizer of strings and assigned a fitness measure that reflects the probability that it captures the structure of a restricted sample of strings generated by a stochastic target language . our ga introduces a novel encoding of grammars as genotypic strings , and uses a local search component to aid in learning rule probabilities . both fitness evaluation and the local search algorithm depend on a sophisticated chart parser . we give results for two simple grammars whose nonstochastic equivalents have been used in a previous study . we also present arguments about the degree of testing needed for ga based grammar induction . genetic algorithms ( gas ) are now regularly being applied to much more difficult problems than originally conceived . much of this advance has been towards extending ga performance on classic function optimization problems , for example on problem instances with very difficult or even perverse ga - hard " characteristics , on problems of much higher dimensionality than have been attempted in the past , etc . this paper is part of a second extension of ga theory , to problems outside the standard function optimization formulation . most ambitiously , we seek a way of evolving arbitrary programs : functions that produce appropriate outputs for all possible inputs . recent empirical results arising from lisp - oriented genetic programming and related techniques have made this prospect much more tantalizing than we might have imagined even a few years ago . a fundamental obstacle to theoretical progress on the evolution of programs is due to the enormous variability introduced by the programs depenendence on its input . for most problems of practical interest , the space of possible inputs is infinite , or at least too large to be exhaustively tested as part of each individual ' s fitness evaluation . we have previously used grammars as a formalism to structure a ga ' s search for a particular class of program called sorting networks ( snets ) [ kbw #NUM# ] . briefly , production rules of a grammar are encoded as genes of a genome ; this grammar is used as a generator of a string which can be interpreted as an snet specification ; the snet is then tested against a small set of input vectors ; and finally the individual ' s fitness measures how successfully it has sorted them . while the goal of evolving a sorting program is obviously much more restricted than a search for an arbitrary function , it does retain the characteristic that the solution must sort correctly for any input vector , despite the fact that this space of potential inputs is exponentially large . in this paper we restrict ourselves still further , to much simpler but analytically tractable grammars . our goal is to apply the enormous background of research regarding the computational power of various grammatical classes and problems inducing these from finite samples of their corresponding languages . we focus particularly on the task of stochastic , context - free grammar induction . we again encode production rules of the grammar as genes of a variable - length genome in a novel way , and use a fitness measure that reflects the probability that a grammar assigns to restricted samples from a stochastic target language . our main results explore the effect of a local search algorithm for tuning production probabilities on ga performance . previous studies have concentrated on nonstochastic context - free grammars ( cfgs ) ( [ lankhorst tr ] , [ wyard , icga #NUM# ] ) or stochastic regular grammars
many case - based reasoning algorithms retrieve cases using a derivative of the k - nearest neighbor ( k - nn ) classifier , whose similarity function is sensitive to irrelevant , interacting , and noisy features . many proposed methods for reducing this sensitivity parameterize k - nn ' s similarity function with feature weights . we focus on methods that automatically assign weight settings using little or no domain - specific knowledge . our goal is to predict the relative capabilities of these methods for specific dataset characteristics . we introduce a five - dimensional framework that categorizes automated weight - setting methods , empirically compare methods along one of these dimensions , summarize our results with four hypotheses , and describe additional evidence that supports them . our investigation revealed that most methods correctly assign low weights to completely irrelevant features , and methods that use performance feedback demonstrate three advantages over other methods ( i . e . , they require less pre - processing , better tolerate interacting features , and in crease learning rate ) .
this paper deals with the problem of learning characteristic concept descriptions from examples and describes a new generalization approach implemented in the system cola - #NUM# . the approach tries to take advantage of the information which can be induced from descriptions of unclassified objects using a conceptual clustering algorithm . experimental results in various real - world domains strongly support the hypothesis that the new approach delivers more correct ( and possibly more comprehesible ) concept descriptions than exisiting methods , if the induced concept descriptions are also used to classify objects which belong to concepts which were not present in the training data set . this paper describes the generalization approach implemented in cola and presents experimental results obtained with a relational and a propositional real world data set .
local selection ( ls ) is a very simple selection scheme in evolutionary algorithms . individual fitnesses are compared to a fixed threshold , rather than to each other , to decide who gets to reproduce . ls , coupled with fitness functions stemming from the consumption of shared environmental resources , maintains diversity in a way similar to fitness sharing ; however it is generally more efficient than fitness sharing , and lends itself to parallel implementations for distributed tasks . while ls is not prone to premature convergence , it applies minimal selection pressure upon the population . ls is therefore more appropriate than other , stronger selection schemes only on certain problem classes . this papers characterizes one broad class of problems in which ls consistently out performs tournament selection .
presenting and analyzing the results of ai experiments : data averaging and data snooping , proceedings of the fourteenth national conference on artificial intelligence , aaai - #NUM# , aaai press , menlo park , california , pp . #NUM# , #NUM# . copyright aaai . presenting and analyzing the results of ai experiments : abstract experimental results reported in the machine learning ai literature can be misleading . this paper investigates the common processes of data averaging ( reporting results in terms of the mean and standard deviation of the results from multiple trials ) and data snooping in the context of neural networks , one of the most popular ai machine learning models . both of these processes can result in misleading results and inaccurate conclusions . we demonstrate how easily this can happen and propose techniques for avoiding these very important problems . for data averaging , common presentation assumes that the distribution of individual results is gaussian . however , we investigate the distribution for common problems and find that it often does not approximate the gaussian distribution , may not be symmetric , and may be multimodal . we show that assuming gaussian distributions can significantly affect the interpretation of results , especially those of comparison studies . for a controlled task , we find that the distribution of performance is skewed towards better performance for smoother target functions and skewed towards worse performance for more complex target functions . we propose new guidelines for reporting performance which provide more information about the actual distribution ( e . g . box - whiskers plots ) . for data snooping , we demonstrate that optimization of performance via experimentation with multiple parameters can lead to significance being assigned to results which are due to chance . we suggest that precise descriptions of experimental techniques can be very important to the evaluation of results , and that we need to be aware of potential data snooping biases when formulating these experimental techniques ( e . g . selecting the test procedure ) . additionally , it is important to only rely on appropriate statistical tests and to ensure that any assumptions made in the tests are valid ( e . g . normality of the distribution ) .
a brief survey of biological research on non - coding dna is presented here . there has been growing interest in the effects of non - coding segments in evolutionary algorithms ( eas ) . to better understand and conduct research on non - coding segments and eas , it is important to understand the biological background of such work . this paper begins with a review of basic genetics and terminology , describes the different types of non - coding dna , and then surveys recent intron research .
we use simulated soccer to study multiagent learning . each team ' s players ( agents ) share action set and policy , but may behave differently due to position - dependent inputs . all agents making up a team are rewarded or punished collectively in case of goals . we conduct simulations with varying team sizes , and compare several learning algorithms : td - q learning with linear neural networks ( td - q ) , probabilistic incremental program evolution ( pipe ) , and a pipe version that learns by coevolution ( co - pipe ) . td - q is based on learning evaluation functions ( efs ) mapping input / action pairs to expected reward . pipe and co - pipe search policy space directly . they use adaptive probability distributions to synthesize programs that calculate action probabilities from current inputs . our results show that linear td - q encounters several difficulties in learning appropriate shared efs . pipe and co - pipe , however , do not depend on efs and find good policies faster and more reliably . this suggests that in some multiagent learning scenarios direct search in policy space can offer advantages over ef - based approaches .
a novel supervised learning method is presented by combining linear discriminant functions with neural networks . the proposed method results in a tree - structured hybrid architecture . due to constructive learning , the binary tree hierarchical architecture is automatically generated by a controlled growing process for a specific supervised learning task . unlike the classic decision tree , the linear discriminant functions are merely employed in the intermediate level of the tree for heuristically partitioning a large and complicated task into several smaller and simpler subtasks in the proposed method . these subtasks are dealt with by component neural networks at the leaves of the tree accordingly . for constructive learning , growing and credit - assignment algorithms are developed to serve for the hybrid architecture . the proposed architecture provides an efficient way to apply existing neural networks ( e . g . multi - layered perceptron ) for solving a large scale problem . we have already applied the proposed method to a universal approximation problem and several benchmark classification problems in order to evaluate its performance . simulation results have shown that the proposed method yields better results and faster training in comparison with the multi - layered perceptron .
machine learning and knowledge engineering have always been strongly related , but the introduction of new representations in knowledge engineering has created a gap between them . this paper describes research aimed at applying machine learning techniques to the current knowledge engineering representations . we propose a system that redesigns a part of a knowledge based system , the so called control knowledge . we claim a strong similarity between redesign of knowledge based systems and incremental machine learning . finally we will relate this work to existing research .
often when learning from data , one attaches a penalty term to a standard error term in an attempt to prefer simple models and prevent overfitting . current penalty terms for neural networks , however , often do not take into account weight interaction . this is a critical drawback since the effective number of parameters in a network usually differs dramatically from the total number of possible parameters . in this paper , we present a penalty term that uses principal component analysis to help detect functional redundancy in a neural network . results show that our new algorithm gives a much more accurate estimate of network complexity than do standard approaches . as a result , our new term should be able to improve techniques that make use of a penalty term , such as weight decay , weight pruning , feature selection , bayesian , and prediction - risk tech niques .
the dmp #NUM# ( dynamic multilayer perceptron #NUM# ) network training method is based upon a divide and conquer approach which builds networks in the form of binary trees , dynamically allocating nodes and layers as needed . this paper introduces the dmp #NUM# method , and compares the preformance of dmp #NUM# when using the standard delta rule training method for training individual nodes against the performance of dmp #NUM# when using a genetic algorithm for training . while the basic model does not require the use of a genetic algorithm for training individual nodes , the results show that the convergence properties of dmp #NUM# are enhanced by the use of a genetic algorithm with an appropriate fitness function .
in the late #NUM# s , we developed one of the early case - based design systems called kritik . kritik autonomously generated preliminary ( conceptual , qualitative ) designs for physical devices by retrieving and adapting past designs stored in its case memory . each case in the system had an associated structure - behavior - function ( sbf ) device model that explained how the structure of the device accomplished its functions . these casespecific device models guided the process of modifying a past design to meet the functional specification of a new design problem . the device models also enabled verification of the design modifications . kritik #NUM# is a new and more complete implementation of kritik . in this paper , we take a retrospective view on kritik . in early papers , we had described kritik as integrating case - based and model - based reasoning . in this integration , kritik also grounds the computational process of case - based reasoning in the sbf content theory of device comprehension . the sbf models not only provide methods for many specific tasks in case - based design such as design adaptation and verification , but they also provide the vocabulary for the whole process of case - based design , from retrieval of old cases to storage of new ones . this grounding , we believe , is essential for building well - constrained theories of case - based design .
much of the current research in learning bayesian networks fails to effectively deal with missing data . most of the methods assume that the data is complete , or make the data complete using fairly ad - hoc methods ; other methods do deal with missing data but learn only the conditional probabilities , assuming that the structure is known . we present a principled approach to learn both the bayesian network structure as well as the conditional probabilities from incomplete data . the proposed algorithm is an iterative method that uses a combination of expectation - maximization ( em ) and imputation techniques . results are presented on synthetic data sets which show that the performance of the new algorithm is much better than ad - hoc methods for handling missing data .
researchers in the field of distributed artificial intelligence ( dai ) have been interested in developing efficient mechanisms to coordinate the activities of multiple autonomous agents . the need for coordination arises because agents have to share resources and expertise required to achieve their goals . previous work in the area includes using sophisticated information exchange protocols , investigating heuristics for negotiation , and developing formal models of possibilities of conflict and cooperation among agent interests . in order to handle the changing requirements of continuous and dynamic environments , we propose learning as a means to provide additional possibilities for effective coordination . we use reinforcement learning techniques on a block pushing problem to show that agents can learn complimentary policies to follow a desired path without any knowledge about each other . we theoretically analyze and experimentally verify the effects of learning rate on system convergence , and also demonstrate the benefits of using learned coordination knowledge on similar problems . similar reinforcement learning based coordination can be achieved in both cooperative and non - cooperative domains , and in domains with noisy communication channels and other stochastic characteristics that present a formidable challenge to using other coordination schemes .
the performance of the error backpropagation ( bp ) and id #NUM# learning algorithms was compared on the task of mapping english text to phonemes and stresses . under the distributed output code developed by sejnowski and rosenberg , it is shown that bp consistently out - performs id #NUM# on this task by several percentage points . three hypotheses explaining this difference were explored : ( a ) id #NUM# is overfitting the training data , ( b ) bp is able to share hidden units across several output units and hence can learn the output units better , and ( c ) bp captures statistical information that id #NUM# does not . we conclude that only hypothesis ( c ) is correct . by augmenting id #NUM# with a simple statistical learning procedure , the performance of bp can be approached but not matched . more complex statistical procedures can improve the performance of both bp and id #NUM# substantially . a study of the residual errors suggests that there is still substantial room for improvement in learning methods for text - to - speech mapping .
we thank steen ladegaard knudsen for his assistance in programming , analysis and running of simulations , scott baden for his assistance in vectorizing our code for the cray y - mp , the division of engineering block grant for time on the cray at the san diego supercomputer center , and the members of the pdpnlp and guru research groups at ucsd for helpful comments on earlier versions of this work .
there has been a lot of recent interest in so - called " steady state " genetic algorithms ( gas ) which , among other things , replace only a few individuals ( typically #NUM# or #NUM# ) each generation from a fixed size population of size n . understanding the advantages and / or disadvantages of replacing only a fraction of the population each generation ( rather than the entire population ) was a goal of some of the earliest ga research . in spite of considerable progress in our understanding of gas since then , the pros / cons of overlapping generations remains a somewhat cloudy issue . however , recent theoretical and empirical results provide the background for a much clearer understanding of this issue . in this paper we review , combine , and extend these results in a way that significantly sharpens our insight .
daily experience shows that in the real world , the meaning of many concepts heavily depends on some implicit context , and changes in that context can cause more or less radical changes in the concepts . incremental concept learning in such domains requires the ability to recognize and adapt to such changes . this paper presents a solution for incremental learning tasks where the domain provides explicit clues as to the current context ( e . g . , attributes with characteristic values ) . we present a general two - level learning model , and its realization in a system named metal ( b ) , that can learn to detect certain types of contextual clues , and can react accordingly when a context change is suspected . the model consists of a base level learner that performs the regular on - line learning and classification task , and a meta - learner that identifies potential contextual clues . context learning and detection occur during regular on - line learning , without separate training phases for context recognition . experiments with synthetic domains as well as a ` real - world ' problem show that metal ( b ) is robust in a variety of dimensions and produces substantial improvement over simple object - level learning in situations with changing contexts .
this paper investigates memory issues that influence long - term creative problem solving and design activity , taking a case - based reasoning perspective . our exploration is based on a well - documented example the invention of the telephone by alexander graham bell . we abstract bell ' s reasoning and understanding mechanisms that appear time and again in long - term creative design . we identify that the understanding mechanism is responsible for analogical anticipation of design constraints and analogical evaluation , beside case - based design . but an already understood design can satisfy opportunistically suspended design problems , still active in background . the new mechanisms are integrated in a computational model , alec #NUM# , that accounts for some creative be
intelligent human agents exist in a cooperative social environment that facilitates learning . they learn not only by trial - and - error , but also through cooperation by sharing instantaneous information , episodic experience , and learned knowledge . the key investigations of this paper are , " given the same number of reinforcement learning agents , will cooperative agents outperform independent agents who do not communicate during learning ? " and " what is the price for such cooperation ? " using independent agents as a benchmark , cooperative agents are studied in following ways : ( #NUM# ) sharing sensation , ( #NUM# ) sharing episodes , and ( #NUM# ) sharing learned policies . this paper shows that ( a ) additional sensation from another agent is beneficial if it can be used efficiently , ( b ) sharing learned policies or episodes among agents speeds up learning at the cost of communication , and ( c ) for joint tasks , agents engaging in partnership can significantly outperform independent agents although they may learn slowly in the beginning . these tradeoffs are not just limited to multi - agent reinforcement learning .
we describe sfoil , a descendant of foil that uses the advanced stochastic search heuristic , and its application in learning to compose the two - voice counterpoint . the application required learning a #NUM# - ary relation from more than #NUM# . #NUM# training instances . sfoil is able to efficiently deal with this learning task which is to our knowledge one of the most complex learning task solved by an ilp system . this demonstrates that ilp systems can scale up to real databases and that top - down ilp systems that use the covering approach and advanced search strategies are appropriate for knowledge discovery in databases and are promising for further investigation .
the human visual system is more sensitive to the relative motion of objects than to their absolute motion . an understanding of motion perception requires an understanding of how neural circuits can group moving visual elements relative to one another , based upon hierarchical reference frames . we have modeled visual relative motion perception using a neural network architecture that groups visual elements according to gestalt common - fate principles and exploits information about the behavior of each group to predict the behavior of individual elements . a simple competitive neural circuit binds visual elements together into a representation of a visual object . information about the spiking pattern of neurons allows transfer of the bindings of an object representation from location to location in the neural circuit as the object moves . the model exhibits characteristics of human object grouping and solves some key neural circuit design problems in visual relative motion perception .
a knowledge - level analysis of complex tasks like diagnosis and design can give us a better understanding of these tasks in terms of the goals they aim to achieve and the different ways to achieve these goals . in this paper we present a knowledge - level analysis of redesign . redesign is viewed as a family of methods based on some common principles , and a number of dimensions along which redesign problem solving methods can vary are distinguished . by examining the problem - solving behavior of a number of existing redesign systems and approaches , we came up with a collection of problem - solving methods for redesign and developed a task - method structure for redesign . in constructing a system for redesign a large number of knowledge - related choices and decisions are made . in order to describe all relevant choices in redesign problem solving , we have to extend the current notion of possible relations between tasks and methods in a psm architecture . the realization of a task by a psm , and the decomposition of a psm into subtasks are the most common relations in a psm architecture .
in bayesian density estimation and prediction using dirichlet process mixtures of standard , exponential family distributions , the precision or total mass parameter of the mixing dirichlet process is a critical hyperparame - ter that strongly influences resulting inferences about numbers of mixture components . this note shows how , with respect to a flexible class of prior distributions for this parameter , the posterior may be represented in a simple conditional form that is easily simulated . as a result , inference about this key quantity may be developed in tandem with the existing , routine gibbs sampling algorithms for fitting such mixture models . the concept of data augmentation is important , as ever , in developing this extension of the existing algorithm . a final section notes an simple asymptotic approx imation to the posterior .
in this paper we describe our study of applying knowledge - based neural networks to the problem of diagnosing faults in local telephone loops . currently , nynex uses an expert system called max to aid human experts in diagnosing these faults ; however , having an effective learning algorithm in place of max would allow easy portability between different maintenance centers , and easy updating when the phone equipment changes . we find that ( i ) machine learning algorithms have better accuracy than max , ( ii ) neural networks perform better than decision trees , ( iii ) neural network ensembles perform better than standard neural networks , ( iv ) knowledge - based neural networks perform better than standard neural networks , and ( v ) an ensemble of knowledge - based neural networks performs the best .
human vision systems integrate information nonlocally , across long spatial ranges . for example , a moving stimulus appears smeared when viewed briefly ( #NUM# ms ) , yet sharp when viewed for a longer exposure ( #NUM# ms ) ( burr , #NUM# ) . this suggests that visual systems combine information along a trajectory that matches the motion of the stimulus . our self - organizing neural network model shows how developmental exposure to moving stimuli can direct the formation of horizontal trajectory - specific motion integration pathways that unsmear representations of moving stimuli . these results account for burr ' s data and can potentially also model other phenomena , such as visual inertia .
we introduce a new technique which enables a learner without access to hidden information to learn nearly as well as a learner with access to hidden information . we apply our technique to solve an open problem of maass and turan [ #NUM# ] , showing that for any concept class f , the least number of queries sufficient for learning f by an algorithm which has access only to arbitrary equivalence queries is at most a factor of #NUM# = log #NUM# ( #NUM# = #NUM# ) more than the least number of queries sufficient for learning f by an algorithm which has access to both arbitrary equivalence queries and membership queries . previously known results imply that the #NUM# = log #NUM# ( #NUM# = #NUM# ) in our bound is best possible . we describe analogous results for two generalizations of this model to function learning , and apply those results to bound the difficulty of learning in the harder of these models in terms of the difficulty of learning in the easier model . we bound the difficulty of learning unions of k concepts from a class f in terms of the difficulty of learning f . we bound the difficulty of learning in a noisy environment for deterministic algorithms in terms of the difficulty of learning in a noise - free environment . we apply a variant of our technique to develop an algorithm transformation that allows probabilistic learning algorithms to nearly optimally cope with noise . a second variant enables us to improve a general lower bound of turan [ #NUM# ] for the pac - learning model ( with queries ) . finally , we show that logarithmically many membership queries never help to obtain computationally efficient learning algorithms .
in this paper we describe the evolution of a discrete - time recurrent neural network to control a real mobile robot . in all our experiments the evolutionary procedure is carried out entirely on the physical robot without human intervention . we show that the autonomous development of a set of behaviors for locating a battery charger and periodically returning to it can be achieved by lifting constraints in the design of the robot / environment interactions that were employed in a preliminary experiment . the emergent homing behavior is based on the autonomous development of an internal neural topographic map ( which is not pre - designed ) that allows the robot to choose the appropriate trajectory as function of location and remaining energy .
genetic programming is a method of program discovery consisting of a special kind of genetic algorithm capable of operating on nonlinear chromosomes ( parse trees ) representing programs and an interpreter which can run the programs being optimised . this paper describes pdgp ( parallel distributed genetic programming ) , a new form of genetic programming which is suitable for the development of fine - grained parallel programs . pdgp is based on a graph - like representation for parallel programs which is manipulated by crossover and mutation operators which guarantee the syntactic correctness of the offspring . the paper describes these operators and reports some preliminary results obtained with this paradigm .
conceptual analogy ( ca ) is a general approach that applies conceptual clustering and concept representations to facilitate the efficient use of past experiences ( cases ) during analogical reasoning ( borner #NUM# ) . the approach was developed and implemented in syn * ( see also ( borner #NUM# , borner and faauer #NUM# ) ) to support the design of supply nets in building engineering . this paper sketches the task ; it outlines the nearest - neighbor - based agglomerative conceptual clustering applied in organizing large amounts of structured cases into case classes ; it provides the concept representation used to characterize case classes and shows the analogous solution of new problems based on the concepts available . however , the main purpose of this paper is to evaluate ca in terms of its reasoning efficiency ; its capability to derive solutions that go beyond the cases in the case base but still preserve the quality of cases .
accurate and fast estimation of probability density functions is crucial for satisfactory computational performance in many scientific problems . when the type of density is known a priori , then the problem becomes statistical estimation of parameters from the observed values . in the non - parametric case , usual estimators make use of kernel functions . if x j ; j = #NUM# ; #NUM# ; : : : ; n is a sequence of i . i . d . random variables with estimated probability density function f n , in the kernel method the computation of the values f n ( x #NUM# ) ; f n ( x #NUM# ) ; : : : ; f n ( x n ) requires o ( n #NUM# ) operations , since each kernel needs to be evaluated at every x j . we propose a sequence of special weight functions for the non - parametric estimation of f which requires almost linear time : if m is a slowly growing function that increases without bound with n , our method requires only o ( m #NUM# n ) arithmetic operations . we derive conditions for convergence under a number of metrics , which turn out to be similar to those required for the convergence of kernel based methods . we also discuss experiments on different distributions and compare the efficiency and the accuracy of our computations with kernel based estimators for various values of n and m .
we propose an active learning method with hidden - unit reduction , which is devised specially for multilayer perceptrons ( mlp ) . first , we review our active learning method , and point out that many fisher - information - based methods applied to mlp have a critical problem : the information matrix may be singular . to solve this problem , we derive the singularity condition of an information matrix , and propose an active learning technique that is applicable to
stable neural network control and estimation may be viewed formally as a merging of concepts from nonlinear dynamic systems theory with tools from multivariate approximation theory . this paper extends earlier results on adaptive control and estimation of nonlinear systems using gaussian radial basis functions to the on - line generation of irregularly sampled networks , using tools from multiresolution analysis and wavelet theory . this yields much more compact and efficient system representations while preserving global closed - loop stability . approximation models employing basis functions that are localized in both space and spatial frequency admit a measure of the approximated function ' s spatial frequency content that is not directly dependent on reconstruction error . as a result , these models afford a means of adaptively selecting basis functions according to the local spatial frequency content of the approximated function . an algorithm for stable , on - line adaptation of output weights simultaneously with node configuration in a class of non - parametric models with wavelet basis functions is presented . an asymptotic bound on the error in the network ' s reconstruction is derived and shown to be dependent solely on the minimum approximation error associated with the steady state node configuration . in addition , prior bounds on the temporal bandwidth of the system to be identified or controlled are used to develop a criterion for on - line selection of radial and ridge wavelet basis functions , thus reducing the rate of increase in network ' s size with the dimension of the state vector . experimental results obtained by using the network to predict the path of an unknown light bluff object thrown through air , in an active - vision based robotic catching system , are given to illustrate the network ' s performance in a simple real - time application .
research on bias in machine learning algorithms has generally been concerned with the impact of bias on predictive accuracy . we believe that there are other factors that should also play a role in the evaluation of bias . one such factor is the stability of the algorithm ; in other words , the repeatability of the results . if we obtain two sets of data from the same phenomenon , with the same underlying probability distribution , then we would like our learning algorithm to induce approximately the same concepts from both sets of data . this paper introduces a method for quantifying stability , based on a measure of the agreement between concepts . we also discuss the relationships among stability , predictive accuracy , and bias .
in many genetic algorithms applications the objective is to find a ( near - ) optimal solution using a limited amount of computation . given these requirements it is difficult to find a good balance between exploration and exploitation . usually such a balance is found by tuning the various parameters ( like the selective pressure , population size , the mutation - and crossover rate ) of the genetic algorithm . as an alternative we propose simultaneous tuning of the selective pressure and the disruptiveness of the recombination operators . our experiments show that the combination of a proper selective pressure and a highly disruptive recombination operator yields superior performance . the reduction mechanism used in a steady - state ga has a strong influence on the optimal crossover disruptiveness . using the worst fitness deletion strategy the building blocks present in the current best individuals are always preserved . this releases the crossover operator from the burden to maintain good building blocks and allows us to tune crossover disruptiveness to improve the search for better individuals .
cross - validation is a frequently used , intuitively pleasing technique for estimating the accuracy of theories learned by machine learning algorithms . during testing of a machine learning algorithm ( foil ) on new databases of prokaryotic rna transcription promoters which we have developed , cross - validation displayed an interesting phenomenon . one theory is found repeatedly and is responsible for very little of the cross - validation error , whereas other theories are found very infrequently which tend to be responsible for the majority of the cross - validation error . it is tempting to believe that the most frequently found theory ( the " modal theory " ) may be more accurate as a classifier of unseen data than the other theories . however , experiments showed that modal theories are not more accurate on unseen data than the other theories found less frequently during cross - validation . modal theories may be useful in predicting when cross - validation is a poor estimate of true accuracy . we offer explanations #NUM# for correspondence : department of computer science and engineering , university of california , san
one of the most significant cost factors in robotics applications is the design and development of real - time robot control software . control theory helps when linear controllers have to be developed , but it doesn ' t sufficiently support the generation of non - linear controllers , although in many cases ( such as in compliance control ) , nonlinear control is essential for achieving high performance . this paper discusses how machine learning has been applied to the design of ( non - ) linear controllers . several alternative function approximators , including multilayer perceptrons ( mlp ) , radial basis function networks ( rbfns ) , and fuzzy controllers are analyzed and compared , leading to the definition of two major families : open field function function approximators and locally receptive field function approximators . it is shown that rbfns and fuzzy controllers bear strong similarities , and that both have a symbolic interpretation . this characteristics allows for applying both symbolic and statistic learning algorithms to synthesize the network layout from a set of examples and , possibly , some background knowledge . three integrated learning algorithms , two of which are original , are described and evaluated on experimental test cases . the first test case is provided by a robot kuka ir - #NUM# engaged into the " peg - into - hole " task , whereas the second is represented by a classical prediction task on the mackey - glass time series . from the experimental comparison , it appears that both fuzzy controllers and rbfns synthesised from examples are excellent approximators , and that , in practice , they can be even more accurate than mlps .
this paper discusses the use of evolutionary computation to evolve behaviors that exhibit emergent intelligent behavior . genetic algorithms are used to learn navigation and collision avoidance behaviors for robots . the learning is performed under simulation , and the resulting behaviors are then used to control the actual robot . some of the emergent behavior is described in detail .
judgments of similarity and soundness are important aspects of human analogical processing . this paper explores how these judgments can be modeled using sme , a simulation of gentner ' s structure - mapping theory . we focus on structural evaluation , explicating several principles which psychologically plausible algorithms should follow . we introduce the specificity conjecture , which claims that naturalistic representations include a preponderance of appearance and low - order information . we demonstrate via computational experiments that this conjecture affects how structural evaluation should be performed , including the choice of normalization technique and how the systematicity preference is implemented .
genetic algorithms have been used to solve hard optimization problems ranging from the travelling salesman problem to the quadratic assignment problem . we show that the simple genetic algorithm can be used to solve an optimization problem derived from the #NUM# - conjunctive normal form problem . by separating the populations into small sub - populations , parallel genetic algorithms exploits the inherent parallelism in genetic algorithms and prevents premature convergence . genetic algorithms using hill - climbing conduct genetic search in the space of local optima , and hill - climbing can be less com - putationally expensive than genetic search . we examine the effectiveness of these techniques in improving the quality of solutions of #NUM# cnf problems .
icsim is a simulator for structured connectionism under development at icsi . structured connectionism is characterized by the need for flexibility , efficiency and support for the design and reuse of modular substructure . we take the position that a fast object - oriented language like sather [ #NUM# ] is an appropriate implementation medium to achieve these goals . the core of icsim consists of a hierarchy of classes that correspond to simulation entities . new connectionist models are realized by combining and specializing pre - existing classes . whenever possible , auxillary functionality has been separated out into functional modules in order to keep the basic hierarchy as clean and simple as possible .
in recent years , researchers have made considerable progress on the worst - case analysis of inductive learning tasks , but for theoretical results to have impact on practice , they must deal with the average case . in this paper we present an average - case analysis of a simple algorithm that induces one - level decision trees for concepts defined by a single relevant attribute . given knowledge about the number of training instances , the number of irrelevant attributes , the amount of class and attribute noise , and the class and attribute distributions , we derive the expected classification accuracy over the entire instance space . we then examine the predictions of this analysis for different settings of these domain parameters , comparing them to exper imental results to check our reasoning .
we compare the performance of several machine learning algorithms in the problem of prognos - tics of the femoral neck fracture recovery : the k - nearest neighbours algorithm , the semi - naive bayesian classifier , backpropagation with weight elimination learning of the multilayered neural networks , the lfc ( lookahead feature construction ) algorithm , and the assistant - i and assistant - r algorithms for top down induction of decision trees using information gain and relieff as search heuristics , respectively . we compare the prognostic accuracy and the explanation ability of different classifiers . among the different algorithms the semi - naive bayesian classifier and assistant - r seem to be the most appropriate . we analyze the combination of decisions of several classifiers for solving prediction problems and show that the combined classifier improves both performance and the explanation ability .
the structure - mapping engine ( sme ) has successfully modeled several aspects of human consistent interpretations of an analogy . while useful for theoretical explorations , this aspect of the algorithm is both psychologically implausible and computationally inefficient . ( #NUM# ) sme contains no mechanism for focusing on interpretations relevant to an analogizer ' s goals . this paper describes modifications to sme which overcome these flaws . we describe a greedy merge algorithm which efficiently computes an approximate " best " interpretation , and can generate alternate interpretations when necessary . we describe pragmatic marking , a technique which focuses the mapping to produce relevant , yet novel , inferences . we illustrate these techniques via example and evaluate their performance using empirical data and theoretical analysis . analogical processing . however , it has two significant drawbacks : ( #NUM# ) sme constructs all structurally
one of key issues in both discrete and continuous class prediction and in machine learning in general seems to be the problem of estimating the quality of attributes . heuristic measures mostly assume independence of attributes and therefore cannot be successfully used in domains with strong dependencies between attributes . relief and its extension relieff are statistical methods capable of correctly estimating the quality of attributes in classification problems with strong dependencies between attributes . following the analysis of relieff we have extended it to continuous class problems . regressional relieff ( rrelieff ) and relieff provide a unified view on estimation of quality of attributes . the experiments show that rrelieff successfully estimates the quality of attributes and can be used for non - myopic learning of regression trees .
we investigate how abduction and induction can be integrated into a common learning framework through the notion of abductive concept learning ( acl ) . acl is an extension of inductive logic programming ( ilp ) to the case in which both the background and the target theory are abductive logic programs and where an abductive notion of entailment is used as the coverage relation . in this framework , it is then possible to learn with incomplete information about the examples by exploiting the hypothetical reasoning of abduction . the paper presents the basic framework of acl with its main characteristics and illustrates its potential in addressing several problems in ilp such as learning with incomplete information and multiple predicate learning . an algorithm for acl is developed by suitably extending the top - down ilp method for concept learning and integrating this with an abductive proof procedure for abductive logic programming ( alp ) . a prototype system has been developed and applied to learning problems with incomplete information . the particular role of integrity constraints in acl is investigated showing acl as a hybrid learning framework that integrates the explanatory ( discriminant ) and descriptive ( characteristic ) settings of ilp .
in the markov decision process ( mdp ) formalization of reinforcement learning , a single adaptive agent interacts with an environment defined by a probabilistic transition function . in this solipsistic view , secondary agents can only be part of the environment and are therefore fixed in their behavior . the framework of markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals . this paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment . it describes a q - learning - like algorithm for finding optimal policies and demonstrates its application to a simple two - player game in which the optimal policy is probabilistic .
genetic programming is a promising new method for automatically generating functions and algorithms through natural selection . in contrast to other learning methods , genetic programming ' s automatic programming makes it a natural approach for developing algorithmic robot behaviors . in this paper we present an overview of how we apply genetic programming to behavior - based team coordination in the robocup soccer server domain . the result is not just a hand - coded soccer algorithm , but a team of softbots which have learned on their own how to play a reasonable game of soccer .
we have evolved artificial neural networks to control the wandering behavior of small robots . the task was to touch as many squares in a grid as possible during a fixed period of time . a number of the simulated robots were embodied in small lego ( trademark ) robot , controlled by a motorola ( trademark ) #NUM# processor ; and their performance was compared to the simulations . we observed that : ( a ) evolution was an effective means to program control ; ( b ) progress was characterized by sharply stepped periods of improvement , separated by periods of stasis that corresponded to levels of behavioral / computational complexity ; and ( c ) the simulated and realized robots behaved quite similarly , the realized robots in some cases outperforming the simulated ones . introducing random noise to the simulations improved the fit somewhat ( from #NUM# . #NUM# to #NUM# . #NUM# ) . hybrid simulated / embodied selection regimes for evolutionary robots are discussed .
the predator / prey domain is utilized to conduct research in distributed artificial intelligence . genetic programming is used to evolve behavioral strategies for the predator agents . to further the utility of the predator strategies , the prey population is allowed to evolve at the same time . the expected competitive learning cycle did not surface . this failing is investigated , and a simple prey algorithm surfaces , which is consistently able to evade capture from the predator algorithms .
this paper explores two boosting techniques for cost - sensitive tree classification in the situation where misclassification costs change very often . ideally , one would like to have only one induction , and use the induced model for different misclassification costs . thus , it demands robustness of the induced model against cost changes . combining multiple trees gives robust predictions against this change . we demonstrate that ordinary boosting combined with the minimum expected cost criterion to select the prediction class is a good solution under this situation . we also introduce a variant of the ordinary boosting procedure which utilizes the cost information during training . we show that the proposed technique performs better than the ordinary boosting in terms of misclassification cost . however , this technique requires to induce a set of new trees every time the cost changes . our empirical investigation also reveals some interesting behavior of boosting decision trees for cost - sensitive classification .
previous approaches to multi - agent reinforcement learning are either very limited or heuristic by nature . the main reason is : each agent ' s or " animat ' s " environment continually changes because the other learning animats keep changing . traditional reinforcement learning algorithms cannot properly deal with this . their convergence theorems require repeatable trials and strong ( typically markovian ) assumptions about the environment . in this paper , however , we use a novel , general , sound method for multiple , reinforcement learning " animats " , each living a single life with limited computational resources in an unrestricted , changing environment . the method is called " incremental self - improvement " ( is | schmidhuber , #NUM# ) . is properly takes into account that whatever some animat learns at some point may affect learning conditions for other animats or for itself at any later point . the learning algorithm of an is - based animat is embedded in its own policy | the animat cannot only improve its performance , but in principle also improve the way it improves etc . at certain times in the animat ' s life , is uses reinforcement / time ratios to estimate from a single training example ( namely the entire life so far ) which previously learned things are still useful , and selectively keeps them but gets rid of those that start appearing harmful . is is based on an efficient , stack - based backtracking procedure which is guaranteed to make each animat ' s learning history a history of long - term reinforcement accelerations . experiments demonstrate is ' effectiveness . in one experiment , is learns a sequence of more and more complex function approximation problems . in another , a multi - agent system consisting of three co - evolving , is - based animats chasing each other learns interesting , stochastic predator and prey strategies .
the breeder genetic algorithm bga depends on a set of control parameters and genetic operators . in this paper it is shown that strategy adaptation by competing subpopulations makes the bga more robust and more efficient . each subpopulation uses a different strategy which competes with other subpopulations . numerical results are pre sented for a number of test functions .
we present a computational approach to the acquisition of problem schemes by learning by doing and to their application in analogical problem solving . our work has its background in automatic program construction and relies on the concept of recursive program schemes . in contrast to the usual approach to cognitive modelling where computational models are designed to fit specific data we propose a framework to describe certain empirically established characteristics of human problem solving and learning in a uniform and formally sound way .
genetic algorithms ( gas ) play a major role in many artificial - life systems , but there is often little detailed understanding of why the ga performs as it does , and little theoretical basis on which to characterize the types of fitness landscapes that lead to successful ga performance . in this paper we propose a strategy for addressing these issues . our strategy consists of defining a set of features of fitness landscapes that are particularly relevant to the ga , and experimentally studying how various configurations of these features affect the ga ' s performance along a number of dimensions . in this paper we informally describe an initial set of proposed feature classes , describe in detail one such class ( " royal road " functions ) , and present some initial experimental results concerning the role of crossover and " building blocks " on landscapes constructed from features of this class .
we consider the question " how should one act when the only goal is to learn as much as possible ? " building on the theoretical results of fedorov [ #NUM# ] and mackay [ #NUM# ] , we apply techniques from optimal experiment design ( oed ) to guide the query / action selection of a neural network learner . we demonstrate that these techniques allow the learner to minimize its generalization error by exploring its domain efficiently and completely . we conclude that , while not a panacea , oed - based query / action has much to offer , especially in domains where its high computational costs can be tolerated . this report describes research done at the center for biological and computational learning and the artificial intelligence laboratory of the massachusetts institute of technology . support for the center is provided in part by a grant from the national science foundation under contract asc - #NUM# . the author was also funded by atr human information processing laboratories , siemens corporate research and nsf grant cda - #NUM# .
cbet is a software tool for the interactive exploration of a case base . cbet is an integrated environment that provides a range of browsing and display functions that make possible knowledge extraction from a set of cases . cbet is motivated by an application to training firemen . here cases describe past forest fire fighting interventions and cbet is used to detect dependencies between data , acquire practical planning competences , visualize complex data , clustering similar cases . in cbet well rooted machine learning techniques for selecting relevant features , clustering cases and forecasting unknown values have been adapted and reused for case base exploration .
recent studies on planning , comparing plan re - use and plan generation , have shown that both the above tasks may have the same degree of computational complexity , even if we deal with very similar problems . the aim of this paper is to show that the same kind of results apply also for diagnosis . we propose a theoretical complexity analysis coupled with some experimental tests , intended to evaluate the adequacy of adaptation strategies which re - use the solutions of past diagnostic problems in order to build a solution to the problem to be solved . results of such analysis show that , even if diagnosis re - use falls into the same complexity class of diagnosis generation ( they are both np - complete problems ) , practical advantages can be obtained by exploiting a hybrid architecture combining case - based and model - based diagnostic problem solving in a unifying framework .
the representation of hidden variable models by attractor neural networks is studied . memories are stored in a dynamical attractor that is a continuous manifold of fixed points , as illustrated by linear and nonlinear networks with hidden neurons . pattern analysis and synthesis are forms of pattern completion by recall of a stored memory . analysis and synthesis in the linear network are performed by bottom - up and top - down connections . in the nonlinear network , the analysis computation additionally requires rectification nonlinearity and inner product inhibition between hidden neurons . one popular approach to sensory processing is based on generative models , which assume that sensory input patterns are synthesized from some underlying hidden variables . for example , the sounds of speech can be synthesized from a sequence of phonemes , and images of a face can be synthesized from pose and lighting variables . hidden variables are useful because they constitute a simpler representation of the variables that are visible in the sensory input . using a generative model for sensory processing requires a method of pattern analysis . given a sensory input pattern , analysis is the recovery of the hidden variables from which it was synthesized . in other words , analysis and synthesis are inverses of each other . there are a number of approaches to pattern analysis . in analysis - by - synthesis , the synthetic model is embedded inside a negative feedback loop [ #NUM# ] . another approach is to construct a separate analysis model [ #NUM# ] . this paper explores a third approach , in which visible - hidden pairs are embedded as attractive fixed points , or attractors , in the state space of a recurrent neural network . the attractors can be regarded as memories stored in the network , and analysis and synthesis as forms of pattern completion by recall of a memory . the approach is illustrated with linear and nonlinear network architectures . in both networks , the synthetic model is linear , as in principal
in this paper , we examine decision graphs , a generalization of decision trees . we present an inference scheme to construct decision graphs using the minimum message length principle . empirical tests demonstrate that this scheme compares favourably with other decision tree inference schemes . this work provides a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain .
for an agent living in a non - deterministic markov environment ( nme ) , what is , in theory , the fastest way of acquiring information about its statistical properties ? the answer is : to design " optimal " sequences of " experiments " by performing action sequences that maximize expected information gain . this notion is implemented by combining concepts from information theory and reinforcement learning . experiments show that the resulting method , reinforcement driven information acquisition , can explore certain nmes much faster than conventional random exploration .
we consider learnability with membership queries in the presence of incomplete information . in the incomplete boundary query model introduced by blum et al . [ #NUM# ] , it is assumed that membership queries on instances near the boundary of the target concept may receive a " don ' t know " answer . we show that zero - one threshold functions are efficiently learnable in this model . the learning algorithm uses split graphs when the boundary region has radius #NUM# , and their generalization to split hypergraphs ( for which we give a split - finding algorithm ) when the boundary region has constant radius greater than #NUM# . we use a notion of indistinguishability of concepts that is appropriate for this model .
most techniques for verification and validation are directed at functional properties of programs . however , other properties of programs are also essential . this paper describes a model for the average computing time of a kads knowledge - based system based on its structure . an example taken from an existing knowledge - based system is used to demonstrate the use of the cost - model in designing the system .
realistic and complex planning situations require a mixed - initiative planning framework in which human and automated planners interact to mutually construct a desired plan . ideally , this joint cooperation has the potential of achieving better plans than either the human or the machine can create alone . human planners often take a case - based approach to planning , relying on their past experience and planning by retrieving and adapting past planning cases . planning by analogical reasoning in which generative and case - based planning are combined , as in prodigy / analogy , provides a suitable framework to study this mixed - initiative integration . however , having a human user engaged in this planning loop creates a variety of new research questions . the challenges we found creating a mixed - initiative planning system fall into three categories : planning paradigms differ in human and machine planning ; visualization of the plan and planning process is a complex , but necessary task ; and human users range across a spectrum of experience , both with respect to the planning domain and the underlying planning technology . this paper presents our approach to these three problems when designing an interface to incorporate a human into the process of planning by analogical reasoning with prodigy / analogy . the interface allows the user to follow both generative and case - based planning , it supports visualization of both plan and the planning rationale , and it addresses the variance in the experience of the user by allowing the user to control the presentation of information . * this research is sponsored as part of the darpa / rl knowledge based planning and scheduling initiative under grant number f #NUM# - #NUM# - #NUM# - #NUM# . a short version of this document appeared as cox , m . t . , & veloso , m . m . ( #NUM# ) . supporting combined human and machine planning : an interface for planning by analogical reasoning . in d . b . leake & e . plaza ( eds . ) , case - based reasoning research and development : second international conference on case - based reasoning ( pp . #NUM# - #NUM# ) . berlin : springer - verlag .
one of the major goals of most early concept learners was to find hypotheses that were perfectly consistent with the training data . it was believed that this goal would indirectly achieve a high degree of predictive accuracy on a set of test data . later research has partially disproved this belief . however , the issue of consistency has not yet been resolved completely . we examine the issue of consistency from a new perspective . to avoid overfitting the training data , a considerable number of current systems have sacrificed the goal of learning hypotheses that are perfectly consistent with the training instances by setting a goal of hypothesis simplicity ( occam ' s razor ) . instead of using simplicity as a goal , we have developed a novel approach that addresses consistency directly . in other words , our concept learner has the explicit goal of selecting the most appropriate degree of consistency with the training data . we begin this paper by exploring concept learning with less than perfect consistency . next , we describe a system that can adapt its degree of consistency in response to feedback about predictive accuracy on test data . finally , we present the results of initial experiments that begin to address the question of how tightly hypotheses should fit the training data for different problems .
in this contribution we propose a new solution for the problem of blind separation of sources ( for one dimensional signals and images ) in the case that not only the waveform of sources is unknown , but also their number . for this purpose multi - layer neural networks with associated adaptive learning algorithms are developed . the primary source signals can have any non - gaussian distribution , i . e . they can be sub - gaussian and / or super - gaussian . computer experiments are presented which demonstrate the validity and high performance of the proposed approach .
a new learning algorithm is derived which performs online stochastic gradient ascent in the mutual information between outputs and inputs of a network . in the absence of a priori knowledge about the ` signal ' and ` noise ' components of the input , propagation of information depends on calibrating network non - linearities to the detailed higher - order moments of the input density functions . by incidentally minimising mutual information between outputs , as well as maximising their individual entropies , the network ` fac - torises ' the input into independent components . as an example application , we have achieved near - perfect separation of ten digitally mixed speech signals . our simulations lead us to believe that our network performs better at blind separation than the herault - jutten network , reflecting the fact that it is derived rigorously from the mutual information objective .
we present a method for maintaining mixtures of prunings of a prediction or decision tree that extends the " node - based " prunings of [ bun #NUM# , wst #NUM# , hs #NUM# ] to the larger class of edge - based prunings . the method includes an efficient online weight allocation algorithm that can be used for prediction , compression and classification . although the set of edge - based prunings of a given tree is much larger than that of node - based prunings , our algorithm has similar space and time complexity to that of previous mixture algorithms for trees . using the general on - line framework of freund and schapire [ fs #NUM# ] , we prove that our algorithm maintains correctly the mixture weights for edge - based prunings with any bounded loss function . we also give a similar algorithm for the logarithmic loss function with a corresponding weight allocation algorithm . finally , we describe experiments comparing node - based and edge - based mixture models for estimating the probability of the next word in english text , which show the ad vantages of edge - based models .
markov chain monte carlo ( mcmc ) methods , including the gibbs sampler and the metropolis - hastings algorithm , are very commonly used in bayesian statistics for sampling from complicated , high - dimensional posterior distributions . a continuing source of uncertainty is how long such a sampler must be run in order to converge approximately to its target stationary distribution . rosenthal ( #NUM# b ) presents a method to compute rigorous theoretical upper bounds on the number of iterations required to achieve a specified degree of convergence in total variation distance by verifying drift and minorization conditions . we propose the use of auxiliary simulations to estimate the numerical values needed in rosenthal ' s theorem . our simulation method makes it possible to compute quantitative convergence bounds for models for which the requisite analytical computations would be prohibitively difficult or impossible . on the other hand , although our method appears to perform well in our example problems , it can not provide the guarantees offered by analytical proof . acknowledgements . we thank brad carlin for assistance and encouragement .
in the field of operation research and artificial intelligence , several stochastic search algorithms have been designed based on the theory of global random search ( zhigljavsky #NUM# ) . basically , those techniques iteratively sample the search space with respect to a probability distribution which is updated according to the result of previous samples and some predefined strategy . genetic algorithms ( gas ) ( goldberg #NUM# ) or greedy randomized adaptive search procedures ( grasp ) ( feo & resende #NUM# ) are two particular instances of this paradigm . in this paper , we present sage , a search algorithm based on the same fundamental mechanisms as those techniques . however , it addresses a class of problems for which it is difficult to design transformation operators to perform local search because of intrinsic constraints in the definition of the problem itself . for those problems , a procedural approach is the natural way to construct solutions , resulting in a state space represented as a tree or a dag . the aim of this paper is to describe the underlying heuristics used by sage to address problems belonging to that class . the performance of sage is analyzed on the problem of grammar induction and its successful application to problems from the recent abbadingo dfa learning competition is presented .
we analyze a hierarchical bayes model which is related to the usual empirical bayes formulation of james - stein estimators . we consider running a gibbs sampler on this model . using previous results about convergence rates of markov chains , we provide rigorous , numerical , reasonable bounds on the running time of the gibbs sampler , for a suitable range of prior distributions . we apply these results to baseball data from efron and morris ( #NUM# ) . for a different range of prior distributions , we prove that the gibbs sampler will fail to converge , and use this information to prove that in this case the associated posterior distribution is non - normalizable . acknowledgements . i am very grateful to jun liu for suggesting this project , and to neal madras for suggesting the use of the submartingale convergence theorem herein . i thank kate cowles and richard tweedie for helpful conversations , and thank the referees for useful comments .
evolutionary algorithms are often presented as general purpose search methods . yet , we also know that no search method is better than another over all possible problems and that in fact there is often a good deal of problem specific information involved in the choice of problem representation and search operators . in this paper we explore some very general properties of representations as they relate to neighborhood search methods . in particular , we looked at the expected number of local optima under a neighborhood search operator when averaged overall possible representations . the number of local optima under a neighborhood search operator for standard binary and standard binary reflected gray codes is developed and explored as one measure of problem complexity . we also relate number of local optima to another metric , , designed to provide one measure of complexity with respect to a simple genetic algorithm . choosing a good representation is a vital component of solving any search problem . however , choosing a good representation for a problem is as difficult as choosing a good search algorithm for a problem . wolpert and macready ' s ( #NUM# ) no free lunch ( nfl ) theorem proves that no search algorithm is better than any other over all possible discrete functions . radcliffe and surry ( #NUM# ) extend these notions to also cover the idea that all representations are equivalent when their behavior is considered on average over all possible functions . to understand these results , we first outline some of the simple assumptions behind this theorem . first , assume the optimization problem is discrete ; this describes all combinatorial optimization problems - and really all optimization problems being solved on computers since computers have finite precision . second , we ignore the fact that we can resample points in the space . the " no free lunch " result can be stated as follows :
we investigate the effectiveness of connectionist networks for predicting the future continuation of temporal sequences . the problem of overfitting , particularly serious for short records of noisy data , is addressed by the method of weight - elimination : a term penalizing network complexity is added to the usual cost function in back - propagation . the ultimate goal is prediction accuracy . we analyze two time series . on the benchmark sunspot series , the networks outperform traditional statistical approaches . we show that the network performance does not deteriorate when there are more input units than needed . weight - elimination also manages to extract some part of the dynamics of the notoriously noisy currency exchange rates and makes the network solution interpretable .
we present a detailed analysis of the evolution of genetic programming ( gp ) populations using the problem of finding a program which returns the maximum possible value for a given terminal and function set and a depth limit on the program tree ( known as the max problem ) . we confirm the basic message of [ gathercole and ross , #NUM# ] that crossover together with program size restrictions can be responsible for premature convergence to a suboptimal solution . we show that this can happen even when the population retains a high level of variety and show that in many cases evolution from the sub - optimal solution to the solution is possible if sufficient time is allowed . in both cases theoretical models are presented and compared with actual runs .
the main operations in inductive logic programming ( ilp ) are generalization and specialization , which only make sense in a generality order . in ilp , the three most important generality orders are subsumption , implication and implication relative to background knowledge . the two languages used most often are languages of clauses and languages of only horn clauses . this gives a total of six different ordered languages . in this paper , we give a systematic treatment of the existence or non - existence of least generalizations and greatest specializations of finite sets of clauses in each of these six ordered sets . we survey results already obtained by others and also contribute some answers of our own . our main new results are , firstly , the existence of a computable least generalization under implication of every finite set of clauses containing at least one non - tautologous function - free clause ( among other , not necessarily function - free clauses ) . secondly , we show that such a least generalization need not exist under relative implication , not even if both the set that is to be generalized and the background knowledge are function - free . thirdly , we give a complete discussion of existence and non - existence of greatest specializations in each of the six ordered languages .
this articles discusses developments in bayesian time series mod - elling and analysis relevant in studies of time series in the physical and engineering sciences . with illustrations and references , we discuss : bayesian inference and computation in various state - space models , with examples in analysing quasi - periodic series ; isolation and modelling of various components of error in time series ; decompositions of time series into significant latent subseries ; nonlinear time series models based on mixtures of auto - regressions ; problems with errors and uncertainties in the timing of observations ; and the development of non - linear models based on stochastic deformations of time scales .
some areas of recent development and current interest in time series are noted , with some discussion of bayesian modelling efforts motivated by substantial practical problems . the areas include non - linear auto - regressive time series modelling , measurement error structures in state - space modelling of time series , and issues of timing uncertainties and time deformations . some discussion of the needs and opportunities for work on non / semi - parametric models and robustness issues is given in each context .
we present a method for the unsupervised segmentation of data streams originating from different unknown sources which alternate in time . we use an architecture consisting of competing neural networks . memory is included in order to resolve ambiguities of input - output relations . in order to obtain maximal specialization , the competition is adiabatically increased during training . our method achieves almost perfect identification and segmentation in the case of switching chaotic dynamics where input manifolds overlap and input - output relations are ambiguous . only a small dataset is needed for the training proceedure . applications to time series from complex systems demonstrate the potential relevance of our approach for time series analysis and short - term prediction .
we outline a differential theory of learning for statistical pattern classification . when applied to neural networks , the theory leads to an efficient differential learning strategy based on classification figure - of - merit ( cfm ) objective functions [ #NUM# ] . differential learning guarantees the highest probability of generalization for a classifier with limited functional complexity , trained with a limited number of examples . the theory is significant for this and two other reasons : we demonstrate the importance of differential learning ' s efficiency with a simple pattern recognition task that lends itself to closed - form analysis . we conclude with a practical application of the theory in which a differentially trained perceptron diagnoses a crippling joint disorder from magnetic resonance images better than both its probabilistically trained counterpart and more complex probabilistically trained multi - layer perceptrons . the recent renaissance of connectionism has led to a considerable amount of research regarding generalization in neural network pattern classifiers that are trained in a supervised fashion . most of this research has been done by computational learning theorists and statisticians intent on matching the functional complexity of the classifier with the size of the training sample in order to avoid the well - known curse of dimensionality ( see for example the work of barron , baum , haussler , and vapnik much of which is summarized in [ #NUM# ] ) . yet relatively little attention has been paid to the effect that the objective function ( used to drive the supervised learning procedure ) has on discrimination and generalization .
anaplastic thyroid carcinoma is a rare but very aggressive tumor . many factors that might influence the survival of patients have been suggested . the aim of our study was to determine which of the factors , known at the time of admission to the hospital , might predict survival of patients with anaplastic thyroid carcinoma . our aim was also to assess the relative importance of the factors and to identify potentially useful decision and regression trees generated by machine learning algorithms . our study included #NUM# patients ( #NUM# females and #NUM# males ; mean age was #NUM# . #NUM# years ) with anaplastic thyroid carcinoma treated at the institute of oncology ljubljana from #NUM# to #NUM# . patients were classified into categories according to #NUM# attributes : sex , age , history , physical findings , extent of disease on admission , and tumor morphology . in this paper we compare the machine learning approach with the previous statistical evaluations on the problem ( uni - variate and multivariate analysis ) and show that it can provide more thorough analysis and improve understanding of the data .
we study the worst - case behavior of a family of learning algorithms based on sutton ' s method of temporal differences . in our on - line learning framework , learning takes place in a sequence of trials , and the goal of the learning algorithm is to estimate a discounted sum of all the reinforcements that will be received in the future . in this setting , we are able to prove general upper bounds on the performance of a slightly modified version of sutton ' s so - called td ( ) algorithm . these bounds are stated in terms of the performance of the best linear predictor on the given training sequence , and are proved without making any statistical assumptions of any kind about the process producing the learner ' s observed training sequence . we also prove lower bounds on the performance of any algorithm for this learning problem , and give a similar analysis of the closely related problem of learning to predict in a model in which the learner must produce predictions for a whole batch of observations before receiving reinforcement .
the common use of static binary place - value codes for real - valued parameters of the phenotype in holland ' s genetic algorithm ( ga ) forces either the sacrifice of representational precision for efficiency of search or vice versa . dynamic parameter encoding ( dpe ) is a mechanism that avoids this dilemma by using convergence statistics derived from the ga population to adaptively control the mapping from fixed - length binary genes to real values . dpe is shown to be empirically effective and amenable to analysis ; we explore the problem of premature convergence in gas through two convergence models .
traditionally , genetic algorithms have relied upon #NUM# and #NUM# - point crossover operators . many recent empirical studies , however , have shown the benefits of higher numbers of crossover points . some of the most intriguing recent work has focused on uniform crossover , which involves on the average l / #NUM# crossover points for strings of length l . despite theoretical analysis , however , it appears difficult to predict when a particular crossover form will be optimal for a given problem . this paper describes an adaptive genetic algorithm that decides , as it runs , which form is optimal .
this paper reviews the application of gibbs sampling to a cointegrated var system . aggregate imports and import prices for belgium are modelled using two cointegrating relations . gibbs sampling techniques are used to estimate from a bayesian perspective the cointegrating relations and their weights in the var system . extensive use of spectral analysis is made to get insight into convergence issues .
we provide a generic monte carlo method to find the alternative of maximum expected utility in a decision analysis . we define an artificial distribution on the product space of alternatives and states , and show that the optimal alternative is the mode of the implied marginal distribution on the alternatives . after drawing a sample from the artificial distribution , we may use exploratory data analysis tools to approximately identify the optimal alternative . we illustrate our method for some important types of influence diagrams . ( decision analysis , influence diagrams , markov chain monte carlo , simulation )
this paper describes a new sampling - based heuristic for tree search named sage and presents an analysis of its performance on the problem of grammar induction . this last work has been inspired by the abbadingo dfa learning competition [ #NUM# ] which took place between mars and november #NUM# . sage ended up as one of the two winners in that competition . the second winning algorithm , first proposed by rod - ney price , implements a new evidence - driven heuristic for state merging . our own version of this heuristic is also described in this paper and compared to sage .
conversational case - based reasoning ( ccbr ) is a form of interactive case - based reasoning where users input a partial problem description ( in text ) . the ccbr system responds with a ranked solution display , which lists the solutions of stored cases whose problem descriptions best match the user ' s , and a ranked question display , which lists the unanswered questions in these cases . users interact with these displays , either refining their problem description by answering selected questions , or selecting a solution to apply . ccbr systems should support dialogue inferencing ; they should infer answers to questions that are implied by the problem description . otherwise , questions will be listed that the user believes they have already answered . the standard approach to dialogue inferencing allows case library designers to insert rules that define implications between the problem description and unanswered questions . however , this approach imposes substantial knowledge engineering requirements . we introduce an alternative approach whereby an intelligent assistant guides the designer in defining a model of their case library , from which implication rules are derived . we detail this approach , its benefits , and explain how it can be supported through an integration with parka - db , a fast relational database system . we will evaluate our approach in the context of our ccbr system , named nacodae . this paper appeared at the #NUM# aaai spring symposium on multimodal reasoning , and is ncarai tr aic - #NUM# - #NUM# . we introduce an integrated reasoning approach in which a model - based reasoning component performs an important inferencing role in a conversational case - based reasoning ( ccbr ) system named nacodae ( breslow & aha , #NUM# ) ( figure #NUM# ) . ccbr is a form of case - based reasoning where users enter text queries describing a problem and the system assists in eliciting refinements of it ( aha & breslow , #NUM# ) . cases have three components :
the identification , design , and implementation of strategies for cooperation is a central research issue in the field of distributed artificial intelligence ( dai ) . we propose a novel approach to the construction of cooperation strategies for a group of problem solvers based on the genetic programming ( gp ) paradigm . gps are a class of adaptive algorithms used to evolve solution structures that optimize a given evaluation criterion . our approach is based on designing a representation for cooperation strategies that can be manipulated by gps . we present results from experiments in the predator - prey domain , which has been extensively studied as a easy - to - describe but difficult - to - solve cooperation problem domain . the key aspect of our approach is the minimal reliance on domain knowledge and human intervention in the construction of good cooperation strategies . promising comparison results with prior systems lend credence to the viability of this ap proach .
recently , a new approach that involves a form of simulated evolution has been proposed for the building of autonomous robots . however , it is still not clear if this approach may be adequate to face real life problems . in this paper we show how control systems that perform a nontrivial sequence of behaviors can be obtained with this methodology by carefully designing the conditions in which the evolutionary process operates . in the experiment described in the paper , a mobile robot is trained to locate , recognize , and grasp a target object . the controller of the robot has been evolved in simulation and then downloaded and tested on the real robot .
it is open as to which chromosomal dimension performs best . although higher - dimensional encodings ( whether real or imaginary ) can preserve more geographical gene linkages , we suspect that too high a dimension would not perform desirably . we are studying the question of which dimension of encoding is best for a given instance . it is likely that the optimal dimension is somehow dependent on the chromosome size and the input graph topology ; interactions with the flexibility of crossover are yet unknown . the interaction of these considerations with the number of cuts used in the crossover is also an open issue . * in relocating genes onto a multi - dimensional chromosome , the simplest way is via a sequential assignment such as row - major order . section #NUM# showed that performance improves when a dfs - row - major reembedding is used for two - and three - dimensional encodings . we suspect that this phenomenon will be consistent for higher - dimensional cases , and hope to perform more detailed investigations in the future . although dfs reordering proved to be helpful for both linear encodings [ #NUM# ] and multi - dimensional encodings , we do not believe dfs - row - major reembedding is a good approach for the multi - dimensional cases since the row - major embedding is so simplistic . we are considering alternative #NUM# - dimensional and #NUM# - dimensional reembeddings which will hopefully provide further improvement . [ #NUM# ] t . n . bui and b . r . moon . hyperplane synthesis for genetic algorithms . in fifth international conference on genetic algorithms , pages #NUM# - #NUM# , july #NUM# . [ #NUM# ] t . n . bui and b . r . moon . analyzing hyperplane synthesis in genetic algorithms using clustered schemata . in international conference on evolutionary computation , oct . #NUM# . lecture notes in computer science , #NUM# : #NUM# - #NUM# , springer - verlag .
increasing attention has been paid to reinforcement learning algorithms in recent years , partly due to successes in the theoretical analysis of their behavior in markov environments . if the markov assumption is removed , however , neither generally the algorithms nor the analyses continue to be usable . we propose and analyze a new learning algorithm to solve a certain class of non - markov decision problems . our algorithm applies to problems in which the environment is markov , but the learner has restricted access to state information . the algorithm involves a monte - carlo policy evaluation combined with a policy improvement method that is similar to that of markov decision problems and is guaranteed to converge to a local maximum . the algorithm operates in the space of stochastic policies , a space which can yield a policy that performs considerably better than any deterministic policy . although the space of stochastic policies is continuous | even for a discrete action space | our algorithm is computationally tractable .
markov chain monte carlo ( mcmc ) algorithms have revolutionized bayesian practice . in their simplest form ( i . e . , when parameters are updated one at a time ) they are , however , often slow to converge when applied to high - dimensional statistical models . a remedy for this problem is to block the parameters into groups , which are then updated simultaneously using either a gibbs or metropolis - hastings step . in this paper we construct several ( partially and fully blocked ) mcmc algorithms for minimizing the autocorrelation in mcmc samples arising from important classes of longitudinal data models . we exploit an identity used by chib ( #NUM# ) in the context of bayes factor computation to show how the parameters in a general linear mixed model may be updated in a single block , improving convergence and producing essentially independent draws from the posterior of the parameters of interest . we also investigate the value of blocking in non - gaussian mixed models , as well as in a class of binary response data longitudinal models . we illustrate the approaches in detail with three real - data examples .
the paper presents a comparison between two feature selection methods ; the importance score ( is ) and a genetic algorithm - based ( ga ) method . the goal of both is to achieve better performing rules produced by the aq #NUM# learning system . the is method performs a greedy - like search based on an attributional score that represents the importance of each attribute in classifying the decision classes . is uses the rule testing system atest to evaluate the performance of the selected feature sets . the genetic algorithm method explores , in an efficient way , the space of all possible subsets to obtain the set of features that maximizes the predictive accuracy of the learned rules . the ga method uses the genesis system to globally search the space . it uses an evaluation function for providing a feedback about the fitness of each feature subset . the comparison is done on three real world problems , wind bracing design , accident data , and soybean data . key words : feature selection , machine learning , genetic algorithms
this paper describes a training algorithm for simple synchrony networks ( ssns ) , and reports on experiments in language learning using a recursive grammar . the ssn is a new connectionist architecture combining a technique for learning about patterns across time , simple recurrent networks ( srns ) , with temporal synchrony variable binding ( tsvb ) . the use of tsvb means the ssn can learn about entities in the training set , and generalise this information to entities in the test set . in the experiments , the network is trained on sentences with up to one embedded clause , and with some words restricted to certain classes of constituent . during testing , the network generalises information learned to sentences with up to three embedded clauses , and with words appearing in any constituent . these results demonstrate that ssns learn generalisations across syntactic constituents .
we use directed search techniques in the space of computer programs to learn recursive sequences of positive integers . specifically , the integer sequences of squares , x #NUM# ; cubes , x #NUM# ; factorial , x ! ; and fibonacci numbers are studied . given a small finite prefix of a sequence , we show that three directed searches | machine - language genetic programming with crossover , exhaustive iterative hill climbing , and a hybrid ( crossover and hill climbing ) | can automatically discover programs that exactly reproduce the finite target prefix and , moreover , that correctly produce the remaining sequence up to the underlying machine ' s precision . our machine - language representation is generic | it contains instructions for arithmetic , register manipulation and comparison , and control flow . we also introduce an output instruction that allows variable - length sequences as result values . importantly , this representation does not contain recursive operators ; recursion , when needed , is automatically synthesized from primitive instructions . for a fixed set of search parameters ( e . g . , instruction set , program size , fitness criteria ) , we compare the efficiencies of the three directed search techniques on the four sequence problems . for this parameter set , an evolutionary - based search always outperforms exhaustive hill climbing as well as undirected random search . since only the prefix of the target sequence is variable in our experiments , we posit that this approach to sequence induction is potentially quite general .
this paper demonstrates the use of graphs as a mathematical tool for expressing independenices , and as a formal language for communicating and processing causal information in statistical analysis . we show how complex information about external interventions can be organized and represented graphically and , conversely , how the graphical representation can be used to facilitate quantitative predictions of the effects of interventions . we first review the markovian account of causation and show that directed acyclic graphs ( dags ) offer an economical scheme for representing conditional independence assumptions and for deducing and displaying all the logical consequences of such assumptions . we then introduce the manipulative account of causation and show that any dag defines a simple transformation which tells us how the probability distribution will change as a result of external interventions in the system . using this transformation it is possible to quantify , from non - experimental data , the effects of external interventions and to specify conditions under which randomized experiments are not necessary . finally , the paper offers a graphical interpretation for rubin ' s model of causal effects , and demonstrates its equivalence to the manipulative account of causation . we exemplify the tradeoffs between the two approaches by deriving nonparametric bounds on treatment effects under conditions of imperfect compliance .
an interesting classical result due to jackson allows polynomial - time learning of the function class dnf using membership queries . since in most practical learning situations access to a membership oracle is unrealistic , this paper explores the possibility that quantum computation might allow a learning algorithm for dnf that relies only on example queries . a natural extension of fourier - based learning into the quantum domain is presented . the algorithm requires only an example oracle , and it runs in o ( #NUM# n ) time , a result that appears to be classically impossible . the algorithm is unique among quantum algorithms in that it does not assume a priori knowledge of a function and does not operate on a superposition that includes all possible basis states .
a number of exact algorithms have been developed to perform probabilistic inference in bayesian belief networks in recent years . these algorithms use graph - theoretic techniques to analyze and exploit network topology . in this paper , we examine the problem of efficient probabilistic inference in a belief network as a combinatorial optimization problem , that of finding an optimal factoring given an algebraic expression over a set of probability distributions . we define a combinatorial optimization problem , the optimal factoring problem , and discuss application of this problem in belief networks . we show that optimal factoring provides insight into the key elements of efficient probabilistic inference , and present simple , easily implemented algorithms with excellent performance . we also show how use of an algebraic perspective permits significant extension to the belief net representation .
explanation is an important issue in building computer - based interactive design environments in which a human designer and a knowledge system may cooperatively solve a design problem . we consider the two related problems of explaining the system ' s reasoning and the design generated by the system . in particular , we analyze the content of explanations of design reasoning and design solutions in the domain of physical devices . we describe two complementary languages : task - method - knowledge models for explaining design reasoning , and structure - behavior - function models for explaining device designs . interactive kritik is a computer program that uses these representations to visually illustrate the system ' s reasoning and the result of a design episode . the explanation of design reasoning in interactive kritik is in the context of the evolving design solution , and , similarly , the explanation of the design solution is in the context of the design reasoning .
in this paper we describe the implementation of the backpropagation algorithm by means of an object oriented library ( arch ) . the use of this library relieve the user from the details of a specific parallel programming paradigm and at the same time allows a greater portability of the generated code . to provide a comparision with existing solutions , we survey the most relevant implementations of the algorithm proposed so far in the literature , both on dedicated and general purpose computers . extensive experimental results show that the use of the library does not hurt the performance of our simulator , on the contrary our implementation on a connection machine ( cm - #NUM# ) is comparable with the fastest in its category .
typical home comfort systems utilize only rudimentary forms of energy management and conservation . the most sophisticated technology in common use today is an automatic setback thermostat . tremendous potential remains for improving the efficiency of electric and gas usage . however , home residents who are ignorant of the physics of energy utilization cannot design environmental control strategies , but neither can energy management experts who are ignorant of the behavior patterns of the inhabitants . adaptive control seems the only alternative . we have begun building an adaptive control system that can infer appropriate rules of operation for home comfort systems based on the lifestyle of the inhabitants and energy conservation goals . recent research has demonstrated the potential of neural networks for intelligent control . we are constructing a prototype control system in an actual residence using neural network reinforcement learning and prediction techniques . the residence is equipped with sensors to provide information about environmental conditions ( e . g . , temperatures , ambient lighting level , sound and motion in each room ) and actuators to control the gas furnace , electric space heaters , gas hot water heater , lighting , motorized blinds , ceiling fans , and dampers in the heating ducts . this paper presents an overview of the project as it now stands .
today there is a great interest in discovering methods that allow a faster design and development of real - time control software . control theory helps when linear controllers have to be developed but it does not support the generation in this paper , it is discussed how machine learning has been applied to the function , and locally receptive field function approximators . three integrated learning algorithms , two of which are original , are described and then tried on two experimental test cases . the first test case is provided by an industrial robot kuka ir - #NUM# engaged into the " peg - into - hole " task , while the second is a classical prediction task on the mackey - glass chaotic series . from the experimental comparison , it appears that both fuzzy controllers and rbfns synthesised from examples are excellent approximators , and that they can be even more accurate than mlps . of non - linear controllers , which in many cases ( such as in compliant motion control )
the term soft computing ( sc ) represents the combination of emerging problem - solving technologies such as fuzzy logic ( fl ) , probabilistic reasoning ( pr ) , neural networks ( nns ) , and genetic algorithms ( gas ) . each of these technologies provide us with complementary reasoning and searching methods to solve complex , real - world problems . after a brief description of each of these technologies , we will analyze some of their most useful combinations , such as the use of fl to control gas and nns parameters ; the application of gas to evolve nns ( topologies or weights ) or to tune fl controllers ; and the implementation of fl controllers as nns tuned by backpropagation - type algorithms .
two issues of an intelligent navigation robot have been addressed in this work . first is the robot ' s ability to learn a representation of the local environment and use this representation to identify which local environment it is in . this is done by first extracting features from the sensors which are more informative than just distances of obstacles in various directions . using these features a reduced ring representation ( rrr ) of the local environment is derived . as the robot navigates , it learns the rrr signatures of all the new environment types it encounters . for purpose of identification , a ring matching criteria is proposed where the robot tries to match the rrr from the sensory input to one of the rrrs in its library . the second issue addressed is that of learning hill climbing control laws in the local environments . unlike conventional neuro - controllers , a reinforcement learning framework , where the robot first learns a model of the environment and then learns the control law in terms of a neural network is proposed here . the reinforcement function is generated from the sensory inputs of the robot before and after a control action is taken . three key results shown in this work are that ( #NUM# ) the robot is able to build its library of rrr signatures perfectly even with significant sensor noise for eight different local environ - mets , ( #NUM# ) it was able to identify its local environment with an accuracy of more than #NUM# % , once the library is build , and ( #NUM# ) the robot was able to learn adequate hill climbing control laws which take it to the distinctive state of the local environment for five different environment types .
as artificial neural networks ( anns ) gain popularity in a variety of application domains , it is critical that these models run fast and generate results in real time . although a number of implementations of neural networks are available on sequential machines , most of these implementations require an inordinate amount of time to train or run anns , especially when the ann models are large . one approach for speeding up the implementation of anns is to implement them on parallel machines . this paper surveys the area of parallel environments for the implementations of anns , and prescribes desired characteristics to look for in such implementations .
we derive distribution - free uniform test error bounds that improve on vc - type bounds for validation . we show how to use knowledge of test inputs to improve the bounds . the bounds are sharp , but they require intense computation . we introduce a method to trade sharpness for speed of computation . also , we compute the bounds for several test cases .
connectionist research is firmly established within the scientific community , especially within the multi - disciplinary field of cognitive science . this diversity , however , has created an environment which makes it difficult for connectionist researchers to remain aware of recent advances in the field , let alone understand how the field has developed . this paper attempts to address this problem by providing a brief guide to connectionist research . the paper begins by defining the basic tenets of connectionism . next , the development of connectionist research is traced , commencing with connectionism ' s philosophical predecessors , moving to early psychological and neuropsychological influences , followed by the mathematical and computing contributions to connectionist research . current research is then reviewed , focusing specifically on the different types of network architectures and learning rules in use . the paper concludes by suggesting that neural network research | at least in cognitive science | should move towards models that incorporate the relevant functional principles inherent in neurobiological systems .
the sensorimotor integration system can be viewed as an observer attempting to estimate its own state and the state of the environment by integrating multiple sources of information . we describe a computational framework capturing this notion , and some specific models of integration and adaptation that result from it . psychophysical results from two sensorimotor systems , subserving the integration and adaptation of visuo - auditory maps , and estimation of the state of the hand during arm movements , are presented and analyzed within this framework . these results suggest that ( #NUM# ) spatial information from visual and auditory systems is integrated so as to reduce the variance in localization . ( #NUM# ) the effects of a remapping in the relation between visual and auditory space can be predicted from a simple learning rule . ( #NUM# ) the temporal propagation of errors in estimating the hand ' s state is captured by a linear dynamic observer , providing evidence for the existence of an internal model which simulates the dynamic behavior of the arm .
several researchers have demonstrated how complex action sequences can be learned through neuro - evolution ( i . e . evolving neural networks with genetic algorithms ) . however , complex general behavior such as evading predators or avoiding obstacles , which is not tied to specific environments , turns out to be very difficult to evolve . often the system discovers mechanical strategies ( such as moving back and forth ) that help the agent cope , but are not very effective , do not appear believable and would not generalize to new environments . the problem is that a general strategy is too difficult for the evolution system to discover directly . this paper proposes an approach where such complex general behavior is learned incrementally , by starting with simpler behavior and gradually making the task more challenging and general . the task transitions are implemented through successive stages of delta - coding ( i . e . evolving modifications ) , which allows even converged populations to adapt to the new task . the method is tested in the stochastic , dynamic task of prey capture , and compared with direct evolution . the incremental approach evolves more effective and more general behavior , and should also scale up to harder tasks .
go is a difficult game for computers to master , and the best go programs are still weaker than the average human player . since the traditional game playing techniques have proven inadequate , new approaches to computer go need to be studied . this paper presents a new approach to learning to play go . the sane ( symbiotic , adaptive neuro - evolution ) method was used to evolve networks capable of playing go on small boards with no pre - programmed go knowledge . on a #NUM# fi #NUM# go board , networks that were able to defeat a simple computer opponent were evolved within a few hundred generations . most significantly , the networks exhibited several aspects of general go playing , which suggests the approach could scale up well .
recent studies on a floating building block representation for the genetic algorithm ( ga ) suggest that there are many advantages to using the floating representation . this paper investigates the behavior of the ga on floating representation problems in response to three different types of pressures : ( #NUM# ) a reduction in the amount of genetic material available to the ga during the problem solving process , ( #NUM# ) functions which have negative - valued building blocks , and ( #NUM# ) randomizing non - coding segments . results indicate that the ga ' s performance on floating representation problems is very robust . significant reductions in genetic material ( genome length ) may be made with relatively small decrease in performance . the ga can effectively solve problems with negative building blocks . randomizing non - coding segments appears to improve rather than harm ga performance .
we analyze a simple hill - climbing algorithm ( rmhc ) that was previously shown to outperform a genetic algorithm ( ga ) on a simple " royal road " function . we then analyze an " idealized " genetic algorithm ( iga ) that is significantly faster than rmhc and that gives a lower bound for ga speed . we identify the features of the iga that give rise to this speedup , and discuss how these features can be incorporated into a real ga .
we consider the recently proposed parallel variable distribution ( pvd ) algorithm of ferris and mangasarian [ #NUM# ] for solving optimization problems in which the variables are distributed among p processors . each processor has the primary responsibility for updating its block of variables while allowing the remaining " secondary " variables to change in a restricted fashion along some easily computable directions . we propose useful generalizations that consist , for the general unconstrained case , of replacing exact global solution of the subproblems by a certain natural sufficient descent condition , and , for the convex case , of inexact subproblem solution in the pvd algorithm . these modifications are the key features of the algorithm that has not been analyzed before . the proposed modified algorithms are more practical and make it easier to achieve good load balancing among the parallel processors . we present a general framework for the analysis of this class of algorithms and derive some new and improved linear convergence results for problems with weak sharp minima of order #NUM# and strongly convex problems . we also show that nonmonotone synchronization schemes are admissible , which further improves flexibility of pvd approach .
a paradigm of statistical mechanics of financial markets ( smfm ) is fit to multivariate financial markets using adaptive simulated annealing ( asa ) , a global optimization algorithm , to perform maximum likelihood fits of lagrangians defined by path integrals of multivariate conditional probabilities . canonical momenta are thereby derived and used as technical indicators in a recursive asa optimization process to tune trading rules . these trading rules are then used on out - of - sample data , to demonstrate that they can profit from the smfm model , to illustrate that these markets are likely not efficient . this methodology can be extended to other systems , e . g . , electroencephalography . this approach to complex systems emphasizes the utility of blending an intuitive and powerful mathematical - physics formalism to generate indicators which are used by ai - type rule - based models of management .
the computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on mcculloch pitts neurons ( i . e . threshold gates ) respectively sigmoidal gates . in particular it is shown that networks of spiking neurons are computationally more powerful than these other neural network models . a concrete biologically relevant function is exhibited which can be computed by a single spiking neuron ( for biologically reasonable values of its parameters ) , but which requires hundreds of hidden units on a sigmoidal neural net . this article does not assume prior knowledge about spiking neurons , and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neuro biology .
we compare genetic algorithms ( ga ) with a functional search method , very fast simulated reannealing ( vfsr ) , that not only is efficient in its search strategy , but also is statistically guaranteed to find the function optima . ga previously has been demonstrated to be competitive with other standard boltzmann - type simulated annealing techniques . presenting a suite of six standard test functions to ga and vfsr codes from previous studies , without any additional fine tuning , strongly suggests that vfsr can be expected to be orders of magnitude more efficient than ga .
in recent years , machine learning research has started addressing a problem known as theory refinement . the goal of a theory refinement learner is to modify an incomplete or incorrect rule base , representing a domain theory , to make it consistent with a set of input training examples . this paper presents a major revision of the either propositional theory refinement system . two issues are discussed . first , we show how run time efficiency can be greatly improved by changing from a exhaustive scheme for computing repairs to an iterative greedy method . second , we show how to extend either to refine m - of - n rules . the resulting algorithm , neither ( new either ) , is more than an order of magnitude faster and produces significantly more accurate results with theories that fit the m - of - n format . to demonstrate the advantages of neither , we present experimental results from two real - world domains .
predictability minimization ( pm | schmidhuber , #NUM# ) exhibits various intuitive and theoretical advantages over many other methods for unsupervised redundancy reduction . so far , however , there were only toy applications of pm . in this paper , we apply semilinear pm to static real world images and find : without a teacher and without any significant preprocessing , the system automatically learns to generate distributed representations based on well - known feature detectors , such as orientation sensitive edge detectors and off - center - on - surround - like structures , thus extracting simple features related to those considered useful for image pre - processing and compression .
a learner ' s modifiable components are called its policy . an algorithm that modifies the policy is a learning algorithm . if the learning algorithm has modifiable components represented as part of the policy , then we speak of a self - modifying policy ( smp ) . smps can modify the way they modify themselves etc . they are of interest in situations where the initial learning algorithm itself can be improved by experience | this is what we call " learning to learn " . how can we force some ( stochastic ) smp to trigger better and better self - modifications ? the success - story algorithm ( ssa ) addresses this question in a lifelong reinforcement learning context . during the learner ' s life - time , ssa is occasionally called at times computed according to smp itself . ssa uses backtracking to undo those smp - generated smp - modifications that have not been empirically observed to trigger lifelong reward accelerations ( measured up until the current ssa call | this evaluates the long - term effects of smp - modifications setting the stage for later smp - modifications ) . smp - modifications that survive ssa represent a lifelong success history . until the next ssa call , they build the basis for additional smp - modifications . solely by self - modifications our smp / ssa - based learners solve a complex task in a partially observable environment ( poe ) whose state space is far bigger than most reported in the poe literature .
we study task sequences that allow for speeding up the learner ' s average reward intake through appropriate shifts of inductive bias ( changes of the learner ' s policy ) . to evaluate long - term effects of bias shifts setting the stage for later bias shifts we use the " success - story algorithm " ( ssa ) . ssa is occasionally called at times that may depend on the policy itself . it uses backtracking to undo those bias shifts that have not been empirically observed to trigger long - term reward accelerations ( measured up until the current ssa call ) . bias shifts that survive ssa represent a lifelong success history . until the next ssa call , they are considered useful and build the basis for additional bias shifts . ssa allows for plugging in a wide variety of learning algorithms . we plug in ( #NUM# ) a novel , adaptive extension of levin search and ( #NUM# ) a method for embedding the learner ' s policy modification strategy within the policy itself ( incremental self - improvement ) . our inductive transfer case studies involve complex , partially observable environments where traditional reinforcement learning fails .
the inductive logic programming system lopster was created to demonstrate the advantage of basing induction on logical implication rather than - subsumption . lopster ' s sub - unification procedures allow it to induce recursive relations using a minimum number of examples , whereas inductive logic programming algorithms based on - subsumption require many more examples to solve induction tasks . however , lopster ' s input examples must be carefully chosen ; they must be along the same inverse resolution path . we hypothesize that an extension of lopster can efficiently induce recursive relations without this requirement . we introduce a generalization of lopster named crustacean that has this capability and empirically evaluate its ability to induce recursive relations .
submitted to nips - #NUM# td ( ) is a popular family of algorithms for approximate policy evaluation in large mdps . td ( ) works by incrementally updating the value function after each observed transition . it has two major drawbacks : it makes inefficient use of data , and it requires the user to manually tune a stepsize schedule for good performance . for the case of linear value function approximations and = #NUM# , the least - squares td ( lstd ) algorithm of bradtke and barto [ #NUM# ] eliminates all stepsize parameters and improves data efficiency . this paper extends bradtke and barto ' s work in three significant ways . first , it presents a simpler derivation of the lstd algorithm . second , it generalizes from = #NUM# to arbitrary values of ; at the extreme of = #NUM# , the resulting algorithm is shown to be a practical formulation of supervised linear regression . third , it presents a novel , intuitive interpretation of lstd as a model - based reinforcement learning technique .
simulated annealing | moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions | has traditionally been used as an inexact method of handling isolated modes in markov chain samplers . here , it is shown how one can use the markov chain transitions for such an annealing sequence to define an importance sampler . the markov chain aspect allows this method to perform acceptably even for high - dimensional problems , where finding good importance sampling distributions would otherwise be very difficult , while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases . this annealed importance sampling procedure resembles the second half of the previously - studied tempered transitions , and can be seen as a generalization of a recently - proposed variant of sequential importance sampling . it is also related to thermodynamic integration methods for estimating ratios of normalizing constants . annealed importance sampling is most attractive when isolated modes are present , or when estimates of normalizing constants are required , but it may also be more generally useful , since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in markov chain samplers .
the genetic programming optimization method ( gp ) elaborated by john koza [ koza , #NUM# ] is a variant of genetic algorithms . the search space of the problem domain consists of computer programs represented as parse trees , and the crossover operator is realized by an exchange of subtrees . empirical analyses show that large parts of those trees are never used or evaluated which means that these parts of the trees are irrelevant for the solution or redundant . this paper is concerned with the identification of the redundancy occuring in gp . it starts with a mathematical description of the behavior of gp and the conclusions drawn from that description among others explain the " size problem " which denotes the phenomenon that the average size of trees in the population grows with time .
for the well - known concept of decision trees as it is used for inductive inference we study the natural concept of equivalence : two decision trees are equivalent if and only if they represent the same hypothesis . we present a simple efficient algorithm to establish whether two decision trees are equivalent or not . the complexity of this algorithm is bounded by the product of the sizes of both decision trees . the hypothesis represented by a decision tree is essentially a boolean function , just like a proposition . although every boolean function can be represented in this way , we show that disjunctions and conjunctions of decision trees can not efficiently be represented as decision trees , and simply shaped propositions may require exponential size for representation as de cision trees .
we propose to assess the relevance of theories of synaptic modification as models of feature extraction in human vision , by using masks derived from synaptic weight patterns to occlude parts of the stimulus images in psychophysical experiments . in the experiment reported here , we found that a mask derived from principal component analysis of object images was more effective in reducing the generalization performance of human subjects than a mask derived from another method of feature extraction ( bcm ) , based on higher - order statistics of the images .
a two dimensional time - dependent duffing oscillator model of macroscopic neocortex exhibits chaos for some ranges of parameters . we embed this model in moderate noise , typical of the context presented in real neocortex , using pathint , a non - monte - carlo path - integral algorithm that is particularly adept in handling nonlinear fokker - planck systems . this approach shows promise to investigate whether chaos in neocortex , as predicted by such models , can survive in noisy contexts .
in this paper , we employ the genetic programming paradigm to enable a computer to learn to play strategies for the ancient egyptian boardgame senet by evolving board evaluation functions . formulating the problem in terms of board evaluation functions made it feasible to evaluate the fitness of game playing strategies by using tournament - style fitness evaluation . the game has elements of both strategy and chance . our approach learns strategies which enable the computer to play consistently at a reasonably skillful level .
this paper introduces a new algorithm , q #NUM# , for optimizing the expected output of a multi - input noisy continuous function . q #NUM# is designed to need only a few experiments , it avoids strong assumptions on the form of the function , and it is autonomous in that it requires little problem - specific tweaking . four existing approaches to this problem ( response surface methods , numerical optimization , supervised learning , and evolutionary methods ) all have inadequacies when the requirement of " black box " behavior is combined with the need for few experiments . q #NUM# uses instance - based determination of a convex region of interest for performing experiments . in conventional instance - based approaches to learning , a neighborhood was defined by proximity to a query point . in contrast , q #NUM# defines the neighborhood by a new geometric procedure that captures the size and shape of the zone of possible optimum locations . q #NUM# also optimizes weighted combinations of outputs , and finds inputs to produce target outputs . we compare q #NUM# with other optimizers of noisy functions on several problems , including a simulated noisy process with both non - linear continuous dynamics and discrete - event queueing components . results are encouraging in terms of both speed and autonomy .
in this paper , we propose a multi - classification approach for constructive induction . the idea of an improvement of classification accuracy is based on iterative modification of input data space . this process is independently repeated for each pair of n classes . finally , it gives ( n #NUM# n ) / #NUM# input data subspaces of attributes dedicated for optimal discrimination of appropriate pairs of classes . we use genetic algorithms as a constructive induction engine . a final classification is obtained by a weighted majority voting rule , according to n #NUM# - classifier approach . the computational experiment was performed on medical data set . the obtained results point out the advantage of using a multi - classification model ( n #NUM# classifier ) in constructive induction in relation to the analogous single - classifier approach .
this highly interdisciplinary project extends previous work in combat modeling and in control - theoretic descriptions of decision - making human factors in complex activities . a previous paper has established the first theory of the statistical mechanics of combat ( smc ) , developed using modern methods of statistical mechanics , baselined to empirical data gleaned from the national training center ( ntc ) . this previous project has also established a janus ( t ) - ntc computer simulation / wargame of ntc , providing a statistical ` ` what - if ' ' capability for ntc scenarios . this mathematical formulation is ripe for control - theoretic extension to include human factors , a methodology previously developed in the context of teleoperated vehicles . similar ntc scenarios differing at crucial decision points will be used for data to model the inuence of decision making on combat . the results may then be used to improve present human factors and c #NUM# algorithms in computer simulations / wargames . our approach is to ` ` subordinate ' ' the smc nonlinear stochastic equations , fitted to ntc scenarios , to establish the zeroth order description of that combat . in practice , an equivalent mathematical - physics representation is used , more suitable for numerical and formal work , i . e . , a lagrangian representation . theoretically , these equations are nested within a larger set of nonlinear stochastic operator - equations which include c #NUM# human factors , e . g . , supervisory decisions . in this study , we propose to perturb this operator theory about the smc zeroth order set of equations . then , subsets of scenarios fit to zeroth order , originally considered to be similarly degenerate , can be further split perturbatively to distinguish c #NUM# decision - making inuences . new methods of very fast simulated re - annealing ( vfsr ) , developed in the previous project , will be used for fitting these models to empirical data .
the work in progress reported by wright & liley shows great promise , primarily because of their experimental and simulation paradigms . however , their tentative conclusion that macroscopic neocortex may be considered ( approximately ) a linear near - equilibrium system is premature and does not correspond to tentative conclusions drawn from other studies of neocortex . at this time , there exists an interdisciplinary multidimensional gradation on published studies of neocortex , with one primary dimension of mathematical physics represented by two extremes . at one extreme , there is much scientifically unsupported talk of chaos and quantum physics being responsible for many important macroscopic neocortical processes ( involving many thousands to millions of neurons ) ( wilczek , #NUM# ) . at another extreme , many non - mathematically trained neuroscientists uncritically lump all neocortical mathematical theory into one file , and consider only statistical averages of citations for opinions on the quality of that research ( nunez , #NUM# ) . in this context , it is important to appreciate that wright and liley ( w & l ) report on their scientifically sound studies on macroscopic neocortical function , based on simulation and a blend of sound theory and reproducible experiments . however , their pioneering work , given the absence of much knowledge of neocortex at this time , is open to criticism , especially with respect to their present inferences and conclusions . their conclusion that eeg data exhibit linear near - equilibrium dynamics may very well be true , but only in the sense of focusing only on one local minima , possibly with individual - specific and physiological - state dependent
traditional evolutionary optimization algorithms assume a static evaluation function , according to which solutions are evolved . incremental evolution is an approach through which a dynamic evaluation function is scaled over time in order to improve the performance of evolutionary optimization . in this paper , we present empirical results that demonstrate the effectiveness of this approach for genetic programming . using two domains , a two - agent pursuit - evasion game and the tracker [ #NUM# ] trail - following task , we demonstrate that incremental evolution is most successful when applied near the beginning of an evolutionary run . we also show that incremental evolution can be successful when the intermediate evaluation functions are more difficult than the target evaluation function , as well as when they are easier than the target function .
nk - landscapes offer the ability to assess the performance of evolutionary algorithms on problems with different degrees of epistasis . in this paper , we study the performance of six algorithms in nk - landscapes with low and high dimension while keeping the amount of epistatic interactions constant . the results show that compared to genetic local search algorithms , the performance of standard genetic algorithms employing crossover or mutation significantly decreases with increasing problem size . furthermore , with increasing k , crossover based algorithms are in both cases outperformed by mutation based algorithms . however , the relative performance differences between the algorithms grow significantly with the dimension of the search space , indicating that it is important to consider high - dimensional landscapes for evaluating the performance of evolutionary algorithms .
theories of rational belief revision recently proposed by gardenfors and nebel illuminate many important issues but impose unnecessarily strong standards for correct revisions and make strong assumptions about what information is available to guide revisions . we reconstruct these theories according to an economic standard of rationality in which preferences are used to select among alternative possible revisions . by permitting multiple partial specifications of preferences in ways closely related to preference - based nonmonotonic logics , the reconstructed theory employs information closer to that available in practice and offers more flexible ways of selecting revisions . we formally compare this notion of rational belief revision with those of gardenfors and nebel , adapt results about universal default theories to prove that there is no universal method of rational belief revision , and examine formally how different limitations on rationality affect belief revision .
independent component analysis ( ica ) is a statistical signal processing technique whose main applications are blind source separation , blind deconvolution , and feature extraction . estimation of ica is usually performed by optimizing a ' contrast ' function based on higher - order cumulants . in this paper , it is shown how almost any error function can be used to construct a contrast function to perform the ica estimation . in particular , this means that one can use contrast functions that are robust against outliers . as a practical method for tnding the relevant extrema of such contrast functions , a txed - point iteration scheme is then introduced . the resulting algorithms are quite simple and converge fast and reliably . these algorithms also enable estimation of the independent components one - by - one , using a simple deation scheme .
this article will appear in volume #NUM# , no . #NUM# ( #NUM# ) of journal of applied statistical science . adrian e . raftery is professor of statistics and sociology , department of statistics , gn - #NUM# , university of washington , seattle , wa #NUM# . this research was supported by onr contract no . n - #NUM# - #NUM# - j - #NUM# , by nih grant no . #NUM# r #NUM# hd #NUM# - #NUM# , by the ministere de la recherche et de l ' espace , paris , by the universite de paris vi , and by inria , rocquencourt , france . raftery thanks the latter two institutions , paul deheuvels and gilles celeux for hearty hospitality during his paris sabbatical in which this article was written . this article was prepared for presentation at the conference on applied change point analysis , university of maryland - baltimore , march #NUM# - #NUM# , #NUM# . parts of this article review collaborative research with others to whom i would like to express my appreciation , namely volkan akman , jeff banfield , nhu le , steven lewis , doug martin , fionn murtagh , ross taplin and simon tavare .
we examine the efficient implementation of back prop type algorithms on t #NUM# [ #NUM# ] , a vector processor with a fixed point engine , designed for neural network simulation . a matrix formulation of back prop , matrix back prop [ #NUM# ] , has been shown to be very efficient on some riscs [ #NUM# ] . using matrix back prop , we achieve an asymptotically optimal performance on t #NUM# ( about #NUM# . #NUM# gops ) for both forward and backward phases , which is not possible with the standard on - line method . since high efficiency is futile if convergence is poor ( due to the use of fixed point arithmetic ) , we use a mixture of fixed and floating point operations . the key observation is that the precision of fixed point is sufficient for good convergence , if the range is appropriately chosen . though the most expensive computations are implemented in fixed point , we achieve a rate of convergence that is comparable to the floating point version . the time taken for conversion between fixed and floating point is also shown to be reasonable .
in this paper we discuss our approach to learning classification rules from data . we sketch out two modules of our architecture , namely linneo + and gar . linneo + , which is a knowledge acquisition tool for ill - structured domains automatically generating classes from examples that incrementally works with an unsupervised strategy . linneo + ' s output , a representation of the conceptual structure of the domain in terms of classes , is the input to gar that is used to generate a set of classification rules for the original training set . gar can generate both conjunctive and disjunctive rules . herein we present an application of these techniques to data obtained from a real wastewater treatment plant in order to help the construction of a rule base . this rule will be used for a knowledge - based system that aims to supervise the whole process .
when reading a sentence such as " the diplomat threw the ball in the ballpark for the princess " our interpretation changes from a dance event to baseball and back to dance . such on - line disambiguation happens automatically and appears to be based on dynamically combining the strengths of association between the keywords and the two senses . subsymbolic neural networks are very good at modeling such behavior . they learn word meanings as soft constraints on interpretation , and dynamically combine these constraints to form the most likely interpretation . on the other hand , it is very difficult to show how systematic language structures such as relative clauses could be processed in such a system . the network would only learn to associate them to specific contexts and would not be able to process new combinations of them . a closer look at understanding embedded clauses shows that humans are not very systematic in processing grammatical structures either . for example , " the girl who the boy who the girl who lived next door blamed hit cried " is very difficult to understand , whereas " the car that the man who the dog that had rabies bit drives is in the garage " is not . this difference emerges from the same semantic constraints that are at work in the disambiguation task . in this chapter we will show how the subsymbolic parser can be combined with high - level control that allows the system to process novel combinations of relative clauses systematically , while still being sensitive to the semantic constraints .
we investigated the generalization capabilities of backpropagation learning in feed - forward and recurrent feed - forward connectionist networks on the assignment of syllable boundaries to orthographic representations in dutch ( hyphenation ) . this is a difficult task because phonological and morphological constraints interact , leading to ambiguity in the input patterns . we compared the results to different symbolic pattern matching approaches , and to an exemplar - based generalization scheme , related to a k - nearest neighbour approach , but using a similarity metric weighed by the relative information entropy of positions in the training patterns . our results indicate that the generalization performance of backpropagation learning for this task is not better than that of the best symbolic pattern matching approaches , and of exemplar - based generalization .
we present a framework for incorporating pruning strategies in the mtiling constructive neural network learning algorithm . pruning involves elimination of redundant elements ( connection weights or neurons ) from a network and is of considerable practical interest . we describe three elementary sensitivity based strategies for pruning neurons . experimental results demonstrate a moderate to significant reduction in the network size without compromising the network ' s generalization performance .
a number of neural learning rules have been recently proposed for independent component analysis ( ica ) . the rules are usually derived from information - theoretic criteria such as maximum entropy or minimum mutual information . in this paper , we show that in fact , ica can be performed by very simple hebbian or anti - hebbian learning rules , which may have only weak relations to such information - theoretical quantities . rather suprisingly , practically any non - linear function can be used in the learning rule , provided only that the sign of the hebbian / anti - hebbian term is chosen correctly . in addition to the hebbian - like mechanism , the weight vector is here constrained to have unit norm , and the data is preprocessed by prewhitening , or sphering . these results imply that one can choose the non - linearity so as to optimize desired statistical or numerical criteria .
there are currently several types of constructive , or growth , algorithms available for training a feed - forward neural network . this paper describes and explains the main ones , using a fundamental approach to the multi - layer perceptron problem - solving mechanisms . the claimed convergence properties of the algorithms are verified using just two mapping theorems , which consequently enables all the algorithms to be unified under a basic mechanism . the algorithms are compared and contrasted and the deficiencies of some highlighted . the fundamental reasons for the actual success of these algorithms are extracted , and used to suggest where they might most fruitfully be applied . a suspicion that they are not a panacea for all current neural network difficulties , and that one must somewhere along the line pay for the learning efficiency they promise , is developed into an argument that their generalization abilities will lie on average below that of back - propagation .
prioritized sweeping is a model - based reinforcement learning method that attempts to focus an agent ' s limited computational resources to achieve a good estimate of the value of environment states . to choose effectively where to spend a costly planning step , classic prioritized sweeping uses a simple heuristic to focus computation on the states that are likely to have the largest errors . in this paper , we introduce generalized prioritized sweeping , a principled method for generating such estimates in a representation - specific manner . this allows us to extend prioritized sweeping beyond an explicit , state - based representation to deal with compact representations that are necessary for dealing with large state spaces . we apply this method for generalized model approximators ( such as bayesian networks ) , and describe preliminary experiments that compare our approach with classical prioritized sweeping .
constructive learning algorithms offer an approach for incremental construction of potentially near - minimal neural network architectures for pattern classification tasks . such algorithms help overcome the need for ad - hoc and often inappropriate choice of network topology in the use of algorithms that search for a suitable weight setting in an otherwise a - priori fixed network architecture . several such algorithms proposed in the literature have been shown to converge to zero classification errors ( under certain assumptions ) on a finite , non - contradictory training set in a #NUM# - category classification problem . this paper explores multi - category extensions of several constructive neural network learning algorithms for pattern classification . in each case , we establish the convergence to zero classification errors on a multi - category classification task ( under certain assumptions ) . results of experiments with non - separable multi - category data sets demonstrate the feasibility of this approach to multi - category pattern classification and also suggest several interesting directions for future research .
as real logic programmers normally use cut ( ! ) , an effective learning procedure for logic programs should be able to deal with it . because the cut predicate has only a procedural meaning , clauses containing cut cannot be learned using an extensional evaluation method , as is done in most learning systems . on the other hand , searching a space of possible programs ( instead of a space of independent clauses ) is unfeasible . an alternative solution is to generate first a candidate base program which covers the positive examples , and then make it consistent by inserting cut where appropriate . the problem of learning programs with cut has not been investigated before and this seems to be a natural and reasonable approach . we generalize this scheme and investigate the difficulties that arise . some of the major shortcomings are actually caused , in general , by the need for intensional evaluation . as a conclusion , the analysis of this paper suggests , on precise and technical grounds , that learning cut is difficult , and current induction techniques should probably be restricted to purely declarative logic languages .
neural one - unit learning rules for the problem of independent component analysis ( ica ) and blind source separation are introduced . in these new algorithms , every ica neuron develops into a separator that tnds one of the independent components . the learning rules use very simple constrained hebbian / anti - hebbian learning in which decorrelating feedback may be added . to speed up the convergence of these stochastic gradient descent rules , a novel com putationally ecient txed - point algorithm is introduced .
connectionist models is a collection of forty papers representing a wide variety of research topics in connectionism . the book is distinguished by a single feature : the papers are almost exclusively contributions of graduate students active in the field . the students were selected by a rigorous review process and participated in a two week long summer school devoted to connectionism #NUM# . as the ambitious editors state in the foreword : these are bold claims and , if true , the reader is presented with an exciting opportunity to sample the frontiers of connectionism . their words imply two ways to approach the book . the book must be read not just as a random collection of scientific papers , but also as a challenge to evaluate a controversial field . #NUM# this summer school is actually the third in a series , previous ones being held in #NUM# and #NUM# . the proceedings of the #NUM# summer school ( which i had the priviledge of participating in ) are reviewed by nigel goddard in [ #NUM# ] . continuing the pattern , a fourth school is scheduled to be held in #NUM# in boulder , co .
a knowledge - based system uses its database ( a . k . a . its " theory " ) to produce answers to the queries it receives . unfortunately , these answers may be incorrect if the underlying theory is faulty . standard " theory revision " systems use a given set of " labeled queries " ( each a query paired with its correct answer ) to transform the given theory , by adding and / or deleting either rules and / or antecedents , into a related theory that is as accurate as possible . after formally defining the theory revision task , this paper provides both sample and computational complexity bounds for this process . it first specifies the number of labeled queries necessary to identify a revised theory whose error is close to minimal with high probability . it then considers the computational complexity of finding this best theory , and proves that , unless p = n p , no polynomial time algorithm can identify this near - optimal revision , even given the exact distribution of queries , except in the most trivial of situations . it also shows that , except in such trivial situations , no polynomial - time algorithm can produce a theory whose error is even close to ( i . e . , within a particular polynomial factor of ) optimal . these results suggest reasons why theory revision can be more effective than learning from scratch , and also justify many aspects of the standard theory revision systems , including the practice of hill - climbing to a locally - optimal theory , based on a given set of labeled queries .
this paper investigates a dynamic path - based method for constructing conjunctions as new attributes for decision tree learning . it searches for conditions ( attribute - value pairs ) from paths to form new attributes . compared with other hypothesis - driven new attribute construction methods , the new idea of this method is that it carries out systematic search with pruning over each path of a tree to select conditions for generating a conjunction . therefore , conditions for constructing new attributes are dynamically decided during search . empirically evaluation in a set of artificial and real - world domains shows that the dynamic path - based method can improve the performance of selective decision tree learning in terms of both higher prediction accuracy and lower theory complexity . in addition , it shows some performance advantages over a fixed path - based method and a fixed rule - based method for learning decision trees .
numerous recent papers focus on standard recurrent nets ' problems with long time lags between relevant signals . some propose rather sophisticated , alternative methods . we show : many problems used to test previous methods can be solved more quickly by random weight guessing .
this paper contributes to the study of nonlinear dynamical systems from a computational perspective . these systems are inherently more powerful than their linear counterparts ( such as markov chains ) , which have had a wide impact in computer science , and they seem likely to play an increasing role in future . however , there are as yet no general techniques available for handling the computational aspects of discrete nonlinear systems , and even the simplest examples seem very hard to analyze . we focus in this paper on a class of quadratic systems that are widely used as a model in population genetics and also in genetic algorithms . these systems describe a process where random matings occur between parental chromosomes via a mechanism known as " crossover " : i . e . , children inherit pieces of genetic material from different parents according to some random rule . our results concern two fundamental quantitative properties of crossover systems : #NUM# . we develop a general technique for computing the
evolution is a stochastic process which operates on the dna of species . the evolutionary process leaves tell - tale signs in the dna which can be used to construct phylogenies , or evolutionary trees , for a set of species . maximum likelihood estimations ( mle ) methods seek the evolutionary tree which is most likely to have produced the dna under consideration . while these methods are widely accepted and intellectually satisfying , they are computationally intractable . in this paper , we address the intractability of mle methods as follows . we introduce a metric on " evolutionary " stochastic process , and show that this metric is meaningful by giving a lower - bound on the learnability of the true phylogeny in terms of our metric measure . we complement this result with a simple and efficient algorithm for inverting the stochastic process of evolution , that is , for building the tree from observations on the dna of the species . put another way , we show that we can pac - learn phylogenies . though there have been many heuristics suggested for this problem , our algorithm is the first algorithm with a guaranteed convergence rate , and further , this rate is within a polynomial of the lower - bound rate we establish . our algorithm is also the the first polynomial time algorithm which is guaranteed to converge at all to the correct tree .
much has been done to develop learning techniques for delayed reward problems in worlds where the actions and / or states are approximated by discrete representations . although this is acceptable in some applications there are many more situations where such an approximation is difficult and unnatural . for instance , in applications such as robotic , s where real machines interact with the real world , learning techniques that use real valued continuous quantities are required . presented in this paper is an extension to q - learning that uses both real valued states and actions . this is achieved by introducing activation strengths to each actuator system of the robot . this allow all actuators to be active to some continuous amount simultaneously . learning occurs by incrementally adapting both the expected future reward to goal evaluation function and the gradients of that function with respect to each actuator system .
connections between stochastic smoothing / filtering and estimation with incomplete data are investigated . it is shown , under the right censoring scheme , that the kaplan - meier estimator can be characterized as a moment estimate based on a stochastic filter / smoother ( a pseudo - filter / smoother ) . motivated by this result , a potentially useful martingale approach for estimation and convergence with incomplete data is proposed estimators are characterized as pseudo - stochastic smoothers ( which sometimes reduce to filters ) , which are described by a ( system of ) stochastic integral equation ( s ) ; recent results in convergence of stochastic integrals and stochastic differential equations are then applied to address convergence issues . as an illustration , the double censoring problem is revisited under this framework , a closed form estimator is proposed and convergence properties studied . martingale theory plays a vital role in the entire analysis . this approach is in essence a self - consistency method . #NUM#
over the past few years , the telecommunications paradigm has been shifting rapidly from hardware to middleware . in particular , the traditional issues of service characteristics and network control are being replaced by the modern , customer - driven issues of network and service management ( e . g . , electronic commerce , one - stop shops ) . an area of service management which has extremely high visibility and negative impact when managed badly is that of problem handling . problem handling is a very knowledge intensive activity , particularly nowadays with the increase in number and complexity of services becoming available . trials at several bt support centres have already demonstrated the potential of case - based reasoning technology in improving current practice for problem detection and diagnosis . a major cost involved in implementing a case - based system is in the manual building of the initial case base and then in the subsequent maintenance of that case base over time . this paper shows how inductive machine learning can be combined with case - based reasoning to produce an intelligent system capable of both extracting knowledge from raw data automatically and reasoning from that knowledge . in addition to discovering knowledge in existing data repositories , the integrated system may be used to acquire and revise knowledge continually . experiments with the suggested integrated approach demonstrate promise and justify the next step .
when using the genetic programming ( gp ) algorithm on a difficult problem with a large set of training cases , a large population size is needed and a very large number of function - tree evaluations must be carried out . this paper describes how to reduce the number of such evaluations by selecting a small subset of the training data set on which to actually carry out the gp algorithm . three subset selection methods described in the paper are : dynamic subset selection ( dss ) , using the current gp run to select ` difficult ' and / or disused cases , historical subset selection ( hss ) , using previous gp runs , random subset selection ( rss ) . gp , gp + dss , gp + hss , gp + rss are compared on a large classification problem . gp + dss can produce better results in less than #NUM# % of the time taken by gp . gp + hss can nearly match the results of gp , and , perhaps surprisingly , gp + rss can occasionally approach the results of gp . gp and gp + dss are then compared on a smaller problem , and a hybrid dynamic fitness function ( dff ) , based on dss , is proposed .
this paper presents limited error fitness ( lef ) , a modification to the standard supervised learning approach in genetic programming ( gp ) , in which an individual ' s fitness score is based on how many cases remain uncovered in the ordered training set after the individual exceeds an error limit . the training set order and the error limit are both altered dynamically in response to the performance of the fittest individual in the previous generation .
we describe an experimental study of pruning methods for decision tree classifiers when the goal is minimizing loss rather than error . in addition to two common methods for error minimization , cart ' s cost - complexity pruning and c #NUM# . #NUM# ' s error - based pruning , we study the extension of cost - complexity pruning to loss and one pruning variant based on the laplace correction . we perform an empirical comparison of these methods and evaluate them with respect to loss . we found that applying the laplace correction to estimate the probability distributions at the leaves was beneficial to all pruning methods . unlike in error minimization , and somewhat surprisingly , performing no pruning led to results that were on par with other methods in terms of the evaluation criteria . the main advantage of pruning was in the reduction of the decision tree size , sometimes by a factor of ten . while no method dominated others on all datasets , even for the same domain different pruning mechanisms are better for different loss matrices .
the fourier transform of boolean functions has come to play an important role in proving many important learnability results . we aim to demonstrate that the fourier transform techniques are also a useful and practical algorithm in addition to being a powerful theoretical tool . we describe the more prominent changes we have introduced to the algorithm , ones that were crucial and without which the performance of the algorithm would severely deteriorate . one of the benefits we present is the confidence level for each prediction which measures the likelihood the prediction is correct .
this paper looks at the use of small populations in genetic programming ( gp ) , where the trend in the literature appears to be towards using as large a population as possible , which requires more memory resources and cpu - usage is less efficient . dynamic subset selection ( dss ) and limited error fitness ( lef ) are two different , adaptive variations of the standard supervised learning method used in gp . this paper compares the performance of gp , gp + dss , and gp + lef , on a #NUM# case classification problem , using a small population size of #NUM# . a similar comparison between gp and gp + dss is done on a larger and messier #NUM# case classification problem . for both problems , gp + dss with the small population size consistently produces a better answer using fewer tree evaluations than other runs using much larger populations . even standard gp can be seen to perform well with the much smaller population size , indicating that it is certainly worth an exploratory run or three with a small population size before assuming that a large population size is necessary . it is an interesting notion that smaller can mean faster and better .
one method for detecting fraud is to check for suspicious changes in user behavior . this paper describes the automatic design of user profiling methods for the purpose of fraud detection , using a series of data mining techniques . specifically , we use a rule - learning program to uncover indicators of fraudulent behavior from a large database of customer transactions . then the indicators are used to create a set of monitors , which profile legitimate customer behavior and indicate anomalies . finally , the outputs of the monitors are used as features in a system that learns to combine evidence to generate high - confidence alarms . the system has been applied to the problem of detecting cellular cloning fraud based on a database of call records . experiments indicate that this automatic approach performs better than hand - crafted methods for detecting fraud . furthermore , this approach can adapt to the changing conditions typical of fraud detection environments .
given a set of samples of a probability distribution on a set of discrete random variables , we study the problem of constructing a good approximative neural network model of the underlying probability distribution . our approach is based on an unsupervised learning scheme where the samples are first divided into separate clusters , and each cluster is then coded as a single vector . these bayesian prototype vectors consist of conditional probabilities representing the attribute - value distribution inside the corresponding cluster . using these prototype vectors , it is possible to model the underlying joint probability distribution as a simple bayesian network ( a tree ) , which can be realized as a feedforward neural network capable of probabilistic reasoning . in this framework , learning means choosing the size of the prototype set , partitioning the samples into the corresponding clusters , and constructing the cluster prototypes . we describe how the prototypes can be determined , given a partition of the samples , and present a method for evaluating the likelihood of the corresponding bayesian tree . we also present a greedy heuristic for searching through the space of different partition schemes with different numbers of clusters , aiming at an optimal approximation of the probability distribution .
this paper introduces two new crossover operators for genetic programming ( gp ) . contrary to the regular gp crossover , the operators presented attempt to preserve the context in which subtrees appeared in the parent trees . a simple coordinate scheme for nodes in an s - expression tree is proposed , and crossovers are only allowed between nodes with exactly or partially matching coordinates .
hierarchical genetic programming ( hgp ) approaches rely on the discovery , modification , and use of new functions to accelerate evolution . this paper provides a qualitative explanation of the improved behavior of hgp , based on an analysis of the evolution process from the dual perspective of diversity and causality . from a static point of view , the use of an hgp approach enables the manipulation of a population of higher diversity programs . higher diversity increases the exploratory ability of the genetic search process , as demonstrated by theoretical and experimental fitness distributions and expanded structural complexity of individuals . from a dynamic point of view , this report analyzes the causality of the crossover operator . causality relates changes in the structure of an object with the effect of such changes , i . e . changes in the properties or behavior of the object . the analyses of crossover causality suggests that hgp discovers and exploits useful structures in a bottom - up , hierarchical manner . diversity and causality are complementary , affecting exploration and exploitation in genetic search . unlike other machine learning techniques that need extra machinery to control the tradeoff between them , hgp automatically trades off exploration and exploitation .
reinforcement learning ( rl ) algorithms provide a sound theoretical basis for building learning control architectures for embedded agents . unfortunately all of the theory and much of the practice ( see barto et al . , #NUM# , for an exception ) of rl is limited to marko - vian decision processes ( mdps ) . many real - world decision tasks , however , are inherently non - markovian , i . e . , the state of the environment is only incompletely known to the learning agent . in this paper we consider only partially observable mdps ( pomdps ) , a useful class of non - markovian decision processes . most previous approaches to such problems have combined computationally expensive state - estimation techniques with learning control . this paper investigates learning in pomdps without resorting to any form of state estimation . we present results about what td ( #NUM# ) and q - learning will do when applied to pomdps . it is shown that the conventional discounted rl framework is inadequate to deal with pomdps . finally we develop a new framework for learning without state - estimation in pomdps by including stochastic policies in the search space , and by defining the value or utility of a dis tribution over states .
the task is to monitor walking patterns and give early warning of falls using foot switch and mercury trigger sensors . we describe a dynamic belief network model for fall diagnosis which , given evidence from sensor observations , outputs beliefs about the current walking status and makes predictions regarding future falls . the model represents possible sensor error and is parametrised to allow customisation to the individual being monitored .
i would like to thank my dissertation advisor , roger schank , for his very valuable guidance on this research , and to thank the cognitive science reviewers for their helpful comments on a draft of this paper . the research described here was conducted primarily at yale university , supported in part by the defense advanced research projects agency , monitored by the office of naval research under contract n #NUM# - #NUM# - k - #NUM# and by the air force office of scientific research under contract f #NUM# - #NUM# - c - #NUM# .
this paper describes two methods for hierarchically organizing temporal behaviors . the first is more intuitive : grouping together common sequences of events into single units so that they may be treated as individual behaviors . this system immediately encounters problems , however , because the units are binary , meaning the behaviors must execute completely or not at all , and this hinders the construction of good training algorithms . the system also runs into difficulty when more than one unit is ( or should be ) active at the same time . the second system is a hierarchy of transition values . this hierarchy dynamically modifies the values that specify the degree to which one unit should follow another . these values are continuous , allowing the use of gradient descent during learning . furthermore , many units are active at the same time as part of the system ' s normal functionings .
this paper introduces the " incremental self - improvement paradigm " . unlike previous methods , incremental self - improvement encourages a reinforcement learning system to improve the way it learns , and to improve the way it improves the way it learns . . . , without significant theoretical limitations | the system is able to " shift its inductive bias " in a universal way . its major features are : ( #NUM# ) there is no explicit difference between " learning " , " meta - learning " , and other kinds of information processing . using a turing machine equivalent programming language , the system itself occasionally executes self - delimiting , initially highly random " self - modification programs " which modify the context - dependent probabilities of future action sequences ( including future self - modification programs ) . ( #NUM# ) the system keeps only those probability modifications computed by " useful " self - modification programs : those which bring about more payoff ( reward , reinforcement ) per time than all previous self - modification programs . ( #NUM# ) the computation of payoff per time takes into account all the computation time required for learning | the entire system life is considered : boundaries between learning trials are ignored ( if there are any ) . a particular implementation based on the novel paradigm is presented . it is designed to exploit what conventional digital machines are good at : fast storage addressing , arithmetic operations etc . experiments illustrate the system ' s mode of operation . keywords : self - improvement , self - reference , introspection , machine - learning , reinforcement learning . note : this is the revised and extended version of an earlier report from november #NUM# , #NUM# .
in #NUM# , angluin showed that no class exhibiting a combinatorial property called " approximate fingerprints " can be identified exactly using polynomially many equivalence queries ( of polynomial size ) . here we show that this is a necessary condition : every class without approximate fingerprints has an identification strategy that makes a polynomial number of equivalence queries . furthermore , if the class is " honest " in a technical sense , the computational power required by the strategy is within the polynomial - time hierarchy , so proving non learnability is at least as hard as showing p #NUM# = np .
we propose an extension to the genetic programming paradigm which allows users of traditional genetic algorithms to evolve computer programs . to this end , we have to introduce mechanisms like transscription , editing and repairing into genetic programming . we demonstrate the feasibility of the approach by using it to develop programs for the prediction of sequences of integer numbers .
generalized delta rule , popularly known as back - propagation ( bp ) [ #NUM# , #NUM# ] is probably one of the most widely used procedures for training multi - layer feed - forward networks of sigmoid units . despite reports of success on a number of interesting problems , bp can be excruciatingly slow in converging on a set of weights that meet the desired error criterion . several modifications for improving the learning speed have been proposed in the literature [ #NUM# , #NUM# , #NUM# , #NUM# , #NUM# ] . bp is known to suffer from the phenomenon of flat spots [ #NUM# ] . the slowness of bp is a direct consequence of these flat - spots together with the formulation of the bp learning rule . this paper proposes a new approach to minimizing the error that is suggested by the mathematical properties of the conventional error function and that effectively handles flat - spots occurring in the output layer . the robustness of the proposed technique is demonstrated on a number of data - sets widely studied in the machine learning community .
alan e . gelfand is a professor in the department of statistics at the university of connecti - cut , storrs , ct #NUM# . sujit k . sahu is a lecturer at the school of mathematics , university of wales , cardiff , cf #NUM# #NUM# yh , uk . the research of the first author was supported in part by nsf grant dms #NUM# while the second author was supported in part by an epsrc grant from uk . the authors thank brad carlin , kate cowles , gareth roberts and an anonymous referee for valuable comments .
technical report no . #NUM# , department of statistics , university of toronto abstract . gaussian processes are a natural way of defining prior distributions over functions of one or more input variables . in a simple nonparametric regression problem , where such a function gives the mean of a gaussian distribution for an observed response , a gaussian process model can easily be implemented using matrix computations that are feasible for datasets of up to about a thousand cases . hyperparameters that define the covariance function of the gaussian process can be sampled using markov chain methods . regression models where the noise has a t distribution and logistic or probit models for classification applications can be implemented by sampling as well for latent values underlying the observations . software is now available that implements these methods using covariance functions with hierarchical parameterizations . models defined in this way can discover high - level properties of the data , such as which inputs are relevant to predicting the response .
many factory optimization problems , from inventory control to scheduling and reliability , can be formulated as continuous - time markov decision processes . a primary goal in such problems is to find a gain - optimal policy that minimizes the long - run average cost . this paper describes a new average - reward algorithm called smart for finding gain - optimal policies in continuous time semi - markov decision processes . the paper presents a detailed experimental study of smart on a large unreliable production inventory problem . smart outperforms two well - known reliability heuristics from industrial engineering . a key feature of this study is the integration of the reinforcement learning algorithm directly into two commercial discrete - event simulation packages , arena and csim , paving the way for this approach to be applied to many other factory optimization problems for which there already exist simulation models .
locally weighted polynomial regression ( lwpr ) is a popular instance - based algorithm for learning continuous non - linear mappings . for more than two or three inputs and for more than a few thousand dat - apoints the computational expense of predictions is daunting . we discuss drawbacks with previous approaches to dealing with this problem , and present a new algorithm based on a multiresolution search of a quickly - constructible augmented kd - tree . without needing to rebuild the tree , we can make fast predictions with arbitrary local weighting functions , arbitrary kernel widths and arbitrary queries . the paper begins with a new , faster , algorithm for exact lwpr predictions . next we introduce an approximation that achieves up to a two - orders - of - magnitude speedup with negligible accuracy losses . increasing a certain approximation parameter achieves greater speedups still , but with a correspondingly larger accuracy degradation . this is nevertheless useful during operations such as the early stages of model selection and locating optima of a fitted surface . we also show how the approximations can permit real - time query - specific optimization of the kernel width . we conclude with a brief discussion of potential extensions for tractable instance - based learning on datasets that are too large to fit in a com puter ' s main memory .
an xof - n is a set containing one or more attribute - value pairs . for a given instance , its value corresponds to the number of its attribute - value pairs that are true . in this paper , we explore the characteristics and performance of continuous - valued xof - n attributes versus nominal xof - n attributes for constructive induction . nominal xof - ns are more representationally powerful than continuous - valued xof - ns , but the former suffer the " fragmentation " problem , although some mechanisms such as subsetting can help to solve the problem . two approaches to constructive induction using continuous - valued xof - ns are described . continuous - valued xof - ns perform better than nominal ones on domains that need xof - ns with only one cut point . on domains that need xof - n representations with more than one cut point , nominal xof - ns perform better than continuous - valued ones . experimental results on a set of artificial and real - world domains support these statements .
this paper studies the effects on decision tree learning of constructing four types of attribute ( conjunctive , disjunctive , mof - n , and xof - n representations ) . to reduce effects of other factors such as tree learning methods , new attribute search strategies , search starting points , evaluation functions , and stopping criteria , a single tree learning algorithm is developed . with different option settings , it can construct four different types of new attribute , but all other factors are fixed . the study reveals that conjunctive and disjunctive representations have very similar performance in terms of prediction accuracy and theory complexity on a variety of concepts , even on dnf and cnf concepts that are usually thought to be suited only to one of the two kinds of representation . in addition , the study demonstrates that the stronger representation power of mof - n than conjunction and disjunction and the stronger representation power of xof - n than these three types of new attribute can be reflected in the performance of decision tree learning in terms of higher prediction accuracy and lower theory complexity .
quence identification problems forms models that depend on the absolute locations of nucleotides and assume independence of consecutive nucleotide locations . this paper describes a new class of learning methods , called compression - based induction ( cbi ) , that is geared towards sequence learning problems such as those that arise when learning dna sequences . the central idea is to use text compression techniques on dna sequences as the means for generalizing from sample sequences . the resulting methods form models that are based on the more important relative locations of nucleotides and on the dependence of consecutive locations . they also provide a suitable framework into which biological domain knowledge can be injected into the learning process . we present initial explorations of a range of cbi methods that demonstrate the potential of our methods for dna sequence identification tasks .
we compare two techniques for lighting control in an actual room equipped with seven banks of lights and photoresistors to detect the lighting level at four sensing points . each bank of lights can be independently set to one of sixteen intensity levels . the task is to determine the device intensity levels that achieve a particular configuration of sensor readings . one technique we explored uses a neural network to approximate the mapping between sensor readings and device intensity levels . the other technique we examined uses a conventional feedback control loop . the neural network approach appears superior both in that it does not require experimentation on the fly ( and hence fluctuating light intensity levels during settling , and lengthy settling times ) and in that it can deal with complex interactions that conventional control techniques do not handle well . this comparison was performed as part of the " adaptive house " project , which is described briefly . further directions for control in the
we provide a sufficient condition for convergence of a general class of alternating estimation - maximization ( em ) type continuous - parameter estimation algorithms with respect to a given norm . this class includes em , penalized em , green ' s osl - em , and other approximate em algorithms . the convergence analysis can be extended to include alternating coordinate - maximization em algorithms such as meng and rubin ' s ecm and fessler and hero ' s sage . the condition for monotone convergence can be used to establish norms under which the distance between successive iterates and the limit point of the em - type algorithm approaches zero monotonically . for illustration , we apply our results to estimation of poisson rate parameters in emission tomography and establish that in the final iterations the logarithm of the em iterates converge monotonically in a weighted euclidean norm .
the kbann approach uses neural networks to refine knowledge that can be written in the form of simple propositional rules . we extend this idea further by presenting the manncon algorithm by which the mathematical equations governing a pid controller determine the topology and initial weights of a network , which is further trained using backpropagation . we apply this method to the task of controlling the outflow and temperature of a water tank , producing statistically - significant gains in accuracy over both a standard neural network approach and a non - learning pid controller . furthermore , using the pid knowledge to initialize the weights of the network produces statistically less variation in testset accuracy when compared to networks initialized with small random numbers .
random field models in image analysis and spatial statistics usually have local interactions . they can be simulated by markov chains which update a single site at a time . the updating rules typically condition on only a few neighboring sites . if we want to approximate the expectation of a bounded function , can we make better use of the simulations than through the empirical estimator ? we describe symmetrizations of the empirical estimator which are computationally feasible and can lead to considerable variance reduction . the method is reminiscent of the idea behind generalized von mises statistics . to simplify the exposition , we consider mainly nearest neighbor random fields and the gibbs sampler .
intrator ( #NUM# ) proposed a feature extraction method that is related to recent statistical theory ( huber , #NUM# ; friedman , #NUM# ) , and is based on a biologically motivated model of neuronal plasticity ( bienenstock et al . , #NUM# ) . this method has been recently applied to feature extraction in the context of recognizing #NUM# d objects from single #NUM# d views ( intrator and gold , #NUM# ) . here we describe experiments designed to analyze the nature of the extracted features , and their relevance to the theory and psychophysics of object recognition .
the building - block hypothesis appeals to the notion of problem decomposition and the assembly of solutions from sub - solutions . accordingly , there have been many varieties of ga test problems with a structure based on building - blocks . many of these problems use deceptive fitness functions to model interdependency between the bits within a block . however , very few have any model of interdependency between building - blocks ; those that do are not consistent in the type of interaction used intra - block and inter - block . this paper discusses the inadequacies of the various test problems in the literature and clarifies the concept of building - block interdependency . we formulate a principled model of hierarchical interdependency that can be applied through many levels in a consistent manner and introduce hierarchical if - and - only - if ( h - iff ) as a canonical example . we present some empirical results of gas on h - iff showing that if population diversity is maintained and linkage is tight then the ga is able to identify and manipulate building - blocks over many levels of assembly , as the building - block hypothesis suggests .
we introduce a model for analog computation with discrete time in the presence of analog noise that is flexible enough to cover the most important concrete cases , such as noisy analog neural nets and networks of spiking neurons . this model subsumes the classical model for digital computation in the presence of noise . we show that the presence of arbitrarily small amounts of analog noise reduces the power of analog computational models to that of finite automata , and we also prove a new type of upper bound for the
in this paper we extend the basic autologistic model to include covariates and an indication of sampling effort . the model is applied to sampled data instead of the traditional use for image analysis where complete data are available . we adopt a bayesian set - up and develop a hybrid gibbs sampling estimation procedure . using simulated examples , we show that the autologistic model with covariates for sample data improves predictions as compared to the simple logistic regression model and the standard autologistic model ( without covariates ) .
specifying , constructing and simulating structured connectionist networks requires significant programming effort . system tools can greatly reduce the effort required , and by providing a conceptual structure within which to work , make large and complex network simulations possible . the rochester connectionist simulator is a system tool designed to aid specification , construction and simulation of connectionist networks . this report describes this tool in detail : the facilities provided and how to use them , as well as details of the implementation . through this we hope not only to make designing and verifying connectionist networks easier , but also to encourage the development and refinement of connectionist research tools themselves .
many state - of - the - art ilp systems require large numbers of negative examples to avoid overgeneralization . this is a considerable disadvantage for many ilp applications , namely indu ctive program synthesis where relativelly small and sparse example sets are a more realistic scenario . integrity constraints are first order clauses that can play the role of negative examples in an inductive process . one integrity constraint can replace a long list of ground negative examples . however , checking the consistency of a program with a set of integrity constraints usually involves heavy the orem - proving . we propose an efficient constraint satisfaction algorithm that applies to a wide variety of useful integrity constraints and uses a monte carlo strategy . it looks for inconsistencies by ra ndom generation of queries to the program . this method allows the use of integrity constraints instead of ( or together with ) negative examples . as a consequence programs to induce can be specified more rapidly by the user and the ilp system tends to obtain more accurate definitions . average running times are not greatly affected by the use of integrity constraints compared to ground negative examples .
a neural network method for identifying the ancestor of a hadron jet is presented . the idea is to find an efficient mapping between certain observed hadronic kinematical variables and the quark / gluon identity . this is done with a neuronic expansion in terms of a network of sigmoidal functions using a gradient descent procedure , where the errors are back - propagated through the network . with this method we are able to separate gluon from quark jets originating from monte carlo generated e + e events with ~ #NUM# % accuracy . the result is independent on the mc model used . this approach for isolating the gluon jet is then used to study the so - called string effect . in addition , heavy quarks ( b and c ) in e + e reactions can be identified on the #NUM# % level by just observing the hadrons . in particular we are able to separate b - quarks with an efficiency and purity , which is comparable with what is expected from vertex detectors . we also speculate on how the neural network method can be used to disentangle different hadronization schemes by compressing the dimensionality of the state space of hadrons .
self - organizing neural networks are briefly reviewed and compared with supervised learning algorithms like back - propagation . the power of self - organization networks is in their capability of displaying typical features in a transparent manner . this is successfully demonstrated with two applications from hadronic jet physics ; hadronization model discrimination and separation of b , c and light quarks .
the langevin updating rule , in which noise is added to the weights during learning , is presented and shown to improve learning on problems with initially ill - conditioned hessians . this is particularly important for multilayer perceptrons with many hidden layers , that often have ill - conditioned hessians . in addition , manhattan updating is shown to have a similar effect .
the pac learning of rectangles has been studied because they have been found experimentally to yield excellent hypotheses for several applied learning problems . also , pseudorandom sets for rectangles have been actively studied recently because ( i ) they are a subprob - lem common to the derandomization of depth - #NUM# ( dnf ) circuits and derandomizing randomized logspace , and ( ii ) they approximate the distribution of n independent multivalued random variables . we present improved upper bounds for a class of such problems of approximating high - dimensional rectangles that arise in pac learning and pseudorandomness .
a distinction between two forms of task knowledge transfer , representational and functional , is reviewed followed by a discussion of mtl , a modified version of the multiple task learning ( mtl ) neural network method of functional transfer . the mtl method employs a separate learning rate , k , for each task output node k . k varies as a function of a measure of relatedness , r k , between the kth task and the primary task of interest . an mtl network is applied to a diagnostic domain of four levels of coronary artery disease . results of experiments demonstrate the ability of mtl to develop a predictive model for one level of disease which has superior diagnostic ability over models produced by either single task learning or standard multiple task learning .
most of the work on the vapnik - chervonenkis dimension of neural networks has been focused on feedforward networks . however , recurrent networks are also widely used in learning applications , in particular when time is a relevant parameter . this paper provides lower and upper bounds for the vc dimension of such networks . several types of activation functions are discussed , including threshold , polynomial , piecewise - polynomial and sigmoidal functions . the bounds depend on two independent parameters : the number w of weights in the network , and the length k of the input sequence . in contrast , for feedforward networks , vc dimension bounds can be expressed as a function of w only . an important difference between recurrent and feedforward nets is that a fixed recurrent net can receive inputs of arbitrary length . therefore we are particularly interested in the case k w . ignoring multiplicative constants , the main results say roughly the following : for architectures with activation = any fixed nonlinear polynomial , the vc dimension is wk . for architectures with activation = any fixed piecewise polynomial , the vc dimension is between wk and w #NUM# k . for architectures with activation = h ( threshold nets ) , the vc dimension is between w log ( k = w ) and minfwk log wk ; w #NUM# + w log wkg . forthe standard sigmoid ( x ) = #NUM# = ( #NUM# + e x ) , the vc dimension is between wk and w #NUM# k #NUM# .
the performance of hillclimbing design optimization can be improved by abstraction and decomposition of the design space . methods for automatically finding and exploiting such abstractions and decompositions are presented in this paper . a technique called " operator importance analysis " finds useful abstractions . it does so by determining which of a given set of operators are the most important for a given class of design problems . hillclimbing search runs faster when performed using this this smaller set of operators . a technique called " operator interaction analysis " finds useful decompositions . it does so by measuring the pairwise interaction between operators . it uses such measurements to form an ordered partition of the operator set . this partition can then be used in a " hierarchic " hillclimbing algorithm which runs faster than ordinary hillclimbing with an unstructured operator set . we have implemented both techniques and tested them in the domain of racing yacht hull design . our experimental results show that these two methods can produce substantial speedups with little or no loss in quality of the resulting designs .
this paper investigates an algorithm for the construction of decisions trees comprised of linear threshold units and also presents a novel algorithm for the learning of non - linearly separable boolean functions using madaline - style networks which are isomorphic to decision trees . the construction of such networks is discussed , and their performance in learning is compared with standard backpropagation on a sample problem in which many irrelevant attributes are introduced . littlestone ' s winnow algorithm is also explored within this architecture as a means of learning in the presence of many irrelevant attributes . the learning ability of this madaline - style architecture on nonoptimal ( larger than necessary ) networks is also explored .
lipid research clinic program #NUM# ] lipid research clinic program . the lipid research clinics coronary primary prevention trial results , parts i and ii . journal of the american medical association , #NUM# ( #NUM# ) : #NUM# - #NUM# , january #NUM# . [ pearl #NUM# ] judea pearl . aspects of graphical models connected with causality . technical report r - #NUM# - ll , cognitive systems laboratory , ucla , june #NUM# . submitted to biometrika ( june #NUM# ) . short version in proceedings of the #NUM# th session of the international statistical institute : invited papers , flo rence , italy , august #NUM# , tome lv , book #NUM# , pp . #NUM# - #NUM# .
we investigate the generation of neural networks through the induction of binary trees of threshold logic units ( tlus ) . initially , we describe the framework for our tree construction algorithm and how such trees can be transformed into an isomorphic neural network topology . several methods for learning the linear discriminant functions at each node of the tree structure are examined and shown to produce accuracy results that are comparable to classical information theoretic methods for constructing decision trees ( which use single feature tests at each node ) . our tlu trees , however , are smaller and thus easier to understand . moreover , we show that it is possible to simultaneously learn both the topology and weight settings of a neural network simply using the training data set that we are given .
we consider the problem of learning dnf formulae in the mistake - bound and the pac models . we develop a new approach , which is called polynomial explainability , that is shown to be useful for learning some new subclasses of dnf ( and cnf ) formulae that were not known to be learnable before . unlike previous learnability results for dnf ( and cnf ) formulae , these subclasses are not limited in the number of terms or in the number of variables per term ; yet , they contain the subclasses of k - dnf and k - term - dnf ( and the corresponding classes of cnf ) as special cases . we apply our dnf results to the problem of learning visual concepts and obtain learning algorithms for several natural subclasses of visual concepts that appear to have no natural boolean counterpart . on the other hand , we show that learning some other natural subclasses of visual concepts is as hard as learning the class of all dnf formulae . we also consider the robustness of these results under various types of noise .
typical approaches to plan recognition start from a representation of an agent ' s possible plans , and reason evidentially from observations of the agent ' s actions to assess the plausibility of the various candidates . a more expansive view of the task ( consistent with some prior work ) accounts for the context in which the plan was generated , the mental state and planning process of the agent , and consequences of the agent ' s actions in the world . we present a general bayesian framework encompassing this view , and focus on how context can be exploited in plan recognition . we demonstrate the approach on a problem in traffic monitoring , where the objective is to induce the plan of the driver from observation of vehicle movements . starting from a model of how the driver generates plans , we show how the highway context can appropriately influence the recognizer ' s interpretation of observed driver be havior .
i present a parallel algorithm for exact probabilistic inference in bayesian networks . for polytree networks with n variables , the worst - case time complexity is o ( log n ) on a crew pram ( concurrent - read , exclusive - write parallel random - access machine ) with n processors , for any constant number of evidence variables . for arbitrary networks , the time complexity is o ( r #NUM# w log n ) for n processors , or o ( w log n ) for r #NUM# w n processors , where r is the maximum range of any variable , and w is the induced width ( the maximum clique size ) , after moralizing and trian gulating the network .
rational distributed reason maintenance for efficiency dictates that plans for large - scale distributed activities be revised incrementally , with parts of plans being revised only if the expected utility of identifying and revising the sub - plans improve on the expected utility of using the original plan . the problems of identifying and reconsidering the subplans affected by changed circumstances or goals are closely related to the problems of revising beliefs as new or changed information is gained . but traditional techniques of reason maintenance | the standard method for belief revision | choose revisions arbitrarily and enforce global notions of consistency and groundedness which may mean reconsidering all beliefs or plan elements at each step . we develop revision methods aiming to revise only those beliefs and plans worth revising , and to tolerate incoherence and ungroundedness when these are judged less detrimental than a costly revision effort . we use an artificial market economy in planning and revision tasks to arrive at overall judgments of worth , and present a representation for qualitative preferences that permits capture of common forms of dominance information .
a feed - forward neural network method is developed for reconstructing the invariant mass of hadronic jets appearing in a calorimeter . the approach is illustrated in w ! q q , where w - bosons are produced in pp reactions at sps collider energies . the neural network method yields results that are superior to conventional methods . this neural network application differs from the classification ones in the sense that an analog number ( the mass ) is computed by the network , rather than a binary decision being made . as a by - product our application clearly demonstrates the need for using " intelligent " variables in instances when the amount of training instances is limited .
a new class of highspeed , self - adaptive , massively parallel computing models called asocs ( adaptive self - organizing concurrent systems ) has been proposed . current analysis suggests that there may be problems implementing asocs models in vlsi using the hierarchical network structures originally proposed . the problems are not inherent in the models , but rather in the technology used to implement them . this has led to the development of a new asocs model called dna ( discriminant - node asocs ) that does not depend on a hierarchical node structure for success . three areas of the dna model are briefly discussed in this paper : dna ' s flexible nodes , how dna overcomes problems other models have allocating unused nodes , and how dna operates during processing and learning .
this paper presents an approach to mobile robot path planning using case - based reasoning together with map - based path planning . the map - based path planner is used to seed the case - base with innovative solutions . the casebase stores the paths and the information about their traversability . while planning the route those paths are preferred that according to the former experience are least risky .
efficiency dictates that plans for large - scale distributed activities be revised incrementally , with parts of plans being revised only if the expected utility of identifying and revising the subplans improve on the expected utility of using the original plan . the problems of identifying and reconsidering the subplans affected by changed circumstances or goals are closely related to the problems of revising beliefs as new or changed information is gained . but the current techniques of reason maintenance | the standard method for belief revision | choose revisions arbitrarily and enforce global notions of consistency and groundedness which may mean reconsidering all beliefs or plan elements at each step . we outline revision methods that revise only those beliefs and plans worth revising , and that tolerate incoherence and ungroundedness when these are judged less detrimental than a costly revision effort .
in this paper , we examine previous work on the naive bayesian classifier and review its limitations , which include a sensitivity to correlated features . we respond to this problem by embedding the naive bayesian induction scheme within an algorithm that carries out a greedy search through the space of features . we hypothesize that this approach will improve asymptotic accuracy in domains that involve correlated features without reducing the rate of learning in ones that do not . we report experimental results on six natural domains , including comparisons with decision - tree induction , that support these hypotheses . in closing , we discuss other approaches to extending naive bayesian classifiers and outline some directions for future research .
in this paper we present a novel induction algorithm for bayesian networks . this selective bayesian network classifier selects a subset of attributes that maximizes predictive accuracy prior to the network learning phase , thereby learning bayesian networks with a bias for small , high - predictive - accuracy networks . we compare the performance of this classifier with selective and non - selective naive bayesian classifiers . we show that the selective bayesian network classifier performs significantly better than both versions of the naive bayesian classifier on almost all databases analyzed , and hence is an enhancement of the naive bayesian classifier . relative to the non - selective bayesian network classifier , our selective bayesian network classifier generates networks that are computationally simpler to evaluate and that display predictive accuracy comparable to that of bayesian networks which model all features .
we attempt to recover an unknown function from noisy , sampled data . using orthonormal bases of compactly supported wavelets we develop a nonlinear method which works in the wavelet domain by simple nonlinear shrinkage of the empirical wavelet coefficients . the shrinkage can be tuned to be nearly minimax over any member of a wide range of triebel - and besov - type smoothness constraints , and asymptotically minimax over besov bodies with p q . linear estimates cannot achieve even the minimax rates over triebel and besov classes with p & lt ; #NUM# , so our method can significantly outperform every linear method ( kernel , smoothing spline , sieve , : : : ) in a minimax sense . variants of our method based on simple threshold nonlinearities are nearly minimax . our method possesses the interpretation of spatial adaptivity : it reconstructs using a kernel which may vary in shape and bandwidth from point to point , depending on the data . least favorable distributions for certain of the triebel and besov scales generate objects with sparse wavelet transforms . many real objects have similarly sparse transforms , which suggests that these minimax results are relevant for practical problems . sequels to this paper discuss practical implementation , spatial adaptation properties and applications to inverse problems . acknowledgements . this work was completed while the first author was on leave from u . c . berkeley , where his research was supported by nsf dms #NUM# - #NUM# , by nasa contract nca #NUM# - #NUM# , and by a grant from att foundation . the second author was supported in part by nsf grants dms #NUM# - #NUM# , #NUM# - #NUM# , and nih phs grant gm #NUM# - #NUM# . supersedes an earlier version , titled " wavelets and optimal function estimation " , dated november #NUM# , #NUM# , and issued as technical reports by the departments of statistics at both stanford and at u . c . berkeley .
it is established good software engineering practice to ensure that programs use memory via abstract data structures such as stacks , queues and lists . these provide an interface between the program and memory , freeing the program of memory management details which are left to the data structures to implement . the main result presented herein is that gp can automatically generate stacks and queues . typically abstract data structures support multiple operations , such as put and get . we show that gp can simultaneously evolve all the operations of a data structure by implementing each such operation with its own independent program tree . that is , the chromosome consists of a fixed number of independent program trees . moreover , crossover only mixes genetic material of program trees that implement the same operation . program trees interact with each other only via shared memory and shared " automatically defined functions " ( adfs ) .
one of the main experimental tools in probing the interactions between neurons has been the measurement of the correlations in their activity . in general , however the interpretation of the observed correlations is difficult , since the correlation between a pair of neurons is influenced not only by the direct interaction between them but also by the dynamic state of the entire network to which they belong . thus , a comparison between the observed correlations and the predictions from specific model networks is needed . in this paper we develop the theory of neuronal correlation functions in large networks comprising of several highly connected subpopulations , and obeying stochastic dynamic rules . when the networks are in asynchronous states , the cross - correlations are relatively weak , i . e . , their amplitude relative to that of the auto - correlations is of order of #NUM# = n , n being the size of the interacting populations . using the weakness of the cross - correlations , general equations which express the matrix of cross - correlations in terms of the mean neuronal activities , and the effective interaction matrix are presented . the effective interactions are the synaptic efficacies multiplied by the the gain of the postsynaptic neurons . the time - delayed cross - correlation matrix can be expressed as a sum of exponentially decaying modes that correspond to the ( non - orthogonal ) eigenvectors of the effective interaction matrix . the theory is extended to networks with random connectivity , such as randomly dilute networks . this allows for the comparison between the contribution from the internal common input and that from the direct
technical report no . #NUM# department of statistics university of washington #NUM# sonia petrone is assistant professor , universita di pavia , dipartimento di economia politica e metodi quantitativi , i - #NUM# pavia , italy and adrian e . raftery is professor of statistics and sociology , department of statistics , university of washington , box #NUM# , seattle , wa #NUM# - #NUM# . this research was supported by onr grant no . n - #NUM# - #NUM# - j - #NUM# and by grants from murst , rome .
interference in neural networks occurs when learning in one area of the input space causes unlearning in another area . networks that are less susceptible to interference are called spatially local networks . these networks are often used in neurocontrol , in online applications , where , because of the real time nature of the task , interference is often a problem . although there are heuristics as to what makes a network local , there is no theoretical framework for measuring localization . this paper provides a formal definition of interference and localization that will allow us to measure a network ' s local properties . these definitions will be useful in developing learning algorithms that make networks more local . this may lead to faster learning over the entire input domain .
a neural network model called lissom for the cooperative self - organization of afferent and lateral connections in cortical maps is applied to modeling cortical plasticity . after self - organization , the lissom maps are in a dynamic equilibrium with the input , and reorganize like the cortex in response to simulated cortical lesions and intracortical microstimulation . the model predicts that adapting lateral interactions are fundamental to cortical reorganization , and suggests techniques to hasten recovery following sensory cortical surgery .
a novel approach to learning first order logic formulae from positive and negative examples is incorporated in a system named icl ( inductive constraint logic ) . in icl , examples are viewed as interpretations which are true or false for the target theory , whereas in present inductive logic programming systems , examples are true and false ground facts ( or clauses ) . furthermore , icl uses a clausal representation , which corresponds to a conjunctive normal form where each conjunct forms a constraint on positive examples , whereas classical learning techniques have concentrated on concept representations in disjunctive normal form . we present some experiments with this new system on the mutagenesis problem . these experiments illustrate some of the differences with other systems , and indicate that our approach should work at least as well as the more classical approaches .
automated synthesis of analog electronic circuits is recognized as a difficult problem . genetic programming was used to evolve b o t h the topology and the sizing ( n u m e r i c a l v a l u e s ) f o r e a c h component of a circuit that can perform source identification by correctly cl assify an incoming signal into categories .
bell and sejnowski ( #NUM# ) have derived a blind signal processing algorithm for a non - linear feedforward network from an information maximization viewpoint . this paper first shows that the same algorithm can be viewed as a maximum likelihood algorithm for the optimization of a linear generative model . third , this paper gives a partial proof of the ` folk - theorem ' that any mixture of sources with high - kurtosis histograms is separable by the classic ica algorithm .
i present an expectation - maximization ( em ) algorithm for principal component analysis ( pca ) . the algorithm allows a few eigenvectors and eigenvalues to be extracted from large collections of high dimensional data . it is computationally very efficient in space and time . it also naturally accommodates missing information . i also introduce a new variant of pca called sensible principal component analysis ( spca ) which defines a proper density model in the data space . learning for spca is also done with an em algorithm . i report results on synthetic and real data showing that these em algorithms correctly and efficiently find the leading eigenvectors of the covariance of datasets in a few iterations using up to hundreds of thousands of datapoints in thousands of dimensions .
we present new algorithms for parameter estimation of hmms . by adapting a framework used for supervised learning , we construct iterative algorithms that maximize the likelihood of the observations while also attempting to stay close to the current estimated parameters . we use a bound on the relative entropy between the two hmms as a distance measure between them . the result is new iterative training algorithms which are similar to the em ( baum - welch ) algorithm for training hmms . the proposed algorithms are composed of a step similar to the expectation step of baum - welch and a new update of the parameters which replaces the maximization ( re - estimation ) step . the algorithm takes only negligibly more time per iteration and an approximated version uses the same expectation step as baum - welch . we evaluate experimentally the new algorithms on synthetic and natural speech pronunciation data . for sparse models , i . e . models with relatively small number of non - zero parameters , the proposed algorithms require significantly fewer iterations .
we investigate the distribution of performance of the boolean functions of #NUM# boolean inputs ( particularly that of the parity functions ) , the always - on - #NUM# and even - #NUM# parity functions . we us enumeration , uniform monte - carlo random sampling and sampling random full trees . as expected xor dramatically changes the fitness distributions . in all cases once some minimum size threshold has been exceeded , the distribution of performance is approximately independent of program length . however the distribution of the performance of full trees is different from that of asymmetric trees and varies with tree depth . we consider but reject testing the no free lunch ( nfl ) theorems on these functions .
technical report bu - #NUM# - m , biometrics unit , cornell university abstract we analyse the convergence to stationarity of a simple non - reversible markov chain that serves as a model for several non - reversible markov chain sampling methods that are used in practice . our theoretical and numerical results show that non - reversibility can indeed lead to improvements over the diffusive behavior of simple markov chain sampling schemes . the analysis uses both probabilistic techniques and an explicit diagonalisation . we thank david aldous , martin hildebrand , brad mann , and laurent saloff - coste for their help .
principal component analysis ( pca ) is one of the most popular techniques for processing , compressing and visualising data , although its effectiveness is limited by its global linearity . while nonlinear variants of pca have been proposed , an alternative paradigm is to capture data complexity by a combination of local linear pca projections . however , conventional pca does not correspond to a probability density , and so there is no unique way to combine pca models . previous attempts to formulate mixture models for pca have therefore to some extent been ad hoc . in this paper , pca is formulated within a maximum - likelihood framework , based on a specific form of gaussian latent variable model . this leads to a well - defined mixture model for probabilistic principal component analysers , whose parameters can be determined using an em algorithm . we discuss the advantages of this model in the context of clustering , density modelling and local dimensionality reduction , and we demonstrate its application to image compression and handwritten digit recognition .
institute for neural computation technical report series , no . inc - #NUM# , january #NUM# . university of california , san diego . la jolla , ca #NUM# . abstract computational models of neural map formation can be considered on at least three different levels of abstraction : detailed models including neural activity dynamics , weight dynamics which abstract from the the neural activity dynamics by an adiabatic approximation , and objective functions from which weight dynamics may be derived as gradient flows . in this paper we present an example of how an objective function can be derived from detailed non - linear neural dynamics . a systematic investigation reveals how different weight dynamics introduced previously can be derived from objective functions generated from a few prototypical terms . this includes dynamic link matching as a special case of neural map formation . we focus in particular on the role of coordinate transformations to derive different weight dynamics from the same objective function . coordinate transformations are also important in deriving normalization rules from constraints . several examples illustrate how objective functions can help in understanding , generating , and comparing different models of neural map formation . the techniques used in this analysis may also be useful in investigating other types of neural dynamics .
real - valued random hidden variables can be useful for modelling latent structure that explains correlations among observed variables . i propose a simple unit that adds zero - mean gaussian noise to its input before passing it through a sigmoidal squashing function . such units can produce a variety of useful behaviors , ranging from deterministic to binary stochastic to continuous stochastic . i show how " slice sampling " ( neal #NUM# ) can be used for inference and learning in top - down networks of these units and demonstrate learning on two simple problems .
there is an obvious need for improving the performance and accuracy of a bayesian network as new data is observed . because of errors in model construction and changes in the dynamics of the domains , we cannot afford to ignore the information in new data . while sequential update of parameters for a fixed structure can be accomplished using standard techniques , sequential update of network structure is still an open problem . in this paper , we investigate sequential update of bayesian networks were both parameters and structure are expected to change . we introduce a new approach that allows for the flexible manipulation of the tradeoff between the quality of the learned networks and the amount of information that is maintained about past observations . we formally describe our approach including the necessary modifications to the scoring functions for learning bayesian networks , evaluate its effectiveness through and empirical study , and extend it to the case of missing data .
this paper sketches several aspects of a hypothetical cortical architecture for visual object recognition , based on a recent computational model . the scheme relies on modules for learning from examples , such as hyperbf - like networks , as its basic components . such models are not intended to be precise theories of the biological circuitry but rather to capture a class of explanations we call memory - based models ( mbm ) that contains sparse population coding , memory - based recognition and codebooks of prototypes . unlike the sigmoidal units of some artificial neural networks , the units of mbms are consistent with the usual description of cortical neurons as tuned to multidimensional optimal stimuli . we will describe how an example of mbm may be realized in terms of cortical circuitry and biophysical mechanisms , consistent with psychophysical and physiological data . a number of predictions , testable with physiological techniques , are made . this memo describes research done within the center for biological and computational learning in the department of brain and cognitive sciences and at the artificial intelligence laboratory at the massachusetts institute of technology . this research is sponsored by grants from the office of naval research under contracts n #NUM# - #NUM# - j - #NUM# and n #NUM# - #NUM# - #NUM# - #NUM# ; and by a grant from the national science foundation under contract asc - #NUM# ( this award includes funds from arpa provided under the hpcc program ) . additional support is provided by the north atlantic treaty organization , atr audio and visual perception research laboratories , mitsubishi electric corporation , sumitomo metal industries , and siemens ag . support for the a . i . laboratory ' s artificial intelligence research is provided by arpa contract n #NUM# - #NUM# - j - #NUM# . tomaso poggio is supported by the uncas and helen whitaker chair at mit ' s whitaker college .
we exploit qualitative probabilistic relationships among variables for computing bounds of conditional probability distributions of interest in bayesian networks . using the signs of qualitative relationships , we can implement abstraction operations that are guaranteed to bound the distributions of interest in the desired direction . by evaluating incrementally improved approximate networks , our algorithm obtains monotonically tightening bounds that converge to exact distributions . for supermodular utility functions , the tightening bounds monotonically reduce the set of admissible decision alternatives as well .
a parallel algorithm is proposed for a fundamental problem of machine learning , that of mul - ticategory discrimination . the algorithm is based on minimizing an error function associated with a set of highly structured linear inequalities . these inequalities characterize piecewise - linear separation of k sets by the maximum of k affine functions . the error function has a lipschitz continuous gradient that allows the use of fast serial and parallel unconstrained minimization algorithms . a serial quasi - newton algorithm is considerably faster than previous linear programming formulations . a parallel gradient distribution algorithm is used to parallelize the error - minimization problem . preliminary computational results are given for both a decstation
this paper presents a large and systematic body of data on the relative effectiveness of mutation , crossover , and combinations of mutation and crossover in genetic programming ( gp ) . the literature of traditional genetic algorithms contains related studies , but mutation and crossover in gp differ from their traditional counterparts in significant ways . in this paper we present the results from a very large experimental data set , the equivalent of approximately #NUM# , #NUM# typical runs of a gp system , systematically exploring a range of parameter settings . the resulting data may be useful not only for practitioners seeking to optimize parameters for gp runs , but also for theorists exploring issues such as the role of building blocks in gp .
department of statistics , university of toronto markov chain monte carlo methods such as gibbs sampling and simple forms of the metropolis algorithm typically move about the distribution being sampled via a random walk . for the complex , high - dimensional distributions commonly encountered in bayesian inference and statistical physics , the distance moved in each iteration of these algorithms will usually be small , because it is difficult or impossible to transform the problem to eliminate dependencies between variables . the inefficiency inherent in taking such small steps is greatly exacerbated when the algorithm operates via a random walk , as in such a case moving to a point n steps away will typically take around n #NUM# iterations . such random walks can sometimes be suppressed using " overrelaxed " variants of gibbs sampling ( a . k . a . the heatbath algorithm ) , but such methods have hitherto been largely restricted to problems where all the full conditional distributions are gaussian . i present an overrelaxed markov chain monte carlo algorithm based on order statistics that is more widely applicable . in particular , the algorithm can be applied whenever the full conditional distributions are such that their cumulative distribution functions and inverse cumulative distribution functions can be efficiently computed . the method is demonstrated on an inference problem for a simple hierarchical bayesian model .
we utilize collective memory to integrate weak and strong search heuristics to find cliques in fc , a family of graphs . we construct fc such that pruning of partial solutions will be ineffective . each weak heuristic maintains a local cache of the collective memory . we examine the impact on the distributed search from the various characteristics of the distribution of the collective memory , the search algorithms , and our family of graphs . we find the distributed search performs better than the individuals , even though the space of partial solutions is combinatorially explosive .
in this paper , we investigate the integration of knowledge acquisition and machine learning techniques . we argue that existing machine learning techniques can be made more useful as knowledge acquisition tools by allowing the expert to have greater control over and interaction with the learning process . we describe a number of extensions to focl ( a multistrategy horn - clause learning program ) that have greatly enhanced its power as a knowledge acquisition tool , paying particular attention to the utility of maintaining a connection between a rule and the set of examples explained by the rule . the objective of this research is to make the modification of a domain theory analogous to the use of a spread sheet . a prototype knowledge acquisition tool , focl - #NUM# - #NUM# - #NUM# , has been constructed in order to evaluate the strengths and weaknesses of this approach .
starting with a likelihood or preference order on worlds , we extend it to a likelihood ordering on sets of worlds in a natural way , and examine the resulting logic . lewis earlier considered such a notion of relative likelihood in the context of studying counterfactuals , but he assumed a total preference order on worlds . complications arise when examining partial orders that are not present for total orders . there are subtleties involving the exact approach to lifting the order on worlds to an order on sets of worlds . in addition , the axiomatization of the logic of relative likelihood in the case of partial orders gives insight into the connection between relative likelihood and default reasoning .
the choice of an input representation for machine learning can have a profound impact on the accuracy of the learned model in classifying novel instances . a reliable method of rapidly estimating the value of a representation , independent of the learner , would be a powerful tool in the search for better representations . we introduce a fast representation - quality measure that is more accurate than rendell and ragavan ' s blurring metric in rank ordering input representations for neural networks on two difficult , real - world datasets . this work constitutes a step forward both in representation quality measures and in our understanding of the characteristics that engender good representations .
this paper shows how the performance of a genetic programming system can be improved through the addition of mechanisms for non - genetic transmission of information between individuals ( culture ) . teller has previously shown how genetic programming systems can be enhanced through the addition of memory mechanisms for individual programs [ teller #NUM# ] ; in this paper we show how teller ' s memory mechanism can be changed to allow for communication between individuals within and across generations . we show the effects of indexed memory and culture on the performance of a genetic programming system on a symbolic regression problem , on koza ' s lawnmower problem , and on wum - pus world agent problems . we show that culture can reduce the computational effort required to solve all of these problems . we conclude with a discussion of possible improvements .
this paper reviews a large number of cbr systems to determine when and what sort of adaptation is currently used . three taxonomies are proposed : an adaptation - relevant taxonomy of cbr systems , a taxonomy of the tasks performed by cbr systems and a taxonomy of adaptation knowledge . to the extent that the set of existing systems reflects constraints on what is feasible , this review shows interesting dependencies between different system - types , the tasks these systems achieve and the adaptation needed to meet system goals . the cbr system designer may find the partition of cbr systems and the division of adaptation knowledge suggested by this paper useful . moreover , this paper may help focus the initial stages of systems development by suggesting ( on the basis of existing work ) what types of adaptation knowledge should be supported by a new system . in addition , the paper provides a framework for the preliminary evaluation and comparison of systems .
constructive learning algorithms offer an approach to incremental construction of near - minimal artificial neural networks for pattern classification . examples of such algorithms include tower , pyramid , upstart , and tiling algorithms which construct multilayer networks of threshold logic units ( or , multilayer perceptrons ) . these algorithms differ in terms of the topology of the networks that they construct which in turn biases the search for a decision boundary that correctly classifies the training set . this paper presents an analysis of such algorithms from a geometrical perspective . this analysis helps in a better characterization of the search bias employed by the different algorithms in relation to the geometrical distribution of examples in the training set . simple experiments with non linearly separable training sets support the results of mathematical analysis of such algorithms . this suggests the possibility of designing more efficient constructive algorithms that dynamically choose among different biases to build near - minimal networks for pattern classification .
#NUM# ] d . aldous and p . shields . a diffusion limit for a class of randomly growing binary trees . probability theory , #NUM# : #NUM# - #NUM# , #NUM# . [ #NUM# ] r . breathnach , c . benoist , k . o ' hare , f . gannon , and p . chambon . ovalbumin gene : evidence for leader sequence in mrna and dna sequences at the exon - intron boundaries . proceedings of the national academy of science , #NUM# : #NUM# - #NUM# , #NUM# . [ #NUM# ] s . brunak , j . engelbrecht , and s . knudsen . prediction of human mrna donor and acceptor sites from the dna sequence . journal of molecular biology , #NUM# : #NUM# , #NUM# . [ #NUM# ] jack cophen and ian stewart . the information in your hand . the mathematical intelligencer , #NUM# ( #NUM# ) , #NUM# . [ #NUM# ] r . g . gallager . information theory and reliable communication . john wiley & sons , inc . , #NUM# . [ #NUM# ] ali hariri , bruce weber , and john olmstead . on the validity of shannon - information calculations for molecular biological sequence . journal of theoretical biology , #NUM# : #NUM# - #NUM# , #NUM# . [ #NUM# ] w . b . davenport jr . and w . l . root . an introduction to the theory of random signals and noise . mcgraw - hill , #NUM# . [ #NUM# ] andrzej knopka and john owens . complexity charts can be used to map functional domains in dna . gene anal . techn . , #NUM# , #NUM# . [ #NUM# ] s . m . mount . a catalogue of splice - junction sequences . nucleic acids research , #NUM# : #NUM# - #NUM# , #NUM# . [ #NUM# ] h . m . seidel , d . l . pompliano , and j . r . knowles . exons as microgenes ? science , #NUM# , september #NUM# . [ #NUM# ] c . e . shannon . a mathematical theory of communication . bell system tech . j . , #NUM# : #NUM# - #NUM# , #NUM# - #NUM# , #NUM# . [ #NUM# ] peter s . shenkin , batu erman , and lucy d . mastrandrea . information - theoretical entropy as a measure of sequence variability . proteins , #NUM# ( #NUM# ) : #NUM# , #NUM# . [ #NUM# ] r . staden . measurements of the effects that coding for a protein has on a dna sequence and their use for finding genes . nucleic acids research , #NUM# : #NUM# - #NUM# , #NUM# . [ #NUM# ] j . a . steitz . snurps . scientific american , #NUM# ( #NUM# ) , june #NUM# . [ #NUM# ] h . van trees . detection , estimation and modulation theory . wiley , #NUM# . [ #NUM# ] j . d . watson , n . h . hopkins , j . w . roberts , j . ar - getsinger steitz , and a . m . weiner . molecular biology of the gene . benjamin / cummings , menlo park , ca , fourth edition , #NUM# . [ #NUM# ] a . d . wyner and a . j . wyner . an improved version of the lempel - ziv algorithm . transactions of information theory . [ #NUM# ] a . j . wyner . string matching theorems and applications to data compression and statistics . phd thesis , stanford university , #NUM# . [ #NUM# ] j . ziv and a . lempel . a universal algorithm for sequential data compression . ieee transactions on information theory , it - #NUM# ( #NUM# ) : #NUM# - #NUM# , #NUM# .
temporal - difference ( td ) learning can be used not just to predict rewards , as is commonly done in reinforcement learning , but also to predict states , i . e . , to learn a model of the world ' s dynamics . we present theory and algorithms for intermixing td models of the world at different levels of temporal abstraction within a single structure . such multi - scale td models can be used in model - based reinforcement - learning architectures and dynamic programming methods in place of conventional markov models . this enables planning at higher and varied levels of abstraction , and , as such , may prove useful in formulating methods for hierarchical or multi - level planning and reinforcement learning . in this paper we treat only the prediction problem | that of learning a model and value function for the case of fixed agent behavior . within this context , we establish the theoretical foundations of multi - scale models and derive td algorithms for learning them . two small computational experiments are presented to test and illustrate the theory . this work is an extension and generalization of the work of singh ( #NUM# ) , dayan ( #NUM# ) , and sutton & pinette ( #NUM# ) .
fuzzy rules for control can be effectively tuned via reinforcement learning . reinforcement learning is a weak learning method , which only requires information on the success or failure of the control application . the tuning process allows people to generate fuzzy rules which are unable to accurately perform control and have them tuned to be rules which provide smooth control . this paper explores a new simplified method of using reinforcement learning for the tuning of fuzzy control rules . it is shown that the learned fuzzy rules provide smoother control in the pole balancing domain than another approach .
procedural representations of control policies have two advantages when facing the scale - up problem in learning tasks . first they are implicit , with potential for inductive generalization over a very large set of situations . second they facilitate modularization . in this paper we compare several randomized algorithms for learning modular procedural representations . the main algorithm , called adaptive representation through learning ( arl ) is a genetic programming extension that relies on the discovery of subroutines . arl is suitable for learning hierarchies of subroutines and for constructing policies to complex tasks . arl was successfully tested on a typical reinforcement learning problem of controlling an agent in a dynamic and nondeterministic environment where the discovered subroutines correspond to agent behaviors .
we introduce a new model of distributions generated by random walks on graphs . this model suggests a variety of learning problems , using the definitions and models of distribution learning defined in [ #NUM# ] . our framework is general enough to model previously studied distribution learning problems , as well as to suggest new applications . we describe special cases of the general problem , and investigate their relative difficulty . we present algorithms to solve the learning problem under various conditions .
most constructive induction researchers focus only on new boolean attributes . this paper reports a new constructive induction algorithm , called xofn , that constructs new nominal attributes in the form of xof - n representations . an xof - n is a set containing one or more attribute - value pairs . for a given instance , its value corresponds to the number of its attribute - value pairs that are true . the promising preliminary experimental results , on both artificial and real - world domains , show that constructing new nominal attributes in the form of xof - n representations can significantly improve the performance of selective induction in terms of both higher prediction accuracy and lower theory complexity .
in a previous sab paper [ #NUM# ] , we presented the scientific rationale for simulating the coevolution of pursuit and evasion strategies . here , we present an overview of our simulation methods and some results . our most notable results are as follows . first , co - evolution works to produce good pursuers and good evaders through a pure bootstrapping process , but both types are rather specially adapted to their opponents ' current counter - strategies . second , eyes and brains can also co - evolve within each simulated species for example , pursuers usually evolved eyes on the front of their bodies ( like cheetahs ) , while evaders usually evolved eyes pointing sideways or even backwards ( like gazelles ) . third , both kinds of coevolution are promoted by allowing spatially distributed populations , gene duplication , and an explicitly spatial morphogenesis program for eyes and brains that allows bilateral symmetry . the paper concludes by discussing some possible applications of simulated pursuit - evasion coevolu tion in biology and entertainment .
learning to store information over extended time intervals via recurrent backpropagation takes a very long time , mostly due to insufficient , decaying error back flow . we briefly review hochreiter ' s #NUM# analysis of this problem , then address it by introducing a novel , efficient method called " long short - term memory " ( lstm ) . lstm can learn to bridge time lags in excess of #NUM# steps by enforcing constant error flow through " constant error carrousels " within special units . multiplicative gate units learn to open and close access to the constant error flow . lstm ' s update complexity per time step is o ( w ) , where w is the number of weights . in experimental comparisons with rtrl , bptt , recurrent cascade - correlation , elman nets , and neural sequence chunking , lstm leads to many more successful runs , and learns much faster . lstm also solves complex long time lag tasks that have never been solved by previous recurrent network algorithms . it works with local , distributed , real - valued , and noisy pattern representations .
geometric separability is a generalisation of linear separability , familiar to many from minsky and papert ' s analysis of the perceptron learning method . the concept forms a novel dimension along which to conceptualise learning methods . the present paper shows how geometric separability can be defined and demonstrates that it accurately predicts the performance of a at least one empirical learning method .
inspired by a visual motion detection model for the rabbit retina and by a computational architecture used for early audition in the barn owl , we have designed a chip that employs a correlation model to report the one - dimensional field motion of a scene in real time . using subthreshold analog vlsi techniques , we have fabricated and successfully tested a #NUM# transistor chip using a standard mosis process .
in associative reinforcement learning , an environment generates input vectors , a learning system generates possible output vectors , and a reinforcement function computes feedback signals from the input - output pairs . the task is to discover and remember input - output pairs that generate rewards . especially difficult cases occur when rewards are rare , since the expected time for any algorithm can grow exponentially with the size of the problem . nonetheless , if a reinforcement function possesses regularities , and a learning algorithm exploits them , learning time can be reduced below that of non - generalizing algorithms . this paper describes a neural network algorithm called complementary reinforcement back - propagation ( crbp ) , and reports simulation results on problems designed to offer differing opportunities for generalization .
we prove that the canonical distortion measure ( cdm ) [ #NUM# , #NUM# ] is the optimal distance measure to use for #NUM# nearest - neighbour ( #NUM# - nn ) classification , and show that it reduces to squared euclidean distance in feature space for function classes that can be expressed as linear combinations of a fixed set of features . pac - like bounds are given on the sample - complexity required to learn the cdm . an experiment is presented in which a neural network cdm was learnt for a japanese ocr environ ment and then used to do #NUM# - nn classification .
the schema theorem states that implicit parallel search is behind the power of the genetic algorithm . we contend that chromosomes can vote , proportionate to their fitness , for candidate schemata . we maintain a population of binary strings and ternary schemata . the string population not only works on solving its problem domain , but it supplies fitness for the schema population , which indirectly can solve the original problem .
r . k . belew , j . mcinerney , and n . schraudolph , evolving networks : using the genetic algorithm with connectionist learning , in artificial life ii , sfi studies in the science of complexity , c . g . langton , c . taylor , j . d . farmer , s . rasmussen eds . , vol . #NUM# , addison - wesley , #NUM# . [ #NUM# ] m . mcinerney , and a . p . dhawan , use of genetic algorithms with back propagation in training of feed - forward neural networks , in ieee international conference on neural networks , vol . #NUM# , pp . #NUM# - #NUM# , #NUM# . [ #NUM# ] f . z . brill , d . e . brown , and w . n . martin , fast genetic selection of features for neural network classifiers , ieee transactions on neural networks , vol . #NUM# , no . #NUM# , pp . #NUM# - #NUM# , #NUM# . [ #NUM# ] f . dellaert , and j . vandewalle , automatic design of cellular neural networks by means of genetic algorithms : finding a feature detector , in the third ieee international workshop on cellular neural networks and their applications , ieee , new jersey , pp . #NUM# - #NUM# , #NUM# . [ #NUM# ] d . e . moriarty , and r . miikkulainen , efficient reinforcement learning through symbiotic evolution , machine learning , vol . #NUM# , pp . #NUM# - #NUM# , #NUM# . [ #NUM# ] l . davis , handbook of genetic algorithms , van nostrand reinhold , new york , #NUM# . [ #NUM# ] d . whitely , the genitor algorithm and selective pressure , in proceedings of the third interanational conference on genetic algorithms , j . d . schaffer ed . , morgan kauffman , san mateo , ca , #NUM# , pp . #NUM# - #NUM# . [ #NUM# ] van camp , d . , t . plate and g . e . hinton ( #NUM# ) . the xerion neural network simulator and documentation . department of computer science , university of toronto , toronto .
an agent that must learn to act in the world by trial and error faces the reinforcement learning problem , which is quite different from standard concept learning . although good algorithms exist for this problem in the general case , they are often quite inefficient and do not exhibit generalization . one strategy is to find restricted classes of action policies that can be learned more efficiently . this paper pursues that strategy by developing algorithms that can efficiently learn action maps that are expressible in k - dnf . the algorithms are compared with existing methods in empirical trials and are shown to have very good performance .
this note considers positive recurrent markov chains where the probability of remaining in the current state is arbitrarily close to #NUM# . specifically , conditions are given which ensure the non - existence of central limit theorems for ergodic averages of functionals of the chain . the results are motivated by applications for metropolis - hastings algorithms which are constructed in terms of a rejection probability , ( where a rejection involves remaining at the current state ) . two examples for commonly used algorithms are given , for the independence sampler and the metropolis adjusted langevin algorithm . the examples are rather specialised , although in both cases , the problems which arise are typical of problems commonly occurring for the particular algorithm being used . #NUM# i would like to thank kerrie mengersen jeff rosenthal and richard tweedie for useful conversations on the subject of this paper .
a reinforcement learning system with limited computational resources interacts with an unrestricted , unknown environment . its goal is to maximize cumulative reward , to be obtained throughout its limited , unknown lifetime . system policy is an arbitrary modifiable algorithm mapping environmental inputs and internal states to outputs and new internal states . the problem is : in realistic , unknown environments , each policy modification process ( pmp ) occurring during system life may have unpredictable influence on environmental states , rewards and pmps at any later time . existing reinforcement learning algorithms cannot properly deal with this . neither can naive exhaustive search among all policy candidates | not even in case of very small search spaces . in fact , a reasonable way of measuring performance improvements in such general ( but typical ) situations is missing . i define such a measure based on the novel " reinforcement acceleration criterion " ( rac ) . at a given time , rac is satisfied if the beginning of each completed pmp that computed a currently valid policy modification has been followed by long - term acceleration of average reinforcement intake ( the computation time for later pmps is taken into account ) . i present a method called " environment - independent reinforcement acceleration " ( eira ) which is guaranteed to achieve rac . eira does neither care whether the system ' s policy allows for changing itself , nor whether there are multiple , interacting learning systems . consequences are : ( #NUM# ) a sound theoretical framework for " meta - learning " ( because the success of a pmp recursively depends on the success of all later pmps , for which it is setting the stage ) . ( #NUM# ) a sound theoretical framework for multi - agent learning . the principles have been implemented ( #NUM# ) in a single system using an assembler - like programming language to modify its own policy , and ( #NUM# ) a system consisting of multiple agents , where each agent is in fact just a connection in a fully recurrent reinforcement learning neural net . a by - product of this research is a general reinforcement learning algorithm for such nets . preliminary experiments illustrate the theory .
evolutionary computation uses computational models of evolution - ary processes as key elements in the design and implementation of computer - based problem solving systems . in this paper we provide an overview of evolutionary computation , and describe several evolutionary algorithms that are currently of interest . important similarities and di fferences are noted , which lead to a discussion of important issues that need to be resolved , and items for future research .
we present a general method for proving rigorous , a priori bounds on the number of iterations required to achieve convergence of markov chain monte carlo . we describe bounds for specific models of the gibbs sampler , which have been obtained from the general method . we discuss possibilities for obtaining bounds more generally .
much recent research in decision theoretic planning has adopted markov decision processes ( mdps ) as the model of choice , and has attempted to make their solution more tractable by exploiting problem structure . one particular algorithm , structured policy construction achieves this by means of a decision theoretic analog of goal regression , using action descriptions based on bayesian networks with tree - structured conditional probability tables . the algorithm as presented is not able to deal with actions with correlated effects . we describe a new decision theoretic regression operator that corrects this weakness . while conceptually straightforward , this extension requires a somewhat more complicated technical approach .
the problem of programming an artificial ant to follow the santa fe trail has been repeatedly used as a benchmark problem . recently we have shown performance of several techniques is not much better than the best performance obtainable using uniform random search . we suggested that this could be because the program fitness landscape is difficult for hill climbers and the problem is also difficult for genetic algorithms as it contains multiple levels of deception . here we redefine the problem so the ant is obliged to traverse the trail in approximately the correct order . a simple genetic programming system , with no size or depth restriction , is show to perform approximately three times better with the improved training function .
in general , the machine learning process can be accelerated through the use of heuristic knowledge about the problem solution . for example , monomorphic typed genetic programming ( gp ) uses type information to reduce the search space and improve performance . unfortunately , monomorphic typed gp also loses the generality of untyped gp : the generated programs are only suitable for inputs with the specified type . polymorphic typed gp improves over mono - morphic and untyped gp by allowing the type information to be expressed in a more generic manner , and yet still imposes constraints on the search space . this paper describes a polymorphic gp system which can generate polymorphic programs : programs which take inputs of more than one type and produces outputs of more than one type . we also demonstrate its operation through the generation of the map polymorphic program .
although so - called naive bayesian classification makes the unrealistic assumption that the values of the attributes of an example are independent given the class of the example , this learning method is remarkably successful in practice , and no uniformly better learning method is known . boosting is a general method of combining multiple classifiers due to yoav freund and rob schapire . this paper shows that boosting applied to naive bayesian classifiers yields combination classifiers that are representationally equivalent to standard feedforward multilayer perceptrons . ( an ancillary result is that naive bayesian classification is a nonparametric , nonlinear generalization of logistic regression . ) as a training algorithm , boosted naive bayesian learning is quite different from backpropagation , and has definite advantages . boosting requires only linear time and constant space , and hidden nodes are learned incrementally , starting with the most important . on the real - world datasets on which the method has been tried so far , generalization performance is as good as or better than the best published result using any other learning method . unlike all other standard learning algorithms , naive bayesian learning , with and without boosting , can be done in logarithmic time with a linear number of parallel computing units . accordingly , these learning methods are highly plausible computationally as models of animal learning . other arguments suggest that they are plausible behaviorally also .
this paper addresses the problem of handling skewed class distributions within the case - based learning ( cbl ) framework . we first present as a baseline an information - gain - weighted cbl algorithm and apply it to three data sets from natural language processing ( nlp ) with skewed class distributions . although overall performance of the baseline cbl algorithm is good , we show that the algorithm exhibits poor performance on minority class instances . we then present two cbl algorithms designed to improve the performance of minority class predictions . each variation creates test - case - specific feature weights by first observing the path taken by the test case in a decision tree created for the learning task , and then using path - specific information gain values to create an appropriate weight vector for use during case retrieval . when applied to the nlp data sets , the algorithms are shown to significantly increase the accuracy of minority class predictions while maintaining or improving over all classification accuracy .
the rtrl algorithm for fully recurrent continually running networks ( robinson and fallside , #NUM# ) ( williams and zipser , #NUM# ) requires o ( n #NUM# ) computations per time step , where n is the number of non - input units . i describe a method suited for on - line learning which computes exactly the same gradient and requires fixed - size storage of the same order but has an average time complexity #NUM# per time step of o ( n #NUM# ) .
in many markov chain monte carlo problems , the target density function is known up to a normalization constant . in this paper , we take advantage of this knowledge to facilitate the convergence diagnostic of a markov sampler by estimating the l #NUM# error of a kernel estimator . firstly , we propose an estimator of the normalization constant which is shown to be asymptotically normal under mixing and moment conditions . secondly , the l #NUM# error of the kernel estimator is estimated using the normalization constant estimator , and the ratio of the estimated l #NUM# error to the true l #NUM# error is shown to converge to #NUM# in probability under similar conditions . thirdly , we propose a sequential plot of the estimated l #NUM# error as a tool to monitor the convergence of the markov sampler . finally , a #NUM# - dimensional bimodal example is given to illustrate the proposal
in recent years , a number of different semantics for defaults have been proposed , such as preferential structures , * - semantics , possibilistic structures , and - rankings , that have been shown to be characterized by the same set of axioms , known as the klm properties ( for kraus , lehmann , and magidor ) . while this was viewed as a surprise , we show here that it is almost inevitable . we do this by giving yet another semantics for defaults that uses plausibility measures , a new approach to modeling uncertainty that generalize other approaches , such as probability measures , belief functions , and possibility measures . we show that all the earlier approaches to default reasoning can be embedded in the framework of plausibility . we then provide a necessary and sufficient condition on plausibilities for the klm properties to be sound , and an additional condition necessary and sufficient for the klm properties to be complete . these conditions are easily seen to hold for all the earlier approaches , thus explaining why they are characterized by the klm properties .
it is commonplace in artificial intelligence to divide an agent ' s explicit beliefs into two parts : the beliefs explicitly represented or manifest in memory , and the implicitly represented or constructive beliefs that are repeatedly reconstructed when needed rather than memorized . many theories of knowledge view the relation between manifest and constructive beliefs as a logical relation , with the manifest beliefs representing the constructive beliefs through a logic of belief . this view , however , limits the ability of a theory to treat incomplete or inconsistent sets of beliefs in useful ways . we argue that a more illuminating view is that belief is the result of rational representation . in this theory , the agent obtains its constructive beliefs by using its manifest beliefs and preferences to rationally ( in the sense of decision theory ) choose the most useful conclusions indicated by the manifest beliefs .
the economic theory of rationality promises to equal mathematical logic in its importance for the mechanization of reasoning . we survey the growing literature on how the basic notions of probability , utility , and rational choice , coupled with practical limitations on information and resources , influence the design and analysis of reasoning and representation systems .
the paper is concerned with the integration of constraint logic programming systems ( clp ) with systems based on genetic algorithms ( ga ) . the resulting framework is tailored for applications that require a first phase in which a number of constraints need to be generated , and a second phase in which an optimal solution satisfying these constraints is produced . the first phase is carried by the clp and the second one by the ga . we present a specific framework where ecl i ps e ( ecrc common logic programming system ) and genocop ( genetic algorithm for numerical optimization for constrained problems ) are integrated in a framework called coco ( computational intelligence plus constraint logic programming ) . the coco system is applied to the training problem for neural networks . we consider constrained networks , e . g . neural networks with shared weights , constraints on the weights for example domain constraints for hardware implementation etc . then ecl i ps e is used to generate the chromosome representation together with other constraints which ensure , in most cases , that each network is specified by exactly one chromosome . thus the problem becomes a constrained optimization problem , where the optimization criterion is to optimize the error of the network , and genocop is used to find an optimal solution . note : the work of the second author was partially supported by sion , a department of the nwo , the national foundation for scientific research . this work has been carried out while the third author was visiting cwi , amsterdam , and the fourth author was visiting leiden university .
in this paper we study how global optimization methods ( like genetic algorithms ) can be used to train neural networks . we introduce the notion of regularity , for studying properties of the error function that expand the search space in an artificial way . regularities are used to generate constraints on the weights of the network . in order to find a satisfiable set of constraints we use a constraint logic programming system . then the training of the network becomes a constrained optimization problem . we also relate the notion of regularity to so - called network transformations .
in this paper , we study pac - learning algorithms for specialized classes of deterministic finite automata ( dfa ) . in particular , we study branching programs , and we investigate the influence of the width of the branching program on the difficulty of the learning problem . we first present a distribution - free algorithm for learning width - #NUM# branching programs . we also give an algorithm for the proper learning of width - #NUM# branching programs under uniform distribution on labeled samples . we then show that the existence of an efficient algorithm for learning width - #NUM# branching programs would imply the existence of an efficient algorithm for learning dnf , which is not known to be the case . finally , we show that the existence of an algorithm for learning width - #NUM# branching programs would also yield an algorithm for learning a very restricted version of parity with noise .
the longest common subsequence problem is examined from the point of view of parameterized computational complexity . there are several different ways in which parameters enter the problem , such as the number of sequences to be analyzed , the length of the common subsequence , and the size of the alphabet . lower bounds on the complexity of this basic problem imply lower bounds on a number of other sequence alignment and consensus problems . at issue in the theory of parameterized complexity is whether a problem which takes input ( x ; k ) can be solved in time f ( k ) n ff where ff is independent of k ( termed fixed - parameter tractability ) . it can be argued that this is the appropriate asymptotic model of feasible computability for problems for which a small range of parameter values covers important applications | a situation which certainly holds for many problems in biological sequence analysis . our main results show that : ( #NUM# ) the longest common subsequence ( lcs ) parameterized by the number of sequences to be analyzed is hard for w [ t ] for all t . ( #NUM# ) the lcs problem problem , parameterized by the length of the common subsequence , belongs to w [ p ] and is hard for w [ #NUM# ] . ( #NUM# ) the lcs problem parameterized both by the number of sequences and the length of the common subsequence , is complete for w [ #NUM# ] . all of the above results are obtained for unrestricted alphabet sizes . for alphabets of a fixed size , problems ( #NUM# ) and ( #NUM# ) are fixed - parameter tractable . we conjecture that ( #NUM# ) remains hard .
is the universe computable ? if so , it may be much cheaper in terms of information requirements to compute all computable universes instead of just ours . i apply basic concepts of kolmogorov complexity theory to the set of possible universes , and chat about perceived and true randomness , life , generalization , and learning in a given universe . assumptions . a long time ago , the great programmer wrote a program that runs all possible universes on his big computer . " possible " means " computable " : ( #NUM# ) each universe evolves on a discrete time scale . ( #NUM# ) any universe ' s state at a given time is describable by a finite number of bits . one of the many universes is ours , despite some who evolved in it and claim it is incomputable . computable universes . let t m denote an arbitrary universal turing machine with unidirectional output tape . t m ' s input and output symbols are " #NUM# " , " #NUM# " , and " , " ( comma ) . t m ' s possible input programs can be ordered alphabetically : " " ( empty program ) , " #NUM# " , " #NUM# " , " , " , " #NUM# " , " #NUM# " , " #NUM# , " , " #NUM# " , " #NUM# " , " #NUM# , " , " , #NUM# " , " , #NUM# " , " , , " , " #NUM# " , etc . let a k denote t m ' s k - th program in this list . its output will be a finite or infinite string over the alphabet f " #NUM# " , " #NUM# " , " , " g . this sequence of bitstrings separated by commas will be interpreted as the evolution e k of universe u k . if e k includes at least one comma , then let u l k represents u k ' s state at the l - th time step of e k ( k ; l #NUM# f #NUM# ; #NUM# ; : : : ; g ) . e k is represented by the sequence u #NUM# k corresponds to u k ' s big bang . different algorithms may compute the same universe . some universes are finite ( those whose programs cease producing outputs at some point ) , others are not . i don ' t know about ours . tm not important . the choice of the turing machine is not important . this is due to the compiler theorem : for each universal turing machine c there exists a constant prefix c #NUM# f " #NUM# " , " #NUM# " , " , " g fl such that for all possible programs p , c ' s output in response to program c p is identical to t m ' s output in response to p . the prefix c is the compiler that compiles programs for t m into equivalent programs for c . k denote the l - th ( possibly empty ) bitstring before the l - th comma . u l
the metropolis - hastings algorithm for estimating a distribution is based on choosing a candidate markov chain and then accepting or rejecting moves of the candidate to produce a chain known to have as the invariant measure . the traditional methods use candidates essentially unconnected to . based on diffusions for which is invariant , we develop for one - dimensional distributions a class of candidate distributions that " self - target " towards the high density areas of . these produce metropolis - hastings algorithms with convergence rates that appear to be considerably better than those known for the traditional candidate choices , such as random walk . in particular , for wide classes of these choices may effectively help reduce the " burn - in " problem . we illustrate this behaviour for examples with exponential and polynomial tails , and for a logistic regression model using a gibbs sampling algorithm .
we show that a dnf with terms of size at most d can be approximated by a function with at most d o ( d log #NUM# = " ) non zero fourier coefficients such that the expected error squared , with respect to the uniform distribution , is at most " . this property is used to derive a learning algorithm for dnf , under the uniform distribution . the learning algorithm uses queries and learns , with respect to the uniform distribution , a dnf with terms of size at most d in time polynomial in n and d o ( d log #NUM# = " ) . the interesting implications are for the case when " is constant . in this case our algorithm learns a dnf with a polynomial number of terms in time n o ( log log n ) , and a dnf with terms of size at most o ( log n = log log n ) in polynomial time .
in this paper we present a new multivariate decision tree algorithm lmdt , which combines linear machines with decision trees . lmdt constructs each test in a decision tree by training a linear machine and then eliminating irrelevant and noisy variables in a controlled manner . to examine lmdt ' s ability to find good generalizations we present results for a variety of domains . we compare lmdt empirically to a univariate decision tree algorithm and observe that when multivariate tests are the appropriate bias for a given data set , lmdt finds small accurate trees .
kooperberg , bose , and stone ( #NUM# ) introduced polyclass , a methodology that uses adaptively selected linear splines and their tensor products to model conditional class probabilities . the authors attempted to develop a methodology that would work well on small and moderate size problems and would scale up to large problems . however , the version of polyclass that was developed for large problems was impractical in that it required two months of cpu time to apply it to a large data set . a modification to this methodology involving the use of the stochastic gradient ( on - line ) method in fitting polyclass models to given sets of basis functions is developed here that makes the methodology applicable to large data sets . in particular , it is successfully applied to a phoneme recognition problem involving #NUM# phonemes , #NUM# features , #NUM# , #NUM# cases in the training sample , #NUM# basis functions , and #NUM# , #NUM# unknown parameters . comparisons with neural networks are made both on the original problem and on a three - vowel subproblem .
the use of externally imposed hierarchical structures to reduce the complexity of learning control is common . however , it is acknowledged that learning the hierarchical structure itself is an important step towards more general ( learning of many things as required ) and less bounded ( learning of a single thing as specified ) learning . presented in this paper is a reinforcement learning algorithm called nested q - learning that generates a hierarchical control structure in reinforcement learning domains . the emergent structure combined with learned bottom - up reactive reactions results in a reactive hierarchical control system . effectively , the learned hierarchy decomposes what would otherwise be a monolithic evaluation function into many smaller evaluation functions that can be recombined without the loss of previously learned information .
we present an on - line investment algorithm which achieves almost the same wealth as the best constant - rebalanced portfolio determined in hindsight from the actual market outcomes . the algorithm employs a multiplicative update rule derived using a framework introduced by kivinen and warmuth . our algorithm is very simple to implement and requires only constant storage and computing time per stock in each trading period . we tested the performance of our algorithm on real stock data from the new york stock exchange accumulated during a #NUM# - year period . on this data , our algorithm clearly outperforms the best single stock as well as cover ' s universal portfolio selection algorithm . we also present results for the situation in which the investor has access to additional " side information . "
we consider belief revision operators that satisfy the alchourron - gardenfors - makinson postulates , and present an epistemic logic in which , for any such revision operator , the result of a revision can be described by a sentence in the logic . in our logic , the fact that the agent ' s set of beliefs is is represented by the sentence o , where o is levesque ' s ` only know ' operator . intuitively , o is read as ` is all that is believed . ' the fact that the agent believes is represented by the sentence b , read in the usual way as ` is believed ' . the connective represents update as defined by katsuno and mendelzon . the revised beliefs are represented by the sentence o b . we show that for every revision operator that satisfies the agm postulates , there is a model for our epistemic logic such that the beliefs implied by the sentence o b in this model correspond exactly to the sentences implied by the theory that results from revising by . this means that reasoning about changes in the agent ' s beliefs reduces to model checking of certain epistemic sentences . the negative result in the paper is that this type of formal account of revision cannot be extended to the situation where the agent is able to reason about its beliefs . a fully introspective agent cannot use our construction to reason about the results of its own revisions , on pain of triviality .
this paper introduces a novel enhancement for learning bayesian networks with a bias for small , high - predictive - accuracy networks . the new approach selects a subset of features which maximizes predictive accuracy prior to the network learning phase . we examine explicitly the effects of two aspects of the algorithm , feature selection and node ordering . our approach generates networks which are com - putationally simpler to evaluate and which display predictive accuracy comparable to that of bayesian networks which model all attributes .
while the need for hierarchies within control systems is apparent , it is also clear to many researchers that such hierarchies should be learned . learning both the structure and the component behaviors is a difficult task . the benefit of learning the hierarchical structures of behaviors is that the decomposition of the control structure into smaller transportable chunks allows previously learned knowledge to be applied to new but related tasks . presented in this paper are improvements to nested q - learning ( nql ) that allow more realistic learning of control hierarchies in reinforcement environments . also presented is a simulation of a simple robot performing a series of related tasks that is used to compare both hierarchical and non - hierarchal learning techniques .
recent findings suggest that a classification scheme based on an ensemble of networks is an effective way to address overfitting . we study optimal methods for training an ensemble of networks . some recent experiments on postal zip - code character data suggest that weight decay may not be an optimal method for controlling the variance of a classifier .
best - first model merging is a general technique for dynamically choosing the structure of a neural or related architecture while avoiding overfitting . it is applicable to both learning and recognition tasks and often generalizes significantly better than fixed structures . we demonstrate the approach applied to the tasks of choosing radial basis functions for function learning , choosing local affine models for curve and constraint surface modelling , and choosing the structure of a balltree or bumptree to maximize efficiency of access .
we describe algorithms for estimating a given measure known up to a constant of proportionality , based on a large class of diffusions ( extending the langevin model ) for which is invariant . we show that under weak conditions one can choose from this class in such a way that the diffusions converge at exponential rate to , and one can even ensure that convergence is independent of the starting point of the algorithm . when convergence is less than exponential we show that it is often polynomial at known rates . we then consider methods of discretizing the diffusion in time , and find methods which inherit the convergence rates of the continuous time process . these contrast with the behaviour of the naive or euler discretization , which can behave badly even in simple cases .
linsker has reported the development of structured receptive fields in simulations using a hebb - type synaptic plasticity rule in a feed - forward linear network . the synapses develop under dynamics determined by a matrix that is closely related to the covariance matrix of input cell activities . we analyse the dynamics of the learning rule in terms of the eigenvectors of this matrix . these eigenvectors represent independently evolving weight structures . some general theorems are presented regarding the properties of these eigenvectors and their eigenvalues . for a general covariance matrix four principal parameter regimes are predicted . we concentrate on the gaussian covariances at layer b ! c of linsker ' s network . analytic and numerical solutions for the eigenvectors at this layer are presented . three eigenvectors dominate the dynamics : a dc eigenvector , in which all synapses have the same sign ; a bi - lobed , oriented eigenvector ; and a circularly symmetric , centre - surround eigenvector . analysis of the circumstances in which each of these vectors dominates yields an explanation of the emergence of centre - surround structures and symmetry - breaking bi - lobed structures . criteria are developed estimating the boundary of the parameter regime in which centre - surround structures emerge . the application of our analysis to linsker ' s higher layers , at which the covariance functions were oscillatory , is briefly discussed .
this paper considers the problem of scaling the proposal distribution of a multidimensional random walk metropolis algorithm , in order to maximize the efficiency of the algorithm . the main result is a weak convergence result as the dimension of a sequence of target densities , n , converges to #NUM# . when the proposal variance is appropriately scaled according to n , the sequence of stochastic processes formed by the first component of each markov chain , converge to the appropriate limiting langevin diffusion process . the limiting diffusion approximation admits a straight - forward efficiency maximization problem , and the resulting asymptotically optimal policy is related to the asymptotic acceptance rate of proposed moves for the algorithm . the asymptotically optimal acceptance rate is #NUM# . #NUM# under quite general conditions . the main result is proved in the case where the target density has a symmetric product form . extensions of the result are discussed .
combinating reactivity with planning has been proposed as a means of compensating for potentially slow response times of planners while still making progress toward long term goals . the demands of rapid response and the complexity of many environments make it difficult to decompose , tune and coordinate reactive behaviors while ensuring consistency . neural networks can address the tuning problem , but are less useful for decomposition and coordination . we hypothesize that interacting reactions can be decomposed into separate behaviors resident in separate networks and that the interaction can be coordinated through the tuning mechanism and a higher level controller . to explore these issues , we have implemented a neural network architecture as the reactive component of a two layer control system for a simulated race car . by varying the architecture , we test whether decomposing reactivity into separate behaviors leads to superior overall performance , coordination and learning convergence .
we introduce a formal model of teaching in which the teacher is tailored to a particular learner , yet the teaching protocol is designed so that no collusion is possible . not surprisingly , such a model remedies the non - intuitive aspects of other models in which the teacher must successfully teach any consistent learner . we prove that any class that can be exactly identified by a deterministic polynomial - time algorithm with access to a very rich set of example - based queries is teachable by a computationally unbounded teacher and a polynomial - time learner . in addition , we present other general results relating this model of teaching to various previous results . we also consider the problem of designing teacher / learner pairs in which both the teacher and learner are polynomial - time algorithms and describe teacher / learner pairs for the classes of #NUM# - decision lists and horn sentences .
this paper explores some algorithms for automatic quantization of real - valued datasets using thermometer codes for pattern classification applications . experimental results indicate that a relatively simple randomized thermometer code generation technique can result in quantized datasets that when used to train simple perceptrons , can yield generalization on test data that is substantially better than that obtained with their unquantized counterparts .
automated search of a space of candidate designs seems an attractive way to improve the traditional engineering design process . to make this approach work , however , the automated design system must include both knowledge of the modeling limitations of the method used to evaluate candidate designs and also an effective way to use this knowledge to influence the search process . we suggest that a productive approach is to include this knowledge by implementing a set of model constraint functions which measure how much each modeling assumptions is violated , and to influence the search by using the values of these model constraint functions as constraint inputs to a standard constrained nonlinear optimization numerical method . we test this idea in the domain of conceptual design of supersonic transport aircraft , and our experiments indicate that our model constraint communication strategy can decrease the cost of design space search by one or more orders of magnitude .
the curse of dimensionality is one of the severest problems concerning the application of rbf networks . the number of rbf nodes and therefore the number of training examples needed grows exponentially with the intrinsic dimensionality of the input space . one way to address this problem is the application of feature selection as a data preprocessing step . in this paper we propose a two - step approach for the determination of an optimal feature subset : first , all possible feature - subsets are reduced to those with best discrimination properties by the application of the fast and robust filter technique eubafes . secondly we use a wrapper approach to judge , which of the pre - selected feature subsets leads to rbf networks with least complexity and best classification accuracy . experiments are undertaken to show the improvement for rbf networks by our feature selection approach .
this paper re - examines the problem of parameter estimation in bayesian networks with missing values and hidden variables from the perspective of recent work in on - line learning [ #NUM# ] . we provide a unified framework for parameter estimation that encompasses both on - line learning , where the model is continuously adapted to new data cases as they arrive , and the more traditional batch learning , where a pre - accumulated set of samples is used in a one - time model selection process . in the batch case , our framework encompasses both the gradient projection algorithm [ #NUM# , #NUM# ] and the em algorithm [ #NUM# ] for bayesian networks . the framework also leads to new on - line and batch parameter update schemes , including a parameterized version of em . we provide both empirical and theoretical results indicating that parameterized em allows faster convergence to the maximum likelihood parame ters than does standard em .
many techniques for speedup learning and knowledge compilation focus on the learning and optimization of macro - operators or control rules in task domains that can be characterized using a problem - space search paradigm . however , such a characterization does not fit well the class of task domains in which the problem solver is required to perform in a continuous manner . for example , in many robotic domains , the problem solver is required to monitor real - valued perceptual inputs and vary its motor control parameters in a continuous , on - line manner to successfully accomplish its task . in such domains , discrete symbolic states and operators are difficult to define . to improve its performance in continuous problem domains , a problem solver must learn , modify , and use continuous operators that continuously map input sensory information to appropriate control outputs . additionally , the problem solver must learn the contexts in which those continuous operators are applicable . we propose a learning method that can compile sensorimo - tor experiences into continuous operators , which can then be used to improve performance of the problem solver . the method speeds up the task performance as well as results in improvements in the quality of the resulting solutions . the method is implemented in a robotic navigation system , which is evaluated through extensive experimen tation .
discussions of case - based reasoning often reflect an implicit assumption that a case memory system will become better informed , i . e . will increase in knowledge , as more cases are added to the case - base . this paper considers formalisations of this ` knowledge content ' which are a necessary preliminary to more rigourous analysis of the performance of case - based reasoning systems . in particular we are interested in modelling the learning aspects of case - based reasoning in order to study how the performance of a case - based reasoning system changes as it accumlates problem - solving experience . the current paper presents a ` case - base semantics ' which generalises recent formalisations of case - based classification . within this framework , the paper explores various issues in assuring that these sematics are well - defined , and illustrates how the knowledge content of the case memory system can be seen to reside in both the chosen similarity measure and in the cases of the case - base .
we propose and analyze a distribution learning algorithm for a subclass of acyclic probabilistic finite automata ( apfa ) . this subclass is characterized by a certain distinguishability property of the automata ' s states . though hardness results are known for learning distributions generated by general apfas , we prove that our algorithm can indeed efficiently learn distributions generated by the subclass of apfas we consider . in particular , we show that the kl - divergence between the distribution generated by the target source and the distribution generated by our hypothesis can be made small with high confidence in polynomial time . we present two applications of our algorithm . in the first , we show how to model cursively written letters . the resulting models are part of a complete cursive handwriting recognition system . in the second application we demonstrate how apfas can be used to build multiple - pronunciation models for spoken words . we evaluate the apfa based pronunciation models on labeled speech data . the good performance ( in terms of the log - likelihood obtained on test data ) achieved by the apfas and the incredibly small amount of time needed for learning suggests that the learning algorithm of apfas might be a powerful alternative to commonly used probabilistic models .
this paper examines the inductive inference of a complex grammar with neural networks specifically , the task considered is that of training a network to classify natural language sentences as grammatical or ungrammatical , thereby exhibiting the same kind of discriminatory power provided by the principles and parameters linguistic framework , or government - and - binding theory . neural networks are trained , without the division into learned vs . innate components assumed by chomsky , in an attempt to produce the same judgments as native speakers on sharply grammatical / ungrammatical data . how a recurrent neural network could possess linguistic capability , and the properties of various common recurrent neural network architectures are discussed . the problem exhibits training behavior which is often not present with smaller grammars , and training was initially difficult . however , after implementing several techniques aimed at improving the convergence of the gradient descent backpropagation - through - time training algorithm , significant learning was possible . it was found that certain architectures are better able to learn an appropriate grammar . the operation of the networks and their training is analyzed . finally , the extraction of rules in the form of deterministic finite state automata is investigated .
we propose the lazy neural tree ( lnt ) as the appropriate architecture for the realization of smooth regression systems . the lnt is a hybrid of a decision tree and a neural network . from the neural network it inherits smoothness of the generated function , incremental adaptability , and conceptual simplicity . from the decision tree it inherits the topology and initial parameter setting as well as a very efficient sequential implementation that out - performs traditional neural network simulations by the order of magnitudes . the enormous speed is achieved by lazy evaluation . a further speed - up can be obtained by the application of a window - ing scheme if the region of interesting results is restricted .
this paper describes a new method for determining the consensus sequences that signal the start of translation and the boundaries between exons and introns ( donor and acceptor sites ) in eukaryotic mrna . the method takes into account the dependencies between adjacent bases , in contrast to the usual technique of considering each position independently . when coupled with a dynamic program to compute the most likely sequence , new consensus sequences emerge . the consensus sequence information is summarized in conditional probability matrices which , when used to locate signals in uncharacter - ized genomic dna , have greater sensitivity and specificity than conventional matrices . species - specific versions of these matrices are especially effective at distinguishing true and false sites .
in the pantheon of evolutionary forces , the optimizing apollonian powers of natural selection are generally assumed to dominate the dark dionysian dynamics of sexual selection . but this need not be the case , particularly with a class of selective mating mechanisms called ` directional mate preferences ' ( kirkpatrick , #NUM# ) . in previous simulation research , we showed that nondirectional assortative mating preferences could cause populations to spontaneously split apart into separate species ( todd & miller , #NUM# ) . in this paper , we show that directional mate preferences can cause populations to wander capriciously through phenotype space , under a strange form of runaway sexual selection , with or without the influence of natural selection pressures . when directional mate preferences are free to evolve , they do not always evolve to point in the direction of natural - selective peaks . sexual selection can thus take on a life of its own , such that mate preferences within a species become a distinct and important part of the environment to which the species ' phenotypes adapt . these results suggest a broader conception of ` adaptive behavior ' , in which attracting potential mates becomes as important as finding food and avoiding predators . we present a framework for simulating a wide range of directional and non - directional mate preferences , and discuss some practical and scientific applications of simu lating sexual selection .
the paper investigates the possibilities for using simple recurrent networks as transducers which map sequential natural language input into non - sequential feature - based semantics . the networks perform well on sentences containing a single main predicate ( encoded by transitive verbs or prepositions ) applied to multiple - feature objects ( encoded as noun - phrases with adjectival modifiers ) , and shows robustness against ungrammatical inputs . a second set of experiments deals with sentences containing embedded structures . here the network is able to process multiple levels of sentence - final embeddings but only one level of center - embedding . this turns out to be a consequence of the network ' s inability to retain information that is not reflected in the outputs over intermediate phases of processing . two extensions to elman ' s [ #NUM# ] original recurrent network architecture are introduced .
we use a connectionist network trained with reinforcement to control both an autonomous robot vehicle and a simulated robot . we show that given appropriate sensory data and architectural structure , a network can learn to control the robot for a simple navigation problem . we then investigate a more complex goal - based problem and examine the plan - like behavior that emerges . an autonomous agent can be abstractly defined as a mapping from a sequence of sensory inputs to an appropriate action in response to these percepts . such an agent is autonomous to the extent that its behavior is determined by its immediate inputs and past experience , rather than by its built - in control . we are interested in investigating the cognitive capabilities of autonomous agents . we believe that cognitive behavior can emerge from the reactive , situated activity of autonomous agents .
in this paper we consider the problem of tracking a subset of a domain ( called the target ) which changes gradually over time . a single ( unknown ) probability distribution over the domain is used to generate random examples for the learning algorithm and measure the speed at which the target changes . clearly , the more rapidly the target moves , the harder it is for the algorithm to maintain a good approximation of the target . therefore we evaluate algorithms based on how much movement of the target can be tolerated between examples while predicting with accuracy * . furthermore , the complexity of the class h of possible targets , as measured by d , its vc - dimension , also effects the difficulty of tracking the target concept . we show that if the problem of minimizing the number of disagreements with a sample from among concepts in a class h can be approximated to within a factor k , then there is a simple tracking algorithm for h which can achieve a probability * of making a mistake if the target movement rate is at most a constant times * #NUM# = ( k ( d + k ) ln #NUM# * ) , where d is the vapnik - chervonenkis dimension of h . also , we show that if h is properly pac - learnable , then there is an efficient ( randomized ) algorithm that with high probability approximately minimizes disagreements to within a factor of #NUM# d + #NUM# , yielding an efficient tracking algorithm for h which tolerates drift rates up to a constant times * #NUM# = ( d #NUM# ln #NUM# in addition , we prove complementary results for the classes of halfspaces and axis - aligned hy perrectangles showing that the maximum rate of drift that any algorithm ( even with unlimited
practical pattern classification and knowledge discovery problems require selection of a subset of attributes or features ( from a much larger set ) to represent the patterns to be classified . this is due to the fact that the performance of the classifier ( usually induced by some learning algorithm ) and the cost of classification are sensitive to the choice of the features used to construct the classifier . exhaustive evaluation of possible feature subsets is usually infeasible in practice because of the large amount of computational effort required . genetic algorithms , which belong to a class of randomized heuristic search techniques , offer an attractive approach to find near - optimal solutions to such optimization problems . this paper presents an approach to feature subset selection using a genetic algorithm . some advantages of this approach include the ability to accommodate multiple criteria such as accuracy and cost of classification into the feature selection process and to find feature subsets that perform well for particular choices of the inductive learning algorithm used to construct the pattern classifier . our experiments with several benchmark real - world pattern classification problems demonstrate the feasibility of this approach to feature subset selection in the automated many practical pattern classification tasks ( e . g . , medical diagnosis ) require learning of an appropriate classification function that assigns a given input pattern ( typically represented using a vector of attribute or feature values ) to one of a finite set of classes . the choice of features , attributes , or measurements used to represent patterns that are presented to a classifier affect ( among other things ) : the accuracy of the classification function that can be learned using an inductive learning algorithm ( e . g . , a decision tree induction algorithm or a neural network learning algorithm ) : the features used to describe the patterns implicitly define a pattern language . if the language is not expressive enough , it would fail to capture the information that is necessary for classification and hence regardless of the learning algorithm used , the accuracy of the classification function learned would be limited by this lack of information . design of neural networks for pattern classification and knowledge discovery .
the main aim of this paper is to provide a tutorial on regression with gaussian processes . we start from bayesian linear regression , and show how by a change of viewpoint one can see this method as a gaussian process predictor based on priors over functions , rather than on priors over parameters . this leads in to a more general discussion of gaussian processes in section #NUM# . section #NUM# deals with further issues , including hierarchical modelling and the setting of the parameters that control the gaussian process , the covariance functions for neural network models and the use of gaussian processes in classification problems .
general convergence results for linear discriminant updates abstract the problem of learning linear discriminant concepts can be solved by various mistake - driven update procedures , including the winnow family of algorithms and the well - known perceptron algorithm . in this paper we define the general class of quasi - additive algorithms , which includes perceptron and winnow as special cases . we give a single proof of convergence that covers much of this class , including both perceptron and winnow but also many novel algorithms . our proof introduces a generic measure of progress that seems to capture much of when and how these algorithms converge . using this measure , we develop a simple general technique for proving mistake bounds , which we apply to the new algorithms as well as existing algorithms . when applied to known algorithms , our technique automatically produces close variants of existing proofs ( and we generally obtain the known bounds , to within constants ) thus showing , in a certain sense , that these seem ingly diverse results are fundamentally isomorphic .
in this paper we present a prototype of a flexible similarity - based retrieval system . its flexibility is supported by allowing for an imprecisely specified query . moreover , our algorithm allows for assessing if the retrieved items are relevant in the initial context , specified in the query . the presented system can be used as a supporting tool for a software repository . we also discuss system evaluation with concerns on usefulness , scalability , applicability and comparability . evaluation of the t a #NUM# system on three domains gives us encouraging results and an integration of ta #NUM# into a real software repository as a retrieval tool is ongoing .
this paper describes an application of an inductive learning techniques to case - based reasoning . we introduce two main forms of induction , define case - based reasoning and present a combination of both . the evaluation of the proposed system , called ta #NUM# , is carried out on a classification task , namely character recognition . we show how inductive knowledge improves knowledge representation and in turn flexibility of the system , its performance ( in terms of classification accuracy ) and its scalability .
the aaai fall symposium ; flexible computation in intelligent systems : results , issues , and opportunities . nov . #NUM# - #NUM# , #NUM# , cambridge , ma abstract this paper presents a case - based reasoning system ta #NUM# . we address the flexibility of the case - based reasoning process , namely flexible retrieval of relevant experiences , by using a novel similarity assessment theory . to exemplify the advantages of such an approach , we have experimentally evaluated the system and compared its performance to the performance of non - flexible version of ta #NUM# and to other machine learning algorithms on several domains .
diagnosis of a disease and its treatment are not separate , one - shot activities . instead they are very often dependent and interleaved over time , mostly due to uncertainty about the underlying disease , uncertainty associated with the response of a patient to the treatment and varying cost of different treatment and diagnostic ( investigative ) procedures . the framework particularly suitable for modeling such a complex therapy decision process is partially observable markov decision process ( pomdp ) . unfortunately the problem of finding the optimal therapy within the standard pomdp framework is also computationally very costly . in this paper we investigate various structural extensions of the standard pomdp framework and approximation methods which allow us to simplify model construction process for larger therapy problems and to solve them faster . a therapy problem we target specifically is the management of patients with ischemic heart disease .
genetic programming is increasing in popularity as the basis for a wide range of learning algorithms . however , the technique has to date only been successfully applied to modest tasks because of the performance overheads of evolving a large number of data structures , many of which do not correspond to a valid program . we address this problem directly and demonstrate how the evolutionary process can be achieved with much greater efficiency through the use of a formally - based representation and strong typing . we report initial experimental results which demonstrate that our technique exhibits significantly better performance than previous work .
the dna promoter sequences domain theory and database have become popular for testing systems that integrate empirical and analytical learning . this note reports a simple change and reinterpretation of the domain theory in terms of m - of - n concepts , involving no learning , that results in an accuracy of #NUM# . #NUM# % on the #NUM# items of the database . moreover , an exhaustive search of the space of m - of - n domain theory interpretations indicates that the expected accuracy of a randomly chosen interpretation is #NUM# . #NUM# % , and that a maximum accuracy of #NUM# . #NUM# % is achieved in #NUM# cases . this demonstrates the informativeness of the domain theory , without the complications of understanding the interactions between various learning algorithms and the theory . in addition , our results help characterize the difficulty of learning using the dna promoters theory .
we discuss a strategy for polychotomous classification that involves estimating class probabilities for each pair of classes , and then coupling the estimates together . the coupling model is similar to the bradley - terry method for paired comparisons . we study the nature of the class probability estimates that arise , and examine the performance of the procedure in real and simulated datasets . classifiers used include linear discriminants , nearest neighbors , and the support vector machine .
intracortical microstimulation ( icms ) of a localized site in the somatosensory cortex of rats and monkeys for #NUM# - #NUM# hours produces a large increase in the cortical representation of the skin region represented by the icms - site neurons before icms , with very little effect on the icms - site neuron ' s rf location , rf size , and responsiveness ( recanzone et al . , #NUM# ) . the " exin " ( afferent excitatory and lateral inhibitory ) learning rules ( marshall , #NUM# ) are used to model rf changes during icms . the exin model produces reorganization of rf topography similar to that observed experimentally . the possible role of inhibitory learning in producing the effects of icms is studied by simulating the exin model with only lateral inhibitory learning . the model also produces an increase in the cortical representation of the skin region represented by the icms - site rf . icms is compared to artificial scotoma conditioning ( pettet & gilbert , #NUM# ) and retinal lesions ( darian - smith & gilbert , #NUM# ) , and it is suggested that lateral inhibitory learning may be a general principle of cortical plasticity .
in this work we present a classification methodology ( linneo + ) to discover concepts from ill - structured domains and to organize hierarchies with them . in order to achieve this aim linneo + uses conceptual learning techniques and classification . the final target is to build knowledge bases after expert validation . some techniques for the improvement of the results in the classification step are used , like biasing using partial expert knowledge ( classification rules or causal and structural dependencies between attributes ) or delayed cluster assignation of objects . also some comparisons with a few well - known systems are shown .
the new classification algorithm clef combines a version of a linear machine known as a - machine with a non - linear function approximator that constructs its own features . the algorithm finds non - linear decision boundaries by constructing features that are needed to learn the necessary discriminant functions . the clef algorithm is proven to separate all consistently labelled training instances , even when they are not linearly separable in the input variables . the algorithm is illustrated on a variety of tasks .
in this paper , we review five heuristic strategies for handling context - sensitive features in supervised machine learning from examples . we discuss two methods for recovering lost ( implicit ) contextual information . we mention some evidence that hybrid strategies can have a synergetic effect . we then show how the work of several machine learning researchers fits into this framework . while we do not claim that these strategies exhaust the possibilities , it appears that the framework includes all of the techniques that can be found in the published literature on context sensitive learning .
in this paper we describe a new adaptive penalty approach for handling constraints in genetic algorithm optimization problems . the idea is to start with a relatively small penalty coefficient and then increase it or decrease it on demand as the optimization progresses . empirical results in several engineering design domains demonstrate the merit of the proposed approach .
recent research in decision theoretic planning has focussed on making the solution of markov decision processes ( mdps ) more feasible . we develop a family of algorithms for structured reachability analysis of mdps that are suitable when an initial state ( or set of states ) is known . using compact , structured representations of mdps ( e . g . , bayesian networks ) , our methods , which vary in the tradeoff between complexity and accuracy , produce structured descriptions of ( estimated ) reachable states that can be used to eliminate variables or variable values from the problem description , reducing the size of the mdp and making it easier to solve . one contribution of our work is the extension of ideas from graphplan to deal with the distributed nature of action representations typically embodied within bayes nets and the problem of correlated action effects . we also demonstrate that our algorithm can be made more complete by using k - ary constraints instead of binary constraints . another contribution is the illustration of how the compact representation of reachability constraints can be exploited by several existing ( exact and approximate ) abstraction algorithms for mdps .
many ilp systems , such as golem , foil , and mis , take advantage of user supplied meta - knowledge to restrict the hypothesis space . this meta - knowledge can be in the form of type information about arguments in the predicate being learned , or it can be information about whether a certain argument in the predicate is functionally dependent on the other arguments ( supplied as mode information ) . this meta knowledge is explicitly supplied to an ilp system in addition to the data . the present paper argues that in many cases the meta knowledge can be extracted directly from the raw data . three algorithms are presented that learn type , mode , and symmetric meta - knowledge from data . these algorithms can be incorporated in existing ilp systems in the form of a preprocessor that obviates the need for a user to explicitly provide this information . in many cases , the algorithms can extract meta - knowledge that the user is either unaware of , but which information can be used by the ilp system to restrict the hypothesis space .
gold showed in #NUM# that not even regular grammars can be exactly identified from positive examples alone . since it is known that children learn natural grammars almost exclusively from positives examples , gold ' s result has been used as a theoretical support for chomsky ' s theory of innate human linguistic abilities . in this paper new results are presented which show that within a bayesian framework not only grammars , but also logic programs are learnable with arbitrarily low expected error from positive examples only . in addition , we show that the upper bound for expected error of a learner which maximises the bayes ' posterior probability when learning from positive examples is within a small additive term of one which does the same from a mixture of positive and negative examples . an inductive logic programming implementation is described which avoids the pitfalls of greedy search by global optimisation of this function during the local construction of individual clauses of the hypothesis . results of testing this implementation on artificially - generated data - sets are reported . these results are in agreement with the theoretical predictions .
we prove two results about that estimator . [ smooth ] : with high probability ^ f fl n is at least as smooth as f , in any of a wide variety of smoothness measures . [ adapt ] : the estimator comes nearly as close in mean square to f as any measurable estimator can come , uniformly over balls in each of two broad scales of smoothness classes . these two properties are unprecedented in several ways . our proof of these results develops new facts about abstract statistical inference and its connection with acknowledgements . these results were described at the symposium on wavelet theory , held in connection with the shanks lectures at van - derbilt university , april #NUM# - #NUM# #NUM# . the author would like to thank professor l . l . schumaker for hospitality at the conference , and r . a . devore , iain johnstone , gerard kerkyacharian , bradley lucier , a . s . nemirovskii , ingram olkin , and dominique picard for interesting discussions and correspondence on related topics . the author is also at the university of california , berkeley
in this paper we study sequence evolution in a general markov model that incorporates practically every stochastic model found in the literature . in particular , we study the error in the estimation of evolutionary distances and its dependence on sample sequence lengths . by deriving large deviation results that are applicable to any distance - based evolutionary tree building algorithm , we show that the harmonic greedy triplets and the short quartet algorithms recover the evolutionary tree with high probability from sequences of polynomial length in the number of nodes .
results are reported from the application of tools for synthesizing , optimizing and analyzing neural networks to an ecg patient monitoring task . a neural network was synthesized from a rule - based classifier and optimized over a set of normal and abnormal heartbeats . the classification error rate on a separate and larger test set was reduced by a factor of #NUM# . sensitivity analysis of the synthesized and optimized networks revealed informative differences . analysis of the weights and unit activations of the optimized network enabled a reduction in size of the network by a factor of #NUM# % without loss of accuracy .
intracortical microstimulation ( icms ) of a localized site in the somatosensory cortex of rats and monkeys for #NUM# - #NUM# hours produces a large increase in the cortical representation of the skin region represented by the icms - site neurons before icms , with very little effect on the icms - site neuron ' s rf location , rf size , and responsiveness ( recanzone et al . , #NUM# ) . the " exin " ( afferent excitatory and lateral inhibitory ) learning rules ( marshall , #NUM# ) are used to model rf changes during icms . the exin model produces reorganization of rf topography similar to that observed experimentally . the possible role of inhibitory learning in producing the effects of icms is studied by simulating the exin model with only lateral inhibitory learning . the model also produces an increase in the cortical representation of the skin region represented by the icms - site rf . icms is compared to artificial scotoma conditioning ( pettet & gilbert , #NUM# ) and retinal lesions ( darian - smith & gilbert , #NUM# ) , and it is suggested that lateral inhibitory learning may be a general principle of cortical plasticity .
we present a detailed analysis of the evolution of genetic programming ( gp ) populations using the problem of finding a program which returns the maximum possible value for a given terminal and function set and a depth limit on the program tree ( known as the max problem ) . we confirm the basic message of [ gathercole and ross , #NUM# ] that crossover together with program size restrictions can be responsible for premature convergence to a suboptimal solution . we show that this can happen even when the population retains a high level of variety and show that in many cases evolution from the sub - optimal solution to the solution is possible if sufficient time is allowed . in both cases theoretical models are presented and compared with actual runs .
we present a symbolic machinery that admits both probabilistic and causal information about a given domain , and produces probabilistic statements about the effect of actions and the impact of observations . the calculus admits two types of conditioning operators : ordinary bayes conditioning , p ( yjx = x ) , which represents the observation x = x , and causal conditioning , p ( yjdo ( x = x ) ) , read : the probability of y = y conditioned on holding x constant ( at x ) by deliberate action . given a mixture of such observational and causal sentences , together with the topology of the causal graph , the calculus derives new conditional probabilities of both types , thus enabling one to quantify the effects of actions and observations .
a general model for the coevolution of cooperating species is presented . this model is instantiated and tested in the domain of function optimization , and compared with a traditional ga - based function optimizer . the results are encouraging in two respects . they suggest ways in which the performance of ga and other ea - based optimizers can be improved , and they suggest a new approach to evolving complex structures such as neural networks and rule sets .
this paper investigates learning in a lifelong context . lifelong learning addresses situations in which a learner faces a whole stream of learning tasks . such scenarios provide the opportunity to transfer knowledge across multiple learning tasks , in order to generalize more accurately from less training data . in this paper , several different approaches to lifelong learning are described , and applied in an object recognition domain . it is shown that across the board , lifelong learning approaches generalize consistently more accurately from less training data , by their ability to transfer knowledge across learning tasks .
a constant rebalanced portfolio is an investment strategy which keeps the same distribution of wealth among a set of stocks from period to period . recently there has been work on on - line investment strategies that are competitive with the best constant rebalanced portfolio determined in hindsight ( cover , #NUM# ; helmbold et al . , #NUM# ; cover and ordentlich , #NUM# a ; cover and ordentlich , #NUM# b ; ordentlich and cover , #NUM# ; cover , #NUM# ) . for the universal algorithm of cover ( cover , #NUM# ) , we provide a simple analysis which naturally extends to the case of a fixed percentage transaction cost ( commission ) , answering a question raised in ( cover , #NUM# ; helmbold et al . , #NUM# ; cover and ordentlich , #NUM# a ; cover and ordentlich , #NUM# b ; ordentlich and cover , #NUM# ; cover , #NUM# ) . in addition , we present a simple randomized implementation that is significantly faster in practice . we conclude by explaining how these algorithms can be applied to other problems , such as combining the predictions of statistical language models , where the resulting guarantees are more striking .
the fully connected recurrent network ( frn ) using the on - line training method , real time recurrent learning ( rtrl ) , is computationally expensive . it has a computational complexity of o ( n #NUM# ) and storage complexity of o ( n #NUM# ) , where n is the number of non - input units . we have devised a locally connected recurrent model which has a much lower complexity in both computational time and storage space . the ring - structure recurrent network ( rrn ) , the simplest kind of the locally connected has the corresponding complexity of o ( mn + np ) and o ( np ) respectively , where p , n and m are the number of input , hidden and output units respectively . we compare the performance between rrn and frn in sequence recognition and time series prediction . we tested the networks ' ability in temporal memorizing power and time warpping ability in the sequence recognition task . in the time series prediction task , we used both networks to train and predict three series ; a periodic series with white noise , a deterministic chaotic series and the sunspots data . both tasks show that rrn needs a much shorter training time and the performance of rrn is comparable to that of frn .
in most object recognition systems , interactions between objects in a scene are ignored and the best interpretation is considered to be the set of hypothesized objects that matches the greatest number of image features . we show how image interpretation can be cast as the problem of finding the most probable explanation ( mpe ) in a bayesian network that models both visual and physical object interactions . the problem of how to determine exact conditional probabilities for the network is shown to be unimportant , since the goal is to find the most probable configuration of objects , not to calculate absolute probabilities . we furthermore show that evaluating configurations by feature counting is equivalent to calculating the joint probability of the configuration using a restricted bayesian network , and derive the assumptions about probabilities necessary to make a bayesian formulation reasonable .
research on nonmonotonic and default reasoning has identified several important criteria for preferring alternative default inferences . the theories of reasoning based on each of these criteria may uniformly be viewed as theories of rational inference , in which the reasoner selects maximally preferred states of belief . though researchers have noted some cases of apparent conflict between the preferences supported by different theories , it has been hoped that these special theories of reasoning may be combined into a universal logic of nonmonotonic reasoning . we show that the different categories of preferences conflict more than has been realized , and adapt formal results from social choice theory to prove that every universal theory of default reasoning will violate at least one reasonable principle of rational reasoning . our results can be interpreted as demonstrating that , within the preferential framework , we cannot expect much improvement on the rigid lexicographic priority mechanisms that have been proposed for conflict resolution .
we apply the exponential weight algorithm , introduced and littlestone and warmuth [ #NUM# ] and by vovk [ #NUM# ] to the problem of predicting a binary sequence almost as well as the best biased coin . we first show that for the case of the logarithmic loss , the derived algorithm is equivalent to the bayes algorithm with jeffrey ' s prior , that was studied by xie and barron under probabilistic assumptions [ #NUM# ] . we derive a uniform bound on the regret which holds for any sequence . we also show that if the empirical distribution of the sequence is bounded away from #NUM# and from #NUM# , then , as the length of the sequence increases to infinity , the difference between this bound and a corresponding bound on the average case regret of the same algorithm ( which is asymptotically optimal in that case ) is only #NUM# = #NUM# . we show that this gap of #NUM# = #NUM# is necessary by calculating the regret of the min - max optimal algorithm for this problem and showing that the asymptotic upper bound is tight . we also study the application of this algorithm to the square loss and show that the algorithm that is derived in this case is different from the bayes algorithm and is better than it for prediction in the worst - case .
we study the close connections between game theory , on - line prediction and boosting . after a brief review of game theory , we describe an algorithm for learning to play repeated games based on the on - line prediction methods of littlestone and war - muth . the analysis of this algorithm yields a simple proof of von neumann ' s famous minmax theorem , as well as a provable method of approximately solving a game . we then show that the on - line prediction model is obtained by applying this game - playing algorithm to an appropriate choice of game and that boosting is obtained by applying the same algorithm to the dual of this game .
many problems impede the design of multi - agent systems , not the least of which is the passing of information between agents . while others hand implement communication routes and semantics , we explore a method by which communication can evolve . in the experiments described here , we model agents as connectionist networks . we supply each agent with a number of communications channels implemented by the addition of both input and output units for each channel . the output units initiate environmental signals whose amplitude decay over distance and are perturbed by environmental noise . an agent does not receive input from other individuals , rather the agents input reects the summation of all other agents output signals along that channel . because we use real - valued activations , the agents communicate using real - valued vectors . under our evolutionary program , gnarl , the agents coevolve a communication scheme over continuous channels which conveys task - spe cific information .
as the field of genetic programming ( gp ) matures and its breadth of application increases , the need for parallel implementations becomes absolutely necessary . the transputer - based system recently presented by koza ( [ #NUM# ] ) is one of the rare such parallel implementations . until today , no implementation has been proposed for parallel gp using a simd architecture , except for a data - parallel approach ( [ #NUM# ] ) , although others have exploited workstation farms and pipelined supercomputers . one reason is certainly the apparent difficulty of dealing with the parallel evaluation of different s - expressions when only a single instruction can be executed at the same time on every processor . the aim of this paper is to present such an implementation of parallel gp on a simd system , where each processor can efficiently evaluate a different s - expression . we have implemented this approach on a maspar mp - #NUM# computer , and will present some timing results . to the extent that simd machines , like the maspar are available to offer cost - effective cycles for scien tific experimentation , this is a useful approach .
artificial neural networks have been applied to the prediction of splice site location in human pre - mrna . a joint prediction scheme where prediction of transition regions between introns and exons regulates a cutoff level for splice site assignment was able to predict splice site locations with confidence levels far better than previously reported in the literature . the problem of predicting donor and acceptor sites in human genes is hampered by the presence of numerous amounts of false positives | in the paper the distribution of these false splice sites is examined and linked to a possible scenario for the splicing mechanism in vivo . when the presented method detects #NUM# % of the true donor and acceptor sites it makes less than #NUM# . #NUM# % false donor site assignments and less than #NUM# . #NUM# % false acceptor site assignments . for the large data set used in this study this means that on the average there are one and a half false donor sites per true donor site and six false acceptor sites per true acceptor site . with the joint assignment method more than a fifth of the true donor sites and around one fourth of the true acceptor sites could be detected without accompaniment of any false positive predictions . highly confident splice sites could not be isolated with a widely used weight matrix method or by separate splice site networks . a complementary relation between the confidence levels of the coding / non - coding and the separate splice site networks was observed , with many weak splice sites having sharp transitions in the coding / non - coding signal and many stronger splice sites having more ill - defined transitions between coding and non - coding .
to coordinate with other agents in its environment , an agent needs models of what the other agents are trying to do . when communication is impossible or expensive , this information must be acquired indirectly via plan recognition . typical approaches to plan recognition start with a specification of the possible plans the other agents may be following , and develop special techniques for discriminating among the possibilities . perhaps more desirable would be a uniform procedure for mapping plans to general structures supporting inference based on uncertain and incomplete observations . in this paper , we describe a set of methods for converting plans represented in a flexible procedural language to observation models represented as probabilistic belief networks , and we outline issues in applying the resulting probabilistic models of agents when coordinating activity in physical domains .
the construction of evolutionary trees is a fundamental problem in biology , and yet methods for reconstructing evolutionary trees are not reliable when it comes to inferring accurate topologies of large divergent evolutionary trees from realistic length sequences . we address this problem and present a new polynomial time algorithm for reconstructing evolutionary trees called the short quartets method which is consistent and which has greater statistical power than other polynomial time methods , such as neighbor - joining and the #NUM# - approximation algorithm by agarwala et al . ( and the " double pivot " variant of the agarwala et al . algorithm by cohen and farach ) for the l #NUM# - nearest tree problem . our study indicates that our method will produce the correct topology from shorter sequences than can be guaranteed using these other methods .
artificial life ( a - life ) research offers , among other things , a new style of computer simulation for understanding biological systems and processes . but most current a - life work does not show enough methodological sophistication to count as good theoretical biology . as a first step towards developing a stronger methodology for a - life , this paper ( #NUM# ) identifies some methodological pitfalls arising from the ` computer science inuence ' in a - life , ( #NUM# ) suggests some methodological heuristics for a - life as theoretical biology , ( #NUM# ) notes the strengths of a - life methods versus previous research methods in biology , ( #NUM# ) examines some open questions in theoretical biology that may benefit from a - life simulation , and ( #NUM# ) argues that the debate over ` strong a - life ' is not relevant to a - life ' s utility for theoretical biology . #NUM# introduction : simulating our way into the dark continent
in this paper the problem of learning appropriate domain - specific bias is addressed . it is shown that this can be achieved by learning many related tasks from the same domain , and a theorem is given bounding the number tasks that must be learnt . a corollary of the theorem is that if the tasks are known to possess a common internal representation or preprocessing then the number of examples required per task for good generalisation when learning n tasks simultaneously scales like o ( a + b tive support for the theoretical results is reported .
principal component analysis ( pca ) is one of the most popular techniques for processing , compressing and visualising data , although its effectiveness is limited by its global linearity . while nonlinear variants of pca have been proposed , an alternative paradigm is to capture data complexity by a combination of local linear pca projections . however , conventional pca does not correspond to a probability density , and so there is no unique way to combine pca models . previous attempts to formulate mixture models for pca have therefore to some extent been ad hoc . in this paper , pca is formulated within a maximum - likelihood framework , based on a specific form of gaussian latent variable model . this leads to a well - defined mixture model for probabilistic principal component analysers , whose parameters can be determined using an em algorithm . we discuss the advantages of this model in the context of clustering , density modelling and local dimensionality reduction , and we demonstrate its application to image compression and handwritten digit recognition .
the study of belief change has been an active area in philosophy and ai . in recent years two special cases of belief change , belief revision and belief update , have been studied in detail . in a companion paper [ friedman and halpern #NUM# a ] , we introduce a new framework to model belief change . this framework combines temporal and epistemic modalities with a notion of plausibility , allowing us to examine the change of beliefs over time . in this paper , we show how belief revision and belief update can be captured in our framework . this allows us to compare the assumptions made by each method , and to better understand the principles underlying them . in particular , it shows that katsuno and mendelzon ' s notion of belief update [ katsuno and mendelzon #NUM# a ] depends on several strong assumptions that may limit its applicability in artificial intelligence . finally , our analysis allow us to identify a notion of minimal change that underlies a broad range of belief change operations including revision and update .
a new heuristic approach for minimizing possibly nonlinear and non differentiable continuous space functions is presented . by means of an extensive testbed , which includes the de jong functions , it will be demonstrated that the new method converges faster and with more certainty than adaptive simulated annealing as well as the annealed nelder & mead approach , both of which have a reputation for being very powerful . the new method requires few control variables , is robust , easy to use and lends itself very well to parallel computation .
we present results for three new algorithms for setting the step - size parameters , ff and , of temporal - difference learning methods such as td ( ) . the overall task is that of learning to predict the outcome of an unknown markov chain based on repeated observations of its state trajectories . the new algorithms select step - size parameters online in such a way as to eliminate the bias normally inherent in temporal - difference methods . we compare our algorithms with conventional monte carlo methods . monte carlo methods have a natural way of setting the step size : for each state s they use a step size of #NUM# = n s , where n s is the number of times state s has been visited . we seek and come close to achieving comparable step - size algorithms for td ( ) . one new algorithm uses a = #NUM# = n s schedule to achieve the same effect as processing a state backwards with td ( #NUM# ) , but remains completely incremental . another algorithm uses a at each time equal to the estimated transition probability of the current transition . we present empirical results showing improvement in convergence rate over monte carlo methods and conventional td ( ) . a limitation of our results at present is that they apply only to tasks whose state trajectories do not contain cycles .
visualization has proven to be a powerful and widely - applicable tool for the analysis and interpretation of multi - variate data . most visualization algorithms aim to find a projection from the data space down to a two - dimensional visualization space . however , for complex data sets living in a high - dimensional space it is unlikely that a single two - dimensional projection can reveal all of the interesting structure . we therefore introduce a hierarchical visualization algorithm which allows the complete data set to be visualized at the top level , with clusters and sub - clusters of data points visualized at deeper levels . the algorithm is based on a hierarchical mixture of latent variable models , whose parameters are estimated using the expectation - maximization algorithm . we demonstrate the principle of the approach on a toy data set , and we then apply the algorithm to the visualization of a synthetic data set in #NUM# dimensions obtained from a simulation of multi - phase flows in oil pipelines , and to data in #NUM# dimensions derived from satellite images . a matlab software implementation of the algorithm is publicly available from the world - wide web .
assumed unless otherwise stated . basically , de generates new parameter vectors by adding the weighted difference between two population vectors to a third vector . if the resulting vector yields a lower objective function value than a predetermined population member , the newly generated vector replaces the vector , with which it was compared , in the next generation ; otherwise , the old vector is retained . this basic principle , however , is extended when it comes to the practical variants of de . for example an existing vector can be perturbed by adding more than one weighted difference vector to it . in most cases , it is also worthwhile to mix the parameters of the old vector with those of the perturbed one before comparing the objective function values . several variants of de which have proven to be useful will be described in the
we present a novel application of ilp to the problem of diterpene structure elucidation from #NUM# c nmr spectra . diterpenes are organic compounds of low molecular weight that are based on a skeleton of #NUM# carbon atoms . they are of significant chemical and commercial interest because of their use as lead compounds in the search for new pharmaceutical effectors . the structure elucidation of diterpenes based on #NUM# c nmr spectra is usually done manually by human experts with specialized background knowledge on peak patterns and chemical structures . in the process , each of the #NUM# skeletal atoms is assigned an atom number that corresponds to its proper place in the skeleton and the diterpene is classified into one of the possible skeleton types . we address the problem of learning classification rules from a database of peak patterns for diterpenes with known structure . recently , propositional learning was successfully applied to learn classification rules from spectra with assigned atom numbers . as the assignment of atom numbers is a difficult process in itself ( and possibly indistinguishable from the classification process ) , we apply ilp , i . e . , relational learning , to the problem of classifying spectra without assigned atom numbers .
models of physical systems can differ according to computational cost , accuracy and precision , among other things . depending on the problem solving task at hand , different models will be appropriate . several investigators have recently developed methods of automatically selecting among multiple models of physical systems . our research is novel in that we are developing model selection techniques specifically suited to computer - aided de sign . our approach is based on the idea that artifact performance models for computer - aided design should be chosen in light of the design decisions they are required to support . we have developed a technique called " gradient magnitude model selection " ( gmms ) , which embodies this principle . gmms operates in the context of a hillclimbing search process . it selects the simplest model that meets the needs of the hillclimbing algorithm in which it operates . we are using the domain of sailing yacht design as a testbed for this research . we have implemented gmms and used it in hillclimb - ing search to decide between a computationally expensive potential - flow program and an algebraic approximation to analyze the performance of sailing yachts . experimental tests show that gmms makes the design process faster than it would be if the most expensive model were used for all design evaluations . gmms achieves this performance improvement with little or no sacrifice in the quality of the resulting design .
we present a new algorithm for eliminating excess parameters and improving network generalization after supervised training . the method , " principal components pruning ( pcp ) " , is based on principal component analysis of the node activations of successive layers of the network . it is simple , cheap to implement , and effective . it requires no network retraining , and does not involve calculating the full hessian of the cost function . only the weight and the node activity correlation matrices for each layer of nodes are required . we demonstrate the efficacy of the method on a regression problem using polynomial basis functions , and on an economic time series prediction problem using a two - layer , feedforward network .
gradient - based numerical optimization of complex engineering designs offers the promise of rapidly producing better designs . however , such methods generally assume that the objective function and constraint functions are continuous , smooth , and defined everywhere . unfortunately , realistic simulators tend to violate these assumptions . we present a rule - based technique for intelligently computing gradients in the presence of such pathologies in the simulators , and show how this gradient computation method can be used as part of a gradient - based numerical optimization system . we tested the resulting system in the domain of conceptual design of supersonic transport aircraft , and found that using rule - based gradients can decrease the cost of design space search by one or more orders of magnitude .
the first step for most case - based design systems is to select an initial prototype from a database of previous designs . the retrieved prototype is then modified to tailor it to the given goals . for any particular design goal the selection of a starting point for the design process can have a dramatic effect both on the quality of the eventual design and on the overall design time . we present a technique for automatically constructing effective prototype - selection rules . our technique applies a standard inductive - learning algorithm , c #NUM# . #NUM# , to a set of training data describing which particular prototype would have been the best choice for each goal encountered in a previous design session . we have tested our technique in the domain of racing - yacht - hull design , comparing our inductively learned selection rules to several competing prototype - selection methods . our results show that the inductive prototype - selection method leads to better final designs when the design process is guided by a noisy evaluation function , and that the inductively learned rules will often be more efficient than competing methods . many automated design systems begin by retrieving an initial prototype from a library of previous designs , using the given design goal as an index to guide the retrieval process [ #NUM# ] . the retrieved prototype is then modified by a set of design modification operators to tailor the selected design to the given goals . in many cases the quality of competing designs can be assessed using domain - specific evaluation functions , and in such cases the design - modification process is often this research has benefited from numerous discussions with members of the rutgers cap project . we thank andrew gelsey for helping with the cross - validation code , john keane for helping with ruvpp , and andrew gelsey and tim weinrich for comments on a previous draft of this paper . this research was supported under arpa - funded nasa grant nag #NUM# - #NUM# . in the context of such case - based design systems , the choice of an initial prototype can affect both the quality of the final design and the computational cost of obtaining that design , for three reasons . first , prototype selection may impact quality when the prototypes lie in disjoint search spaces . in particular , if the system ' s design modification operators cannot convert any prototype into any other prototype , the choice of initial prototype will restrict the set of possible designs that can be obtained by any search process . a poor choice of initial prototype may therefore lead to a suboptimal final design . second , prototype selection may impact quality when the design process is guided by a nonlinear evaluation function with unknown global properties . since there is no known method that is guaranteed to find the global optimum of an arbitrary nonlinear function [ #NUM# ] , most design systems rely on iterative local search methods whose results are sensitive to the initial starting point . finally , the choice of prototype may have an impact on the time needed to carry out the design modification process | two different starting points may yield the same final design but take very different amounts of time to get there . in design problems where evaluating even just a single design can take tremendous amounts of time , selecting an appropriate initial prototype can be the determining factor in the success or failure of the design process . this paper describes the application of inductive learning [ #NUM# ] to form rules for selecting appropriate prototype designs . the paper is structured as follows . in section #NUM# , we describe our inductive method for learning prototype - selection rules . in section #NUM# we describe the domain of racing - yacht - hull design , in which we tested our prototype - selection methods . in sections #NUM# and #NUM# , we describe the experiments
this paper describes the automatic design of methods for detecting fraudulent behavior . much of the design is accomplished using a series of machine learning methods . in particular , we combine data mining and constructive induction with more standard machine learning techniques to design methods for detecting fraudulent usage of cellular telephones based on profiling customer behavior . specifically , we use a rule - learning program to uncover indicators of fraudulent behavior from a large database of cellular calls . these indicators are used to create profilers , which then serve as features to a system that combines evidence from multiple profilers to generate high - confidence alarms . experiments indicate that this automatic approach performs nearly as well as the best hand - tuned methods for detecting fraud .
in many cases programs length ' s increase ( known as " bloat " , " fluff " and increasing " structural complexity " ) during artificial evolution . we show bloat is not specific to genetic programming and suggest it is inherent in search techniques with discrete variable length representations using simple static evaluation functions . we investigate the bloating characteristics of three non - population and one population based search techniques using a novel mutation operator . an artificial ant following the santa fe trail problem is solved by simulated annealing , hill climbing , strict hill climbing and population based search using two variants of the the new subtree based mutation operator . as predicted bloat is observed when using unbiased mutation and is absent in simulated annealing and both hill climbers when using the length neutral mutation however bloat occurs with both mutations when using a population . we conclude that there are two causes of bloat .
we present a method for learning higher - order polynomial functions from examples using linear regression and feature construction . regression is used on a set of training instances to produce a weight vector for a linear function over the feature set . if this hypothesis is imperfect , a new feature is constructed by forming the product of the two features that most effectively predict the squared error of the current hypothesis . the algorithm is then repeated . in an extension to this method , the specific pair of features to combine is selected by measuring their joint ability to predict the hypothesis ' error .
we present a methodology that enables the use of existent classification inductive learning systems on problems of regression . we achieve this goal by transforming regression problems into classification problems . this is done by transforming the range of continuous goal variable values into a set of intervals that will be used as discrete classes . we provide several methods for discretizing the goal variable values . these methods are based on the idea of performing an iterative search for the set of final discrete classes . the search algorithm is guided by a n - fold cross validation estimation of the prediction error resulting from using a set of discrete classes . we have done extensive empirical evaluation of our discretization methodologies using c #NUM# . #NUM# and cn #NUM# on four real world domains . the results of these experiments show the quality of our discretization methods compared to other existing methods . our method is independent of the used classification inductive system . the method is easily applicable to other inductive algorithms . this generality turns our method into a powerful tool that extends the applicability of a wide range of existing classification systems .
the bayesian multivariate adaptive regression spline ( bmars ) methodology of denison et al . ( #NUM# ) is extended to cope with nonlinear time series and financial datasets . the nonlinear time series model is closely related to the adaptive spline threshold autoregressive ( astar ) method of lewis and stevens ( #NUM# ) while the financial models can be thought of as bayesian versions of both the generalised and simple autoregressive conditional het - eroscadastic ( garch and arch ) models .
some problems can be solved only by multi - agent teams . in using genetic programming to produce such teams , one faces several design decisions . first , there are questions of team diversity and of breeding strategy . in one commonly used scheme , teams consist of clones of single individuals ; these individuals breed in the normal way and are cloned to form teams during fitness evaluation . in contrast , teams could also consist of distinct individuals . in this case one can either allow free interbreeding between members of different teams , or one can restrict interbreeding in various ways . a second design decision concerns the types of coordination - facilitating mechanisms provided to individual team members ; these range from sensors of various sorts to complex communication systems . this paper examines three breeding strategies ( clones , free , and restricted ) and three coordination mechanisms ( none , deictic sensing , and name - based sensing ) for evolving teams of agents in the serengeti world , a simple predator / prey environment . among the conclusions are the fact that a simple form of restricted interbreeding outperforms free interbreeding in all teams with distinct individuals , and the fact that name - based sensing consistently outperforms deictic sensing .
this paper presents an o ( ( r n = m ) m rnm ) algorithm for determining whether a set of n species has a perfect phylogeny , where m is the number of characters used to describe a species and r is the maximum number of states that a character can be in . the perfect phylogeny algorithm leads to an o ( ( #NUM# e = k ) k e #NUM# k ) algorithm for triangulating a k - colored graph having e edges .
posner and raichle ' s images of mind is an excellent educational book and very well written . some aws as a scientific publication are : ( a ) the accuracy of the linear subtraction method used in pet is subject to scrutiny by further research at finer spatial - temporal resolutions ; ( b ) lack of accuracy of the experimental paradigm used for eeg complementary studies . images ( posner & raichle , #NUM# ) is an excellent introduction to interdisciplinary research in cognitive and imaging science . well written and illustrated , it presents concepts in a manner well suited both to the layman / undergraduate and to the technical nonexpert / graduate student and postdoctoral researcher . many , not all , people involved in interdisciplinary neuroscience research agree with the p & r ' s statements on page #NUM# , on the importance of recognizing emergent properties of brain function from assemblies of neurons . it is clear from the sparse references that this book was not intended as a standalone review of a broad field . there are some aws in the scientific development , but this must be expected in such a pioneering venture . p & r hav e proposed many cognitive mechanisms deserving further study with imaging tools yet to be developed which can yield better spatial - temporal resolutions .
this paper establishes formulas that can be used to bound the actual treatment effect in any experimental study in which treatment assignment is random but subject compliance is imperfect . these formulas provide the tightest bounds on the average treatment effect that can be inferred given the distribution of assignments , treatments , and responses . our results reveal that even with high rates of noncompliance , experimental data can yield significant and sometimes accurate information on the effect of a treatment on the population .
most researchers in machine learning have built their learning systems under the assumption that some external entity would do all the work of furnishing the learning experiences . recently , however , investigators in several subfields of machine learning have designed systems that play an active role in choosing the situations from which they will learn . such activity is generally called exploration . this paper describes a few of these exploratory learning projects , as reported in the literature , and attempts to extract a general account of the issues involved in exploration .
we study the learnability of read - k - satisfy - j ( rksj ) dnf formulas . these are boolean formulas in disjunctive normal form ( dnf ) , in which the maximum number of occurrences of a variable is bounded by k , and the number of terms satisfied by any assignment is at most j . after motivating the investigation of this class of dnf formulas , we present an algorithm that for any unknown rksj dnf formula to be learned , with high probability finds a logically equivalent dnf formula using the well - studied protocol of equivalence and membership queries . the algorithm runs in polynomial time for k j = o ( log n log log n ) , where n is the number of input variables .
planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence . in this paper we summarize an approach to this problem based on the mathematical framework of markov decision processes and reinforcement learning . conventional model - based reinforcement learning uses primitive actions that last one time step and that can be modeled independently of the learning agent . these can be generalized to macro actions , multi - step actions specified by an arbitrary policy and a way of completing . macro actions generalize the classical notion of a macro operator in that they are closed loop , uncertain , and of variable duration . macro actions are needed to represent common - sense higher - level actions such as going to lunch , grasping an object , or traveling to a distant city . this paper generalizes prior work on temporally abstract models ( sutton #NUM# ) and extends it from the prediction setting to include actions , control , and planning . we define a semantics of models of macro actions that guarantees the validity of planning using such models . this paper present new results in the theory of planning with macro actions and illustrates its potential advantages in a gridworld task .
this paper proposes that the generalisation capabilities of a case - based reasoning system can be evaluated by comparison with a ` rote - learning ' algorithm which uses a very simple generalisation strategy . two such algorithms are defined , and expressions for their classification accuracy are derived as a function of the size of training sample . a series of experiments using artificial and ` natural ' data sets is described in which the learning curve for a case - based learner is compared with those for the apparently trivial rote - learning learning algorithms . the results show that in a number of ` plausible ' situations , the learning curves for a simple case - based learner and the ` majority ' rote - learner can barely be distinguished , although a domain is demonstrated where favourable performance from the case - based learner is observed . this suggests that the maxim of case - based reasoning that ` similar problems have similar solutions ' may be useful as the basis of a generalisation strategy only in selected domains .
research in robotics programming is divided in two camps . the direct hand programmming approach uses an explicit model or a behavioral model ( subsumption architecture ) . the machine learning community uses neural network and / or genetic algorithm . we claim that hand programming and learning are complementary . the two approaches used together can be orders of magnitude more powerful than each approach taken separately . we propose a method to combine them both . it includes three concepts : syntactic constraints to restrict the search space , hand - made problem decomposition , hand given fitness . we use this method to solve a complex problem ( eight - legged locomotion ) . it needs #NUM# less evaluations compared to when genetic algorithm are used alone .
we apply recent results in markov chain theory to hastings and metropolis algorithms with either independent or symmetric candidate distributions , and provide necessary and sufficient conditions for the algorithms to converge at a geometric rate to a prescribed distribution . in the independence case ( in ir k ) these indicate that geometric convergence essentially occurs if and only if the candidate density is bounded below by a multiple of ; in the symmetric case ( in ir only ) we show geometric convergence essentially occurs if and only if has geometric tails . we also evaluate recently developed computable bounds on the rates of convergence in this context : examples show that these theoretical bounds can be inherently extremely conservative , although when the chain is stochastically monotone the bounds may well be effective .
we consider the game of sequentially assigning probabilities to future data based on past observations under logarithmic loss . we are not making probabilistic assumptions about the generation of the data , but consider a situation where a player tries to minimize his loss relative to the loss of the ( with hindsight ) best distribution from a target class for the worst sequence of data . we give bounds on the minimax regret in terms of the metric entropies of the target class with respect to suitable distances between distributions .
in [ #NUM# ] we introduced a formal framework for constructing ordinal similarity measures , and suggested how this might also be applied to cardinal measures . in this paper we will place this approach in a more general framework , called similarity metrics . in this framework , ordinal similarity metrics ( where comparison returns a boolean value ) can be combined with cardinal metrics ( returning a numeric value ) and , indeed , with metrics returning values of other types , to produce new metrics .
in this paper we are concerned with the problem of inducing recursive horn clauses from small sets of training examples . the method of iterative bootstrap induction is presented . in the first step , the system generates simple clauses , which can be regarded as properties of the required definition . properties represent generalizations of the positive examples , simulating the effect of having larger number of examples . properties are used subsequently to induce the required recursive definitions . this paper describes the method together with a series of experiments . the results support the thesis that iterative bootstrap induction is indeed an effective technique that could be of general use in ilp .
considerable effort has been directed recently to develop asymptotically minimax methods in problems of recovering infinite - dimensional objects ( curves , densities , spectral densities , images ) from noisy data . a rich and complex body of work has evolved , with nearly - or exactly - minimax estimators being obtained for a variety of interesting problems . unfortunately , the results have often not been translated into practice , for a variety of reasons sometimes , similarity to known methods , sometimes , computational intractability , and sometimes , lack of spatial adaptivity . we discuss a method for curve estimation based on n noisy data ; one translates the empirical wavelet coefficients towards the origin by an amount method is different from methods in common use today , is computationally practical , and is spatially adaptive ; thus it avoids a number of previous objections to minimax estimators . at the same time , the method is nearly minimax for a wide variety of loss functions - e . g . pointwise error , global error measured in l p norms , pointwise and global error in estimation of derivatives and for a wide range of smoothness classes , including standard holder classes , sobolev classes , and bounded variation . this is a much broader near - optimality than anything previously proposed in the minimax literature . finally , the theory underlying the method is interesting , as it exploits a correspondence between statistical questions and questions of optimal recovery and information - based complexity . acknowledgements : these results have been described at the oberwolfach meeting ` mathematische stochastik ' december , #NUM# and at the ams annual meeting , january #NUM# . this work was supported by nsf dms #NUM# - #NUM# . the authors would like to thank paul - louis hennequin , who organized the ecole d ' ete de probabilites at saint flour #NUM# , where this collaboration began , and to universite de paris vii ( jussieu ) and universite de paris - sud ( orsay ) for supporting visits of dld and imj . the authors would like to thank ildar ibragimov and arkady nemirovskii for personal correspondence cited below . p
figure #NUM# a and figure #NUM# b show the prior distribution over f ( - cr ) that follows from the flat prior and the skewed prior , respectively . figure #NUM# c and figure #NUM# d show the posterior distribution p ( f ( - cr jd ) ) obtained by our system when run on the lipid data , using the flat prior and the skewed prior , respectively . from the bounds of balke and pearl ( #NUM# ) , it follows that under the large - sample assumption , #NUM# : #NUM# f ( - cr jd ) #NUM# : #NUM# . figure #NUM# : prior ( a , b ) and posterior ( c , d ) distributions for a subpopulation f ( - cr jd ) specified by the counter - factual query " would joe have improved had he taken the drug , given that he did not improve without it " . ( a ) corresponds to the flat prior , ( b ) to the skewed prior . this paper identifies and demonstrates a new application area for network - based inference techniques - the management of causal analysis in clinical experimentation . these techniques , which were originally developed for medical diagnosis , are shown capable of circumventing one of the major problems in clinical experiments the assessment of treatment efficacy in the face of imperfect compliance . while standard diagnosis involves purely probabilistic inference in fully specified networks , causal analysis involves partially specified networks in which the links are given causal interpretation and where the domain of some variables are unknown . the system presented in this paper provides the clinical research community , we believe for the first time , an assumption - free , unbiased assessment of the average treatment effect . we offer this system as a practical tool to be used whenever full compliance cannot be enforced and , more broadly , whenever the data available is insufficient for answering the queries of interest to the clinical investigator . lipid research clinic program . #NUM# . the lipid research clinics coronary primary prevention trial results , parts i and ii . journal of the american medical association #NUM# ( #NUM# ) : #NUM# - #NUM# . january .
when can we give causal interpretation abstract the assumptions underlying statistical estimation are of fundamentally different character from the causal assumptions that underly structural equation models ( sem ) . the differences have been blurred through the years for the lack of a mathematical notation capable of distinguishing causal from equational relationships . recent advances in graphical methods provide formal explication of these differences , and are destined to have profound impact on sem ' s practice and philosophy .
incremental class learning ( icl ) provides a feasible framework for the development of scalable learning systems . instead of learning a complex problem at once , icl focuses on learning subproblems incrementally , one at a time | using the results of prior learning for subsequent learning | and then combining the solutions in an appropriate manner . with respect to multi - class classification problems , the icl approach presented in this paper can be summarized as follows . initially the system focuses on one category . after it learns this category , it tries to identify a compact subset of features ( nodes ) in the hidden layers , that are crucial for the recognition of this category . the system then freezes these crucial nodes ( features ) by fixing their incoming weights . as a result , these features cannot be obliterated in subsequent learning . these frozen features are available during subsequent learning and can serve as parts of weight structures build to recognize other categories . as more categories are learned , the set of features gradually stabilizes and learning a new category requires less effort . eventually , learning a new category may only involve combining existing features in an appropriate manner . the approach promotes the sharing of learned features among a number of categories and also alleviates the well - known catastrophic interference problem . we present results of applying the icl approach to the handwritten digit recognition problem , based on a spatio - temporal representation of patterns .
a number of exact algorithms have been developed to perform probabilistic inference in bayesian belief networks in recent years . the techniques used in these algorithms are closely related to network structures and some of them are not easy to understand and implement . in this paper , we consider the problem from the combinatorial optimization point of view and state that efficient probabilistic inference in a belief network is a problem of finding an optimal factoring given a set of probability distributions . from this viewpoint , previously developed algorithms can be seen as alternate factoring strategies . in this paper , we define a combinatorial optimization problem , the optimal factoring problem , and discuss application of this problem in belief networks . we show that optimal factoring provides insight into the key elements of efficient probabilistic inference , and demonstrate simple , easily implemented algorithms with excellent performance .
backpropagation learning ( rumelhart , hinton and williams , #NUM# ) is a useful research tool but it has a number of undesiderable features such as having the experimenter decide from outside what should be learned . we describe a number of simulations of neural networks that internally generate their own teaching input . the networks generate the teaching input by trasforming the network input through connection weights that are evolved using a form of genetic algorithm . what results is an innate ( evolved ) capacity not to behave efficiently in an environment but to learn to behave efficiently . the analysis of what these networks evolve to learn shows some interesting results .
to appear in the twelfth national conference on artificial intelligence ( aaai - #NUM# ) , seattle , wa , july #NUM# august #NUM# , #NUM# . technical report r - #NUM# - a april , #NUM# abstract evaluation of counterfactual queries ( e . g . , " if a were true , would c have been true ? " ) is important to fault diagnosis , planning , and determination of liability . we present a formalism that uses probabilistic causal networks to evaluate one ' s belief that the counterfactual consequent , c , would have been true if the antecedent , a , were true . the antecedent of the query is interpreted as an external action that forces the proposition a to be true , which is consistent with lewis ' miraculous analysis . this formalism offers a concrete embodiment of the " closest world " approach which ( #NUM# ) properly reflects common understanding of causal influences , ( #NUM# ) deals with the uncertainties inherent in the world , and ( #NUM# ) is amenable to machine representation .
evaluation of counterfactual queries ( e . g . , " if a were true , would c have been true ? " ) is important to fault diagnosis , planning , determination of liability , and policy analysis . we present a method for evaluating counter - factuals when the underlying causal model is represented by structural models a nonlinear generalization of the simultaneous equations models commonly used in econometrics and social sciences . this new method provides a coherent means for evaluating policies involving the control of variables which , prior to enacting the policy were influenced by other variables in the system .
theory refinement is the task of updating a domain theory in the light of new cases , to be done automatically or with some expert assistance . the problem of theory refinement under uncertainty is reviewed here in the context of bayesian statistics , a theory of belief revision . the problem is reduced to an incremental learning task as follows : the learning system is initially primed with a partial theory supplied by a domain expert , and thereafter maintains its own internal representation of alternative theories which is able to be interrogated by the domain expert and able to be incrementally refined from data . algorithms for refinement of bayesian networks are presented to illustrate what is meant by " partial theory " , " alternative theory representation " , etc . the algorithms are an incremental variant of batch learning algorithms from the literature so can work well in batch and incremental mode .
the emergence of generalist and specialist behavior in populations of neural networks is studied . energy extracting ability is included as a property of an organism . in artificial life simulations with organisms living in an environment , the fitness score can be interpreted as the combination of an organisms behavior and the ability of the organism to extract energy from potential food sources distributed in the environment . the energy extracting ability is viewed as an evolvable trait of organisms a particular organism ' s mechanisms for extracting energy from the environment and , therefore , it is not fixed and decided by the researcher . simulations with fixed and evolvable energy extracting abilities show that the energy extracting mechanism , the sensory apparatus , and the behavior of organisms may co - evolve and be co - adapted . the results suggest that populations of organisms evolve to be generalists or specialists due to individual energy extracting abilities .
in this paper we consider the problem of theory patching , in which we are given a domain theory , some of whose components are indicated to be possibly flawed , and a set of labeled training examples for the domain concept . the theory patching problem is to revise only the indicated components of the theory , such that the resulting theory correctly classifies all the training examples . theory patching is thus a type of theory revision in which revisions are made to individual components of the theory . our concern in this paper is to determine for which classes of logical domain theories the theory patching problem is tractable . we consider both propositional and first - order domain theories , and show that the theory patching problem is equivalent to that of determining what information contained in a theory is stable regardless of what revisions might be performed to the theory . we show that determining stability is tractable if the input theory satisfies two conditions : that revisions to each theory component have monotonic effects on the classification of examples , and that theory components act independently in the classification of examples in the theory . we also show how the concepts introduced can be used to determine the soundness and completeness of particular theory patching algorithms .
this paper studies how to balance evolutionary design and human expertise in order to best design situated autonomous agents which can learn specific tasks . a genetic algorithm designs control circuits to learn simple behaviors , and given control strategies for simple behaviors , the genetic algorithm designs a combinational circuit that switches between these simple behaviors to perform a navigation task . keywords : genetic algorithms , computational design , autonomous agents , robotics .
in this paper we propose a threestage incremental approach to the development of autonomous agents . we discuss some issues about the characteristics which differentiate reinforcement programs ( rps ) , and define the trainer as a particular kind of rp . we present a set of results obtained running experiments with a trainer which provides guidance to the autonomouse , our mousesized autonomous robot .
in this paper we carefully formulate a schema theorem for genetic programming ( gp ) using a schema definition that accounts for the variable length and the non - homologous nature of gp ' s representation . in a manner similar to early ga research , we use interpretations of our gp schema theorem to obtain a gp building block definition and to state a " classical " building block hypothesis ( bbh ) : that gp searches by hierarchically combining building blocks . we report that this approach is not convincing for several reasons : it is difficult to find support for the promotion and combination of building blocks solely by rigourous interpretation of a gp schema theorem ; even if there were such support for a bbh , it is empirically questionable whether building blocks always exist because partial solutions of consistently above average fitness and resilience to disruption are not assured ; also , a bbh constitutes a narrow and imprecise account of gp search behavior .
although feedforward neural networks are well suited to function approximation , in some applications networks experience problems when learning a desired function . one problem is interference which occurs when learning in one area of the input space causes unlearning in another area . networks that are less susceptible to interference are referred to as spatially local networks . to understand these properties , a theoretical framework , consisting of a measure of interference and a measure of network localization , is developed that incorporates not only the network weights and architecture but also the learning algorithm . using this framework to analyze sigmoidal multi - layer perceptron ( mlp ) networks that employ the back - prop learning algorithm , we address a familiar misconception that sigmoidal networks are inherently non - local by demonstrating that given a sufficiently large number of adjustable parameters , sigmoidal mlps can be made arbitrarily local while retaining the ability to represent any continuous function on a compact domain .
a paradigm of statistical mechanics of financial markets ( smfm ) is fit to multivariate financial markets using adaptive simulated annealing ( asa ) , a global optimization algorithm , to perform maximum likelihood fits of lagrangians defined by path integrals of multivariate conditional probabilities . canonical momenta are thereby derived and used as technical indicators in a recursive asa optimization process to tune trading rules . these trading rules are then used on out - of - sample data , to demonstrate that they can profit from the smfm model , to illustrate that these markets are likely not efficient . this methodology can be extended to other systems , e . g . , electroencephalography . this approach to complex systems emphasizes the utility of blending an intuitive and powerful mathematical - physics formalism to generate indicators which are used by ai - type rule - based models of management .
the close connection between reinforcement learning ( rl ) algorithms and dynamic programming algorithms has fueled research on rl within the machine learning community . yet , despite increased theoretical understanding , rl algorithms remain applicable to simple tasks only . in this paper i use the abstract framework afforded by the connection to dynamic programming to discuss the scaling issues faced by rl researchers . i focus on learning agents that have to learn to solve multiple structured rl tasks in the same environment . i propose learning abstract environment models where the abstract actions represent " intentions " of achieving a particular state . such models are variable temporal resolution models because in different parts of the state space the abstract actions span different number of time steps . the operational definitions of abstract actions can be learned incrementally using repeated experience at solving rl tasks . i prove that under certain conditions solutions to new rl tasks can be found by using simu lated experience with abstract actions alone .
we describe a supervised learning algorithm , eodg , that uses mutual information to build an oblivious decision tree . the tree is then converted to an oblivious read - once decision graph ( oodg ) by merging nodes at the same level of the tree . for domains that are appropriate for both decision trees and oodgs , performance is approximately the same as that of c #NUM# . #NUM# , but the number of nodes in the oodg is much smaller . the merging phase that converts the oblivious decision tree to an oodg provides a new way of dealing with the replication problem and a new pruning mechanism that works top down starting from the root . the pruning mechanism is well suited for finding symmetries and aids in recovering from splits on irrelevant features that may happen during the tree construction .
an approach is explicitly formulated to blend a local with a global theory to investigate oscillatory neocortical firings , to determine the source and the information - processing nature of the alpha rhythm . the basis of this optimism is founded on a statistical mechanical theory of neocortical interactions which has had success in numerically detailing properties of short - term - memory ( stm ) capacity at the mesoscopic scales of columnar interactions , and which is consistent with other theory deriving similar dispersion relations at the macroscopic scales of electroencephalographic ( eeg ) and magnetoencephalographic ( meg ) activity . manuscript received #NUM# march #NUM# . this project has been supported entirely by personal contributions to physical studies institute and to the university of california at san diego physical studies institute agency account through the institute for pure and applied physical sciences .
we present new results , both positive and negative , on the well - studied problem of learning disjunctive normal form ( dnf ) expressions . we first prove that an algorithm due to kushilevitz and mansour [ #NUM# ] can be used to weakly learn dnf using membership queries in polynomial time , with respect to the uniform distribution on the inputs . this is the first positive result for learning unrestricted dnf expressions in polynomial time in any nontrivial formal model of learning . it provides a sharp contrast with the results of kharitonov [ #NUM# ] , who proved that ac #NUM# is not efficiently learnable in the same model ( given certain plausible cryptographic assumptions ) . we also present efficient learning algorithms in various models for the read - k and sat - k subclasses of dnf . for our negative results , we turn our attention to the recently introduced statistical query model of learning [ #NUM# ] . this model is a restricted version of the popular probably approximately correct ( pac ) model [ #NUM# ] , and practically every class known to be efficiently learnable in the pac model is in fact learnable in the statistical query model [ #NUM# ] . here we give a general characterization of the complexity of statistical query learning in terms of the number of uncorrelated functions in the concept class . this is a distribution - dependent quantity yielding upper and lower bounds on the number of statistical queries required for learning on any input distribution . as a corollary , we obtain that dnf expressions and decision trees are not even weakly learnable with respect to the uniform input distribution in polynomial time in the statistical query model . this result is information - theoretic and therefore does not rely on any unproven assumptions . it demonstrates that no simple modification of the existing algorithms in the computational learning theory literature for learning various restricted forms of dnf and decision trees from passive random examples ( and also several algorithms proposed in the experimental machine learning communities , such as the id #NUM# algorithm for decision trees [ #NUM# ] and its variants ) will solve the general problem . the unifying tool for all of our results is the fourier analysis of a finite class of boolean functions on the hypercube .
planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence . in this paper we summarize an approach to this problem based on the mathematical framework of markov decision processes and reinforcement learning . conventional model - based reinforcement learning uses primitive actions that last one time step and that can be modeled independently of the learning agent . these can be generalized to macro actions , multi - step actions specified by an arbitrary policy and a way of completing . macro actions generalize the classical notion of a macro operator in that they are closed loop , uncertain , and of variable duration . macro actions are needed to represent common - sense higher - level actions such as going to lunch , grasping an object , or traveling to a distant city . this paper generalizes prior work on temporally abstract models ( sutton #NUM# ) and extends it from the prediction setting to include actions , control , and planning . we define a semantics of models of macro actions that guarantees the validity of planning using such models . this paper present new results in the theory of planning with macro actions and illustrates its potential advantages in a gridworld task .
in complex and changing environments explanation must be a a dynamic and goal - driven process . this paper discusses an evolving system implementing a novel model of explanation generation | goal - driven interactive explanation | that models explanation as a goal - driven , multi - strategy , situated process inter - weaving reasoning with action . we describe a preliminary implementation of this model in gobie , a system that generates explanations for its internal use to support plan generation and execution .
the design and implementation of software for the ring array processor ( rap ) , a high performance parallel computer , involved development for three hardware platforms : sun sparc workstations , heurikon mc #NUM# boards running the vxworks real - time operating system , and texas instruments tms #NUM# c #NUM# dsps . the rap now runs in sun workstations under unix and in a vme based system using vxworks . a flexible set of tools has been provided both to the rap user and programmer . primary emphasis has been placed on improving the efficiency of layered artificial neural network algorithms . this was done by providing a library of assembly language routines , some of which use node - custom compilation . an object - oriented rap interface in c + + is provided that allows programmers to incorporate the rap as a computational server into their own unix applications . for those not wishing to program in c + + , a command interpreter has been built that provides interactive and shell - script style rap manipulation .
selecting a set of features which is optimal for a given task is a problem which plays an important role in a wide variety of contexts including pattern recognition , adaptive control , and machine learning . our experience with traditional feature selection algorithms in the domain of machine learning lead to an appreciation for their computational efficiency and a concern for their brittleness . this paper describes an alternate approach to feature selection which uses genetic algorithms as the primary search component . results are presented which suggest that genetic algorithms can be used to increase the robustness of feature selection algorithms without a significant decrease in computational efficiency .
several evolutionary algorithms make use of hierarchical representations of variable size rather than linear strings of fixed length . variable complexity of the structures provides an additional representational power which may widen the application domain of evolutionary algorithms . the price for this is , however , that the search space is open - ended and solutions may grow to arbitrarily large size . in this paper we study the effects of structural complexity of the solutions on their generalization performance by analyzing the fitness landscape of sigma - pi neural networks . the analysis suggests that smaller networks achieve , on average , better generalization accuracy than larger ones , thus confirming the usefulness of occam ' s razor . a simple method for implementing the occam ' s razor principle is described and shown to be effective in improv ing the generalization accuracy without limiting their learning capacity .
we argue based upon the numbers of representations of given length , that increase in representation length is inherent in using a fixed evaluation function with a discrete but variable length representation . two examples of this are analysed , including the use of price ' s theorem . both examples confirm the tendency for solutions to grow in size is caused by fitness based selection .
environments that vary over time present a fundamental problem to adaptive systems . although in the worst case there is no hope of effective adaptation , some forms environmental variability do provide adaptive opportunities . we consider a broad class of non - stationary environments , those which combine a variable result function with an invariant utility function , and demonstrate via simulation that an adaptive strategy employing both evolution and learning can tolerate a much higher rate of environmental variation than an evolution - only strategy . we suggest that in many cases where stability has previously been assumed , the constant utility non - stationary environment may in fact be a more powerful viewpoint .
we have recently introduced a neural network for reactive obstacle avoidance based on a model of classical and operant conditioning . in this article we describe the success of this model when implemented on two real autonomous robots . our results show the promise of self - organizing neural networks in the domain of intelligent robotics .
the paper reports on the application of genetic algorithms , probabilistic search algorithms based on the model of organic evolution , to np - complete combinatorial optimization problems . in particular , the subset sum , maximum cut , and minimum tardy task problems are considered . except for the fitness function , no problem - specific changes of the genetic algorithm are required in order to achieve results of high quality even for the problem instances of size #NUM# used in the paper . for constrained problems , such as the subset sum and the minimum tardy task , the constraints are taken into account by incorporating a graded penalty term into the fitness function . even for large instances of these highly multimodal optimization problems , an iterated application of the genetic algorithm is observed to find the global optimum within a number of runs . as the genetic algorithm samples only a tiny fraction of the search space , these results are quite encouraging .
cupit - #NUM# is a programming language specifically designed to express neural network learning algorithms . it provides most of the flexibility of general - purpose languages like c / c + + , but results in much clearer and more elegant programs due to higher expressiveness , in particular for algorithms that change the network topology dynamically ( constructive algorithms , pruning algorithms ) . furthermore , cupit - #NUM# programs can be compiled into efficient code for parallel machines ; no changes are required in the source program . this article presents a description of the language constructs and reports performance results for an implementation of cupit - #NUM# on symmetric multiprocessors ( smps ) .
augmenting genetic algorithms with local search heuristics is a promising approach to the solution of combinatorial optimization problems . in this paper , a genetic local search approach to the quadratic assignment problem ( qap ) is presented . new genetic operators for realizing the approach are described , and its performance is tested on various qap instances containing between #NUM# and #NUM# facilities / locations . the results indicate that the proposed algorithm is able to arrive at high quality solutions in a relatively short time limit : for the largest publicly known prob lem instance , a new best solution could be found .
the problem of programming an artificial ant to follow the santa fe trail has been repeatedly used as a benchmark problem . recently we have shown performance of several techniques is not much better than the best performance obtainable using uniform random search . we suggested that this could be because the program fitness landscape is difficult for hill climbers and the problem is also difficult for genetic algorithms as it contains multiple levels of deception . here we redefine the problem so the ant is obliged to traverse the trail in approximately the correct order . a simple genetic programming system , with no size or depth restriction , is show to perform approximately three times better with the improved training function .
machine learning research has been making great progress is many directions . this article summarizes four of these directions and discusses some current open problems . the four directions are ( a ) improving classification accuracy by learning ensembles of classifiers , ( b ) methods for scaling up supervised learning algorithms , ( c ) reinforcement learning , and ( d ) learning complex stochastic models .
fill ' s algorithm for perfect simulation for attractive finite state space models , unbiased for user impatience , is presented in terms of stochastic recursive sequences and extended in two ways . repulsive discrete markov random fields with two coding sets like the auto - poisson distribution on a lattice with #NUM# - neighbourhood can be treated as monotone systems if a particular partial ordering and quasi - maximal and quasi - minimal states are used . fill ' s algorithm then applies directly . combining fill ' s rejection sampling with sandwiching leads to a version of the algorithm , which works for general discrete conditionally specified repulsive models . extensions to other types of models are briefly discussed .
we consider a special case of reinforcement learning where the environment can be described by a linear system . the states of the environment and the actions the agent can perform are represented by real vectors and the system dynamic is given by a linear equation with a stochastic component . the problem is equivalent to the so - called linear quadratic regulator problem studied in the optimal and adaptive control literature . we propose a learning algorithm for that problem and analyze it in a pac learning framework . unlike the algorithms in the adaptive control literature , our algorithm actively explores the environment to learn an accurate model of the system faster . we show that the control law produced by our algorithm has , with high probability , a value that is close to that of an optimal policy relative to the magnitude of the initial state of the system . the time taken by the algorithm is polynomial in the dimension n of the state - space and in the dimension r of the action - space when the ratio n = r is a constant .
many of today ' s algorithms for inductive logic programming ( ilp ) put a heavy burden and responsibility on the user , because their declarative bias have to be defined in a rather low - level fashion . to address this issue , we developed a method for generating declarative language bias for top - down ilp systems from high - level declarations . the key feature of our approach is the distinction between a user level and an expert level of language bias declarations . the expert provides abstract meta - declarations , and the user declares the relationship between the meta - level and the given database to obtain a low - level declarative language bias . the suggested languages allow for compact and abstract specifications of the declarative language bias for top - down ilp systems using schemata . we verified several properties of the translation algorithm that generates schemata , and applied it successfully to a few chemical domains . as a consequence , we propose to use a two - level approach to generate declarative language bias .
one of the difficult problems in the area of explanation based learning is the utility problem ; learning too many rules of low utility can lead to swamping , or degradation of performance . this paper introduces two new techniques for improving the utility of learned rules . the first technique is to combine ebl with inductive learning techniques to learn a better set of control rules ; the second technique is to use these inductive techniques to learn approximate control rules . the two techniques are synthesized in an algorithm called approximating abductive explanation based learning ( axa - ebl ) . axa - ebl is shown to improve substantially over standard ebl in several domains .
in this paper we address the problem of program discovery as defined by genetic programming [ #NUM# ] . we have two major results : first , by combining a hierarchical crossover operator with two traditional single point search algorithms : simulated annealing and stochastic iterated hill climbing , we have solved some problems with fewer fitness evaluations and a greater probability of a success than genetic programming . second , we have managed to enhance genetic programming by hybridizing it with the simple scheme of hill climbing from a few individuals , at a fixed interval of generations . the new hill climbing component has two options for generating candidate solutions : mutation or crossover . when it uses crossover , mates are either randomly created , randomly drawn from the population at large , or drawn from a pool of fittest individuals .
most of kdd applications consider databases as static objects , and however many databases are inherently temporal , i . e . , they store the evolution of each object with the passage of time . thus , regularities about the dynamics of these databases cannot be discovered as the current state might depend in some way on the previous states . to this end , a pre - processing of data is needed aimed at extracting relationships intimately connected to the temporal nature of data that will be make available to the discovery algorithm . the predicate logic language of ilp methods together with the recent advances as to ef ficiency makes them adequate for this task .
in this paper we consider a continous time method of approximating a given distribution using the langevin diffusion dl t = dw t + #NUM# #NUM# r log ( l t ) dt : we find conditions under which this diffusion converges exponentially quickly to or does not : in one dimension , these are essentially that for distributions with exponential tails of the form ( x ) / exp ( fljxj fi ) , #NUM# & lt ; fi & lt ; #NUM# , exponential convergence occurs if and only if fi #NUM# . we then consider conditions under which the discrete approximations to the diffusion converge . we first show that even when the diffusion itself converges , naive discretisations need not do so . we then consider a " metropolis - adjusted " version of the algorithm , and find conditions under which this also converges at an exponential rate : perhaps surprisingly , even the metropolised version need not converge exponentially fast even if the diffusion does . we briefly discuss a truncated form of the algorithm which , in practice , should avoid the difficulties of the other forms .
an essential component of an intelligent agent is the ability to observe , encode , and use information about its environment . traditional approaches to genetic programming have focused on evolving functional or reactive programs with only a minimal use of state . this paper presents an approach for investigating the evolution of learning , planning , and memory using genetic programming . the approach uses a multi - phasic fitness environment that enforces the use of memory and allows fairly straightforward comprehension of the evolved representations . an illustrative problem of ' gold ' collection is used to demonstrate the usefulness of the approach . the results indicate that the approach can evolve programs that store simple representations of their environments and use these representations to produce simple plans .
reinforcement learning can be used not only to predict rewards , but also to predict states , i . e . to learn a model of the world ' s dynamics . models can be defined at different levels of temporal abstraction . multi - time models are models that focus on predicting what will happen , rather than when a certain event will take place . based on multi - time models , we can define abstract actions , which enable planning ( presumably in a more efficient way ) at various levels of abstraction .
previous research has shown that a technique called error - correcting output coding ( ecoc ) can dramatically improve the classification accuracy of supervised learning algorithms that learn to classify data points into one of k #NUM# classes . in this paper , we will extend the technique so that ecoc can also provide class probability information . ecoc is a method of converting k - class supervised learning problem into a large number l of two - class supervised learning problems and then combining the results of these l evaluations . the underlying two - class supervised learning algorithms are assumed to provide l probability estimates . the problem of computing class probabilities is formulated as an over - constrained system of l linear equations . least squares methods are applied to solve these equations . accuracy and reliability of the probability estimates are demonstrated .
technical report crg - tr - #NUM# - #NUM# may #NUM# , #NUM# ( revised feb #NUM# , #NUM# ) abstract factor analysis , a statistical method for modeling the covariance structure of high dimensional data using a small number of latent variables , can be extended by allowing different local factor models in different regions of the input space . this results in a model which concurrently performs clustering and dimensionality reduction , and can be thought of as a reduced dimension mixture of gaussians . we present an exact expectation - maximization algorithm for fitting the parameters of this mixture of factor analyzers .
the position , size , and shape of the visual receptive field ( rf ) of some primary visual cortical neurons change dynamically , in response to artificial scotoma conditioning in cats ( pettet & gilbert , #NUM# ) and to retinal lesions in cats and monkeys ( darian - smith & gilbert , #NUM# ) . the " exin " learning rules ( marshall , #NUM# ) are used to model dynamic rf changes . the exin model is compared with an adaptation model ( xing & gerstein , #NUM# ) and the lissom model ( sirosh & miikkulainen , #NUM# ; sirosh et al . , #NUM# ) . to emphasize the role of the lateral inhibitory learning rules , the exin and the lissom simulations were done with only lateral inhibitory learning . during scotoma conditioning , the exin model without feedforward learning produces centrifugal expansion of rfs initially inside the scotoma region , accompanied by increased responsiveness , without changes in spontaneous activation . the exin model without feedforward learning is more consistent with the neurophysiological data than are the adaptation model and the lissom model . the comparison between the exin and the lissom models suggests experiments to determine the role of feedforward excitatory and lateral inhibitory learning in producing dynamic rf changes during scotoma conditioning .
in this paper we present a bottom - up algorithm called mri to induce logic programs from their examples . this method can induce programs with a base clause and more than one recursive clause from a very small number of examples . mri is based on the analysis of saturations of examples . it first generates a path structure , which is an expression of a stream of values processed by predicates . the concept of path structure was originally introduced by identam - almquist and used in tim [ idestam - almquist , #NUM# ] . in this paper , we introduce the concepts of extension and difference of path structure . recursive clauses can be expressed as a difference between a path structure and its extension . the paper presents the algorithm and shows experimental results obtained by the method .
the bayesian analysis of neural networks is difficult because a simple prior over weights implies a complex prior distribution over functions . in this paper we investigate the use of gaussian process priors over functions , which permit the predictive bayesian analysis for fixed values of hyperparameters to be carried out exactly using matrix operations . two methods , using optimization and averaging ( via hybrid monte carlo ) over hyperparameters have been tested on a number of challenging problems and have produced excellent results .
explanations play a key role in operationalization - based anomaly detection techniques . in this paper we show that their role is not limited to anomaly detection ; they can also be used for guiding automated knowledge base refinement . we introduce a refinement procedure which takes : ( i ) a small number of refinement rules ( rather than test cases ) , and ( ii ) explanations constructed in an attempt to reveal the cause ( or causes ) for inconsistencies detected during the verification process , and returns rule revisions aiming to recover the consistency of the kb - theory . inconsistencies caused by more than one anomaly are handled at the same time , which improves the efficiency of the refinement process .
this paper sets out a conceptual framework for the open - ended artificial evolution of complex behaviour in autonomous agents . if recurrent dynamical neural networks ( or similar ) are used as phenotypes , then a genetic algorithm that employs variable length genotypes , such as inman harvey ' s saga , is capable of evolving arbitrary levels of be - havioural complexity . furthermore , with simple restrictions on the encoding scheme that governs how genotypes develop into phenotypes , it may be guaranteed that if an increase in fitness requires an increase in be - havioural complexity , then it will evolve . in order for this process to be practicable as a design alternative , however , the time periods involved must be acceptable . the final part of this paper looks at general ways in which the encoding scheme may be modified to speed up the process . experiments are reported in which different categories of scheme were tested against each other , and conclusions are offered as to the most promising type of encoding scheme for a vi able open - ended evolutionary robotics .
we have recently introduced a neural network mobile robot controller ( netmorc ) that autonomously learns the forward and inverse odometry of a differential drive robot through an unsupervised learning - by - doing cycle . after an initial learning phase , the controller can move the robot to an arbitrary stationary or moving target while compensating for noise and other forms of disturbance , such as wheel slippage or changes in the robot ' s plant . in addition , the forward odometric map allows the robot to reach targets in the absence of sensory feedback . the controller is also able to adapt in response to long - term changes in the robot ' s plant , such as a change in the radius of the wheels . in this article we review the netmorc architecture and describe its simplified algorithmic implementation , we present new , quantitative results on netmorc ' s performance and adaptability under noise - free and noisy conditions , we compare netmorc ' s performance on a trajectory - following task with the performance of an alternative controller , and we describe preliminary results on the hardware implementation of netmorc with the mobile robot robuter .
we develop an algorithm for simulating " perfect " random samples from the invariant measure of a harris recurrent markov chain . the method uses backward coupling of embedded regeneration times , and works most effectively for finite chains and for stochastically monotone chains even on continuous spaces , where paths may be sandwiched below " upper " and " lower " processes . examples show that more naive approaches to constructing such bounding processes may be considerably biased , but that the algorithm can be simplified in certain cases to make it easier to run . we give explicit analytic bounds on the backward coupling times in the stochastically monotone case .
this reports gives a review of the new exact simulation algorithms using markov chains . the first part covers the discrete case . we consider two different algorithms , propp and wilsons coupling from the past ( cftp ) technique and fills rejection sampler . the algorithms are tested on the ising model , with and without an external field . the second part covers continuous state spaces . we present several algorithms developed by murdoch and green , all based on coupling from the past . we discuss the applicability of these methods on a bayesian analysis problem of surgical failure rates .
specialist and generalist behaviors in populations of artificial neural networks are studied . a genetic algorithm is used to simulate evolution processes , and thereby to develop neural network control systems that exhibit specialist or generalist behaviors according to the fitness formula . with evolvable fitness formulae the evaluation measure is let free to evolve , and we obtain a co - evolution of the expressed behavior and the individual evolvable fitness formula . the use of evolvable fitness formulae lets us work in a dynamic fitness landscape , opposed to most work , that traditionally applies to static fitness landscapes , only . the role of competition in specialization is studied by letting the individuals live under social conditions in the same , shared environment and directly compete with each other . we find , that competition can act to provide population diversification in populations of organisms with individual evolvable fitness formulae .
using deliberately designed primitive sets , we investigate the relationship between context - based expression mechanisms and the size , height and density of genetic program trees during the evolutionary process . we show that contextual semantics influence the composition , location and flows of operative code in a program . in detail we analyze these dynamics and discuss the impact of our findings on micro - level descriptions of genetic programming .
most traditional prediction techniques deliver the mean of the probability distribution ( a single point ) . for multimodal processes , instead of predicting the mean of the probability distribution , it is important to predict the full distribution . this article presents a new connectionist method to predict the conditional probability distribution in response to an input . the main idea is to transform the problem from a regression to a classification problem . the conditional probability distribution network can perform both direct predictions and iterated predictions , a task which is specific for time series problems . we compare our method to fuzzy logic and discuss important differences , and also demonstrate the architecture on two time series . the first is the benchmark laser series used in the santa fe competition , a deterministic chaotic system . the second is a time series from a markov process which exhibits structure on two time scales . the network produces multimodal predictions for this series . we compare the predictions of the network with a nearest - neighbor predictor and find that the conditional probability network is more than twice as likely a model .
in recent years , considerable effort has gone into understanding default reasoning . most of this effort concentrated on the question of entailment , i . e . , what conclusions are warranted by a knowledge - base of defaults . surprisingly , few works formally examine the general role of defaults . we argue that an examination of this role is necessary in order to understand defaults , and suggest a concrete role for defaults : defaults simplify our decision - making process , allowing us to make fast , approximately optimal decisions by ignoring certain possible states . in order to formalize this approach , we examine decision making in the framework of decision theory . we use probability and utility to measure the impact of possible states on the decision - making process . we accept a default if it ignores states with small impact according to our measure . we motivate our choice of measures and show that the resulting formalization of defaults satisfies desired properties of defaults , namely cumulative reasoning . finally , we compare our approach with poole ' s decision - theoretic defaults , and show how both can be combined to form an attractive framework for reasoning about decisions .
density estimation is a commonly used test case for non - parametric estimation methods . we explore the asymptotic properties of estimators based on thresholding of empirical wavelet coefficients . minimax rates of convergence are studied over a large range of besov function classes b s ; p ; q and for a range of global l #NUM# p error measures , #NUM# p #NUM# & lt ; #NUM# . a single wavelet threshold estimator is asymptotically minimax within logarithmic terms simultaneously over a range of spaces and error measures . in particular , when p #NUM# & gt ; p , some form of non - linearity is essential , since the minimax linear estimators are suboptimal by polynomial powers of n . a second approach , using an approximation of a gaussian white noise model in a mallows metric , is used acknowledgements : we thank alexandr sakhanenko for helpful discussions and references to his work on berry esseen theorems used in section #NUM# . this work was supported in part by nsf dms #NUM# - #NUM# . the second author would like to thank universite de
are used in turn to approximate a . empirical studies show that good results can be achieved with tsl [ #NUM# , #NUM# ] . however , tsl has several drawbacks . training set learners ( e . g . , backpropagation ) are typically slow as they may require many passes over the training set . also , there is no guarantee that , given an arbitrary training set , the system will find enough good critical features to get a reasonable approximation of a . moreover , the number of features to be searched is exponential in the number of inputs , and tsl becomes computationally expensive [ #NUM# ] . finally , the scarcity of interesting positive theoretical results suggests the difficulty of learning without sufficient a priori knowledge . the goal of learning systems is to generalize . generalization is commonly based on the set of critical features the system has available . training set learners typically extract critical features from a random set of examples . while this approach is attractive , it suffers from the exponential growth of the number of features to be searched . we propose to extend it by endowing the system with some a priori knowledge , in the form of precepts . advantages of the augmented system are speedup , improved generalization , and greater parsimony . this paper presents a precept - driven learning algorithm . its main features include : #NUM# ) distributed implementation , #NUM# ) bounded learning and execution times , and #NUM# ) ability to handle both correct and incorrect precepts . results of simulations on real - world data demonstrate promise . this paper presents precept - driven learning ( pdl ) . pdl is intended to overcome some of tsl ' s weaknesses . in pdl , the training set is augmented by a small set of precepts . a pair p = ( i , o ) in i o is called an example . a precept is an example in which some of the i - entries ( inputs ) are set to the special value don ' t - care . an input whose value is not don ' t - care is said to be asserted . if i has no effect on the value of the output . the use of the special value don ' t - care is therefore as a shorthand . a pair containing don ' t - care inputs represents as many examples as the product of the sizes of the input domains of its don ' t - care inputs . #NUM# . introduction
many inductive learning problems can be expressed in the classical attribute - value language . in order to learn and to generalize , learning systems often rely on some measure of similarity between their current knowledge base and new information . the attribute - value language defines a heterogeneous multidimensional input space , where some attributes are nominal and others linear . defining similarity , or proximity , of two points in such input spaces is non trivial . we discuss two representative homogeneous metrics and show examples of why they are limited to their own domains . we then address the issues raised by the design of a heterogeneous metric for inductive learning systems . in particular , we discuss the need for normalization and the impact of don ' t - care values . we propose a heterogeneous metric and evaluate it empirically on a simplified version of ila .
we study efficient algorithms for solving the following problem , which we call the switching distributions learning problem . a sequence s = #NUM# #NUM# : : : n , over a finite alphabet is generated in the following way . the sequence is a concatenation of k runs , each of which is a consecutive subsequence . each run is generated by independent random draws from a distribution ~ p i over , where ~ p i is an element in a set of distributions f ~ p #NUM# ; : : : ; ~ p n g . the learning algorithm is given this sequence and its goal is to find approximations of the distributions ~ p #NUM# ; : : : ; ~ p n , and give an approximate segmentation of the sequence into its constituting runs . we give an efficient algorithm for solving this problem and show conditions under which the algorithm is guaranteed to work with high probability .
i describe a distance metric called " edit " distance which quantifies the syntactic difference between two genetic programs . in the context of one specific problem , the #NUM# bit multiplexor , i use the metric to analyze the amount of new material introduced by different crossover operators , the difference among the best individuals of a population and the difference among the best individuals and the rest of the population . the relationships between these data and run performance are imprecise but they are sufficiently interesting to encourage encourage further investigation into the use of edit distance .
both control and data dependencies among primitives impact the behavioural consistency of subprograms in genetic programming solutions . behavioural consistency in turn impacts the ability of genetic programming to identify and promote appropriate subprograms . we present the results of modelling dependency through a parameterized problem in which a subprogram exhibits internal and external dependency levels that change as the subprogram is successively incorporated into larger subsolutions . we find that the key difference between non - existent and " full " external dependency is a longer time to solution identification and a lower likelihood of success as shown by increased difficulty in identifying and promoting correct subprograms .
in this paper we compare the performance of a serial and a parallel island model genetic algorithm for solving the multiprocessor scheduling problem . we show results using fixed and scaled problems both using and not using migration . we have found that in addition to providing a speedup through the use of parallel processing , the parallel island model ga with migration finds better quality solutions than the serial ga .
an important reason for the continued popularity of artificial neural networks ( anns ) in the machine learning community is that the gradient - descent backpropagation procedure gives anns a locally optimal change procedure and , in addition , a framework for understanding the ann learning performance . genetic programming ( gp ) is also a successful evolutionary learning technique that provides powerful parameterized primitive constructs . unlike anns , though , gp does not have such a principled procedure for changing parts of the learned system based on its current performance . this paper introduces neural programming , a connectionist representation for evolving programs that maintains the benefits of gp . the connectionist model of neural programming allows for a regression credit - blame procedure in an evolutionary learning system . we describe a general method for an informed feedback mechanism for neural programming , internal reinforcement . we introduce an internal reinforcement procedure and demon strate its use through an illustrative experiment .
top - down induction of decision trees ( tdidt ) is a very popular machine learning technique . up till now , it has mainly been used for propositional learning , but seldomly for relational learning or inductive logic programming . the main contribution of this paper is the introduction of logical decision trees , which make it possible to use tdidt in inductive logic programming . an implementation of this top - down induction of logical decision trees , the
probabilistic neural networks ( pnn ) typically learn more quickly than many neural network models and have had success on a variety of applications . however , in their basic form , they tend to have a large number of hidden nodes . one common solution to this problem is to keep only a randomly - selected subset of the original training data in building the network . this paper presents an algorithm called the reduced probabilistic neural network ( rpnn ) that seeks to choose a better - than - random subset of the available instances to use as center points of nodes in the network . the algorithm tends to retain non - noisy border points while removing nodes with instances in regions of the input space that are highly homogeneous . in experiments on #NUM# datasets , the rpnn had better average generalization accuracy than two other pnn models , while requiring an average of less than one - third the number of nodes .
in standard neuro - evolution , a population of networks is evolved in the task , and the network that best solves the task is found . this network is then fixed and used to solve future instances of the problem . networks evolved in this way do not handle real - time interaction very well . it is hard to evolve a solution ahead of time that can cope effectively with all the possible environments that might arise in the future and with all the possible ways someone may interact with it . this paper proposes evolving feedforward neural networks online to create agents that improve their performance through real - time interaction . this approach is demonstrated in a game world where neural - network - controlled individuals play against humans . through evolution , these individuals learn to react to varying opponents while appropriately taking into account conflicting goals . after initial evaluation offline , the population is allowed to evolve online , and its performance improves considerably . the population not only adapts to novel situations brought about by changing strategies in the opponent and the game layout , but it also improves its performance in situations that it has already seen in offline training . this paper will describe an implementation of online evolution and shows that it is a practical method that exceeds the performance of offline evolution alone .
we devise a feed - forward artificial neural network ( ann ) procedure for predicting utility loads and present the resulting predictions for two test problems given by " the great energy predictor shootout the first building data analysis and prediction competition " [ #NUM# ] . key ingredients in our approach are a method ( ffi - test ) for determining relevant inputs and the multilayer perceptron . these methods are briefly reviewed together with comments on alternative schemes like fitting to polynomials and the use of recurrent networks .
n this paper we first review the main results in the theory of schemata in genetic programming ( gp ) and summarise a new gp schema theory which is based on a new definition of schema . then we study the creation , propagation and disruption of this new form of schemata in real runs , for standard crossover , one - point crossover and selection only . finally , we discuss these results in the light our gp schema theorem .
radial basis function ( rbfs ) neural networks provide an attractive method for high dimensional nonparametric estimation for use in nonlinear control . they are faster to train than conventional feedforward networks with sigmoidal activation networks ( " backpropagation nets " ) , and provide a model structure better suited for adaptive control . this article gives a brief survey of the use of rbfs and then introduces a new statistical interpretation of radial basis functions and a new method of estimating the parameters , using the em algorithm . this new statistical interpretation allows us to provide confidence limits on predictions made using the networks .
we review the main results obtained in the theory of schemata in genetic programming ( gp ) emphasising their strengths and weaknesses . then we propose a new , simpler definition of the concept of schema for gp which is closer to the original concept of schema in genetic algorithms ( gas ) . along with a new form of crossover , one - point crossover , and point mutation this concept of schema has been used to derive an improved schema theorem for gp which describes the propagation of schemata from one generation to the next . we discuss this result and show that our schema theorem is the natural counterpart for gp of the schema theorem for
we describe the maximum - likelihood parameter estimation problem and how the expectation - maximization ( em ) algorithm can be used for its solution . we first describe the abstract form of the em algorithm as it is often given in the literature . we then develop the em parameter estimation procedure for two applications : #NUM# ) finding the parameters of a mixture of gaussian densities , and #NUM# ) finding the parameters of a hidden markov model ( hmm ) ( i . e . , the baum - welch algorithm ) for both discrete and gaussian mixture observation models . we derive the update equations in fairly explicit detail but we do not prove any convergence properties . we try to emphasize intuition rather than mathematical rigor .
a genetic programming method is investigated for optimizing both the architecture and the connection weights of multilayer feedforward neural networks . the genotype of each network is represented as a tree whose depth and width are dynamically adapted to the particular application by specifically defined genetic operators . the weights are trained by a next - ascent hillclimb - ing search . a new fitness function is proposed that quantifies the principle of occam ' s razor . it makes an optimal trade - off between the error fitting ability and the parsimony of the network . we discuss the results for two problems of differing complexity and study the convergence and scaling properties of the algorithm .
spert ( synthetic perceptron testbed ) is a fully programmable single chip microprocessor designed for efficient execution of artificial neural network algorithms . the first implementation will be in a #NUM# . #NUM# m cmos technology with a #NUM# mhz clock rate , and a prototype system is being designed to occupy a double sbus slot within a sun sparcstation . spert will sustain over #NUM# fi #NUM# #NUM# connections per second during pattern classification , and around #NUM# fi #NUM# #NUM# connection updates per second while running the popular error backpropagation training algorithm . this represents a speedup of around two orders of magnitude over a sparcstation - #NUM# for algorithms of interest . an earlier system produced by our group , the ring array processor ( rap ) , used commercial dsp chips . compared with a rap multiprocessor of similar performance , spert represents over an order of magnitude reduction in cost for problems where fixed - point arithmetic is satisfactory .
genetic programming is a method of program discovery consisting of a special kind of genetic algorithm capable of operating on nonlinear chromosomes ( parse trees ) representing programs and an interpreter which can run the programs being optimised . this paper describes pdgp ( parallel distributed genetic programming ) , a new form of genetic programming which is suitable for the development of fine - grained parallel programs . pdgp is based on a graph - like representation for parallel programs which is manipulated by crossover and mutation operators which guarantee the syntactic correctness of the offspring . the paper describes these operators and reports some preliminary results obtained with this paradigm .
we define fitness structure in genetic programming to be the mapping between the subprograms of a program and their respective fitness values . this paper shows how various fitness structures of a problem with independent subsolutions relate to the acquisition of sub - solutions . the rate of subsolution acquisition is found to be directly correlated with fitness structure whether that structure is uniform , linear or exponential . an understanding of fitness structure provides partial insight into the complicated relationship between fitness function and the outcome of genetic programming ' s search .
specialization in populations of artificial neural networks is studied . organisms with both fixed and evolvable fitness formulae are placed in isolated and shared environments , and the emerged behaviors are compared . an evolvable fitness formula specifies , that the evaluation measure is let free to evolve , and we obtain co - evolution of the expressed behavior and the individual evolvable fitness formula . in an isolated environment a generalist behavior emerges when organisms have a fixed fitness formula , and a specialist behavior emerges when organisms have individual evolvable fitness formulae . a population diversification analysis shows , that almost all organisms in a population in an isolated environment converge towards the same behavioral strategy , while we find , that competition can act to provide population diversification in populations of organisms in a shared environment .
clones is a object - oriented library for constructing , training and utilizing layered connectionist networks . the clones library contains all the object classes needed to write a simulator with a small amount of added source code ( examples are included ) . the size of experimental ann programs is greatly reduced by using an object - oriented library ; at the same time these programs are easier to read , write and evolve . the library includes database , network behavior and training procedures that can be customized by the user . it is designed to run efficiently on data parallel computers ( such as the rap [ #NUM# ] and spert [ #NUM# ] ) as well as uniprocessor workstations . while efficiency and portability to parallel computers are the primary goals , there are several secondary design goals : #NUM# . allow heterogeneous algorithms and training procedures to be interconnected and trained together . within these constraints we attempt to maximize the variety of artificial neural net work algorithms that can be supported .
technical report : csrp - #NUM# - #NUM# august #NUM# genetic programming is a method of program discovery consisting of a special kind of genetic algorithm capable of operating on parse trees representing programs and an interpreter which can run the programs being optimised . this paper describes parallel distributed genetic programming ( pdgp ) , a new form of genetic programming which is suitable for the development of parallel programs in which symbolic and neural processing elements can be combined a in free and natural way . pdgp is based on a graph - like representation for parallel programs which is manipulated by crossover and mutation operators which guarantee the syntactic correctness of the offspring . the paper describes these operators and reports some results obtained with the exclusive - or problem .
there has been much interest in using optics to implement computer interconnection networks . however , there has been little discussion of any routing methodologies besides those already used in electronics . in this paper , a neural network routing methodology is proposed that can generate control bits for a broad range of optical multistage interconnection networks ( omins ) . though we present no optical implementation of this methodology , we illustrate its control for an optical interconnection network . these omins can be used as communication media for distributed computing systems . the routing methodology makes use of an artificial neural network ( ann ) that functions as a parallel computer for generating the routes . the neural network routing scheme can be applied to electrical as well as optical interconnection networks . however , since the ann can be implemented using optics , this routing approach is especially appealing for an optical computing environment . although the ann does not always generate the best solution , the parallel nature of the ann computation may make this routing scheme faster than conventional routing approaches , especially for omins that have an irregular structure . furthermore , the ann router is fault - tolerant . results are shown for generating routes in a #NUM# fi #NUM# , #NUM# - stage omin .
the multispert parallel system is a straight - forward extension of the spert workstation accelerator , which is predominantly used in speech recognition research at icsi . in order to deliver high performance for artificial neural network training without requiring changes to the user interfaces , the exisiting quicknet ann library was modified to run on multispert . in this report , we present the algorithms used in the parallelization of the quicknet code and analyse their communication and computation requirements . the resulting performance model yields a better understanding of system speed - ups and potential bottlenecks . experimental results from actual training runs validate the model and demonstrate the achieved performance levels .
in this paper we explore the distributed database allocation problem , which is intractable . we also discuss genetic algorithms and how they have been used successfully to solve combinatorial problems . our experimental results show the ga to be far superior to the greedy heuristic in obtaining optimal and near optimal fragment placements for the allocation problem with various data sets .
the task of discovering interesting regularities in ( large ) sets of data ( data mining , knowledge discovery ) has recently met with increased interest in machine learning in general and in inductive logic programming ( ilp ) in particular . however , while there is a widely accepted definition for the task of concept learning from examples in ilp , definitions for the data mining task have been proposed only recently . in this paper , we examine these so - called " non - monotonic semantics " definitions and show that non - monotonicity is only an incidental property of the data mining learning task , and that this task makes perfect sense without such an assumption . we therefore introduce and define a generalized definition of the data mining task called the ilp description learning problem and discuss its properties and relation to the traditional concept learning ( prediction ) learning problem . since our characterization is entirely on the level of models , the definition applies independently of the chosen hypothesis language .
opto - electronic reconfigurable interconnection networks are limited by significant control latency when used in large multiprocessor systems . this latency is the time required to analyze the current traffic and reconfigure the network to establish the required paths . the goal of latency hiding is to minimize the effect of this control overhead . in this paper , we introduce a technique that performs latency hiding by learning the patterns of communication traffic and using that information to anticipate the need for communication paths . hence , the network provides the required communication paths before a request for a path is made . in this study , the communication patterns ( memory accesses ) of a parallel program are used as input to a time delay neural network ( tdnn ) to perform on - line training and prediction . these predicted communication patterns are used by the interconnection network controller that provides routes for the memory requests . based on our experiments , the neural network was able to learn highly repetitive communication patterns , and was thus able to predict the allocation of communication paths , resulting in a reduction of communication latency .
machine learning techniques are applicable to computer system optimization . we show that shared memory multiprocessors can successfully utilize machine learning algorithms for memory access pattern prediction . in particular three different on - line machine learning prediction techniques were tested to learn and predict repetitive memory access patterns for three typical parallel processing applications , the #NUM# - d relaxation algorithm , matrix multiply and fast fourier transform on a shared memory multiprocessor . the predictions were then used by a routing control algorithm to reduce control latency in the interconnection network by configuring the interconnection network to provide needed memory access paths before they were requested . three trainable prediction techniques were used and tested : #NUM# ) . a markov predictor , #NUM# ) . a linear predictor and #NUM# ) . a time delay neural network ( tdnn ) predictor . different predictors performed best on different applications , but the tdnn produced uniformly good results .
in this paper we explore the distributed file and task placement problem , which is intractable . we also discuss genetic algorithms and how they have been used successfully to solve combinatorial problems . our experimental results show the ga to be far superior to the greedy heuristic in obtaining optimal and near optimal file and task placements for the problem with various data sets .
in this paper we show that the posterior distribution for feedforward neural networks is asymptotically consistent . this paper extends earlier results on universal approximation properties of neural networks to the bayesian setting . the proof of consistency embeds the problem in a density estimation problem , then uses bounds on the bracketing entropy to show that the posterior is consistent over hellinger neighborhoods . it then relates this result back to the regression setting . we show consistency in both the setting of the number of hidden nodes growing with the sample size , and in the case where the number of hidden nodes is treated as a parameter . thus we provide a theoretical justification for using neural networks for nonparametric regression in a bayesian framework .
pre - pruning and post - pruning are two standard methods of dealing with noise in concept learning . pre - pruning methods are very efficient , while post - pruning methods typically are more accurate , but much slower , because they have to generate an overly specific concept description first . we have experimented with a variety of pruning methods , including two new methods that try to combine and integrate pre - and post - pruning in order to achieve both accuracy and efficiency . this is verified with test series in a chess position classification task .
pruning is an effective method for dealing with noise in machine learning . recently pruning algorithms , in particular reduced error pruning , have also attracted interest in the field of inductive logic programming . however , it has been shown that these methods can be very inefficient , because most of the time is wasted for generating clauses that explain noisy examples and subsequently pruning these clauses . we introduce a new method which searches for good theories in a top - down fashion to get a better starting point for the pruning algorithm . experiments show that this approach can significantly lower the complexity of the task as well as increase predictive accuracy .
traditional databases commonly support efficient query and update procedures that operate in time which is sublinear in the size of the database . our goal in this paper is to take a first step toward dynamic reasoning in probabilistic databases with comparable efficiency . we propose a dynamic data structure that supports efficient algorithms for updating and querying singly connected bayesian networks . in the conventional algorithm , new evidence is absorbed in time o ( #NUM# ) and queries are processed in time o ( n ) , where n is the size of the network . we propose an algorithm which , after a preprocessing phase , allows us to answer queries in time o ( log n ) at the expense of o ( log n ) time per evidence absorption . the usefulness of sub - linear processing time manifests itself in applications requiring ( near ) real - time response over large probabilistic databases . we briefly discuss a potential application of dynamic probabilistic reasoning in computational biology .
in the network . often , however , an application will not need information about every node in the network nor will it need exact probabilities . we present the localized partial evaluation ( lpe ) propagation algorithm , which computes interval bounds on the marginal probability of a specified query node by examining a subset of the nodes in the entire network . conceptually , lpe ignores parts of the network that are " too far away " from the queried node to have much impact on its value . lpe has the " anytime " property of being able to produce better solutions ( tighter intervals ) given more time to consider more of the network .
we describe an integrated problem solving architecture named inbanca in which bayesian networks and case - based reasoning ( cbr ) work cooperatively on multiagent planning tasks . this includes two - team dynamic tasks , and this paper concentrates on simulated soccer as an example . bayesian networks are used to characterize action selection whereas a case - based approach is used to determine how to implement actions . this paper has two contributions . first , we survey integrations of case - based and bayesian approaches from the perspective of a popular cbr task decomposition framework , thus explaining what types of integrations have been attempted . this allows us to explain the unique aspects of our proposed integration . second , we demonstrate how bayesian nets can be used to provide environmental context , and thus feature selection information , for the case - based reasoner .
many techniques have been developed for learning rules and relationships automatically from diverse data sets , to simplify the often tedious and error - prone process of acquiring knowledge from empirical data . while these techniques are plausible , theoretically well - founded , and perform well on more or less artificial test data sets , they depend on their ability to make sense of real - world data . this paper describes a project that is applying a range of machine learning strategies to problems in agriculture and horticulture . we briefly survey some of the techniques emerging from machine learning research , describe a software workbench for experimenting with a variety of techniques on real - world data sets , and describe a case study of dairy herd management in which culling rules were inferred from a mediumsized database of herd information .
decision - theoretic preferences specify the relative desirability of all possible outcomes of alternative plans . in order to express general patterns of preference holding in a domain , we require a language that can refer directly to preferences over classes of outcomes as well as individuals . we present the basic concepts of a theory of meaning for such generic compar - atives to facilitate their incremental capture and exploitation in automated reasoning systems . our semantics lifts comparisons of individuals to comparisons of classes " other things being equal " by means of contextual equivalences , equivalence relations among individuals that vary with the context of application . we discuss implications of the theory for represent ing preference information .
the baldwin effect , first proposed in the late nineteenth century , suggests that the course of evolutionary change can be influenced by individually learned behavior . the existence of this effect is still a hotly debated topic . in this paper clear evidence is presented that learning - based plasticity at the phenotypic level can and does produce directed changes at the genotypic level . this research confirms earlier experimental work done by others , notably hinton & nowlan ( #NUM# ) . further , the amount of plasticity of the learned behavior is shown to be crucial to the size of the baldwin effect : either too little or too much and the effect disappears or is significantly reduced . finally , for learnable traits , the case is made that over many generations it will become easier for the population as a whole to learn these traits ( i . e . the phenotypic plasticity of these traits will increase ) . in this gradual transition from a genetically driven population to one driven by learning , the importance of the baldwin effect decreases .
this article presents a new line of research investigating on - line learning mechanisms for autonomous intelligent agents . we discuss a case - based method for dynamic selection and modification of behavior assemblages for a navigational system . the case - based reasoning module is designed as an addition to a traditional reactive control system , and provides more flexible performance in novel environments without extensive high - level reasoning that would otherwise slow the system down . the method is implemented in the acbarr ( a case - based reactive robotic ) system , and evaluated through empirical simulation of the system on several different environments , including " box canyon " environments known to be problematic for reactive control systems in general .
we provide analytical expressions governing changes to the bias and variance of the lookup table estimators provided by various monte carlo and temporal difference value estimation algorithms with o * ine updates over trials in absorbing markov reward processes . we have used these expressions to develop software that serves as an analysis tool given a complete description of a markov reward process , it rapidly yields an exact mean - square - error curve , the curve one would get from averaging together sample mean - square - error curves from an infinite number of learning trials on the given problem . we use our analysis tool to illustrate classes of mean - square - error curve behavior in a variety of example reward processes , and we show that although the various temporal difference algorithms are quite sensitive to the choice of step - size and eligibility - trace parameters , there are values of these parameters that make them similarly competent , and generally good .
we examine the inductive inference of a complex grammar specifically , we consider the task of training a model to classify natural language sentences as grammatical or ungrammatical , thereby exhibiting the same kind of discriminatory power provided by the principles and parameters linguistic framework , or government - and - binding theory . we investigate the following models : feed - forward neural networks , fransconi - gori - soda and back - tsoi locally recurrent networks , elman , narendra & parthasarathy , and williams & zipser recurrent networks , euclidean and edit - distance nearest - neighbors , simulated annealing , and decision trees . the feed - forward neural networks and non - neural network machine learning models are included primarily for comparison . we address the question : how can a neural network , with its distributed nature and gradient descent based iterative calculations , possess linguistic capability which is traditionally handled with symbolic computation and recursive processes ? initial simulations with all models were only partially successful by using a large temporal window as input . models trained in this fashion did not learn the grammar to a significant degree . attempts at training recurrent networks with small temporal input windows failed until we implemented several techniques aimed at improving the convergence of the gradient descent training algorithms . we discuss the theory and present an empirical study of a variety of models and learning algorithms which highlights behaviour not present when attempting to learn a simpler grammar .
a parallel version is proposed for a fundamental theorem of serial unconstrained optimization . the parallel theorem allows each of k parallel processors to use simultaneously a different algorithm , such as a descent , newton , quasi - newton or a conjugate gradient algorithm . each processor can perform one or many steps of a serial algorithm on a portion of the gradient of the objective function assigned to it , independently of the other processors . eventually a synchronization step is performed which , for differentiable convex functions , consists of taking a strong convex combination of the k points found by the k processors . for nonconvex , as well as convex , differentiable functions , the best point found by the k processors is taken , or any better point . the fundamental result that we establish is that any accumulation point of the parallel algorithm is stationary for the nonconvex case , and is a global solution for the convex case . computational testing on the thinking machines cm - #NUM# multiprocessor indicate a speedup of the order of the number of processors employed .
sensors represent a crucial link between the evolutionary forces shaping a species ' relationship with its environment , and the individual ' s cognitive abilities to behave and learn . we report on experiments using a new class of " latent energy environments " ( lee ) models to define environments of carefully controlled complexity which allow us to state bounds for random and optimal behaviors that are independent of strategies for achieving the behaviors . using lee ' s analytic basis for defining environments , we then use neural networks ( nnets ) to model individuals and a steady - state genetic algorithm to model an evolutionary process shaping the nnets , in particular their sensors . our experiments consider two types of " contact " and " ambient " sensors , and variants where the nnets are not allowed to learn , learn via error correction from internal prediction , and via reinforcement learning . we find that predictive learning , even when using a larger repertoire of the more sophisticated ambient sensors , provides no advantage over nnets unable to learn . however , reinforcement learning using a small number of crude contact sensors does provide a significant advantage . our analysis of these results points to a tradeoff between the genetic " robustness " of sensors and their informativeness to a learning system .
this is a brief annotated bibliography that i wanted to make available to the attendees of my machine learning tutorial at the #NUM# ai & statistics workshop . these slides are available in my www pages under slides . please contact me if you have any questions . please also note the date ( listed above ) on which this was most recently updated . while i plan to make occasional updates to this file , it is bound to be outdated quickly . also , i apologize for the lack of figures , but my time on this project is limited and the slides should compensate . finally , this bibliography is , by definition , this book is now out of date . both pat langley and tom mitchell are in the process of writing textbooks on this subject , but we ' re still waiting for them . until then , i suggest looking at both the readings and the recent ml conference proceedings ( both international and european ) . there are also a few introductory papers on this subject , though i haven ' t gotten around to putting them in here yet . however , pat langley and dennis kibler ( #NUM# ) have written a good paper on ml as an empirical science , and pat has written several editorials of use to the ml author ( langley #NUM# ; #NUM# ; #NUM# ) . incomplete , and i ' ve left out many other references that may be of some use .
a bayesian approach to multivariate adaptive regression spline ( mars ) fitting ( friedman , #NUM# ) is proposed . this takes the form of a probability distribution over the space of possible mars models which is explored using reversible jump markov chain monte carlo methods ( green , #NUM# ) . the generated sample of mars models produced is shown to have good predictive power when averaged and allows easy interpretation of the relative importance of predictors to the overall fit .
resent allowed sequences of resolution steps for the initial theory . there are , however , many characterizations of allowed sequences of resolution steps that cannot be expressed by a set of resolvents . one approach to this problem is presented , the system mer - lin , which is based on an earlier technique for learning finite - state automata that represent allowed sequences of resolution steps . merlin extends the previous technique in three ways : i ) negative examples are considered in addition to positive examples , ii ) a new strategy for performing generalization is used , and iii ) a technique for converting the learned automaton to a logic program is included . results from experiments are presented in which merlin outperforms both a system using the old strategy for performing generalization , and a traditional covering technique . the latter result can be explained by the limited expressiveness of hypotheses produced by covering and also by the fact that covering needs to produce the correct base clauses for a recursive definition before
we discuss how the ideas of producing perfect simulations based on coupling from the past for finite state space models naturally extend to mul - tivariate distributions with infinite or uncountable state spaces such as auto - gamma , auto - poisson and auto - negative - binomial models , using gibbs sampling in combination with sandwiching methods originally introduced for perfect simulation of point processes .
we apply recent results on the minimax risk in density estimation to the related problem of pattern classification . the notion of loss we seek to minimize is an information theoretic measure of how well we can predict the classification of future examples , given the classification of previously seen examples . we give an asymptotic characterization of the minimax risk in terms of the metric entropy properties of the class of distributions that might be generating the examples . we then use these results to characterize the minimax risk in the special case of noisy two - valued classification problems in terms of the assouad density and the
genetic algorithms ( gas ) have been extensively used in different domains as a means of doing global optimization in a simple yet reliable manner . they have a much better chance of getting to global optima than gradient based methods which usually converge to local sub optima . however , gas have a tendency of getting only moderately close to the optima in a small number of iterations . to get very close to the optima , the ga needs a very large number of iterations . whereas gradient based optimizers usually get very close to local optima in a relatively small number of iterations . in this paper we describe a new crossover operator which is designed to endow the ga with gradient - like abilities without actually computing any gradients and without sacrificing global optimality . the operator works by using guidance from all members of the ga population to select a direction for exploration . empirical results in two engineering design domains and across both binary and floating point representations demonstrate that the operator can significantly improve the steady state error of the ga optimizer .
neural networks are trained for balancing #NUM# and #NUM# poles attached to a cart on a fixed track . for one variant of the single pole system , only pole angle and cart position variables are supplied as inputs ; the network must learn to compute velocities . all of the problems are solved using a fixed architecture and using a new version of cellular encoding that evolves an application specific architecture with real - valued weights . the learning times and generalization capabilities are compared for neural networks developed using both methods . after a post processing simplification , topologies produced by cellular encoding were very simple and could be analyzed . architectures with no hidden units were produced for the single pole and the two pole problem when velocity information is supplied as an input . moreover , these linear solutions display good generalization . for all the control problems , cellular encoding can automatically generate architectures whose complexity and structure reflect the features of the problem to solve .
a recent result of jun liu ' s has shown how to compute explicitly the eigen - values and eigenvectors for the markov chain derived from a special case of the hastings sampling algorithm , known as the indepdendence metropolis sampler . in this note , we show how to extend the result to obtain exact n - step transition probabilities for any n . this is done first for a chain on a finite state space , and then extended to a general ( discrete or continuous ) state space . the paper concludes with some implications for diagnostic tests of convergence of markov chain samplers .
it is well known that search - space reformulation can improve the speed and reliability of numerical optimization in engineering design . we argue that the best choice of reformulation depends on the design goal , and present a technique for automatically constructing rules that map the design goal into a reformulation chosen from a space of possible reformulations . we tested our technique in the domain of racing - yacht - hull design , where each reformulation corresponds to incorporating constraints into the search space . we applied a standard inductive - learning algorithm , c #NUM# . #NUM# , to a set of training data describing which constraints are active in the optimal design for each goal encountered in a previous design session . we then used these rules to choose an appropriate reformulation for each of a set of test cases . our experimental results show that using these reformulations improves both the speed and the reliability of design optimization , outperforming competing methods and approaching the best performance possible .
we apply the reduction to their two core children , the total sum of their matching weights becomes o ( n ) , and if for each comparison of a spine node and a critical node we apply the reduction to the core child of the spine node , the total sum of their matching weights becomes o ( n ) . with regards to the o ( #NUM# ) comparisons of two critical nodes , their sum cannot exceed o ( #NUM# n ) in total weight . thus , since we have a total of o ( n ) edges involved in the matchings , in time o ( n ) , we can reduce the total sum of the matching weights to o ( #NUM# n ) . theorem #NUM# . #NUM# let m : ir #NUM# ! ir be a monotone function bounding the time complexity uwbm . moreover , let m satisfy that m ( x ) = x #NUM# + " f ( x ) , where " #NUM# is a constant , f ( x ) = o ( x o ( #NUM# ) ) , f is monotone , and for some constants b #NUM# ; b #NUM# , #NUM# x ; y b #NUM# : f ( xy ) b #NUM# f ( x ) f ( y ) . then , with = " p in time o ( n #NUM# + o ( #NUM# ) + m ( n ) ) . proof : we spend o ( n polylog n + time ( uwbm ( k #NUM# n ) ) ) on the matchings . so , by theorem #NUM# . #NUM# , we have that comp - core - trees can be computed in time o ( n polylog n + time ( uwbm ( k #NUM# n ) ) ) . applying theo #NUM# = #NUM# , we get corollary #NUM# . #NUM# mast is computable in time o ( n #NUM# : #NUM# log n ) . [ #NUM# ] w . h . e . day . computational complexity of inferring phylogenies from dissimilarity matrices . bulletin of mathematical biology , #NUM# ( #NUM# ) : #NUM# - #NUM# , #NUM# . [ #NUM# ] m . farach , s . kannan , and t . warnow . a robust model for finding optimal evolutionary trees . al - gorithmica , #NUM# . in press . see also stoc ' #NUM# .
we report results on vowel and stop consonant recognition with tokens extracted from the timit database . our current system differs from others doing similar tasks in that we do not use any specific time normalization techniques . we use a very detailed biologically motivated input representation of the speech tokens - lyon ' s cochlear model as implemented by slaney [ #NUM# ] . this detailed , high dimensional representation , known as a cochleagram , is classified by either a back - propagation or by a hybrid supervised / unsupervised neural network classifier . the hybrid network is composed of a biologically motivated unsupervised network and a supervised back - propagation network . this approach produces results comparable to those obtained by others without the addition of time normalization .
two self - organising controller networks are presented in this study . the " clustered controller network " ( ccn ) uses a spatial clustering approach to select the controllers at each instant . in the other gated controller network , the " models - controller network " ( mcn ) , it is the performance of the model attached to each controller which is used to achieve the controller selection . an algorithm to automaticly conctrust the architecture of both networks is described . it makes the two schemes self - organising . different examples of control of non - linear systems are considered in order to illustrate the behaviour of the iccn and the imcn . it makes clear that both these schemes are performing much better than a single adaptive controller . the two main advantages of the iccn over the imcn concern the possibilities to use any controller as a building block of its network architecture and to apply the iccn for modelling purpose . however the iccn appears to have serious problems to cope with non - linear systems having more than a single variable implying a non - linear behaviour . the imcn does not suffer from this trouble . this high sensitivity to the clustering space order is the main drawback limiting the use of the iccn and therefore makes the imcn a much more suitable approach to control a wide range of non - linear systems .
we investigate the problem of estimating the proportion vector which maximizes the likelihood of a given sample for a mixture of given densities . we adapt a framework developed for supervised learning and give simple derivations for many of the standard iterative algorithms like gradient projection and em . in this framework , the distance between the new and old proportion vectors is used as a penalty term . the square distance leads to the gradient projection update , and the relative entropy to a new update which we call the exponentiated gradient update ( eg ) . curiously , when a second order taylor expansion of the relative entropy is used , we arrive at an update em which , for = #NUM# , gives the usual em update . experimentally , both the em - update and the eg - update for & gt ; #NUM# outperform the em algorithm and its variants . we also prove a polynomial bound on the worst - case global rate of convergence of the eg algorithm .
this paper compares direct reinforcement learning ( no explicit model ) and model - based reinforcement learning on a simple task : pendulum swing up . we find that in this task model - based approaches support reinforcement learning from smaller amounts of training data and efficient handling of changing goals .
this article compares the traditional , fixed problem representation style of a genetic algorithm ( ga ) with a new floating representation in which the building blocks of a problem are not fixed at specific locations on the individuals of the population . in addition , the effects of non - coding segments on both of these representations is studied . non - coding segments are a computational model of non - coding dna and floating building blocks mimic the location independence of genes . the fact that these structures are prevalent in natural genetic systems suggests that they may provide some advantages to the evolutionary process . our results show that there is a significant difference in how gas solve a problem in the fixed and floating representations . gas are able to maintain a more diverse population with the floating representation . the combination of non - coding segments and floating building blocks appears to encourage a ga to take advantage of its parallel search and recombination abilities .
we extend hoeffding bounds to develop superior probabilistic performance guarantees for accurate classifiers . the original hoeffding bounds on classifier accuracy depend on the accuracy itself as a parameter . since the accuracy is not known a priori , the parameter value that gives the weakest bounds is used . we present a method that loosely bounds the accuracy using the old method and uses the loose bound as an improved parameter value for tighter bounds . we show how to use the bounds in practice , and we generalize the bounds for individual classifiers to form uniform bounds over multiple classifiers .
evolving cooperative groups : preliminary results abstract multi - agent systems require coordination of sources with distinct expertise to perform complex tasks effectively . in this paper , we use co - evolutionary approach using genetic algorithms to evolve multiple individuals who can effectively cooperate to solve a common problem . we concurrently run a ga for each individual in the group . in this paper , we experiment with a room painting domain which requires cooperation of two agents . we have used two mechanisms for evaluating an individual in one population : ( a ) pair it randomly with members from the other population , ( b ) pair it with members of the other population in a shared memory containing the best pairs found so far . both the approaches are successful in generating optimal behavior patterns . however , our preliminary results exhibit a slight edge for the shared memory approach .
co - evolution refers to the simultaneous evolution of two or more genetically distinct populations with coupled fitness landscapes . in this paper we consider " competitive co - evolution , " in which the fitness of an individual in a " host " population is based on direct competition with individual ( s ) from a " parasite " population . competitive coevolution is applied to three game - learning problems : tic - tac - toe ( ttt ) , nim and a small version of go . two new techniques in competitive co - evolution are explored . " competitive fitness sharing " changes the way fitness is measured , and " shared sampling " alters the way parasites are chosen for testing hosts . experiments using ttt and nim show a substantial improvement in performance when these methods are used . preliminary results using co - evolution for the discovery of cellular automata rules for playing go are presented .
we review the use of global and local methods for estimating a function mapping r m ) r n from samples of the function containing noise . the relationship between the methods is examined and an empirical comparison is performed using the multi - layer perceptron ( mlp ) global neural network model , the single nearest - neighbour model , a linear local approximation ( la ) model , and the following commonly used datasets : the mackey - glass chaotic time series , the sunspot time series , british english vowel data , timit speech phonemes , building energy prediction data , and the sonar dataset . we find that the simple local approximation models often outperform the mlp . no criterion such as classification / prediction , size of the training set , dimensionality of the training set , etc . can be used to distinguish whether the mlp or the local approximation method will be superior . however , we find that if we consider histograms of the k - nn density estimates for the training datasets then we can choose the best performing method a priori by selecting local approximation when the spread of the density histogram is large and choosing the mlp otherwise . this result correlates with the hypothesis that the global mlp model is less appropriate when the characteristics of the function to be approximated varies throughout the input space . we discuss the results , the smoothness assumption often made in function approximation , and the bias / variance dilemma .
we present an implementation of kohonen self - organizing feature maps for the spert - ii vector microprocessor system . the implementation supports arbitrary neural map topologies and arbitrary neighborhood functions . for small networks , as used in real - world tasks , a single spert - ii board is measured to run kohonen net classification at up to #NUM# million connections per second ( mcps ) . on a speech coding benchmark task , spert - ii performs on - line kohonen net training at over #NUM# million connection updates per second ( mcups ) . this represents almost a factor of #NUM# improvement compared to previously reported implementations . the asymptotic peak speed of the system is #NUM# mcps and #NUM# mcups .
the simple bayesian classifier ( sbc ) is commonly thought to assume that attributes are independent given the class , but this is apparently contradicted by the surprisingly good performance it exhibits in many domains that contain clear attribute dependences . no explanation for this has been proposed so far . in this paper we show that the sbc does not in fact assume attribute independence , and can be optimal even when this assumption is violated by a wide margin . the key to this finding lies in the distinction between classification and probability estimation : correct classification can be achieved even when the probability estimates used contain large errors . we show that the previously - assumed region of optimality of the sbc is a second - order infinitesimal fraction of the actual one . this is followed by the derivation of several necessary and several sufficient conditions for the optimality of the sbc . for example , the sbc is optimal for learning arbitrary conjunctions and disjunctions , even though they violate the independence assumption . the paper also reports empirical evidence of the sbc ' s competitive performance in domains containing substantial degrees of attribute dependence .
we propose a method to use inductive logic programming to give heuristic functions for searching goals to solve problems . the method takes solutions of a problem or a history of search and a set of background knowledge on the problem . in a large class of problems , a problem is described as a set of states and a set of operators , and is solved by finding a series of operators . a solution , a series of operators that brings an initial state to a final state , is transformed into positive and negative examples of a relation " better - choice " , which describes that an operator is better than others in a state . we also give a way to use the " better - choice " relation as a heuristic function . the method can use any logic program as background knowledge to induce heuristics , and induced heuristics has high readability . the paper inspects the method by applying to a puzzle .
we describe the development of a monitoring system which uses sensor observation data about discrete events to construct dynamically a probabilistic model of the world . this model is a bayesian network incorporating temporal aspects , which we call a dynamic belief network ; it is used to reason under uncertainty about both the causes and consequences of the events being monitored . the basic dynamic construction of the network is data - driven . however the model construction process combines sensor data about events with externally provided information about agents ' behaviour , and knowledge already contained within the model , to control the size and complexity of the network . this means that both the network structure within a time interval , and the amount of history and detail maintained , can vary over time . we illustrate the system with the example domain of monitoring robot vehicles and people in a restricted dynamic environment using light - beam sensor data . in addition to presenting a generic network structure for monitoring domains , we describe the use of more complex network structures which address two specific monitoring problems , sensor validation and the data association problem .
we evaluate the power of decision tables as a hypothesis space for supervised learning algorithms . decision tables are one of the simplest hypothesis spaces possible , and usually they are easy to understand . experimental results show that on artificial and real - world domains containing only discrete features , idtm , an algorithm inducing decision tables , can sometimes outperform state - of - the - art algorithms such as c #NUM# . #NUM# . surprisingly , performance is quite good on some datasets with continuous features , indicating that many datasets used in machine learning either do not require these features , or that these features have few values . we also describe an incremental method for performing cross - validation that is applicable to incremental learning algorithms including idtm . using incremental cross - validation , it is possible to cross - validate a given dataset and idtm in time that is linear in the number of instances , the number of features , and the number of label values . the time for incremental cross - validation is independent of the number of folds chosen , hence leave - one - out cross - validation and ten - fold cross - validation take the same time .
in the wrapper approach to feature subset selection , a search for an optimal set of features is made using the induction algorithm as a black box . the estimated future performance of the algorithm is the heuristic guiding the search . statistical methods for feature subset selection including forward selection , backward elimination , and their stepwise variants can be viewed as simple hill - climbing techniques in the space of feature subsets . we utilize best - first search to find a good feature subset and discuss overfitting problems that may be associated with searching too many feature subsets . we introduce compound operators that dynamically change the topology of the search space to better utilize the information available from the evaluation of feature subsets . we show that compound operators unify previous approaches that deal with relevant and irrelevant features . the improved feature subset selection yields significant improvements for real - world datasets when using the id #NUM# and the naive - bayes induction algorithms .
we have constructed an inexpensive , video - based , motorized tracking system that learns to track a head . it uses real time graphical user inputs or an auxiliary infrared detector as supervisory signals to train a convolutional neural network . the inputs to the neural network consist of normalized luminance and chrominance images and motion information from frame differences . subsampled images are also used to provide scale invariance . during the online training phase , the neural network rapidly adjusts the input weights depending upon the reliability of the different channels in the surrounding environment . this quick adaptation allows the system to robustly track a head even when other objects are moving within a cluttered background .
in this paper , we consider the complexity of a number of combinatorial problems ; namely , intervalizing colored graphs ( dna physical mapping ) , triangulating colored graphs ( perfect phylogeny ) , ( directed ) ( modified ) colored cutwidth , feasible register assignment and module allocation for graphs of bounded treewidth . each of these problems has as a characteristic a uniform upper bound on the tree or path width of the graphs in " yes " - instances . for all of these problems with the exceptions of feasible register assignment and module allocation , a vertex or edge coloring is given as part of the input . our main results are that the parameterized variant of each of the considered problems is hard for the complexity classes w [ t ] for all t #NUM# z + . we also show that intervalizing colored graphs , triangulating colored graphs , and
it is well - known that certain learning methods ( e . g . , the perceptron learning algorithm ) cannot acquire complete , parity mappings . but it is often overlooked that state - of - the - art learning methods such as c #NUM# . #NUM# and backpropagation cannot generalise from incomplete parity mappings . the failure of such methods to generalise on parity mappings may be sometimes dismissed on the grounds that it is ` impossible ' to generalise over such mappings , or that parity problems are mathematical constructs having little to do with real - world learning . however , this paper argues that such a dismissal is unwarranted . it shows that parity mappings are hard to learn because they are statistically neutral and that statistical neutrality is a property which we should expect to encounter frequently in real - world contexts . it also shows that the generalization failure on parity mappings occurs even when large , minimally incomplete mappings are used for training purposes , i . e . , when claims about the impossibility of generalization are particularly suspect .
there are several stochastic methods that can be used for solving np - hard optimization problems approximatively . examples of such algorithms include ( in order of increasing computational complexity ) stochastic greedy search methods , simulated annealing , and genetic algorithms . we investigate which of these methods is likely to give best performance in practice , with respect to the computational effort each requires . we study this problem empirically by selecting a set of stochastic algorithms with varying computational complexity , and by experimentally evaluating for each method how the goodness of the results achieved improves with increasing computational time . for the evaluation , we use a graph optimization problem , which is closely related to several real - world practical problems . to get a wider perspective of the goodness of the achieved results , the stochastic methods are also compared against special - case greedy heuristics . this investigation suggests that although genetic algorithms can provide good results , simpler stochastic algorithms can achieve similar performance more quickly .
there are two generations of gibbs sampling methods for semi - parametric models involving the dirichlet process . the first generation suffered from a severe drawback ; namely that the locations of the clusters , or groups of parameters , could essentially become fixed , moving only rarely . two strategies that have been proposed to create the second generation of gibbs samplers are integration and appending a second stage to the gibbs sampler wherein the cluster locations are moved . we show that these same strategies are easily implemented for the sequential importance sampler , and that the first strategy dramatically improves results . as in the case of gibbs sampling , these strategies are applicable to a much wider class of models . they are shown to provide more uniform importance sampling weights and lead to additional rao - blackwellization of estimators . steve maceachern is associate professor , department of statistics , ohio state university , merlise clyde is assistant professor , institute of statistics and decision sciences , duke university , and jun liu is assistant professor , department of statistics , stanford university . the work of the second author was supported in part by the national science foundation grants dms - #NUM# and dms - #NUM# , and that of the last author by the national science foundation grants dms - #NUM# , dms - #NUM# , and the terman fellowship .
we present a unified framework for convergence analysis of the generalized subgradient - type algorithms in the presence of perturbations . one of the principal novel features of our analysis is that perturbations need not tend to zero in the limit . it is established that the iterates of the algorithms are attracted , in a certain sense , to an " - stationary set of the problem , where " depends on the magnitude of perturbations . characterization of those attraction sets is given in the general ( nonsmooth and nonconvex ) case . the results are further strengthened for convex , weakly sharp and strongly convex problems . our analysis extends and unifies previously known results on convergence and stability properties of gradient and subgradient methods , including their incremental , parallel and " heavy ball " modifications .
dr . mccalley ' s research is partially supported through grants from national science foundation and pacific gas and electric company . dr . honavar ' s research is partially supported through grants from national science foundation and the john deere foundation . this paper will appear in : proceedings of the #NUM# th annual north american power symposium , oct . #NUM# - #NUM# . #NUM# , laramie , wyoming .
co - evolutionary learning , which involves the embedding of adaptive learning agents in a fitness environment which dynamically responds to their progress , is a potential solution for many technological chicken and egg problems , and is at the heart of several recent and surprising successes , such as sim ' s artificial robot and tesauro ' s backgammon player . we recently solved the two spirals problem , a difficult neural network benchmark classification problem , using the genetic programming primitives set up by [ koza , #NUM# ] . instead of using absolute fitness , we use a relative fitness [ angeline & pollack , #NUM# ] based on a competition for coverage of the data set . as the population reproduces , the fitness function driving the selection changes , and subproblem niches are opened , rather than crowded out . the solutions found by our method have a symbiotic structure which suggests that by holding niches open , crossover is better able to discover modular build ing blocks .
we show that two cooperating robots can learn exactly any strongly - connected directed graph with n indistinguishable nodes in expected time polynomial in n . we introduce a new type of homing sequence for two robots which helps the robots recognize certain previously - seen nodes . we then present an algorithm in which the robots learn the graph and the homing sequence simultaneously by wandering actively through the graph . unlike most previous learning results using homing sequences , our algorithm does not require a teacher to provide counterexamples . furthermore , the algorithm can use efficiently any additional information available that distinguishes nodes . we also present an algorithm in which the robots learn by taking random walks . the rate at which a random walk converges to the stationary distribution is characterized by the conductance of the graph . our random - walk algorithm learns in expected time polynomial in n and in the inverse of the conductance and is more efficient than the homing - sequence algorithm for high - conductance graphs .
we introduce a model for learning from examples and membership queries in situations where the boundary between positive and negative examples is somewhat ill - defined . in our model , queries near the boundary of a target concept may receive incorrect or " don ' t care " responses , and the distribution of examples has zero probability mass on the boundary region . the motivation behind our model is that in many cases the boundary between positive and negative examples is complicated or " fuzzy . " however , one may still hope to learn successfully , because the typical examples that one sees do not come from that region . we present several positive results in this new model . we show how to learn the intersection of two arbitrary halfspaces when membership queries near the boundary may be answered incorrectly . our algorithm is an extension of an algorithm of baum [ #NUM# , #NUM# ] that learns the intersection of two halfspaces whose bounding planes pass through the origin in the pac - with - membership - queries model . we also describe algorithms for learning several subclasses of monotone dnf formulas .
this paper describes new and efficient algorithms for learning deterministic finite automata . our approach is primarily distinguished by two features : ( #NUM# ) the adoption of an average - case setting to model the " typical " labeling of a finite automaton , while retaining a worst - case model for the underlying graph of the automaton , along with ( #NUM# ) a learning model in which the learner is not provided with the means to experiment with the machine , but rather must learn solely by observing the automaton ' s output behavior on a random input sequence . the main contribution of this paper is in presenting the first efficient algorithms for learning non - trivial classes of automata in an entirely passive learning model . we adopt an on - line learning model in which the learner is asked to predict the output of the next state , given the next symbol of the random input sequence ; the goal of the learner is to make as few prediction mistakes as possible . assuming the learner has a means of resetting the target machine to a fixed start state , we first present an efficient algorithm that makes an expected polynomial number of mistakes in this model . next , we show how this first algorithm can be used as a subroutine by a second algorithm that also makes a polynomial number of mistakes even in the absence of a reset . along the way , we prove a number of combinatorial results for randomly labeled automata . we also show that the labeling of the states and the bits of the input sequence need not be truly random , but merely semi - random . finally , we discuss an extension of our results to a model in which automata are used to represent distributions over binary strings .
this paper presents a comparison of genetic programming ( gp ) with simulated annealing ( sa ) and stochastic iterated hill climbing ( sihc ) based on a suite of program discovery problems which have been previously tackled only with gp . all three search algorithms employ the hierarchical variable length representation for programs brought into recent prominence with the gp paradigm [ #NUM# ] . we feel it is not intuitively obvious that mutation - based adaptive search can handle program discovery yet , to date , for each gp problem we have tried , sa or sihc also work .
given a markov chain sampling scheme , does the standard empirical estimator make best use of the data ? we show that this is not so and construct better estimators . we restrict attention to nearest neighbor random fields and to gibbs samplers with deterministic sweep , but our approach applies to any sampler that uses reversible variable - at - a - time updating with deterministic sweep . the structure of the transition distribution of the sampler is exploited to construct further empirical estimators that are combined with the standard empirical estimator to reduce asymptotic variance . the extra computational cost is negligible . when the random field is spatially homogeneous , symmetrizations of our estimator lead to further variance reduction . the performance of the estimators is evaluated in a simulation study of the ising model .
in order for learning to improve the adaptiveness of an animal ' s behavior and thus direct evolution in the way baldwin suggested , the learning mechanism must incorporate an innate evaluation of how the animal ' s actions influence its reproductive fitness . for example , many circumstances that damage an animal , or otherwise reduce its fitness are painful and tend to be avoided . we refer to the mechanism by which an animal evaluates the fitness consequences of its actions as a " motivation system , " and argue that such a system must evolve along with the behaviors it evaluates . we describe simulations of the evolution of populations of agents instantiating a number of different architectures for generating action and learning , in worlds of differing complexity . we find that in some cases , members of the populations evolve motivation systems that are accurate enough to direct learning so as to increase the fitness of the actions the agents perform . furthermore , the motivation systems tend to incorporate systematic distortions in their representations of the worlds they inhabit ; these distortions can increase the adaptiveness of the behavior generated .
the relation between the orthography and the phonology of a language has traditionally been modelled by hand - crafted rule sets . machine - learning ( ml ) approaches offer a means to gather this knowledge automatically . problems arise when the training material is sparse . generalising from sparse data is a well - known problem for many ml algorithms . we present experiments in which connectionist , instance - based , and decision - tree learning algorithms are applied to a small corpus of scottish gaelic . instance - based learning in the ib #NUM# - ig algorithm yields the best generalisation performance , and that most algorithms tested perform tolerably well . given the availability of a lexicon , even if it is sparse , ml is a valuable and efficient tool for automatic phonetic transcription of written text .
we study the problem of estimating the log spectrum of a stationary gaussian time series by thresholding the empirical wavelet coefficients . we propose the use of thresholds t j ; n depending on sample size n , wavelet basis and resolution level j . at fine resolution levels ( j = #NUM# ; #NUM# ; : : : ) , we propose the purpose of this thresholding level is to make the reconstructed log - spectrum as nearly noise - free as possible . in addition to being pleasant from a visual point of view , the noise - free character leads to attractive theoretical properties over a wide range of smoothness assumptions . previous proposals set much smaller thresholds and did not enjoy these properties . t j ; n = ff j log n ;
data mining algorithms including machine learning , statistical analysis , and pattern recognition techniques can greatly improve our understanding of data warehouses that are now becoming more widespread . in this paper , we focus on classification algorithms and review the need for multiple classification algorithms . we describe a system called mlc + + , which was designed to help choose the appropriate classification algorithm for a given dataset by making it easy to compare the utility of different algorithms on a specific dataset of interest . mlc + + not only provides a workbench for such comparisons , but also provides a library of c + + classes to aid in the development of new algorithms , especially hybrid algorithms and multi - strategy algorithms . such algorithms are generally hard to code from scratch . we discuss design issues , interfaces to other programs , and visualization of the resulting classifiers .
reinforcement learning methods can be applied to control problems with the objective of optimizing the value of a function over time . they have been used to train single neural networks that learn solutions to whole tasks . jacobs and jordan [ #NUM# ] have shown that a set of expert networks combined via a gating network can more quickly learn tasks that can be decomposed . even the decomposition can be learned . inspired by boyan ' s work of modular neural networks for learning with temporal - difference methods [ #NUM# ] , we modify the reinforcement learning algorithm called q - learning to train a modular neural network to solve a control problem . the resulting algorithm is demonstrated on the classical pole - balancing problem . the advantage of such a method is that it makes it possible to deal with complex dynamic control problem effectively by using task decomposition and competitive learning .
this report replicates and extends results reported by naval air warfare center ( nawc ) personnel on the automatic classification of sonar images . they used novel case - based reasoning systems in their empirical studies , but did not obtain comparative analyses using standard classification algorithms . therefore , the quality of the nawc results were unknown . we replicated the nawc studies and also tested several other classifiers ( i . e . , both case - based and otherwise ) from the machine learning literature . these comparisons and their ramifications are detailed in this paper . next , we investigated fala and walker ' s two suggestions for future work ( i . e . , on combining their similarity functions and on an alternative case representation ) . finally , we describe several ways to incorporate additional domain - specific knowledge when applying case - based classifiers to similar tasks .
in case - based reasoning systems , the case adaptation process is traditionally controlled by static libraries of hand - coded adaptation rules . this paper proposes a method for learning adaptation knowledge in the form of adaptation strategies of the type developed and hand - coded by kass [ #NUM# ] . adaptation strategies differ from standard adaptation rules in that they encode general memory search procedures for finding the information needed during case adaptation ; this paper focuses on the issues involved in learning memory search procedures to form the basis of new adaptation strategies . it proposes a method that starts with a small library of abstract adaptation rules and uses introspective reasoning about the system ' s memory organization to generate the memory search plans needed to apply those rules . the search plans are then packaged with the original abstract rules to form new adaptation strategies for future use . this process allows a cbr system not only to learn about its domain , by storing the results of case adaptation , but also to learn how to apply the cases in its memory more effectively .
in artificial intelligence , psychology , and education , a growing body of research supports the view that learning is a goal - directed process . psychological experiments show that people with different goals process information differently ; studies in education show that goals have strong effects on what students learn ; and functional arguments from machine learning support the necessity of goal - based focusing of learner effort . at the fourteenth annual conference of the cognitive science society , a symposium brought together researchers in ai , psychology , and education to discuss goal - driven learning . this article presents the fundamental points illuminated by the symposium , placing them in the context of open questions and current research di rections in goal - driven learning .
we present a new method , inspired by the bootstrap , whose goal it is to determine the quality and reliability of a neural network predictor . our method leads to more robust forecasting along with a large amount of statistical information on forecast performance that we exploit . we exhibit the method in the context of multi - variate time series prediction on financial data from the new york stock exchange . it turns out that the variation due to different resamplings ( i . e . , splits between training , cross - validation , and test sets ) is significantly larger than the variation due to different network conditions ( such as architecture and initial weights ) . furthermore , this method allows us to forecast a probability distribution , as opposed to the traditional case of just a single value at each time step . we demonstrate this on a strictly held - out test set that includes the #NUM# stock market crash . we also compare the performance of the class of neural networks to identically bootstrapped linear models .
we present a new method for obtaining local error bars for nonlinear regression , i . e . , estimates of the confidence in predicted values that depend on the input . we approach this problem by applying a maximum - likelihood framework to an assumed distribution of errors . we demonstrate our method first on computer - generated data with locally varying , normally distributed target noise . we then apply it to laser data from the santa fe time series competition where the underlying system noise is known quantization error and the error bars give local estimates of model misspecification . in both cases , the method also provides a weighted - regression effect that improves generalization performance .
pinsker ( #NUM# ) gave a precise asymptotic evaluation of the minimax mean squared error of estimation of a signal in gaussian noise when the signal is known a priori to lie in a compact ellipsoid in hilbert space . this ` minimax bayes ' method can be applied to a variety of global non - parametric estimation settings with parameter spaces far from ellipsoidal . for example it leads to a theory of exact asymptotic minimax estimation over norm balls in besov and triebel spaces using simple co - ordinatewise estimators and wavelet bases . this paper outlines some features of the method common to several applications . in particular , we derive new results on the exact asymptotic minimax risk over weak ` p balls in r n as n ! #NUM# , and also for a class of ` local ' estimators on the triebel scale . by its very nature , the method reveals the structure of asymptotically least favorable distributions . thus we may simulate ` least favorable ' sample paths . we illustrate this for estimation of a signal in gaussian white noise over norm balls in certain besov spaces . in wavelet bases , when p & lt ; #NUM# , the least favorable priors are sparse , and the resulting sample paths strikingly different from those observed in pinsker ' s ellipsoidal setting ( p = #NUM# ) . acknowledgements . i am grateful for many conversations with david donoho and carl taswell , and to a referee for helpful comments . this work was supported in part by nsf grants dms #NUM# - #NUM# , #NUM# , and nih phs grant gm #NUM# - #NUM# .
a proper choice of a proposal distribution for mcmc methods , e . g . for the metropolis - hastings algorithm , is well known to be a crucial factor for the convergence of the algorithm . in this paper we introduce an adaptive metropolis algorithm ( am ) , where the gaussian proposal distribution is updated along the process using the full information cumulated so far . due to the adaptive nature of the process , the am algorithm is non - markovian , but we establish here that it has the correct ergodic properties . we also include the results of our numerical tests , which indicate that the am algorithm competes well with traditional metropolis - hastings algorithms , and demonstrate that am provides an easy to use algorithm for practical computation . #NUM# mathematics subject classification : #NUM# c #NUM# , #NUM# u #NUM# . keywords : adaptive mcmc , comparison , convergence , ergodicity , markov chain
we had previously shown that regularization principles lead to approximation schemes which are equivalent to networks with one layer of hidden units , called regularization networks . in particular , standard smoothness functionals lead to a subclass of regularization networks , the well known radial basis functions approximation schemes . this paper shows that regularization networks encompass a much broader range of approximation schemes , including many of the popular general additive models and some of the neural networks . in particular , we introduce new classes of smoothness functionals that lead to different classes of basis functions . additive splines as well as some tensor product splines can be obtained from appropriate classes of smoothness functionals . furthermore , the same generalization that extends radial basis functions ( rbf ) to hyper basis functions ( hbf ) also leads from additive models to ridge approximation models , containing as special cases breiman ' s hinge functions , some forms of projection pursuit regression and several types of neural networks . we propose to use the term generalized regularization networks for this broad class of approximation schemes that follow from an extension of regularization . in the probabilistic interpretation of regularization , the different classes of basis functions correspond to different classes of prior probabilities on the approximating function spaces , and therefore to different types of smoothness assumptions . in summary , different multilayer networks with one hidden layer , which we collectively call generalized regularization networks , correspond to different classes of priors and associated smoothness functionals in a classical regularization principle . three broad classes are a ) radial basis functions that can be generalized to hyper basis functions , b ) some tensor product splines , and c ) additive splines that can be generalized to schemes of the type of ridge approximation , hinge functions and several perceptron - like neural networks with one - hidden layer . #NUM# this paper will appear on neural computation , vol . #NUM# , pages #NUM# - #NUM# , #NUM# . an earlier version of
in many cases programs length ' s increase ( known as " bloat " , " fluff " and increasing " structural complexity " ) during artificial evolution . we show bloat is not specific to genetic programming and suggest it is inherent in search techniques with discrete variable length representations using simple static evaluation functions . we investigate the bloating characteristics of three non - population and one population based search techniques using a novel mutation operator . an artificial ant following the santa fe trail problem is solved by simulated annealing , hill climbing , strict hill climbing and population based search using two variants of the the new subtree based mutation operator . as predicted bloat is observed when using unbiased mutation and is absent in simulated annealing and both hill climbers when using the length neutral mutation however bloat occurs with both mutations when using a population . we conclude that there are two causes of bloat .
we propose a probabilistic case - space metric for the case matching and case adaptation tasks . central to our approach is a probability propagation algorithm adopted from bayesian reasoning systems , which allows our case - based reasoning system to perform theoretically sound probabilistic reasoning . the same probability propagation mechanism actually offers a uniform solution to both the case matching and case adaptation problems . we also show how the algorithm can be implemented as a connectionist network , where efficient massively parallel case retrieval is an inherent property of the system . we argue that using this kind of an approach , the difficult problem of case indexing can be completely avoided . pp . #NUM# - #NUM# in topics in case - based reasoning , edited by stefan wess , klaus - dieter althoff and michael m . richter . volume #NUM# , lecture
this paper describes how through simple means a genetic search towards optimal neural network architectures can be improved , both in the convergence speed as in the quality of the final result . this result can be theoretically explained with the baldwin effect , which is implemented here not just by the learning process of the network alone , but also by changing the network architecture as part of the learning procedure . this can be seen as a combination of two different techniques , both help ing and improving on simple genetic search .
during the past few years several nonparametric alternatives to the cox proportional hazards model have appeared in the literature . these methods extend techniques that are well known from regression analysis to the analysis of censored survival data . in this paper we discuss methods based on ( partition ) trees and ( polynomial ) splines , analyze two datasets using both survival trees [ #NUM# ] and hare [ #NUM# ] , and compare the strengths and weaknesses of the two methods . one of the strengths of hare is that its model fitting procedure has an implicit check for proportionality of the underlying hazards model . it also provides an explicit model for the conditional hazards function , which makes it very convenient to obtain graphical summaries . on the other hand , the tree - based methods automatically partition a dataset into groups of cases that are similar in survival history . results obtained by survival trees and hare are often complimentary . trees and splines in survival analysis should provide the data analyst with two useful tools when analyzing survival data .
many of the lower - level areas in the mammalian visual system are organized retinotopically , that is , as maps which preserve to a certain degree the topography of the retina . a unit that is a part of such a retinotopic map normally responds selectively to stimulation in a well - delimited part of the visual field , referred to as its receptive field ( rf ) . receptive fields are probably the most prominent and ubiquitous computational mechanism employed by biological information processing systems . this paper surveys some of the possible computational reasons behind the ubiquity of rfs , by discussing examples of rf - based solutions to problems in vision , from spatial acuity , through sensory coding , to object recognition .
we address the problem of computing the largest fraction of missing information for the em algorithm and the worst linear function for data augmentation . these are the largest eigenvalue and its associated eigenvector for the jacobian of the em operator at a maximum likelihood estimate , which are important for assessing convergence in iterative simulation . an estimate of the largest fraction of missing information is available from the em iterates ; this is often adequate since only a few figures of accuracy are needed . in some instances the em iteration also gives an estimate of the worst linear function . we show that the power method for eigencomputation can be used to compute efficient and accurate estimates of both quantities . unlike eigenvalue decomposition , the power method computes only the largest eigenvalue and eigenvector of a matrix , it can take advantage of a good eigenvector estimate as an initial value and it can be terminated after only a few figures of accuracy are obtained . moreover , the matrix products needed in the power method can be computed by extrapolation , obviating the need to form the jacobian of the em operator . we give results of simultation studies on multivariate normal data showing this approach becomes more efficient as the data dimension increases than methods that use a finite - difference approximation to the jacobian , which is the only general - purpose alternative available .
we describe a directed acyclic graphical model that contains a hierarchy of linear units and a mechanism for dynamically selecting an appropriate subset of these units to model each observation . the non - linear selection mechanism is a hierarchy of binary units each of which gates the output of one of the linear units . there are no connections from linear units to binary units , so the generative model can be viewed as a logistic belief net ( neal #NUM# ) which selects a skeleton linear model from among the available linear units . we show that gibbs sampling can be used to learn the parameters of the linear and binary units even when the sampling is so brief that the markov chain is far from equilibrium .
we describe a simple reduction from the problem of pac - learning from multiple - instance examples to that of pac - learning with one - sided random classification noise . thus , all concept classes learnable with one - sided noise , which includes all concepts learnable in the usual #NUM# - sided random noise model plus others such as the parity function , are learnable from multiple - instance examples . we also describe a more efficient ( and somewhat technically more involved ) reduction to the statistical - query model that results in a polynomial - time algorithm for learning axis - parallel rectangles with sample complexity ~ o ( d #NUM# r = * #NUM# ) , saving roughly a factor of r over the results of auer et al . ( #NUM# ) .
the absence of powerful control structures and processes that synchronize , coordinate , switch between , choose among , regulate , direct , modulate interactions between , and combine distinct yet interdependent modules of large connectionist networks ( cn ) is probably one of the most important reasons why such networks have not yet succeeded at handling difficult tasks ( e . g . complex object recognition and description , complex problem - solving , planning ) . in this paper we examine how cn built from large numbers of relatively simple neuron - like units can be given the ability to handle problems that in typical multi - computer networks and artificial intelligence programs along with all other types of programs are always handled using extremely elaborate and precisely worked out central control ( coordination , synchronization , switching , etc . ) . we point out the several mechanisms for central control of this un - brain - like sort that cn already have built into them albeit in hidden , often overlooked , ways . we examine the kinds of control mechanisms found in computers , programs , fetal development , cellular function and the immune system , evolution , social organizations , and especially brains , that might be of use in cn . particularly intriguing suggestions are found in the pacemakers , oscillators , and other local sources of the brain ' s complex partial synchronies ; the diffuse , global effects of slow electrical waves and neurohormones ; the developmental program that guides fetal development ; communication and coordination within and among living cells ; the working of the immune system ; the evolutionary processes that operate on large populations of organisms ; and the great variety of partially competing partially cooperating controls found in small groups , organizations , and larger societies . all these systems are rich in control but typically control that emerges from complex interactions of many local and diffuse sources . we explore how several different kinds of plausible control mechanisms might be incorporated into cn , and assess their potential benefits with respect to their cost .
in drug activity prediction ( as in handwritten character recognition ) , the features extracted to describe a training example depend on the pose ( location , orientation , etc . ) of the example . in handwritten character recognition , one of the best techniques for addressing this problem is the tangent distance method of simard , lecun and denker ( #NUM# ) . jain , et al . ( #NUM# a ; #NUM# b ) introduce a new technique | dynamic reposing | that also addresses this problem . dynamic reposing iteratively learns a neural network and then reposes the examples in an effort to maximize the predicted output values . new models are trained and new poses computed until models and poses converge . this paper compares dynamic reposing to the tangent distance method on the task of predicting the biological activity of musk compounds . in a #NUM# - fold cross - validation ,
genetic algorithms and related evolutionary techniques offer a promising approach for automatically exploring the design space of neural architectures for artificial intelligence and cognitive modeling . central to this process of evolutionary design of neural architectures ( edna ) is the choice of the representation scheme that is used to encode a neural architecture in the form of a gene string ( genotype ) and to decode a genotype into the corresponding neural architecture ( phenotype ) . the representation scheme used not only constrains the class of neural architectures that are representable ( evolvable ) in the system , but also determines the efficiency and the time - space complexity of the evolutionary design procedure as a whole . this paper identifies and discusses a set of properties that can be used to characterize different representations used in edna and to design or select representations with the necessary properties for particular classes of applications .
and load balancing even for irregular neural networks . the idea to achieve these goals lies in the programming model : cupit programs are object - centered , with connections and nodes of a graph ( which is the neural network ) being the objects . algorithms are based on parallel local computations in the nodes and connections and communication along the connections ( plus broadcast and reduction operations ) . this report describes the design considerations and the resulting language definition and discusses in detail a tutorial example program .
when a reasoner explains surprising events for its internal use , a key motivation for explaining is to perform learning that will facilitate the achievement of its goals . human explainers use a range of strategies to build explanations , including both internal reasoning and external information search , and goal - based considerations have a profound effect on their choices of when and how to pursue explanations . however , standard ai models of explanation rely on goal - neutral use of a single fixed strategy | generally backwards chaining | to build their explanations . this paper argues that explanation should be modeled as a goal - driven learning process for gathering and transforming information , and discusses the issues involved in developing an active multi - strategy process for goal - driven explanation .
rf - lissom , a self - organizing model of laterally connected orientation maps in the primary visual cortex , was used to study the psychological phenomenon known as the tilt aftereffect . the same self - organizing processes that are responsible for the long - term development of the map and its lateral connections are shown to result in tilt aftereffects over short time scales in the adult . the model allows observing large numbers of neurons and connections simultaneously , making it possible to relate higher - level phenomena to low - level events , which is difficult to do experimentally . the results give computational support for the idea that direct tilt aftereffects arise from adaptive lateral interactions between feature detectors , as has long been surmised . they also suggest that indirect effects could result from the conservation of synaptic resources during this process . the model thus provides a unified computational explanation of self - organization and both direct and indirect tilt aftereffects in the primary visual cortex .
genetic programming is an automatic programming technique that evolves computer programs to solve , or approximately solve , problems . this paper presents two examples in which genetic programming creates a computer program for controlling a robot so that the robot moves to a specified destination point in minimal time . in the first approach , genetic programming evolves a computer program composed of ordinary a r i t h m e t i c o p e r a t i o n s a n d
in designing autonomous agents that deal competently with issues involving time and space , there is a tradeoff to be made between guaranteed response - time reactions on the one hand , and flexibility and expressiveness on the other . we propose a model of action with probabilistic reasoning and decision analytic evaluation for use in a layered control architecture . our model is well suited to tasks that require reasoning about the interaction of behaviors and events in a fixed temporal horizon . decisions are continuously reevaluated , so that there is no problem with plans becoming obsolete as new information becomes available . in this paper , we are particularly interested in the tradeoffs required to guarantee a fixed reponse time in reasoning about nondeterministic cause - and - effect relationships . by exploiting approximate decision making processes , we are able to trade accuracy in our predictions for speed in decision making in order to improve expected per formance in dynamic situations .
we propose and examine a method of approximate dynamic programming for markov decision processes based on structured problem representations . we assume an mdp is represented using a dynamic bayesian network , and construct value functions using decision trees as our function representation . the size of the representation is kept within acceptable limits by pruning these value trees so that leaves represent possible ranges of values , thus approximating the value functions produced during optimization . we propose a method for detecting convergence , prove errors bounds on the resulting approximately optimal value functions and policies , and describe some preliminary experi mental results .
the majority of commercial computers today are register machines of von neumann type . we have developed a method to evolve turing - complete programs for a register machine . the described implementation enables the use of most program constructs , such as arithmetic operators , large indexed memory , automatic decomposition into subfunctions and subroutines ( adfs ) , conditional constructs i . e . if - then - else , jumps , loop structures , recursion , protected functions , string and list functions . any c - function can be compiled and linked into the function set of the system . the use of register machine language allows us to work at the lowest level of binary machine code without any interpreting steps . in a von neumann machine , programs and data reside in the same memory and the genetic operators can thus directly manipulate the binary machine code in memory . the genetic operators themselves are written in c - language but they modify individuals in binary representation . the result is an execution speed enhancement of up to #NUM# times compared to an interpreting c - language implementation , and up to #NUM# times compared to a lisp implementation . the use of binary machine code demands a very compact coding of about one byte per node in the individual . the resulting evolved programs are disassembled into c - modules and can be incorporated into a conventional software development environment . the low memory requirements and the significant speed enhancement of this technique could be of use when applying genetic programming to new application areas , platforms and research domains .
this paper considers the importance of exploration to game - playing programs which learn by playing against opponents . the central question is whether a learning program should play the move which offers the best chance of winning the present game , or if it should play the move which has the best chance of providing useful information for future games . an approach to addressing this question is developed using probability theory , and then implemented in two different learning methods . initial experiments in the game of go suggest that a program which takes exploration into account can learn better against a knowledgeable opponent than a program which does not .
this article describes an approach to combining symbolic and connectionist approaches to machine learning . a three - stage framework is presented and the research of several groups is reviewed with respect to this framework . the first stage involves the insertion of symbolic knowledge into neural networks , the second addresses the refinement of this prior knowledge in its neural representation , while the third concerns the extraction of the refined symbolic knowledge . experimental results and open research issues are discussed . a shorter version of this paper will appear in machine learning .
a distributed neural network model called spec for processing sentences with recursive relative clauses is described . the model is based on separating the tasks of segmenting the input word sequence into clauses , forming the case - role representations , and keeping track of the recursive embeddings into different modules . the system needs to be trained only with the basic sentence constructs , and it generalizes not only to new instances of familiar relative clause structures , but to novel structures as well . spec exhibits plausible memory degradation as the depth of the center embeddings increases , its memory is primed by earlier constituents , and its performance is aided by semantic constraints between the constituents . the ability to process structure is largely due to a central executive network that monitors and controls the execution of the entire system . this way , in contrast to earlier subsymbolic systems , parsing is modeled as a controlled high - level process rather than one based on automatic reflex responses .
in this paper , we propose a memory - based q - learning algorithm called predictive q - routing ( pq - routing ) for adaptive traffic control . we attempt to address two problems encountered in q - routing ( boyan & littman , #NUM# ) , namely , the inability to fine - tune routing policies under low network load and the inability to learn new optimal policies under decreasing load conditions . unlike other memory - based reinforcement learning algorithms in which memory is used to keep past experiences to increase learning speed , pq - routing keeps the best experiences learned and reuses them by predicting the traffic trend . the effectiveness of pq - routing has been verified under various network topologies and traffic conditions . simulation results show that pq - routing is superior to
several researchers have demonstrated how neural networks can be trained to compensate for nonlinear signal distortion in e . g . digital satellite communications systems . these networks , however , require that both the original signal and its distorted version are known . therefore , they have to be trained off - line , and they cannot adapt to changing channel characteristics . in this paper , a novel dual reinforcement learning approach is proposed that can adapt on - line while the system is performing . assuming that the channel characteristics are the same in both directions , two predistorters at each end of the communication channel co - adapt using the output of the other predistorter to determine their own reinforcement . using the common volterra series model to simulate the channel , the system is shown to successfully learn to compensate for distortions up to #NUM# % , which is significantly higher than what might be expected in an actual channel .
most connectionist modeling assumes noise - free inputs . this assumption is often violated . this paper introduces the idea of clearning , of simultaneously cleaning the data and learning the underlying structure . the cleaning step can be viewed as top - down processing ( where the model modifies the data ) , and the learning step can be viewed as bottom - up processing ( where the data modifies the model ) . clearning is used in conjunction with standard pruning . this paper discusses the statistical foundation of clearning , gives an interpretation in terms of a mechanical model , describes how to obtain both point predictions and conditional densities for the output , and shows how the resulting model can be used to discover properties of the data otherwise not accessible ( such as the signal - to - noise ratio of the inputs ) . this paper uses clearning to predict foreign exchange rates , a noisy time series problem with well - known benchmark performances . on the out - of - sample #NUM# - #NUM# test period , clearning obtains an annualized return on investment above #NUM# % , significantly better than an otherwise identical network . the final ultra - sparse network with #NUM# remaining non - zero input - to - hidden weights ( of the #NUM# initial weights between #NUM# inputs and #NUM# hidden units ) is very robust against overfitting . this small network also lends itself to interpretation .
most connectionist modeling assumes noise - free inputs . this assumption is often violated . this paper introduces the idea of clearning , of simultaneously cleaning the data and learning the underlying structure . the cleaning step can be viewed as top - down processing ( where the model modifies the data ) , and the learning step can be viewed as bottom - up processing ( where the data modifies the model ) . clearning is used in conjunction with standard pruning . this paper discusses the statistical foundation of clearning , gives an interpretation in terms of a mechanical model , describes how to obtain both point predictions and conditional densities for the output , and shows how the resulting model can be used to discover properties of the data otherwise not accessible ( such as the signal - to - noise ratio of the inputs ) . this paper uses clearning to predict foreign exchange rates , a noisy time series problem with well - known benchmark performances . on the out - of - sample #NUM# - #NUM# test period , clearning obtains an annualized return on investment above #NUM# % , significantly better than an otherwise identical network . the final ultra - sparse network with #NUM# remaining non - zero input - to - hidden weights ( of the #NUM# initial weights between #NUM# inputs and #NUM# hidden units ) is very robust against overfitting . this small network also lends itself to interpretation .
we discuss a bayesian formalism which gives rise to a type of wavelet threshold estimation in non - parametric regression . a prior distribution is imposed on the wavelet coefficients of the unknown response function , designed to capture the sparseness of wavelet expansion common to most applications . for the prior specified , the posterior median yields a thresholding procedure . our prior model for the underlying function can be adjusted to give functions falling in any specific besov space . we establish a relation between the hyperparameters of the prior model and the parameters of those besov spaces within which realizations from the prior will fall . such a relation gives insight into the meaning of the besov space parameters . moreover , the established relation makes it possible in principle to incorporate prior knowledge about the function ' s regularity properties into the prior model for its wavelet coefficients . however , prior knowledge about a function ' s regularity properties might be hard to elicit ; with this in mind , we propose a standard choise of prior hyperparameters that works well in our examples . several simulated examples are used to illustrate our method , and comparisons are made with other thresholding methods . we also present an application to a data set collected in an anaesthesiological study .
the perfect phylogeny problem is a classical problem in computational evolutionary biology , in which a set of species / taxa is described by a set of qualitative characters . in recent years , the problem has been shown to be np - complete in general , while the different fixed parameter versions can each be solved in polynomial time . in particular , agarwala and fernandez - baca have developed an o ( #NUM# #NUM# r ( nk #NUM# + k #NUM# ) ) algorithm for the perfect phylogeny problem for n species defined by k r - state characters . since commonly the character data is drawn from alignments of molecular sequences , k is the length of the sequences and can thus be very large ( in the hundreds or thousands ) . thus , it is imperative to develop algorithms which run efficiently for large values of k . in this paper we make additional observations about the structure of the problem and produce an algorithm for the problem that runs in time o ( #NUM# #NUM# r k #NUM# n ) . we also show how it is possible to efficiently build a structure that implicitly represents the set of all perfect phylogenies , and to randomly sample from that set .
belief networks ( or probabilistic networks ) and neural networks are two forms of network representations that have been used in the development of intelligent systems in the field of artificial intelligence . belief networks provide a concise representation of general probability distributions over a set of random variables , and facilitate exact calculation of the impact of evidence on propositions of interest . neural networks , which represent parameterized algebraic combinations of nonlinear activation functions , have found widespread use as models of real neural systems and as function approximators because of their amenability to simple training algorithms . furthermore , the simple , local nature of most neural network training algorithms provides a certain biological plausibility and allows for a massively parallel implementation . in this paper , we show that similar local learning algorithms can be derived for belief networks , and that these learning algorithms can operate using only information that is directly available from the normal , inferential processes of the networks . this removes the main obstacle preventing belief networks from competing with neural networks on the above - mentioned tasks . the precise , local , probabilistic interpretation of belief networks also allows them to be partially or wholly constructed by humans ; allows the results of learning to be easily understood ; and allows them to contribute to rational decision - making in a well - defined way .
we present a new parallel algorithm for learning bayesian inference networks from data . our learning algorithm exploits both properties of the mdl - based score metric , and a distributed , asynchronous , adaptive search technique called nagging . nagging is intrinsically fault tolerant , has dynamic load balancing features , and scales well . we demonstrate the viability , effectiveness , and scalability of our approach empirically with several experiments using on the order of #NUM# machines . more specifically , we show that our distributed algorithm can provide optimal solutions for larger problems as well as good solutions for bayesian networks of up to #NUM# variables .
in this article we investigate the relationship between the two popular algorithms , the em algorithm and the gibbs sampler . we show that the approximate rate of convergence of the gibbs sampler by gaussian approximation is equal to that of the corresponding em type algorithm . this helps in implementing either of the algorithms as improvement strategies for one algorithm can be directly transported to the other . in particular , by running the em algorithm we know approximately how many iterations are needed for convergence of the gibbs sampler . we also obtain a result that under conditions , the em algorithm used for finding the maximum likelihood estimates can be slower to converge than the corresponding gibbs sampler for bayesian inference which uses proper prior distributions . we illustrate our results in a number of realistic examples all based on the generalized linear mixed models .
underwater mammal sound classification is demonstrated using a novel application of wavelet time / frequency decomposition and feature extraction using a bcm unsupervised network . different feature extraction methods and different wavelet representations are studied . the system achieves outstanding classification performance even when tested with mammal sounds recorded at very different locations ( from those used for training ) . the improved results suggest that nonlinear feature extraction from wavelet representations outperforms different linear choices of basis functions .
multiclass learning problems involve finding a definition for an unknown function f ( x ) whose range is a discrete set containing k & gt ; #NUM# values ( i . e . , k " classes " ) . the definition is acquired by studying large collections of training examples of the form hx i ; f ( x i ) i . existing approaches to this problem include ( a ) direct application of multiclass algorithms such as the decision - tree algorithms id #NUM# and cart , ( b ) application of binary concept learning algorithms to learn individual binary functions for each of the k classes , and ( c ) application of binary concept learning algorithms with distributed output codes such as those employed by sejnowski and rosenberg in the nettalk system . this paper compares these three approaches to a new technique in which bch error - correcting codes are employed as a distributed output representation . we show that these output representations improve the performance of id #NUM# on the nettalk task and of backpropagation on an isolated - letter speech - recognition task . these results demonstrate that error - correcting output codes provide a general - purpose method for improving the performance of inductive learning programs on multi - class problems .
in this paper we give a completeness theorem of an inductive inference rule inverse entailment proposed by muggleton . our main result is that a hypothesis clause h can be derived from an example e under a background theory b with inverse entailment iff h subsumes e relative to b in plotkin ' s sense . the theory b can be any clausal theory , and the example e can be any clause which is neither a tautology nor implied by b . the derived hypothesis h is a clause which is not always definite . in order to prove the result we give declarative semantics for arbitrary consistent clausal theories , and show that sb - resolution , which was originally introduced by plotkin , is complete procedural semantics . the completeness is shown as an extension of the completeness theorem of sld - resolution . we also show that every hypothesis h derived with saturant generalization , proposed by rouveirol , must subsume e w . r . t . b in buntine ' s sense . moreover we show that saturant generalization can be obtained from inverse entailment by giving some restriction to its usage .
we present an algorithm for arc reversal in bayesian networks with tree - structured conditional probability tables , and consider some of its advantages , especially for the simulation of dynamic probabilistic networks . in particular , the method allows one to produce cpts for nodes involved in the reversal that exploit regularities in the conditional distributions . we argue that this approach alleviates some of the overhead associated with arc reversal , plays an important role in evidence integration and can be used to restrict sampling of variables in dpns . we also provide an algorithm that detects the dynamic irrelevance of state variables in forward simulation . this algorithm exploits the structured cpts in a reversed network to determine , in a time - independent fashion , the conditions under which a variable does or does not need to be sampled .
a novel approach to learning first order logic formulae from positive and negative examples is presented . whereas present inductive logic programming systems employ examples as true and false ground facts ( or clauses ) , we view examples as interpretations which are true or false for the target theory . this viewpoint allows to reconcile the inductive logic programming paradigm with classical attribute value learning in the sense that the latter is a special case of the former . because of this property , we are able to adapt aq and cn #NUM# type algorithms in order to enable learning of full first order formulae . however , whereas classical learning techniques have concentrated on concept representations in disjunctive normal form , we will use a clausal representation , which corresponds to a conjuctive normal form where each conjunct forms a constraint on positive examples . this representation duality reverses also the role of positive and negative examples , both in the heuristics and in the algorithm . the resulting theory is incorporated in a system named icl ( inductive constraint logic ) .
the multiple instance problem arises in tasks where the training examples are ambiguous : a single example object may have many alternative feature vectors ( instances ) that describe it , and yet only one of those feature vectors may be responsible for the observed classification of the object . this paper describes and compares three kinds of algorithms that learn axis - parallel rectangles to solve the multiple - instance problem . algorithms that ignore the multiple instance problem perform very poorly . an algorithm that directly confronts the multiple instance problem ( by attempting to identify which feature vectors are responsible for the observed classifications ) performs best , giving #NUM# % correct predictions on a musk - odor prediction task . the paper also illustrates the use of artificial data to debug and compare these algorithms .
a new class of data structures called bumptrees is described . these structures are useful for efficiently implementing a number of neural network related operations . an empirical comparison with radial basis functions is presented on a robot arm mapping learning task . applications to density estimation , classification , and constraint representation and learning are also outlined .
model learning combined with dynamic programming has been shown to be effective for learning control of continuous state dynamic systems . the simplest method assumes the learned model is correct and applies dynamic programming to it , but many approximators provide uncertainty estimates on the fit . how can they be exploited ? this paper addresses the case where the system must be prevented from having catastrophic failures during learning . we propose a new algorithm adapted from the dual control literature and use bayesian locally weighted regression models with stochastic dynamic programming . a common reinforcement learning assumption is that aggressive exploration should be encouraged . this paper addresses the converse case in which the system has to reign in exploration . the algorithm is illustrated on a #NUM# dimensional simulated control problem .
handling multi - class problems and real numbers is important in practical applications of machine learning to kdd problems . while attribute - value learners address these problems as a rule , very few ilp systems do so . the few ilp systems that handle real numbers mostly do so by trying out all real values that are applicable , thus running into efficiency or overfitting problems . this paper discusses some recent extensions of icl that address these problems . icl , which stands for inductive constraint logic , is an ilp system that learns first order logic formulae from positive and negative examples . the main charateristic of icl is its view on examples . these are seen as interpretations which are true or false for the clausal target theory ( in cnf ) . we first argue that icl can be used for learning a theory in a disjunctive normal form ( dnf ) . with this in mind , a possible solution for handling more than two classes is given ( based on some ideas from cn #NUM# ) . finally , we show how to tackle problems with continuous values by adapting discretization techniques from attribute value learners .
the paper presents a learning controller that is capable of increasing insertion speed during consecutive peg - into - hole operations , without increasing the contact force level . our aim is to find a better relationship between measured forces and the controlled velocity , without using a complicated ( human generated ) model . we followed a connectionist approach . two learning phases are distinguished . first the learning controller is trained ( or initialised ) in a supervised way by a suboptimal task frame controller . then a reinforcement learning phase follows . the controller consists of two networks : ( #NUM# ) the policy network and ( #NUM# ) the exploration network . on - line robotic exploration plays a crucial role in obtaining a better policy . optionally , this architecture can be extended with a third network : the reinforcement network . the learning controller is implemented on a cad - based contact force simulator . in contrast with most other related work , the experiments are simulated in #NUM# d with #NUM# degrees of freedom . performance of a peg - into - hole task is measured in insertion time and average / maximum force level . the fact that a better performance can be obtained in this way , demonstrates the importance of model - free learning techniques for repetitive robotic assembly tasks . the paper presents the approach and simulation results . keywords : robotic assembly , peg - into - hole , artificial neural networks , reinforcement learning .
indirect experiments are studies in which randomized control is replaced by randomized encouragement , that is , subjects are encouraged , rather than forced to receive treatment programs . the purpose of this paper is to bring to the attention of experimental researchers simple mathematical results that enable us to assess , from indirect experiments , the strength with which causal influences operate among variables of interest . the results reveal that despite the laxity of the encouraging instrument , indirect experimentation can yield significant and sometimes accurate information on the impact of a program on the population as a whole , as well as on the particular individuals who participated in the program .
the use of previously learned knowledge during learning has been shown to reduce the number of examples required for good generalization , and to increase robustness to noise in the examples . in reviewing various means of using learned knowledge from a domain to guide further learning in the same domain , two underlying classes are discerned . methods which use previous knowledge to initialize a learner ( as an initialization bias ) , and those that use previous knowledge to constrain a learner ( as a search bias ) . we show such methods in fact exploit the same domain knowledge differently , and can complement each other . this is shown by presenting a combined approach which both initializes and constrains a learner . this combined approach is seen to outperform the individual methods under the conditions that accurate previously learned domain knowledge is available , and that there are irrelevant features in the domain representation .
we consider recurrent analog neural nets where the output of each gate is subject to gaussian noise , or any other common noise distribution that is nonzero on a large set . we show that many regular languages cannot be recognized by networks of this type , and we give a precise characterization of those languages which can be recognized . this result implies severe constraints on possibilities for constructing recurrent analog neural nets that are robust against realistic types of analog noise . on the other hand we present a method for constructing feedforward analog neural nets that are robust with regard to analog noise of this type .
this paper describes rapture | a system for revising probabilistic knowledge bases that combines connectionist and symbolic learning methods . rapture uses a modified version of backpropagation to refine the certainty factors of a probabilistic rule base and it uses id #NUM# ' s information - gain heuristic to add new rules . results on refining three actual expert knowledge bases demonstrate that this combined approach generally performs better than previous methods .
this paper tackles the supervised induction of a distance from examples described as horn clauses or constrained clauses . in opposition to syntax - driven approaches , this approach is discrimination - driven : it proceeds by defining a small set of complex discriminant hypotheses . these hypotheses serve as new concepts , used to redescribe the initial examples . further , this redescription can be embedded into the space of natural integers , and a distance between examples thus naturally follows .
probability theory represents and manipulates uncertainties , but cannot tell us how to behave . for that we need utility theory which assigns values to the usefulness of different states , and decision theory which concerns optimal rational decisions . there are many methods for probability modeling , but few for learning utility and decision models . we use reinforcement learning to find the optimal sequence of questions in a diagnosis situation while maintaining a high accuracy . automated diagnosis on a heart - disease domain is used to demonstrate that temporal - difference learning can improve diagnosis . on the cleveland heart - disease database our results are better than those reported from all previous methods .
the simple bayesian classifier ( sbc ) , sometimes called naive - bayes , is built based on a conditional independence model of each attribute given the class . the model was previously shown to be surprisingly robust to obvious violations of this independence assumption , yielding accurate classification models even when there are clear conditional dependencies . the sbc can serve as an excellent tool for initial exploratory data analysis when coupled with a visualizer that makes its structure comprehensible . we describe such a visual representation of the sbc model that has been successfully implemented . we describe the requirements we had for such a visualization and the design decisions we made to satisfy them .
this paper describes some of our recent work in the development of computer architectures for efficient execution of artificial neural network algorithms . our earlier system , the ring array processor ( rap ) , was a multiprocessor based on commercial dsps with a low - latency ring interconnection scheme . we have used the rap to simulate variable precision arithmetic and guide us in the design of higher performance neurocomputers based on custom vlsi . the rap system played a critical role in this study , enabling us to experiment with much larger networks than would otherwise be possible . our study shows that back - propagation training algorithms only require moderate precision . specifically , #NUM# b weight values and #NUM# b output values are sufficient to achieve training and classification results comparable to #NUM# b floating point . although these results were gathered for frame classification in continuous speech , we expect that they will extend to many other connectionist calculations . we have used these results as part of the design of a programmable single chip microprocessor , spert . the reduced precision arithmetic permits the use of multiple units per processor . also , reduced precision operands make more efficient use of valuable processor - memory bandwidth . for our moderate - precision fixed - point arithmetic applications , spert represents more than an order of magnitude reduction in cost over systems based on dsp chips .
#NUM# ] r . k . belew , j . mcinerney , and n . schraudolph , evolving networks : using the genetic algorithm with connectionist learning , in artificial life ii , sfi studies in the science of complexity , c . g . langton , c . taylor , j . d . farmer , s . rasmussen eds . , vol . #NUM# , addison - wesley , #NUM# . [ #NUM# ] m . mcinerney , and a . p . dhawan , use of genetic algorithms with back propagation in training of feed - forward neural networks , in ieee international conference on neural networks , vol . #NUM# , pp . #NUM# - #NUM# , #NUM# . [ #NUM# ] f . z . brill , d . e . brown , and w . n . martin , fast genetic selection of features for neural network classifiers , ieee transactions on neural networks , vol . #NUM# , no . #NUM# , pp . #NUM# - #NUM# , #NUM# . [ #NUM# ] f . dellaert , and j . vandewalle , automatic design of cellular neural networks by means of genetic algorithms : finding a feature detector , in the third ieee international workshop on cellular neural networks and their applications , ieee , new jersey , pp . #NUM# - #NUM# , #NUM# . [ #NUM# ] d . e . moriarty , and r . miikkulainen , efficient reinforcement learning through symbiotic evolution , machine learning , vol . #NUM# , pp . #NUM# - #NUM# , #NUM# . [ #NUM# ] l . davis , handbook of genetic algorithms , van nostrand reinhold , new york , #NUM# . [ #NUM# ] d . whitely , the genitor algorithm and selective pressure , in proceedings of the third interanational conference on genetic algorithms , j . d . schaffer ed . , morgan kauffman , san mateo , ca , #NUM# , pp . #NUM# - #NUM# . [ #NUM# ] van camp , d . , t . plate and g . e . hinton ( #NUM# ) . the xerion neural network simulator and documentation . department of computer science , university of toronto , toronto .
kj = #NUM# . the standard ppr algorithm of friedman and stuet - zle ( #NUM# ) estimates the smooth functions f j using the supersmoother nonparametric scatterplot smoother . friedman ' s algorithm constructs a model with m max linear combinations , then prunes back to a simpler model of size m m max , where m and m max are specified by the user . this paper discusses an alternative algorithm in which the smooth functions are estimated using smoothing splines . the direction coefficients ff j , the amount of smoothing in each direction , and
in this paper we suggest a mechanism that improves significantly the performance of a top - down inductive logic programming ( ilp ) learning system . this improvement is achieved at the cost of giving to the system extra information that is not difficult to formulate . this information appears in the form of an algorithm sketch : an incomplete and somewhat vague representation of the computation related to a particular example . we describe which sketches are admissible , give details of the learning algorithm that exploits the information contained in the sketch . the experiments carried out with the implemented system ( skil ) have demonstrated the usefulness of the method and its potential in future applications .
in this paper we are concerned with the problem of inducing recursive horn clauses from small sets of training examples . the method of iterative bootstrap induction is presented . in the first step , the system generates simple clauses , which can be regarded as properties of the required definition . properties represent generalizations of the positive examples , simulating the effect of having larger number of examples . properties are used subsequently to induce the required recursive definitions . this paper describes the method together with a series of experiments . the results support the thesis that iterative bootstrap induction is indeed an effective technique that could be of general use in ilp .
this paper aims to examine the use of genetic algorithms to optimize subsystems of cellular neural network architectures . the application at hand is character recognition : the aim is to evolve an optimal feature detector in order to aid a conventional classifier network to generalize across different fonts . to this end , a performance function and a genetic encoding for a feature detector are presented . an experiment is described where an optimal feature detector is indeed found by the genetic algorithm . we are interested in the application of cellular neural networks in computer vision . genetic algorithms ( ga ' s ) [ #NUM# - #NUM# ] can serve to optimize the design of cellular neural networks . although the design of the global architecture of the system could still be done by human insight , we propose that specific sub - modules of the system are best optimized using one or other optimization method . gas are a good candidate to fulfill this optimization role , as they are well suited to problems where the objective function is a complex function of many parameters . the specific problem we want to investigate is one of character recognition . more specifically , we would like to use the ga to find optimal feature detectors to be used in the recognition of digits .
this article exposes problems of the commonly used technique of splitting the available data into training , validation , and test sets that are held fixed , warns about drawing too strong conclusions from such static splits , and shows potential pitfalls of ignoring variability across splits . using a bootstrap or resampling method , we compare the uncertainty in the solution stemming from the data splitting with neural network specific uncertainties ( parameter initialization , choice of number of hidden units , etc . ) . we present two results on data from the new york stock exchange . first , the variation due to different resamplings is significantly larger than the variation due to different network conditions . this result implies that it is important to not over - interpret a model ( or an ensemble of models ) estimated on one specific split of the data . second , on each split , the neural network solution with early stopping is very close to a linear model ; no significant nonlinearities are extracted .
cross validation can be used to detect when overfitting starts during supervised training of a neural network ; training is then stopped before convergence to avoid the over - fitting ( " early stopping " ) . the exact criterion used for cross validation based early stopping , however , is chosen in an ad - hoc fashion by most researchers or training is stopped interactively . to aid a more well - founded selection of the stopping criterion , #NUM# different automatic stopping criteria from #NUM# classes were evaluated empirically for their efficiency and effectiveness in #NUM# different classification and approximation tasks using multi layer perceptrons with rprop training . the experiments show that on the average slower stopping criteria allow for small improvements in generalization ( on the order of #NUM# % ) , but cost about factor #NUM# longer training time .
we introduce a new formal model in which a learning algorithm must combine a collection of potentially poor but statistically independent hypothesis functions in order to approximate an unknown target function arbitrarily well . our motivation includes the question of how to make optimal use of multiple independent runs of a mediocre learning algorithm , as well as settings in which the many hypotheses are obtained by a distributed population of identical learning agents .
markov chain monte carlo ( mcmc ) methods make possible the use of flexible bayesian models that would otherwise be computationally infeasible . in recent years , a great variety of such applications have been described in the literature . applied statisticians who are new to these methods may have several questions and concerns , however : how much effort and expertise are needed to design and use a markov chain sampler ? how much confidence can one have in the answers that mcmc produces ? how does the use of mcmc affect the rest of the model - building process ? at the joint statistical meetings in august , #NUM# , a panel of experienced mcmc users discussed these and other issues , as well as various " tricks of the trade " . this paper is an edited recreation of that discussion . its purpose is to offer advice and guidance to novice users of mcmc - and to not - so - novice users as well . topics include building confidence in simulation results , methods for speeding and assessing convergence , estimating standard errors , identification of models for which good mcmc algorithms exist , and the current state of software development .
many neural network models must be trained by finding a set of real - valued weights that yield high accuracy on a training set . other learning models require weights on input attributes that yield high leave - one - out classification accuracy in order to avoid problems associated with irrelevant attributes and high dimensionality . in addition , there are a variety of general problems for which a set of real values must be found which maximize some evaluation function . this paper presents an algorithm for doing a schemata search over a real - valued weight space to find a set of weights ( or other real values ) that yield high values for a given evaluation function . the algorithm , called the real - valued schemata search ( rvss ) , uses the brace statistical technique [ moore & lee , #NUM# ] to determine when to narrow the search space . this paper details the rvss approach and gives initial empirical results .
a large body of nonparametric statistical literature is devoted to density estimation . overviews are given in silverman ( #NUM# ) and izenman ( #NUM# ) . this paper addresses the problem of univariate density estimation in a novel way . our approach falls in the class of so called projection estimators , introduced by cencov ( #NUM# ) . the orthonor - mal basis used is a basis of compactly supported wavelets from daubechies ' family . kerkyacharian and picard ( #NUM# , #NUM# ) , donoho et al . ( #NUM# ) , and delyon and judit - sky ( #NUM# ) , among others , applied wavelets in density estimation . the local nature of wavelet functions makes the wavelet estimator superior to projection estimators that use classical orthonormal bases ( fourier , hermite , etc . ) instead of estimating the unknown density directly , we estimate the square root of the density , which enables us to control the positiveness and the l #NUM# norm of the density estimate . however , in that approach one needs a pre - estimator of the density to calculate sample wavelet coefficients . we describe visustop , a data - driven procedure for determining the maximum number of levels in the wavelet density estimator . coefficients in the selected levels are thresholded to make the estimator parsimonious .
this literature review discusses different methods under the general rubric of learning bayesian networks from data , and includes some overlapping work on more general probabilistic networks . connections are drawn between the statistical , neural network , and uncertainty communities , and between the different methodological communities , such as bayesian , description length , and classical statistics . basic concepts for learning and bayesian networks are introduced and methods are then reviewed . methods are discussed for learning parameters of a probabilistic network , for learning the structure , and for learning hidden variables . the presentation avoids formal definitions and theorems , as these are plentiful in the literature , and instead illustrates key concepts with simplified examples .
recent work in supervised learning has shown that a surprisingly simple bayesian classifier with strong assumptions of independence among features , called naive bayes , is competitive with state of the art classifiers such as c #NUM# . #NUM# . this fact raises the question of whether a classifier with less restrictive assumptions can perform even better . in this paper we examine and evaluate approaches for inducing classifiers from data , based on recent results in the theory of learning bayesian networks . bayesian networks are factored representations of probability distributions that generalize the naive bayes classifier and explicitly represent statements about independence . among these approaches we single out a method we call tree augmented naive bayes ( tan ) , which outperforms naive bayes , yet at the same time maintains the computational simplicity ( no search involved ) and robustness which are characteristic of naive bayes . we experimentally tested these approaches using benchmark problems from the u . c . irvine repository , and compared them against c #NUM# . #NUM# , naive bayes , and wrapper - based feature selection methods .
in recent years there has been a flurry of works on learning probabilistic belief networks . current state of the art methods have been shown to be successful for two learning scenarios : learning both network structure and parameters from complete data , and learning parameters for a fixed network from incomplete datathat is , in the presence of missing values or hidden variables . however , no method has yet been demonstrated to effectively learn network structure from incomplete data . in this paper , we propose a new method for learning network structure from incomplete data . this method is based on an extension of the expectation - maximization ( em ) algorithm for model selection problems that performs search for the best structure inside the em procedure . we prove the convergence of this algorithm , and adapt it for learning belief networks . we then describe how to learn networks in two scenarios : when the data contains missing values , and in the presence of hidden variables . we provide experimental results that show the effectiveness of our procedure in both scenarios .
this paper defines a class of problems involving combinations of induction and ( cost ) optimisation . a framework is presented that systematically describes problems that involve construction of decision trees or rules , optimising accuracy as well as measurement - and misclassification costs . it does not present any new algorithms but shows how this framework can be used to configure greedy algorithms for constructing such trees or rules . the framework covers a number of existing algorithms . moreover , the framework can also be used to define algorithm configurations with new functionalities , as expressed in their evaluation functions .
we introduce a new framework for the study of reasoning . the learning ( in order ) to reason approach developed here views learning as an integral part of the inference process , and suggests that learning and reasoning should be studied together . the learning to reason framework combines the interfaces to the world used by known learning models with the reasoning task and a performance criterion suitable for it . in this framework the intelligent agent is given access to its favorite learning interface , and is also given a grace period in which it can interact with this interface and construct a representation kb of the world w . the reasoning performance is measured only after this period , when the agent is presented with queries ff from some query language , relevant to the world , and has to answer whether w implies ff . the approach is meant to overcome the main computational difficulties in the traditional treatment of reasoning which stem from its separation from the " world " . since the agent interacts with the world when constructing its knowledge representation it can choose a representation that is useful for the task at hand . moreover , we can now make explicit the dependence of the reasoning performance on the environment the agent interacts with . we show how previous results from learning theory and reasoning fit into this framework and illustrate the usefulness of the learning to reason approach by exhibiting new results that are not possible in the traditional setting . first , we give learning to reason algorithms for classes of propositional languages for which there are no efficient reasoning algorithms , when represented as a traditional ( formula - based ) knowledge base . second , we exhibit a learning to reason algorithm for a class of propositional languages that is not known to be learnable in the traditional sense .
many ai problems , when formalized , reduce to evaluating the probability that a propositional expression is true . in this paper we show that this problem is computationally intractable even in surprisingly restricted cases and even if we settle for an approximation to this probability . we consider various methods used in approximate reasoning such as computing degree of belief and bayesian belief networks , as well as reasoning techniques such as constraint satisfaction and knowledge compilation , that use approximation to avoid computational difficulties , and reduce them to model - counting problems over a propositional domain . we prove that counting satisfying assignments of propositional languages is intractable even for horn and monotone formulae , and even when the size of clauses and number of occurrences of the variables are extremely limited . this should be contrasted with the case of deductive reasoning , where horn theories and theories with binary clauses are distinguished by the existence of linear time satisfiability algorithms . what is even more surprising is that , as we show , even approximating the number of satisfying assignments ( i . e . , " approximating " approximate reasoning ) , is intractable for most of these restricted theories . we also identify some restricted classes of propositional formulae for which efficient algorithms for counting satisfying assignments can be given .
we describe recent extensions to our framework for the automatic generation of music - making programs . we have previously used genetic programming techniques to produce music - making programs that satisfy user - provided critical criteria . in this paper we describe new work on the use of connectionist techniques to automatically induce musical structure from a corpus . we show how the resulting neural networks can be used as critics that drive our genetic programming system . we argue that this framework can potentially support the induction and recapitulation of deep structural features of music . we present some initial results produced using neural and hybrid symbolic / neural critics , and we discuss directions for future work .
much effort has been devoted to understanding learning and reasoning in artificial intelligence . however , very few models attempt to integrate these two complementary processes . rather , there is a vast body of research in machine learning , often focusing on inductive learning from examples , quite isolated from the work on reasoning in artificial intelligence . though these two processes may be different , they are very much interrelated . the ability to reason about a domain of knowledge is often based on rules about that domain , that must be learned somehow . and the ability to reason can often be used to acquire new knowledge , or learn . this paper introduces an incremental learning algorithm ( ila ) that attempts to combine inductive learning with prior knowledge and reasoning . ila has many important characteristics useful for such a combination , including : #NUM# ) incremental , self - organizing learning , #NUM# ) nonuniform learning , #NUM# ) inherent non - monotonicity , #NUM# ) extensional and intensional capabilities , and #NUM# ) low order polynomial complexity . the paper describes ila , gives simulation results for several applications , and discusses each of the above characteristics in detail .
this paper demonstrates how the nature of the opposition during training affects learning to play two - person , perfect information board games . it considers different kinds of competitive training , the impact of trainer error , appropriate metrics for post - training performance measurement , and the ways those metrics can be applied . the results suggest that teaching a program by leading it repeatedly through the same restricted paths , albeit high quality ones , is overly narrow preparation for the variations that appear in real - world experience . the results also demonstrate that variety introduced into training by random choice is unreliable preparation , and that a program that directs its own training may overlook important situations . the results argue for a broad variety of training experience with play at many levels . this variety may either be inherent in the game or introduced deliberately into the training . lesson and practice training , a blend of expert guidance and knowledge - based , self - directed elaboration , is shown to be particularly effective for learning during competition .
the negative effect is naturally more significant in the more complex domain . the graph for the simple domain crosses the #NUM# line earlier than the complex domain . that means that learning starts to be useful with weight greater than #NUM# . #NUM# for the simple domain and #NUM# . #NUM# for the complex domain . as we relax the optimality requirement more s i g n i f i c a n t l y ( w i t h a w = #NUM# . #NUM# ) , macro usage in the more complex domain becomes more advantageous . the purpose of the research described in this paper is to identify the parameters that effects deductive learning and to perform experiments systematically in order to understand the nature of those effects . the goal of this paper is to demonstrate the methodology of performing parametric experimental study of deductive learning . the example here include the study of two parameters : the point on the satisficing - optimizing scale that is used during the search carried out during problem solving time and during learning time . we showed that a * , which looks for optimal solutions , cannot benefit from macro learning but as the strategy comes closer to best - first ( satisficing search ) , the utility of macros increases . we also demonstrated that deductive learners that learn offline by solving training problems are sensitive to the type of search used during the learning . we showed that in general optimizing search is best for learning . it generates macros that increase the quality solutions regardless of the search method used during problem solving . it also improves the efficiency for problem solvers that require a high level of optimality . the only drawback in using optimizing search is the increase in learning resources spent . we are aware of the fact that the results described here are not very surprising . the goal of the parametric study is not necessarily to find exciting results , but to obtain results , sometimes even previously known , in a controlled experimental environment . the work described here is only part of our research plan . we are currently in the process of extensive experimentation with all the parameters described here and also with others . we also intend to test the validity of the conclusions reached during the study by repeating some of the tests in several of the commonly known search problems . we hope that such systematic experimentation will help the research community to better understand the process of deductive learning and will serve as a demonstration of the experimental methodology that should be used in machine learning research .
we examine a number of techniques for representing actions with stochastic effects using bayesian networks and influence diagrams . we compare these techniques according to ease of specification and size of the representation required for the complete specification of the dynamics of a particular system , paying particular attention the role of persistence relationships . we precisely characterize two components of the frame problem for bayes nets and stochastic actions , propose several ways to deal with these problems , and compare our solutions with re - iter ' s solution to the frame problem for the situation calculus . the result is a set of techniques that permit both ease of specification and compact representation of probabilistic system dynamics that is of comparable size ( and timbre ) to reiter ' s representation ( i . e . , with no explicit frame axioms ) .
given a function f mapping n - variate inputs from a finite field f into f , we consider the task of reconstructing a list of all n - variate degree d polynomials which agree with f on a tiny but non - negligible fraction , ffi , of the input space . we give a randomized algorithm for solving this task which accesses f as a black box and runs in time polynomial in #NUM# d = jf j ) . for the special case when d = #NUM# , we solve this problem for all * def jf j & gt ; #NUM# . in this case the running time of our algorithm is bounded by a polynomial in #NUM# * ; n and exponential in d . our algorithm generalizes a previously known algorithm , due to goldreich and levin , that solves this
many problems correspond to the classical control task of determining the appropriate control action to take , given some ( sequence of ) observations . one standard approach to learning these control rules , called behavior cloning , involves watching a perfect operator operate a plant , and then trying to emulate its behavior . in the experimental learning approach , by contrast , the learner first guesses an initial operation - to - action policy and tries it out . if this policy performs sub - optimally , the learner can modify it to produce a new policy , and recur . this paper discusses the relative effectiveness of these two approaches , especially in the presence of perceptual aliasing , showing in particular that the experimental learner can often learn more effectively than the cloning one .
this paper discusses the role of culture in the evolution of cognitive systems . we define culture as any information transmitted between individuals and between generations by non - genetic means . experiments are presented that use genetic programming systems that include special mechanisms for cultural transmission of information . these systems evolve computer programs that perform cognitive tasks including mathematical function mapping and action selection in a virtual world . the data show that the presence of culture - supporting mechanisms can have a clear beneficial impact on the evolvability of correct programs . the implications that these results may have for cognitive science are briefly discussed .
numerical design optimization algorithms are highly sensitive to the particular formulation of the optimization problems they are given . the formulation of the search space , the objective function and the constraints will generally have a large impact on the duration of the optimization process as well as the quality of the resulting design . furthermore , the best formulation will vary from one application domain to another , and from one problem to another within a given application domain . unfortunately , a design engineer may not know the best formulation in advance of attempting to set up and run a design optimization process . in order to attack this problem , we have developed a software environment that supports interactive formulation , testing and reformulation of design optimization strategies . our system represents optimization strategies in terms of second - order dataflow graphs . reformulations of strategies are implemented as transformations between dataflow graphs . the system permits the user to interactively generate and search a space of design optimization strategies , and experimentally evaluate their performance on test problems , in order to find a strategy that is suitable for his application domain . the system has been implemented in a domain independent fashion , and is being tested in the domain of racing yacht design .
this paper presents the basic results and ideas of dynamic programming as they relate most directly to the concerns of planning in ai . these form the theoretical basis for the incremental planning methods used in the integrated architecture dyna . these incremental planning methods are based on continually updating an evaluation function and the situation - action mapping of a reactive system . actions are generated by the reactive system and thus involve minimal delay , while the incremental planning process guarantees that the actions and evaluation function will eventually be optimal | no matter how extensive a search is required . these methods are well suited to stochastic tasks and to tasks in which a complete and accurate model is not available . for tasks too large to implement the situation - action mapping as a table , supervised - learning methods must be used , and their capabilities remain a significant limitation of the approach .
we investigate the query complexity of exact learning in the membership and ( proper ) equivalence query model . we give a complete characterization of concept classes that are learnable with a polynomial number of polynomial sized queries in this model . we give applications of this characterization , including results on learning a natural subclass of dnf formulas , and on learning with membership queries alone . query complexity has previously been used to prove lower bounds on the time complexity of exact learning . we show a new relationship between query complexity and time complexity in exact learning : if any " honest " class is exactly and properly learnable with polynomial query complexity , but not learnable in polynomial time , then p #NUM# = np . in particular , we show that an honest class is exactly polynomial - query learnable if and only if it is learnable using an oracle for p
this paper presents a case study in evaluating a case - based system . it describes the evaluation of anapron , a system that pronounces names by a combination of rule - based and case - based reasoning . three sets of experiments were run on anapron a set of exploratory measurements to profile the system ' s operation ; a comparison between anapron and other name - pronunciation systems ; and a set of studies that modified various parts of the system to isolate the contribution of each . lessons learned from these experiments for cbr evaluation methodology and for cbr theory are discussed . this work may not be copied or reproduced in whole or in part for any commercial purpose . permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following a notice that such copying is by permission of mitsubishi electric research laboratories of cambridge , massachusetts ; an acknowledgment of the authors and individual contributions to the work ; and all applicable portions of the copyright notice . copying , reproduction , or republishing for any other purpose shall require a license with payment of fee to mitsubishi electric research laboratories . all rights reserved .
consider a given value function on states of a markov decision problem , as might result from applying a reinforcement learning algorithm . unless this value function equals the corresponding optimal value function , at some states there will be a discrepancy , which is natural to call the bellman residual , between what the value function specifies at that state and what is obtained by a one - step lookahead along the seemingly best action at that state using the given value function to evaluate all succeeding states . this paper derives a tight bound on how far from optimal the discounted return for a greedy policy based on the given value function will be as a function of the maximum norm magnitude of this bellman residual . a corresponding result is also obtained for value functions defined on state - action pairs , as are used in q - learning . one significant application of these results is to problems where a function approximator is used to learn a value function , with training of the approxi - mator based on trying to minimize the bellman residual across states or state - action pairs . when control is based on the use of the resulting value function , this result provides a link between how well the objectives of function approximator training are met and the quality of the resulting control .
to measure the quality of a set of vector quantization points a means of measuring the distance between a random point and its quantization is required . common metrics such as the hamming and euclidean metrics , while mathematically simple , are inappropriate for comparing natural signals such as speech or images . in this paper it is shown how an environment of functions on an input space x induces a canonical distortion measure ( cdm ) on x . the depiction canonical is justified because it is shown that optimizing the reconstruction error of x with respect to the cdm gives rise to optimal piecewise constant approximations of the functions in the environment . the cdm is calculated in closed form for several different function classes . an algorithm for training neural networks to implement the cdm is presented along with some en couraging experimental results .
theory revision systems typically use a set of theory - to - theory transformations f k g to hill - climb from a given initial theory to a new theory whose empirical accuracy , over a given set of labeled training instances fc j g , is a local maximum . at the heart of each such process is an " evaluator " , which compares the accuracy of the current theory kb with that of each of its " neighbors " f k ( kb ) g , with the goal of determining which neighbor has the highest accuracy . the obvious " wrapper " evaluator simply evaluates each individual neighbor theory kb k = k ( kb ) on each instance c j . as it can be very expensive to evaluate a single theory on a single instance , and there can be a great many training instances and a huge number of neighbors , this approach can be prohibitively slow . we present an alternative system which employs a smarter evaluator that quickly computes the accuracy of a transformed theory k ( kb ) by " looking inside " kb and reasoning about the effects of the k transformation . we compare the performance of with the naive wrapper system on real - world theories obtained from a fielded expert system , and find that runs over #NUM# times faster than , while attaining the same accuracy . this paper also discusses ' s source of power . keywords : theory revision , efficient algorithm , hill - climbing system multiple submissions : we have submited a related version of this paper to aaai #NUM# .
this paper describes a wavelet method for the estimation of density and hazard rate functions from randomly right censored data . we adopt a nonparametric approach in assuming that the density and hazard rate have no specific parametric form . the method is based on dividing the time axis into a dyadic number of intervals and then counting the number of events within each interval . the number of events and the survival function of the observations are then separately smoothed over time via linear wavelet smoothers , and then the hazard rate function estimators are obtained by taking the ratio . we prove that the estimators possess pointwise and global mean square consistency , obtain the best possible asymptotic mise convergence rate and are also asymptotically normally distributed . we also describe simulation experiments that show these estimators are reasonably reliable in practice . the method is illustrated with two real examples . the first uses survival time data for patients with liver metastases from a colorectal primary tumour without other distant metastases . the second is concerned with times of unemployment for women and the wavelet estimate , through its flexibility , provides a new and interesting interpretation .
experience plays an important role in the development of human expertise . one computational model of how experience affects expertise is provided by research on case - based reasoning , which examines how stored cases encapsulating traces of specific prior problem - solving episodes can be retrieved and re - applied to facilitate new problem - solving . much progress has been made in methods for accessing relevant cases , and case - based reasoning is receiving wide acceptance both as a technology for developing intelligent systems and as a cognitive model of a human reasoning process . however , one important aspect of case - based reasoning remains poorly understood : the process by which retrieved cases are adapted to fit new situations . the difficulty of encoding effective adaptation rules by hand is widely recognized as a serious impediment to the development of fully autonomous case - based reasoning systems . consequently , an important question is how case - based reasoning systems might learn to improve their expertise at case adaptation . we present a framework for acquiring this expertise by using a combination of general adaptation rules , introspective reasoning , and case - based reasoning about the case adaptation task itself .
in this paper , we indicate some possible applications of ilp or similar techniques in the knowledge discovery field , and then discuss several methods for adapting and linking ilp - systems to relational database systems . the proposed methods range from " pure ilp " to " based on techniques originating in ilp " . we show that it is both easy and advantageous to adapt ilp - systems in this way .
most exact algorithms for general pomdps use a form of dynamic programming in which a piecewise - linear and convex representation of one value function is transformed into another . we examine variations of the " incremental pruning " approach for solving this problem and compare them to earlier algorithms from theoretical and empirical perspectives . we find that incremental pruning is presently the most efficient algorithm for solving pomdps .
we improve error bounds based on vc analysis for classes with sets of similar classifiers . we apply the new error bounds to separating planes and artificial neural networks . key words machine learning , learning theory , generalization , vapnik - chervonenkis , separating planes , neural networks .
the higher - order structure of genes and other features of biological sequences can be described by means of formal grammars . these grammars can then be used by general - purpose parsers to detect and assemble such structures by means of syntactic pattern recognition . we describe a grammar and parser for eukaryotic protein - encoding genes , which by some measures is as effective as current connectionist and combinatorial algorithms in predicting gene structures for sequence database entries . parameters on the grammar rules are optimized for several different species , and mixing experiments performed to determine the degree of species specificity and the relative importance of compositional , signal - based , and syntactic components in gene prediction .
we present a novel classification and regression method that combines exploratory projection pursuit ( unsupervised training ) with projection pursuit regression ( supervised training ) , to yield a new family of cost / complexity penalty terms . some improved generalization properties are demonstrated on real world problems .
in this paper , we present an objective function formulation of the bcm theory of visual cortical plasticity that permits us to demonstrate the connection between the unsupervised bcm learning procedure and various statistical methods , in particular , that of projection pursuit . this formulation provides a general method for stability analysis of the fixed points of the theory and enables us to analyze the behavior and the evolution of the network under various visual rearing conditions . it also allows comparison with many existing unsupervised methods . this model has been shown successful in various applications such as phoneme and #NUM# d object recognition . we thus have the striking and possibly highly significant result that a biological neuron is performing a sophisticated statistical procedure .
a system for automatic face recognition is presented . it consists of several steps ; automatic detection of the eyes and mouth is followed by a spatial normalization of the images . the classification of the normalized images is carried out by a hybrid ( supervised and unsupervised ) neural network . two methods for reducing the overfitting a common problem in high dimensional classification schemes are presented , and the superiority of their combination is demonstrated .
radial basis function ( rbf ) neural networks offer an attractive equation form for use in model - based control because they can approximate highly nonlinear plants and yet are well suited for linear adaptive control . we show how interpreting rbfs as mixtures of gaussians allows the application of many statistical tools including the em algorithm for parameter estimation . the resulting emrbf models give uncertainty estimates and warn when they are extrapolating beyond the region where training data was available .
the application of genetic algorithms to neural network optimization ( gann ) has produced an active field of research . this paper proposes a classification of the encoding strategies and it also gives a critical analysis the idea of evolving artificial neural networks ( nn ) by genetic algorithms ( ga ) is based on a powerful metaphor : the evolution of the human brain . this mechanism has developed the highest form of intelligence known from scratch . the metaphor has inspired a great deal of research activities that can be traced to the late #NUM# s ( for instance [ #NUM# ] ) . an increasing amount of research reports , journal papers and theses have been published on the topic , generating a conti - nously growing field . researchers have devoloped a variety of different techniques to encode neural networks for the ga , with increasing complexity . this young field is driven mostly by small , independet research groups that scarcely cooperate with each other . this paper will attempt to analyse and to structure the already performed work , and to point out the shortcomings of the approaches . of the current state of development .
we propose an object recognition scheme based on a method for feature extraction from gray level images that corresponds to recent statistical theory , called projection pursuit , and is derived from a biologically motivated feature extracting neuron . to evaluate the performance of this method we use a set of very detailed psychophysical #NUM# d object recognition experiments ( bulthoff and edelman , #NUM# ) .
wavelet shrinkage , the method proposed by seminal work of donohoand johnstone is a disarmingly simple and efficient way of de - noising data . shrinking wavelet coefficients was proposed from several optimality criteria . the most notable are the asymptotic minimax and cross - validation criteria . in this paper a wavelet shrinkage by imposing natural properties of bayesian models on data is proposed . the performance of methods are tested on standard donoho - johnstone test functions . key words and phrases : wavelets , discrete wavelet transform , thresholding , bayes model . #NUM# ams subject classification : #NUM# a #NUM# , #NUM# g #NUM# .
this paper introduces the idea of clearning , of simultaneously cleaning data and learning the underlying structure . the cleaning step can be viewed as top - down processing ( the model modifies the data ) , and the learning step can be viewed as bottom - up processing ( where the data modifies the model ) . after discussing the statistical foundation of the proposed method from a maximum likelihood perspective , we apply clearning to a notoriously hard problem where benchmark performances are very well known : the prediction of foreign exchange rates . on the difficult #NUM# - #NUM# test period , clearning in conjunction with pruning yields an annualized return between #NUM# and #NUM# % ( out - of - sample ) , significantly better than an otherwise identical network trained without cleaning . the network was started with #NUM# inputs and #NUM# hidden units and ended up with only #NUM# non - zero weights between inputs and hidden units . the resulting ultra - sparse final architectures obtained with clearning and pruning are immune against overfitting , even on very noisy problems since the cleaned data allow for a simpler model . apart from the very competitive performance , clearning gives insight into the data : we show how to estimate the overall signal - to - noise ratio of each input variable , and we show that error estimates for each pattern can be used to detect and remove outliers , and to replace missing or corrupted data by cleaned values . clearning can be used in any nonlinear regression or classification problem .
a large class of machine - learning problems in natural language require the characterization of linguistic context . two characteristic properties of such problems are that their feature space is of very high dimensionality , and their target concepts depend on only a small subset of the features in the space . under such conditions , multiplicative weight - update algorithms such as winnow have been shown to have exceptionally good theoretical properties . in the work reported here , we present an algorithm combining variants of winnow and weighted - majority voting , and apply it to a problem in the aforementioned class : context - sensitive spelling correction . this is the task of fixing spelling errors that happen to result in valid words , such as substituting to for too , casual for causal , and so on . we evaluate our algorithm , winspell , by comparing it against bayspell , a statistics - based method representing the state of the art for this task . we find : ( #NUM# ) when run with a full ( unpruned ) set of features , winspell achieves accuracies significantly higher than bayspell was able to achieve in either the pruned or unpruned condition ; ( #NUM# ) when compared with other systems in the literature , winspell exhibits the highest performance ; ( #NUM# ) while several aspects of winspell ' s architecture contribute to its superiority over bayspell , the primary factor is that it is able to learn a better linear separator than bayspell learns ; ( #NUM# ) when run on a test set drawn from a different corpus than the training set was drawn from , winspell is better able than bayspell to adapt , using a strategy we will present that combines supervised learning on the training set with unsupervised learning on the ( noisy ) test set .
various notions of geometric ergodicity for markov chains on general state spaces exist . in this paper , we review certain relations and implications among them . we then apply these results to a collection of chains commonly used in markov chain monte carlo simulation algorithms , the so - called hybrid chains . we prove that under certain conditions , a hybrid chain will " inherit " the geometric ergodicity of its constituent parts . acknowledgements . we thank charlie geyer for a number of very useful comments regarding spectral theory and central limit theorems . we thank alison gibbs , phil reiss , peter rosenthal , and richard tweedie for very helpful discussions . we thank the referee and the editor for many excellent suggestions .
we present a polynomial - time algorithm for determining whether a set of species , described by the characters they exhibit , has a phylogenetic tree , assuming the maximum number of possible states for a character is fixed . this solves an open problem posed by kannan and warnow . our result should be contrasted with the proof by steel and bodlaender , fellows , and warnow that the phylogeny problem is np - complete in general .
when trying to forecast the future behavior of a real - world system , two of the key problems are nonstationarity of the process ( e . g . , regime switching ) and overfitting of the model ( particularly serious for noisy processes ) . this articles shows how gated experts can point to solutions to these problems . the architecture , also called society of experts and mixture of experts consists of a ( nonlinear ) gating network and several ( nonlinear ) competing experts . each expert learns a conditional mean ( as usual ) , but each expert also has its own adaptive width . the gating network learns to assign a probability to each expert that depends on the input . this article first discusses the assumptions underlying this architecture and derives the weight update rules . it then evaluates the performance of gated experts in comparison to that of single networks , as well as to networks with two outputs , one predicting the mean , the other one the local error bar . this article also investigates the ability of gated experts to discover and characterize underlying the regimes . the results are : * there is significantly less overfitting compared to single nets , for two reasons : only subsets of the potential inputs are given to the experts and gating network ( less of a curse of dimensionality ) , and the experts learn to match their variances to the ( local ) noise levels , thus only learning as this article focuses on the architecture and the overfitting problem . applications to a computer - generated toy problem and the laser data from santa fe competition are given in [ mangeas and weigend , #NUM# ] , and the application to the real - world problem of predicting the electricity demand of france are given in [ mangeas et al . , #NUM# ] . much as the data support .
given a set of samples of an unknown probability distribution , we study the problem of constructing a good approximative bayesian network model of the probability distribution in question . this task can be viewed as a search problem , where the goal is to find a maximal probability network model , given the data . in this work , we do not make an attempt to learn arbitrarily complex multi - connected bayesian network structures , since such resulting models can be unsuitable for practical purposes due to the exponential amount of time required for the reasoning task . instead , we restrict ourselves to a special class of simple tree - structured bayesian networks called bayesian prototype trees , for which a polynomial time algorithm for bayesian reasoning exists . we show how the probability of a given bayesian prototype tree model can be evaluated , given the data , and how this evaluation criterion can be used in a stochastic simulated annealing algorithm for searching the model space . the simulated annealing algorithm provably finds the maximal probability model , provided that a sufficient amount of time is used .
in many applications , such as decision support , negotiation , planning , scheduling , etc . , one needs to express requirements that can only be partially satisfied . in order to express such requirements , we propose a technique called forward - tracking . intuitively , forward - tracking is a kind of dual of chronological back - tracking : if a program globally fails to find a solution , then a new execution is started from a program point and a state ` forward ' in the computation tree . this search technique is applied to constraint logic programming , obtaining a powerful extension that preserves all the useful properties of the original scheme . we report on the successful practical application of forward - tracking to the evolutionary training of ( constrained ) neural networks .
bayesian network inference can be formulated as a combinatorial optimization problem , concerning in the computation of an optimal factoring for the distribution represented in the net . since the determination of an optimal factoring is a computationally hard problem , heuristic greedy strategies able to find approximations of the optimal factoring are usually adopted . in the present paper we investigate an alternative approach based on a combination of genetic algorithms ( ga ) and case - based reasoning ( cbr ) . we show how the use of genetic algorithms can improve the quality of the computed factoring in case a static strategy is used ( as for the mpe computation ) , while the combination of ga and cbr can still provide advantages in the case of dynamic strategies . some preliminary results on different kinds of nets are then reported .
this paper attempts to rigorously determine the computation and communication requirements of connectionist algorithms running on a distributed - memory machine . the strategy involves ( #NUM# ) specifying key connectionist algorithms in a high - level object - oriented language , ( #NUM# ) extracting their running times as polynomials , and ( #NUM# ) analyzing these polynomials to determine the algorithms ' space and time complexity . results are presented for various implementations of the back - propagation algorithm [ #NUM# ] .
we present a new adaptive connectionist planning method . by interaction with an environment a world model is progressively constructed using the backpropagation learning algorithm . the planner constructs a look - ahead plan by iteratively using this model to predict future reinforcements . future reinforcement is maximized to derive suboptimal plans , thus determining good actions directly from the knowledge of the model network ( strategic level ) . this is done by gradient descent in action space .
in the proceedings of the conference on uncertainty in artificial intelli - gence ( uai - #NUM# ) , seattle , wa , #NUM# - #NUM# , july #NUM# - #NUM# , #NUM# . technical report r - #NUM# - b april , #NUM# abstract evaluation of counterfactual queries ( e . g . , " if a were true , would c have been true ? " ) is important to fault diagnosis , planning , and determination of liability . in this paper we present methods for computing the probabilities of such queries using the formulation proposed in [ balke and pearl , #NUM# ] , where the antecedent of the query is interpreted as an external action that forces the proposition a to be true . when a prior probability is available on the causal mechanisms governing the domain , counterfactual probabilities can be evaluated precisely . however , when causal knowledge is specified as conditional probabilities on the observables , only bounds can computed . this paper develops techniques for evaluating these bounds , and demonstrates their use in two applications : ( #NUM# ) the determination of treatment efficacy from studies in which subjects may choose their own treatment , and ( #NUM# ) the determination of liability in product - safety litigation .
this research is sponsored in part by the national science foundation under award iri - #NUM# , and by the wright laboratory , aeronautical systems center , air force materiel command , usaf , and the advanced research projects agency ( arpa ) under grant number f #NUM# - #NUM# - #NUM# - #NUM# . the views and conclusions contained in this document are those of the author and should not be interpreted as necessarily representing official policies or endorsements , either expressed or implied , of nsf , wright laboratory or the united states government .
we investigate the application of classification techniques to utility elicitation . in a decision problem , two sets of parameters must generally be elicited : the probabilities and the utilities . while the prior and conditional probabilities in the model do not change from user to user , the utility models do . thus it is necessary to elicit a utility model separately for each new user . elicitation is long and tedious , particularly if the outcome space is large and not decomposable . there are two common approaches to utility function elicitation . the first is to base the determination of the user ' s utility function solely on elicitation of qualitative preferences . the second makes assumptions about the form and decomposability of the utility function . here we take a different approach : we attempt to identify the new user ' s utility function based on classification relative to a database of previously collected utility functions . we do this by identifying clusters of utility functions that minimize an appropriate distance measure . having identified the clusters , we develop a classification scheme that requires many fewer and simpler assessments than full utility elicitation and is more robust than utility elicitation based solely on preferences . we have tested our algorithm on a small database of utility functions in a prenatal diagnosis domain and the results are quite promising .
the standard method for training hidden markov models optimizes a point estimate of the model parameters . this estimate , which can be viewed as the maximum of a posterior probability density over the model parameters , may be susceptible to over - fitting , and contains no indication of parameter uncertainty . also , this maximum may be unrepresentative of the posterior probability distribution . in this paper we study a method in which we optimize an ensemble which approximates the entire posterior probability distribution . the ensemble learning algorithm requires the same resources as the traditional baum - welch algorithm . the traditional training algorithm for hidden markov models is an expectation - maximization ( em ) algorithm ( dempster et al . #NUM# ) known as the baum - welch algorithm . it is a maximum likelihood method , or , with a simple modification , a penalized maximum likelihood method , which can be viewed as maximizing a posterior probability density over the model parameters . recently , hinton and van camp ( #NUM# ) developed a technique known as ensemble learning ( see also mackay ( #NUM# ) for a review ) . whereas maximum a posteriori methods optimize a point estimate of the parameters , in ensemble learning an ensemble is optimized , so that it approximates the entire posterior probability distribution over the parameters . the objective function that is optimized is a variational free energy ( feynman #NUM# ) which measures the relative entropy between the approximating ensemble and the true distribution . in this paper we derive and test an ensemble learning algorithm for hidden markov models , building on neal
this paper considers the design and analysis of adaptive wavelet control algorithms for uncertain nonlinear dynamical systems . the lyapunov synthesis approach is used to develop a state - feedback adaptive control scheme based on nonlinearly parametrized wavelet network models . semi - global stability results are obtained under the key assumption that the system uncertainty satisfies a " matching " condition . the localization properties of adaptive networks are discussed and formal definitions of interference and localization measures are proposed .
temporal difference ( td ) methods constitute a class of methods for learning predictions in multi - step prediction problems , parameterized by a recency factor . currently the most important application of these methods is to temporal credit assignment in reinforcement learning . well known reinforcement learning algorithms , such as ahc or q - learning , may be viewed as instances of td learning . this paper examines the issues of the efficient and general implementation of td ( ) for arbitrary , for use with reinforcement learning algorithms optimizing the discounted sum of rewards . the traditional approach , based on eligibility traces , is argued to suffer from both inefficiency and lack of generality . the ttd ( truncated temporal differences ) procedure is proposed as an alternative , that indeed only approximates td ( ) , but requires very little computation per action and can be used with arbitrary function representation methods . the idea from which it is derived is fairly simple and not new , but probably unexplored so far . encouraging experimental results are presented , suggesting that using & gt ; #NUM# with the ttd procedure allows one to obtain a significant learning speedup at essentially the same cost as usual td ( #NUM# ) learning .
this paper presents first results of an interdisciplinary project in scientific data mining . we analyze data about the carcinogenicity of chemicals derived from the carcinogenesis bioassay program performed by the us national institute of environmental health sciences . the database contains detailed descriptions of #NUM# tests performed with more than #NUM# compounds and animals of different species , strains and sexes . the chemical structures are described at the atom and bond level , and in terms of various relevant structural properties . the goal of this paper is to investigate the effects that various levels of detail and amounts of information have on the resulting hypotheses , both quantitatively and qualitatively . we apply relational and propositional machine learning algorithms to learning problems formulated as regression or as classification tasks . in addition , these experiments have been conducted with two learning problems which are at different levels of detail . quantitatively , our experiments indicate that additional information not necessarily improves accuracy . qualitatively , a number of potential discoveries have been made by the algorithm for relational regression because it can utilize all the information contained in the relations of the database as well as in the numerical dependent variable .
neural networks and bayesian inference provide a useful framework within which to solve regression problems . however their parameterization means that the bayesian analysis of neural networks can be difficult . in this paper , we investigate a method for regression using gaussian process priors which allows exact bayesian analysis using matrix manipulations . we discuss the workings of the method in detail . we will also detail a range of mathematical and numerical techniques that are useful in applying gaussian processes to general problems including efficient approximate matrix inversion methods developed by skilling .
prototypes have been proposed as representation of concepts that are used effectively by humans . developing computational schemes for generating prototypes from examples , however , has proved to be a difficult problem . we present a novel genetic algorithm based prototype learning system , please , for constructing appropriate prototypes from classified training instances . after constructing a set of prototypes for each of the possible classes , the class of a new input instance is determined by the nearest prototype to this instance . attributes are assumed to be ordinal in nature and prototypes are represented as sets of feature - value pairs . a genetic algorithm is used to evolve the number of prototypes per class and their positions on the input space . we present experimental results on a series of artificial problems of varying complexity . please performs competitively with several nearest neighbor classification algorithms on the problem set . an analysis of the strengths and weaknesses of the initial version of our system motivates the need for additional operators . the inclusion of these operators substantially improves the performance of the system on particularly difficult problems .
this paper describes rapture | a system for revising probabilistic knowledge bases that combines connectionist and symbolic learning methods . rapture uses a modified version of backpropagation to refine the certainty factors of a probabilistic rule base and it uses id #NUM# ' s information - gain heuristic to add new rules . results on refining three actual expert knowledge bases demonstrate that this combined approach generally performs better than previous methods .
in open world applications a number of machine - learning techniques may potentially apply to a given learning situation . the research presented here illustrates the complexity involved in automatically choosing an appropriate technique in a multistrategy learning system . it also constitutes a step toward a general computational solution to the learning - strategy selection problem . the approach is to treat learning - strategy selection as a separate planning problem with its own set of goals , as is the case with ordinary problem - solvers . therefore , the management and pursuit of these learning goals becomes a central issue in learning , similar to the goal - management problems associated with traditional planning systems . this paper explores some issues , problems , and possible solutions in such a framework . examples are presented from a multistrategy learning system called meta - aqua .
we present empirical evidence for considering volatility of eurodollar futures as a stochastic process , requiring a generalization of the standard black - scholes ( bs ) model which treats volatility as a constant . we use a previous development of a statistical mechanics of financial markets ( smfm ) to model these issues .
we examine a new approach to modeling uncertainty based on plausibility measures , where a plausibility measure just associates with an event its plausibility , an element is some partially ordered set . this approach is easily seen to generalize other approaches to modeling uncertainty , such as probability measures , belief functions , and possibility measures . the lack of structure in a plausibility measure makes it easy for us to add structure on an as needed basis , letting us examine what is required to ensure that a plausibility measure has certain properties of interest . this gives us insight into the essential features of the properties in question , while allowing us to prove general results that apply to many approaches to reasoning about uncertainty . plausibility measures have already proved useful in analyzing default reasoning . in this paper , we examine their algebraic properties , analogues to the use of + and fi in probability theory . an understanding of such properties will be essential if plausibility measures are to be used in practice as a representation tool .
multiple - instance learning is a way of modeling ambiguity in supervised learning examples . each example is a bag of instances , but only the bag is labeled not the individual instances . a bag is labeled negative if all the instances are negative , and positive if at least one of the instances in positive . we apply the multiple - instance learning framework to the problem of learning how to classify natural images . images are inherently ambiguous since they can represent many different things . a user labels an image as positive if the image somehow contains the concept . each image is a bag , and the instances are various sub - regions in the image . from a small collection of positive and negative examples , we can learn the concept and then use it to retrieve images that contain the concept from a large database . we show that the diverse density algorithm performs well in this task , that simple hypothesis classes are sufficient to classify natural images , and that user interaction helps to improve perfor mance .
this paper is a discussion of the relationship between learning and forgetting . an analysis of the economics of learning is carried out and it is argued that knowledge can sometimes have a negative value . a series of experiments involving a program which learns to traverse state spaces is described . it is shown that most of the knowledge acquired is of negative value even though it is correct and was acquired solving similar problems . it is shown that the value of the knowledge depends on what else is known and that random forgetting can sometimes lead to substantial improvements in performance . it is concluded that research into knowledge acquisition should take seriously the possibility that knowledge may sometimes be harmful . the view is taken that learning and forgetting are complementary processes which construct and maintain useful representations of experience .
we apply a general technique for learning overcomplete bases to the problem of finding efficient image codes . the bases learned by the algorithm are localized , oriented , and bandpass , consistent with earlier results obtained using related methods . we show that the learned bases are gabor - like in structure and that higher degrees of overcompleteness produce greater sampling density in position , orientation , and scale . the efficient coding framework provides a method for comparing different bases objectively by calculating their probability given the observed data or by measuring the entropy of the basis function coefficients . compared to complete and overcomplete fourier and wavelet bases , the learned bases have much better coding efficiency . we demonstrate the improvement in the representation of the learned bases by showing superior performance in image denoising and filling - in of missing pixels .
in this paper the problem of the kolmogorov complexity related to binary strings is faced . we propose a genetic programming approach which consists in evolving a population of lisp programs looking for the optimal program that generates a given string . this evolutionary approach has permited to overcome the intractable space and time difficulties occurring in methods which perform an approximation of the kolmogorov complexity function . the experimental results are quite significant and also show interesting computational strategies so proving the effectiveness of the implemented technique .
most learning algorithms work most effectively when their training data contain completely specified labeled samples . in many diagnostic tasks , however , the data will include the values of only some of the attributes ; we model this as a blocking process that hides the values of those attributes from the learner . while blockers that remove the values of critical attributes can handicap a learner , this paper instead focuses on blockers that remove only irrelevant attribute values , i . e . , values that are not needed to classify an instance , given the values of the other unblocked attributes . we first motivate and formalize this model of " superfluous - value blocking " , and then demonstrate that these omissions can be useful , by proving that certain classes that seem hard to learn in the general pac model | viz . , decision trees and dnf formulae | are trivial to learn in this setting . we also show that this model can be extended to deal with ( #NUM# ) theory revision ( i . e . , modifying an existing formula ) ; ( #NUM# ) blockers that occasionally include superfluous values or exclude required values ; and ( #NUM# ) other cor ruptions of the training data .
we propose a case - based method of selecting behavior sets as an addition to traditional reactive robotic control systems . the new system ( acbarr | a case based reactive robotic system ) provides more flexible performance in novel environments , as well as overcoming a standard " hard " problem for reactive systems , the box canyon . additionally , acbarr is designed in a manner which is intended to remain as close to pure reactive control as possible . higher level reasoning and memory functions are intentionally kept to a minimum . as a result , the new reasoning does not significantly slow the system down from pure reactive speeds .
when using machine learning techniques for knowledge discovery , output that is comprehensible to a human is as important as predictive accuracy . we introduce a new algorithm , set - gen , that improves the comprehensibility of decision trees grown by standard c #NUM# . #NUM# without reducing accuracy . it does this by using genetic search to select the set of input features c #NUM# . #NUM# is allowed to use to build its tree . we test set - gen on a wide variety of real - world datasets and show that set - gen trees are significantly smaller and reference significantly fewer features than trees grown by c #NUM# . #NUM# without using set - gen . statistical significance tests show that the accuracies of set - gen ' s trees are either not distinguishable from or are more accurate than those of the original c #NUM# . #NUM# trees on all ten datasets tested .
we present a method for automatically determining the structure and the connection weights of a boltzmann machine corresponding to a given bayesian network representation of a probability distribution on a set of discrete variables . the resulting boltzmann machine structure can be implemented efficiently on massively parallel hardware , since the structure can be divided into two separate clusters where all the nodes in one cluster can be updated simultaneously . the updating process of the boltzmann machine approximates a gibbs sampling process of the original bayesian network in the sense that the boltzmann machine converges to the same final state as the gibbs sampler does . the mapping from a bayesian network to a boltzmann machine can be seen as a method for incorporating probabilistic a priori information into a neural network architecture , which can then be trained further with existing learning algorithms .
#NUM# ] dori , d . and tarsi , m . , " a simple algorithm to construct a consistent extension of a partially oriented graph , " computer science department , tel - aviv university . also technical report r - #NUM# , ucla , cognitive systems laboratory , october #NUM# . [ #NUM# ] pearl , j . and wermuth , n . , " when can association graphs admit a causal interpretation ? , " ucla , cognitive systems laboratory , technical report r - #NUM# - l , november #NUM# . [ #NUM# ] verma , t . s . and pearl , j . , " deciding morality of graphs is np - complete , " technical report r - #NUM# , ucla , cognitive systems laboratory , october #NUM# .
a default theory can sanction different , mutually incompatible , answers to certain queries . we can identify each such theory with a set of related credulous theories , each of which produces but a single response to each query , by imposing a total ordering on the defaults . our goal is to identify the credulous theory with optimal " expected accuracy " averaged over the natural distribution of queries in the domain . there are two obvious complications : first , the expected accuracy of a theory depends on the query distribution , which is usually not known . second , the task of identifying the optimal theory , even given that distribution information , is intractable . this paper presents a method , optacc , that side - steps these problems by using a set of samples to estimate the unknown distribution , and by hill - climbing to a local optimum . in particular , given any error and confidence parameters * ; ffi & gt ; #NUM# , optacc produces a theory whose expected accuracy is , with probability at least
given a problem , a case - based reasoning ( cbr ) system will search its case memory and use the stored cases to find the solution , possibly modifying retrieved cases to adapt to the required input specifications . in this paper we introduce a neural network architecture for efficient case - based reasoning . we show how a rigorous bayesian probability propagation algorithm can be implemented as a feedforward neural network and adapted for cbr . in our approach the efficient indexing problem of cbr is naturally implemented by the parallel architecture , and heuristic matching is replaced by a probability metric . this allows our cbr to perform theoretically sound bayesian reasoning . we also show how the probability propagation actually offers a solution to the adaptation problem in a very natural way .
while many trading strategies are based on price prediction , traders in financial markets are typically interested in risk - adjusted performance such as the sharpe ratio , rather than price predictions themselves . this paper introduces an approach which generates a nonlinear strategy that explicitly maximizes the sharpe ratio . it is expressed as a neural network model whose output is the position size between a risky and a risk - free asset . the iterative parameter update rules are derived and compared to alternative approaches . the resulting trading strategy is evaluated and analyzed on both computer - generated data and real world data ( dax , the daily german equity index ) . trading based on sharpe ratio maximization compares favorably to both profit optimization and probability matching ( through cross - entropy optimization ) . the results show that the goal of optimizing out - of - sample risk - adjusted profit can be achieved with this nonlinear approach .
randomized adaptive greedy search , using evolutionary algorithms , offers a powerful and versatile approach to the automated design of neural network architectures for a variety of tasks in artificial intelligence and robotics . in this paper we present results from the evolutionary design of a neuro - controller for a robotic bulldozer . this robot is given the task of clearing an arena littered with boxes by pushing boxes to the sides . through a careful analysis of the evolved networks we show how evolution exploits the design constraints and properties of the environment to produce network structures of high fitness . we conclude with a brief summary of related ongoing research examining the intricate interplay between environment and evolutionary processes in determining the structure and function of the resulting neural architectures .
in this paper , we present a framework for the definition of similarity measures using lattice - valued functions . we show their strengths ( particularly for combining similarity measures ) . then we investigate a particular instantiation of the framework , in which sets are used both to represent objects and to denote degrees of similarity . the paper con cludes by suggesting some generalisations of the findings .
we investigate the solution of constraint - based configuration problems in which the preference function over outcomes is unknown or incompletely specified . the aim is to configure a system , such as a personal computer , so that it will be optimal for a given user . the goal of this project is to develop algorithms that generate the most preferred feasible configuration by posing preference queries to the user . in order to minimize the number and the complexity of preference queries posed to the user , the algorithm reasons about the user ' s preferences while taking into account constraints over the set of feasible configurations . we assume that the user can structure their preferences in a particular way that , while natural in many settings , can be exploited during the optimization process . we also address in a preliminary fashion the trade - offs between computational effort in the solution of a problem and the degree of interaction with the user .
self - selection of input examples on the basis of performance failure is a powerful bias for learning systems . the definition of what constitutes a learning bias , however , has been typically restricted to bias provided by the input language , hypothesis language , and preference criteria between competing concept hypotheses . but if bias is taken in the broader context as any basis that provides a preference for one concept change over another , then the paradigm of failure - driven processing indeed provides a bias . bias is exhibited by the selection of examples from an input stream that are examples of failure ; successful performance is filtered out . we show that the degrees of freedom are less in failure - driven learning than in success - driven learning and that learning is facilitated because of this constraint . we also broaden the definition of failure , provide a novel taxonomy of failure causes , and illustrate the interaction of both in a multistrategy learning system called meta - aqua .
we present a fast algorithm for non - linear dimension reduction . the algorithm builds a local linear model of the data by merging pca with clustering based on a new distortion measure . experiments with speech and image data indicate that the local linear algorithm produces encodings with lower distortion than those built by five layer auto - associative networks . the local linear algorithm is also more than an order of magnitude faster to train .
wavelets are of wide potential use in statistical contexts . the basics of the discrete wavelet transform are reviewed using a filter notation that is useful subsequently in the paper . a ` stationary wavelet transform ' , where the coefficient sequences are not decimated at each stage , is described . two different approaches to the construction of an inverse of the stationary wavelet transform are set out . the application of the stationary wavelet transform as an exploratory statistical method is discussed , together with its potential use in nonparametric regression . a method of local spectral density estimation is developed . this involves extensions to the wavelet context of standard time series ideas such as the periodogram and spectrum . the technique is illustrated by its application to data sets from astronomy and veterinary anatomy .
business users and analysts commonly use spreadsheets and #NUM# d plots to analyze and understand their data . on - line analytical processing ( olap ) provides these users with added flexibility in pivoting data around different attributes and drilling up and down the multi - dimensional cube of aggregations . machine learning researchers , however , have concentrated on hypothesis spaces that are foreign to most users : hyper - planes ( perceptrons ) , neural networks , bayesian networks , decision trees , nearest neighbors , etc . in this paper we advocate the use of decision table classifiers that are easy for line - of - business users to understand . we describe several variants of algorithms for learning decision tables , compare their performance , and describe a visualization mechanism that we have implemented in mineset . the performance of decision tables is comparable to other known algorithms , such as c #NUM# . #NUM# / c #NUM# . #NUM# , yet the resulting classifiers use fewer attributes and are more comprehensible .
the va management services department invests considerably in the collection and assessment of data to inform on hospital and care - area specific levels of quality of care . resulting time series of quality monitors provide information relevant to evaluating patterns of variability in hospital - specific quality of care over time and across care areas , and to compare and assess differences across hospitals . in collaboration with the va management services group we have developed various models for evaluating such patterns of dependencies and combining data across the va hospital system . this paper provides a brief overview of resulting models , some summary examples on three monitor time series , and discussion of data , modelling and inference issues . this work introduces new models for multivariate non - gaussian time series . the framework combines cross - sectional , hierarchical models of the population of hospitals with time series structure to allow and measure time - variations in the associated hierarchical model parameters . in the va study , the within - year components of the models describe patterns of heterogeneity across the population of hospitals and relationships among several such monitors , while the time series components describe patterns of variability through time in hospital - specific effects and their relationships across quality monitors . additional model components isolate unpredictable aspects of variability in quality monitor outcomes , by hospital and care areas . we discuss model assessment , residual analysis and mcmc algorithms developed to fit these models , which will be of interest in related applications in other socio - economic areas .
we report on our development of a high - performance system for neural network and other signal processing applications . we have designed and implemented a vector microprocessor and packaged it as an attached processor for a conventional workstation . we present performance comparisons with commercial workstations on neural network backpropagation training . the spert - ii system demonstrates significant speedups over extensively hand optimization code running on the workstations .
a knowledge - based system uses its database ( a . k . a . its " theory " ) to produce answers to the queries it receives . unfortunately , these answers may be incorrect if the underlying theory is faulty . standard " theory revision " systems use a given set of " labeled queries " ( each a query paired with its correct answer ) to transform the given theory , by adding and / or deleting either rules and / or antecedents , into a related theory that is as accurate as possible . after formally defining the theory revision task , this paper provides both sample and computational complexity bounds for this process . it first specifies the number of labeled queries necessary to identify a revised theory whose error is close to minimal with high probability . it then considers the computational complexity of finding this best theory , and proves that , unless p = n p , no polynomial time algorithm can identify this near - optimal revision , even given the exact distribution of queries , except in the most trivial of situations . it also shows that , except in such trivial situations , no polynomial - time algorithm can produce a theory whose error is even close to ( i . e . , within a particular polynomial factor of ) optimal . these results suggest reasons why theory revision can be more effective than learning from scratch , and also justify many aspects of the standard theory revision systems , including the practice of hill - climbing to a locally - optimal theory , based on a given set of labeled queries .
financial forecasting is an example of a signal processing problem which is challenging due to small sample sizes , high noise , non - stationarity , and non - linearity . neural networks have been very successful in a number of signal processing applications . we discuss fundamental limitations and inherent difficulties when using neural networks for the processing of high noise , small sample size signals . we introduce a new intelligent signal processing method which addresses the difficulties . the method uses conversion into a symbolic representation with a self - organizing map , and grammatical inference with recurrent neural networks . we apply the method to the prediction of daily foreign exchange rates , addressing difficulties with non - stationarity , overfitting , and unequal a priori class probabilities , and we find significant predictability in comprehensive experiments covering #NUM# different foreign exchange rates . the method correctly predicts the direction of change for the next day with an error rate of #NUM# . #NUM# % . the error rate reduces to around #NUM# % when rejecting examples where the system has low confidence in its prediction . the symbolic representation aids the extraction of symbolic knowledge from the recurrent neural networks in the form of deterministic finite state automata . these automata explain the operation of the system and are often relatively simple . rules related to well known behavior such as trend following and mean reversal are extracted .
the problem of how to learn from examples has been studied throughout the history of machine learning , and many successful learning algorithms have been developed . a problem that has received less attention is how to select which algorithm to use for a given learning task . the ability of a chosen algorithm to induce a good generalization depends on how appropriate the model class underlying the algorithm is for the given task . we define an algorithm ' s model class to be the representation language it uses to express a generalization of the examples . supervised learning algorithms differ in their underlying model class and in how they search for a good generalization . given this characterization , it is not surprising that some algorithms find better generalizations for some , but not all tasks . therefore , in order to find the best generalization for each task , an automated learning system must search for the appropriate model class in addition to searching for the best generalization within the chosen class . this thesis proposal investigates the issues involved in automating the selection of the appropriate model class . the presented approach has two facets . firstly , the approach combines different model classes in the form of a model combination decision tree , which allows the best representation to be found for each subconcept of the learning task . secondly , which model class is the most appropriate is determined dynamically using a set of heuristic rules . explicit in each rule are the conditions in which a particular model class is appropriate and if it is not , what should be done next . in addition to describing the approach , this proposal describes how the approach will be evaluated in order to demonstrate that it is both an efficient and effective method for automatic model selection .
this paper presents a new approach to inductive learning that combines aspects of instance - based learning and rule induction in a single simple algorithm . the rise system searches for rules in a specific - to - general fashion , starting with one rule per training example , and avoids some of the difficulties of separate - and - conquer approaches by evaluating each proposed induction step globally , i . e . , through an efficient procedure that is equivalent to checking the accuracy of the rule set as a whole on every training example . classification is performed using a best - match strategy , and reduces to nearest - neighbor if all generalizations of instances were rejected . an extensive empirical study shows that rise consistently achieves higher accuracies than state - of - the - art representatives of its " parent " paradigms ( pebls and cn #NUM# ) , and also outperforms a decision - tree learner ( c #NUM# . #NUM# ) in #NUM# out of #NUM# test domains ( in
most research on machine learning has focused on scenarios in which a learner faces a single , isolated learning task . the lifelong learning framework assumes that the learner encounters a multitude of related learning tasks over its lifetime , providing the opportunity for the transfer of knowledge among these . this paper studies lifelong learning in the context of binary classification . it presents the invariance approach , in which knowledge is transferred via a learned model of the invariances of the domain . results on learning to recognize objects from color images demonstrate superior generalization capabilities if invariances are learned and used to bias subsequent learning .
previous bias shift approaches to predicate invention are not applicable to learning from positive examples only , if a complete hypothesis can be found in the given language , as negative examples are required to determine whether new predicates should be invented or not . one approach to this problem is presented , merlin #NUM# . #NUM# , which is a successor of a system in which predicate invention is guided by sequences of input clauses in sld - refutations of positive and negative examples w . r . t . an overly general theory . in contrast to its predecessor which searches for the minimal finite - state automaton that can generate all positive and no negative sequences , merlin #NUM# . #NUM# uses a technique for inducing hidden markov models from positive sequences only . this enables the system to invent new predicates without being triggered by negative examples . another advantage of using this induction technique is that it allows for incremental learning . experimental results are presented comparing merlin #NUM# . #NUM# with the positive only learning framework of progol #NUM# . #NUM# and comparing the original induction technique with a new version that produces deterministic hidden markov models . the results show that predicate invention may indeed be both necessary and possible when learning from positive examples only as well as it can be beneficial to keep the induced model deterministic .
we present algorithms that learn certain classes of function - free recursive logic programs in polynomial time from equivalence queries . in particular , we show that a single k - ary recursive constant - depth determinate clause is learnable . two - clause programs consisting of one learnable recursive clause and one constant - depth determinate non - recursive clause are also learnable , if an additional " basecase " oracle is assumed . these results immediately imply the pac - learnability of these classes . although these classes of learnable recursive programs are very constrained , it is shown in a companion paper that they are maximally general , in that generalizing either class in any natural way leads to a compu - tationally difficult learning problem . thus , taken together with its companion paper , this paper establishes a boundary of efficient learnability for recursive logic programs .
we present and evaluate two methods for improving the performance of ilp systems . one of them is discretization of numerical attributes , based on fayyad and irani ' s text [ #NUM# ] , but adapted and extended in such a way that it can cope with some aspects of discretization that only occur in relational learning problems ( when indeterminate literals occur ) . the second technique is lookahead . it is a well - known problem in ilp that a learner cannot always assess the quality of a refinement without knowing which refinements will be enabled afterwards , i . e . without looking ahead in the refinement lattice . we present a simple method for specifying when lookahead is to be used , and what kind of lookahead is interesting . both the discretization and lookahead techniques are evaluated experimentally . the results show that both techniques improve the quality of the induced theory , while computational costs are acceptable .
this paper analyses the recently suggested particle approach to filtering time series . we suggest that the algorithm is not robust to outliers for two reasons : the design of the simulators and the use of the discrete support to represent the sequentially updating prior distribution . both problems are tackled in this paper . we believe we have largely solved the first problem and have reduced the order of magnitude of the second . in addition we introduce the idea of stratification into the particle filter which allows us to perform on - line bayesian calculations about the parameters which index the models and maximum likelihood estimation . the new methods are illustrated by using a stochastic volatility model and a time series model of angles .
in this paper we suggest determinations as a representation of knowledge that should be easy to understand . we briefly review determinations , which can be displayed in a tabular format , and their use in prediction , which involves a simple matching process . we describe condet , an algorithm that uses feature selection to construct determinations from training data , augmented by a condensation process that collapses rows to produce simpler structures . we report experiments that show condensation reduces complexity with no loss of accuracy , then discuss condet ' s relation to other work and outline directions for future studies .
recurrent neural networks are complex parametric dynamic systems that can exhibit a wide range of different behavior . we consider the task of grammatical inference with recurrent neural networks . specifically , we consider the task of classifying natural language sentences as grammatical or ungrammatical can a recurrent neural network be made to exhibit the same kind of discriminatory power which is provided by the principles and parameters linguistic framework , or government and binding theory ? we attempt to train a network , without the bifurcation into learned vs . innate components assumed by chomsky , to produce the same judgments as native speakers on sharply grammatical / ungrammatical data . we consider how a recurrent neural network could possess linguistic capability , and investigate the properties of elman , narendra & parthasarathy ( n & p ) and williams & zipser ( w & z ) recurrent networks , and frasconi - gori - soda ( fgs ) locally recurrent networks in this setting . we show that both
instance - based learning techniques typically handle continuous and linear input values well , but often do not handle nominal input attributes appropriately . the value difference metric ( vdm ) was designed to find reasonable distance values between nominal attribute values , but it largely ignores continuous attributes , requiring discretization to map continuous values into nominal values . this paper proposes three new heterogeneous distance functions , called the heterogeneous value difference metric ( hvdm ) , the interpolated value difference metric ( ivdm ) , and the windowed value difference metric ( wvdm ) . these new distance functions are designed to handle applications with nominal attributes , continuous attributes , or both . in experiments on #NUM# applications the new distance metrics achieve higher classification accuracy on average than three previous distance functions on those datasets that have both nominal and continuous attributes .
we propose a new method for variable selection and estimation in cox ' s proportional hazards model . our proposal minimizes the log partial likelihood subject to the sum of the absolute values of the parameters being bounded by a constant . because of the nature of this constraint it tends to produce some coefficients that are exactly zero and hence gives interpretable models . the method is a variation of the " lasso " proposal of tibshirani ( #NUM# ) , designed for the linear regression context . simulations indicate that the lasso can be more accurate than stepwise selection in this setting .
instance - based learning techniques typically handle continuous and linear input values well , but often do not handle nominal input attributes appropriately . the value difference metric ( vdm ) was designed to find reasonable distance values between nominal attribute values , but it largely ignores continuous attributes , requiring discretization to map continuous values into nominal values . this paper proposes three new heterogeneous distance functions , called the heterogeneous value difference metric ( hvdm ) , the interpolated value difference metric ( ivdm ) , and the windowed value difference metric ( wvdm ) . these new distance functions are designed to handle applications with nominal attributes , continuous attributes , or both . in experiments on #NUM# applications the new distance metrics achieve higher classification accuracy on average than three previous distance functions on those datasets that have both nominal and continuous attributes .
research into the utility of non - coding segments , or introns , in genetic - based encodings has shown that they expedite the evolution of solutions in domains by protecting building blocks against destructive crossover . we consider a genetic programming system where non - coding segments can be removed , and the resultant chromosomes returned into the population . this parsimonious repair leads to premature convergence , since as we remove the naturally occurring non - coding segments , we strip away their protective backup feature . we then duplicate the coding segments in the repaired chromosomes , and place the modified chromosomes into the population . the duplication method significantly improves the learning rate in the domain we have considered . we also show that this method can be applied to other domains .
this paper introduces the new operation of restricted iteration creation that automatically genetic programming extends holland ' s genetic algorithm to the task of automatic programming . early work on genetic programming demonstrated that it is possible to evolve a sequence of work - performing steps in a single result - producing branch ( that is , a one - part " main " program ) . the book genetic programming : on the programming of computers by means of natural selection ( koza #NUM# ) describes an extension of holland ' s genetic algorithm in which the genetic population consists of computer programs ( that is , compositions of primitive functions and terminals ) . see also koza and rice ( #NUM# ) . in the most basic form of genetic programming ( where only a single result - producing branch is evolved ) , genetic programming demonstrated the capability to discover a sequence ( as to both its length and its content ) of work - performing steps that is sufficient to produce a satisfactory solution to several problems , including many problems that have been used over the years as benchmarks in machine learning and artificial intelligence . before applying genetic programming to a problem , the user must perform five major preparatory steps , namely identifying the terminals ( inputs ) of the to - be - evolved programs , identifying the primitive functions ( operations ) contained in the to - be - evolved programs , creating the fitness measure for evaluating how well a given program does at solving the problem at hand , choosing certain control parameters ( notably population size and number of generations to be run ) , and determining the termination criterion and method of result designation ( typically the best - so - far individual from the populations produced during the run ) . creates a restricted iteration - performing
the genetic algorithm ( ga ) is a problem solving method that is modelled after the process of natural selection . we are interested in studying a specific aspect of the ga the effect of non - coding segments on ga performance . non - coding segments are segments of bits in an individual that provide no contribution , positive or negative , to the fitness of that individual . previous research on non - coding segments suggests that including these structures in the ga may improve ga performance . understanding when and why this improvement occurs will help us to use the ga to its full potential . in this article , we discuss our hypotheses on non - coding segments and describe the results of our experiments . the experiments may be separated into two categories testing our program on problems from previous related studies , and testing new hypotheses on the effect of non - coding segments .
we previously introduced an exemplar model , named gcm - isw , that exploits a highly flexible weighting scheme . our simulations showed that it records faster learning rates and higher asymptotic accuracies on several artificial categorization tasks than models with more limited abilities to warp input spaces . this paper extends our previous work ; it describes experimental results that suggest human subjects also invoke such highly flexible schemes . in particular , our model provides significantly better fits than models with less flexibility , and we hypothesize that humans selectively weight attributes depending on an item ' s location in the input space . we need more flexible models many theories of human concept learning posit that concepts are represented by prototypes ( reed , #NUM# ) or exemplars ( medin & schaffer , #NUM# ) . prototype models represent concepts by the " best example " or " central tendency " of the concept . #NUM# a new item belongs in a category c if it is relatively similar to c ' s prototype . prototype models are relatively inflexible ; they discard a great deal of information that people use during concept learning ( e . g . , the number of exemplars in a concept ( homa & cultice , #NUM# ) , the variability of features ( fried & holyoak , #NUM# ) , correlations between features ( medin et al . , #NUM# ) , and the particular exemplars used ( whittlesea , #NUM# ) ) . of concept learning
current inductive logic programming systems are limited in their handling of noise , as they employ a greedy covering approach to constructing the hypothesis one clause at a time . this approach also causes difficulty in learning recursive predicates . additionally , many current systems have an implicit expectation that the cardinality of the positive and negative examples reflect the " proportion " of the concept to the instance space . a framework for learning from noisy data and fixed example size is presented . a bayesian heuristic for finding the most probable hypothesis in this general framework is derived . this approach evaluates a hypothesis as a whole rather than one clause at a time . the heuristic , which has nice theoretical properties , is incorporated in an ilp system , lime . experimental results show that lime handles noise better than foil and progol . it is able to learn recursive definitions from noisy data on which other systems do not perform well . lime is also capable of learning from only positive data and also from only negative data .
for over a century , it has been known that damage to the right hemisphere of the brain can cause patients to be unaware of the contralesional side of space . this condition , known as unilateral neglect , represents a collection of clinically related spatial disorders characterized by the failure in free vision to respond , explore , or orient to stimuli predominantly located on the side of space opposite the damaged hemisphere . recent studies using the simple task of line bisection , a conventional diagnostic test , have proved surprisingly revealing with respect to the spatial and attentional impairments involved in neglect . in line bisection , the patient is asked to mark the midpoint of a thin horizontal line on a sheet of paper . neglect patients generally transect far to the right of the center . extensive studies of line bisection have been conducted , manipulating | among other factors | line length , orientation , and position . we have simulated the pattern of results using an existing computational model of visual perception and selective attention called morsel ( mozer , #NUM# ) . morsel has already been used to model data in a related disorder , neglect dyslexia ( mozer & behrmann , #NUM# ) . in this earlier work , morsel was " lesioned " in accordance with the damage we suppose to have occurred in the brains of
this paper overviews a proposed architecture for adaptive parallel logic referred to as asocs ( adaptive self - organizing concurrent system ) . the asocs approach is based on an adaptive network composed of many simple computing elements which operate in a parallel asynchronous fashion . problem specification is given to the system by presenting if - then rules in the form of boolean conjunctions . rules are added incrementally and the system adapts to the changing rule - base . adaptation and data processing form two separate phases of operation . during processing the system acts as a parallel hardware circuit . the adaptation process is distributed amongst the computing elements and efficiently exploits parallelism . adaptation is done in a self - organizing fashion and takes place in time linear with the depth of the network . this paper summarizes the overall asocs concept and overviews three specific architectures .
we describe the design and tuning of a controller for enforcing compliance with a prescribed velocity profile for a rail - based transportation system . this requires following a trajectory , rather than fixed set - points ( as in automobiles ) . we synthesize a fuzzy controller for tracking the velocity profile , while providing a smooth ride and staying within the prescribed speed limits . we use a genetic algorithm to tune the fuzzy controller ' s performance by adjusting its parameters ( the scaling factors and the membership functions ) in a sequential order of significance . we show that this approach results in a controller that is superior to the manually designed one , and with only modest computational effort . this makes it possible to customize automated tuning to a variety of different configurations of the route , the terrain , the power configuration , and the cargo .
the patient - adaptive classifier was compared with a well - established baseline algorithm on six major databases , consisting of over #NUM# million heartbeats . when trained on an initial #NUM# records and tested on an additional #NUM# records , the patient - adaptive algorithm was found to reduce the number of vn errors on one channel by a factor of #NUM# , and the number of nv errors by a factor of #NUM# . we conclude that patient adaptation provides a significant advance in classifying normal vs . ventricular beats for ecg patient monitoring .
this paper presents an experiment comparing a new name - pronunciation system , anapron , with seven existing systems : three state - of - the - art commercial systems ( from bellcore , bell labs , and dec ) , two variants of a machine - learning system ( nettalk ) , and two humans . anapron works by combining rule - based and case - based reasoning . it is based on the idea that it is much easier to improve a rule - based system by adding case - based reasoning to it than by tuning the rules to deal with every exception . in the experiment described here , anapron used a set of rules adapted from mitalk and elementary foreign - language textbooks , and a case library of #NUM# names . with these components | which required relatively little knowledge engineering | anapron was found to perform almost at the level of the commercial systems , and significantly better than the two versions of nettalk . this work may not be copied or reproduced in whole or in part for any commercial purpose . permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following : a notice that such copying is by permission of mitsubishi electric research laboratories of cambridge , massachusetts ; an acknowledgment of the authors and individual contributions to the work ; and all applicable portions of the copyright notice . copying , reproduction , or republishing for any other purpose shall require a license with payment of fee to mitsubishi electric research laboratories . all rights reserved .
this paper is devoted to the problem of learning to predict ordinal ( i . e . , ordered discrete ) classes in an ilp setting . we start with a relational regression algorithm named srt ( structural regression trees ) and study various ways of transforming it into a first - order learner for ordinal classification tasks . combinations of these algorithm variants with several data preprocessing methods are compared on two ilp benchmark data sets to verify the relative strengths and weaknesses of the strategies and to study the trade - off between optimal categorical classification accuracy ( hit rate ) and minimum distance - based error . preliminary results indicate that this is a promising avenue towards algorithms that combine aspects of classification and regression in relational learning .
learning problems in the text processing domain often map the text to a space whose dimensions are the measured features of the text , e . g . , its words . three characteristic properties of this domain are ( a ) very high dimensionality , ( b ) both the learned concepts and the instances reside very sparsely in the feature space , and ( c ) a high variation in the number of active features in an instance . in this work we study three mistake - driven learning algorithms for a typical task of this nature - text categorization . we argue that these algorithms which categorize documents by learning a linear separator in the feature space have a few properties that make them ideal for this domain . we then show that a quantum leap in performance is achieved when we further modify the algorithms to better address some of the specific characteristics of the domain . in particular , we demonstrate ( #NUM# ) how variation in document length can be tolerated by either normalizing feature weights or by using negative weights , ( #NUM# ) the positive effect of applying a threshold range in training , ( #NUM# ) alternatives in considering feature frequency , and ( #NUM# ) the benefits of discarding features while training . overall , we present an algorithm , a variation of littlestone ' s winnow , which performs significantly better than any other algorithm tested on this task using a similar feature set .
we show that networks of relatively realistic mathematical models for biological neurons can in principle simulate arbitrary feedforward sigmoidal neural nets in a way which has previously not been considered . this new approach is based on temporal coding by single spikes ( respectively by the timing of synchronous firing in pools of neurons ) , rather than on the traditional interpretation of analog variables in terms of firing rates . the resulting new simulation is substantially faster and hence more consistent with experimental results about the maximal speed of information processing in cortical neural systems . as a consequence we can show that networks of noisy spiking neurons are " universal approximators " in the sense that they can approximate with regard to temporal coding any given continuous function of several variables . this result holds for a fairly large class of schemes for coding analog variables by firing times of spiking neurons . our new proposal for the possible organization of computations in networks of spiking neurons systems has some interesting consequences for the type of learning rules that would be needed to explain the self - organization of such networks . finally , our fast and noise - robust implementation of sigmoidal neural nets via temporal coding points to possible new ways of implementing feedforward and recurrent sigmoidal neural nets with pulse stream vlsi .
selecting a set of features which is optimal for a given classification task is one of the central problems in machine learning . we address the problem using the flexible and robust filter technique eubafes . eubafes is based on a feature weighting approach which computes binary feature weights and therefore a solution in the feature selection sense and also gives detailed information about feature relevance by continuous weights . moreover the user gets not only one but several potentially optimal feature subsets which is important for filter - based feature selection algorithms since it gives the flexibility to use even complex classifiers by the application of a combined filter / wrapper approach . we applied eubafes on a number of artificial and real world data sets and used radial basis function networks to examine the impact of the feature subsets to classifier accuracy and complexity .
a machine can only learn if it is biased in some way . typically the bias is supplied by hand , for example through the choice of an appropriate set of features . however , if the learning machine is embedded within an environment of related tasks , then it can learn its own bias by learning sufficiently many tasks from the environment [ #NUM# , #NUM# ] . in this paper two models of bias learning ( or equivalently , learning to learn ) are introduced and the main theoretical results presented . the first model is a pac - type model based on empirical process theory , while the second is a hierarchical bayes model .
this paper compares the efficiency of two encoding schemes for artificial neural networks optimized by evolutionary algorithms . direct encoding encodes the weights for an a priori fixed neural network architecture . cellular encoding encodes both weights and the architecture of the neural network . in previous studies , direct encoding and cellular encoding have been used to create neural networks for balancing #NUM# and #NUM# poles attached to a cart on a fixed track . the poles are balanced by a controller that pushes the cart to the left or the right . in some cases velocity information about the pole and cart is provided as an input ; in other cases the network must learn to balance a single pole without velocity information . a careful study of the behavior of these systems suggests that it is possible to balance a single pole with velocity information as an input and without learning to compute the velocity . a new fitness function is introduced that forces the neural network to compute the velocity . by using this new fitness function and tuning the syntactic constraints used with cellular encoding , we achieve a tenfold speedup over our previous study and solve a more difficult problem : balancing two poles when no information about the velocity is provided as input .
demands for applications requiring massive parallelism in symbolic environments have given rebirth to research in models labeled as neura l networks . these models are made up of many simple nodes which are highly interconnected such that computation takes place as data flows amongst the nodes of the network . to present , most models have proposed nodes based on simple analog functions , where inputs are multiplied by weights and summed , the total then optionally being transformed by an arbitrary function at the node . learning in these systems is accomplished by adjusting the weights on the input lines . this paper discusses the use of digital ( boolean ) nodes as a primitive building block in connectionist systems . digital nodes naturally engender new paradigms and mechanisms for learning and processing in connectionist networks . the digital nodes are used as the basic building block of a class of models called asocs ( adaptive self - organizing concurrent systems ) . these models combine massive parallelism with the ability to adapt in a self - organizing fashion . basic features of standard neural network learning algorithms and those proposed using digital nodes are compared and contrasted . the latter mechanisms can lead to vastly improved efficiency for many applications .
many abductive understanding systems explain novel situations by a chaining process that is neutral to explainer needs beyond generating some plausible explanation for the event being explained . this paper examines the relationship of standard models of abductive understanding to the case - based explanation model . in case - based explanation , construction and selection of abductive hypotheses are focused by specific explanations of prior episodes and by goal - based criteria reflecting current information needs . the case - based method is inspired by observations of human explanation of anomalous events during everyday understanding , and this paper focuses on the method ' s contributions to the problems of building good explanations in everyday domains . we identify five central issues , compare how those issues are addressed in traditional and case - based explanation models , and discuss motivations for using the case - based approach to facilitate generation of plausible and useful explanations in domains that are complex and imperfectly un derstood .
previous research has shown that a technique called error - correcting output coding ( ecoc ) can dramatically improve the classification accuracy of supervised learning algorithms that learn to classify data points into one of k #NUM# classes . in this paper , we will extend the technique so that ecoc can also provide class probability information . ecoc is a method of converting k - class supervised learning problem into a large number l of two - class supervised learning problems and then combining the results of these l evaluations . the underlying two - class supervised learning algorithms are assumed to provide l probability estimates . the problem of computing class probabilities is formulated as an over - constrained system of l linear equations . least squares methods are applied to solve these equations . accuracy and reliability of the probability estimates are demonstrated .
in this paper we propose a reactive critic , that is able to respond to changing situations . we will explain why this is usefull in reinforcement learning , where the critic is used to improve the control strategy . we take a problem for which we can derive the solution analytically . this enables us to investigate the relation between the parameters and the resulting approximations of the critic . we will also demonstrate how the reactive critic reponds to changing situations .
quadratic dynamical systems ( qds ) , whose definition extends that of markov chains , are used to model phenomena in a variety of fields like statistical physics and natural evolution . such systems also play a role in genetic algorithms , a widely - used class of heuristics that are notoriously hard to analyze . recently rabinovich et al . took an important step in the study of qds ' s by showing , under some technical assumptions , that such systems converge to a stationary distribution ( similar theorems for markov chains are well - known ) . we show , however , that the following sampling problem for qds ' s is pspace - hard given an initial distribution , produce a random sample from the t ' th generation . the hardness result continues to hold for very restricted classes of qds ' s with very simple initial distributions , thus suggesting that qds ' s are intrinsically more complicated than markov chains .
we consider the problem of learning functions over a fixed distribution . an algorithm by kushilevitz and mansour [ #NUM# ] learns any boolean function over f #NUM# ; #NUM# g n in time polynomial in the l #NUM# - norm of the fourier transform of the function . we show that the km - algorithm is a special case of a more general class of learning algorithms . this is achieved by extending their ideas using representations of finite groups . we introduce some new classes of functions which can be learned using this generalized km algorithm .
standard statistical practice ignores model uncertainty . data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data . this approach ignores the uncertainty in model selection , leading to over - confident inferences and decisions that are more risky than one thinks they are . bayesian model averaging ( bma ) provides a coherent mechanism for accounting for this model uncertainty . several methods for implementing bma have recently emerged . we discuss these methods and present a number of examples . in these examples , bma provides improved out - of - sample predictive performance . we also provide a catalogue of currently available bma software .
with the rapid expansion of machine learning methods and applications , there is a strong need for computer - based interactive tools that support education in this area . the emerald system was developed to provide hands - on experience and an interactive demonstration of several machine learning and discovery capabilities for students in ai and cognitive science , and for ai professionals . the current version of emerald integrates five programs that exhibit different types of machine learning and discovery : learning rules from examples , determining structural descriptions of object classes , inventing conceptual clusterings of entities , predicting sequences of objects , and discovering equations characterizing collections of quantitative and qualitative data . emerald extensively uses color graphic capabilities , voice synthesis , and a natural language representation of the knowledge acquired by the learning programs . each program is presented as a " learning robot , " which has its own " personality , " expressed by its icon , its voice , the comments it generates during the learning process , and the results of learning presented as natural language text and / or voice output . users learn about the capabilities of each " robot " both by being challenged to perform some learning tasks themselves , and by creating their own similar tasks to challenge the " robot . " emerald is an extension of illian , an initial , much smaller version that toured eight major us museums of science , and was seen by over half a million visitors . emerald ' s architecture allows it to incorporate new programs and new capabilities . the system runs on sun workstations , and is available to universities and educational institutions .
the nozzle design associate ( nda ) is a computational environment for the design of jet engine exhaust nozzles for supersonic aircraft . nda may be used either to design new aircraft or to design new nozzles that adapt existing aircraft so they may be reutilized for new missions . nda was developed in a collaboration between computer scientists at rut - gers university and exhaust nozzle designers at general electric aircraft engines and general electric corporate research and development . the nda project has two principal goals : to provide a useful engineering tool for exhaust nozzle design , and to explore fundamental research issues that arise in the application of automated design optimization methods to realistic engineering problems .
we use a simulated evolution search ( genetic programming ) for the automatic synthesis of small iterative machine - language programs . for an integer register machine with an addition instruction as its sole arithmetic operator , we show that genetic programming can produce exact and general multiplication routines by synthesizing the necessary iterative control structures from primitive machine - language instructions . our program representation is a virtual register machine that admits arbitrary control flow . our evolution strategy furthermore does not artificially restrict the synthesis of any control structure ; we only place an upper bound on program evaluation time . a program ' s fitness is the distance between the output produced by a test case and the desired output ( multiplication ) . the test cases exhaustively cover multiplication over a finite subset of the natural numbers ( n #NUM# ) ; yet the derived solutions constitute general multiplication for the positive integers . for this problem , simulated evolution with a two - point crossover operator examines significantly fewer individuals in finding a solution than random search . introduction of a small rate of mutation fur ther increases the number of solutions .
in this paper we present the gp - music system , an interactive system which allows users to evolve short musical sequences using interactive genetic programming , and its extensions aimed at making the system fully automated . the basic gp - system works by using a genetic programming algorithm , a small set of functions for creating musical sequences , and a user interface which allows the user to rate individual sequences . with this user interactive technique it was possible to generate pleasant tunes over runs of #NUM# individuals over #NUM# generations . as the user is the bottleneck in interactive systems , the system takes rating data from a users run and uses it to train a neural network based automatic rater , or auto rater , which can replace the user in bigger runs . using this auto rater we were able to make runs of up to #NUM# generations with #NUM# individuals per generation . the best of run pieces generated by the auto raters were pleasant but were not , in general , as nice as those generated in user interactive runs .
dynamic programming provides a methodology to develop planners and controllers for nonlinear systems . however , general dynamic programming is computationally intractable . we have developed procedures that allow more complex planning and control problems to be solved . we use second order local trajectory optimization to generate locally optimal plans and local models of the value function and its derivatives . we maintain global consistency of the local models of the value function , guaranteeing that our locally optimal plans are actually globally optimal , up to the resolution of our search procedures . learning to do the right thing at each instant in situations that evolve over time is difficult , as the future cost of actions chosen now may not be obvious immediately , and may only become clear with time . value functions are a representational tool that makes the consequences of actions explicit . value functions are difficult to learn directly , but they can be built up from learned models of the dynamics of the world and the cost function . this paper focuses on how fast optimizers that only produce locally optimal answers can play a useful role in speeding up the process of computing or learning a globally optimal value function . consider a system with dynamics x k + #NUM# = f ( x k ; u k ) and a cost function l ( x k ; u k ) , where x is the state of the system and u is a vector of actions or controls . the subscript k serves as a time index , but will be dropped in the equations that follow . a goal of reinforcement learning and optimal control is to find a policy that minimizes the total cost , which is the sum of the costs for each time step . one approach to doing this is to construct an optimal
this paper presents a general framework , learning search - control heuristics for logic programs , which can be used to improve both the efficiency and accuracy of knowledge - based systems expressed as definite - clause logic programs . the approach combines techniques of explanation - based learning and recent advances in inductive logic programming to learn clause - selection heuristics that guide program execution . two specific applications of this framework are detailed : dynamic optimization of prolog programs ( improving efficiency ) and natural language acquisition ( improving accuracy ) . in the area of program optimization , a prototype system , dolphin is able to transform some intractable specifications into polynomial - time algorithms , and outperforms competing approaches in several benchmark speedup domains . a prototype language acquisition system , chill is also described . it is capable of automatically acquiring semantic grammars , which uniformly incorprate syntactic and semantic constraints to parse sentences into case - role representations . initial experiments show that this approach is able to construct accurate parsers which generalize well to novel sentences and significantly outperform previous approaches to learning case - role mapping based on connectionist techniques . planned extensions of the general framework and the specific applications as well as plans for further evaluation are also discussed .
we study on - line generalized linear regression with multidimensional outputs , i . e . , neural networks with multiple output nodes but no hidden nodes . we allow at the final layer transfer functions such as the softmax function that need to consider the linear activations to all the output neurons . we also use a parameterization function which transforms parameter vectors maintained by the algorithm into the actual weights . the on - line algorithm we consider updates the parameters in an additive manner , analogous to the delta rule , but because the actual weights are obtained via the possibly nonlinear parameterization function they may behave in a very different manner . our approach is based on applying the notion of a matching loss function in two different contexts . first , we measure the loss of the algorithm in terms of the loss that matches the transfer function used to produce the outputs . second , the loss function that matches the parameterization function can be used both as a measure of distance between models in motivating the update rule of the algorithm and as a potential function in analyzing its relative performance compared to an arbitrary fixed model . as a result , we have a unified treatment that generalizes earlier results for the gradient descent and exponentiated gradient algorithms to multidimensional outputs , including multiclass logistic regression .
systems for automated design optimization of complex real - world objects can , in principle , be constructed by combining domain - independent numerical codes with existing domain - specific analysis and simulation models . unfortunately , existing " legacy " analysis models are frequently unsuitable for use in automated design . they may crash for large classes of input , be numerically unstable or locally non - smooth , or be highly sensitive to control parameters . direct modification of legacy codes to correct these problems is often rendered infeasible by the high cost of re - validating the modified code . this paper describes an approach to incorporating knowledge - based handling of failures into design optimization systems that does not require code modification , yet allows for fine - grained control of model execution . we have constructed a toolkit for the development of robust design optimization systems that builds " intelligent intercessors " into existing analysis models . these intercessors are compiled from high - level rules to code that is inserted between discretely callable components of the design system . intercessors serve to detect failures ; take corrective action when possible ; and transfer control to an appropriate destination when corrective actions fail . we show that this approach is effective in improving analysis model robustness and design optimization performance in the domain of conceptual design of jet engine nozzles .
in this paper , we study the sample complexity of weak learning . that is , we ask how much data must be collected from an unknown distribution in order to extract a small but significant advantage in prediction . we show that it is important to distinguish between those learning algorithms that output deterministic hypotheses and those that output randomized hypotheses . we prove that in the weak learning model , any algorithm using deterministic hypotheses to weakly learn a class of vapnik - chervonenkis dimension d ( n ) requires ( d ( n ) ) examples . in contrast , when randomized hypotheses are allowed , we show that fi ( #NUM# ) examples suffice in some cases . we then show that there exists an efficient algorithm using deterministic hypotheses that weakly learns against any distribution on a set of size d ( n ) with only o ( d ( n ) #NUM# = #NUM# ) examples . thus for the class of symmetric boolean functions over n variables , where the strong learning sample complexity is fi ( n ) , the sample complexity for weak learning using deterministic hypotheses is ( n ) and o ( n #NUM# = #NUM# ) , and the sample complexity for weak learning using randomized hypotheses is fi ( #NUM# ) . next we prove the existence of classes for which the distribution - free sample size required to obtain a slight advantage in prediction over random guessing is essentially equal to that required to obtain arbitrary accuracy . finally , for a class of small circuits , namely all parity functions of subsets of n boolean variables , we prove a weak learning sample complexity of fi ( n ) . this bound holds even if the weak learning algorithm is allowed to replace random sampling with membership queries , and the target distribution is uniform on f #NUM# ; #NUM# g n . p
convergence results for the em approach to the expectation - maximization ( em ) algorithm is an iterative approach to maximum likelihood parameter estimation . jordan and jacobs ( #NUM# ) recently proposed an em algorithm for the mixture of experts architecture of jacobs , jordan , nowlan and hinton ( #NUM# ) and the hierarchical mixture of experts architecture of jordan and jacobs ( #NUM# ) . they showed empirically that the em algorithm for these architectures yields significantly faster convergence than gradient ascent . in the current paper we provide a theoretical analysis of this algorithm . we show that the algorithm can be regarded as a variable metric algorithm with its searching direction having a positive projection on the gradient of the log likelihood . we also analyze the convergence of the algorithm and provide an explicit expression for the convergence rate . in addition , we describe an acceleration technique that yields a significant speedup in simulation experiments . this report describes research done at the dept . of brain and cognitive sciences , the center for biological and computational learning , and the artificial intelligence laboratory of the massachusetts institute of technology . support for cbcl is provided in part by a grant from the nsf ( asc - #NUM# ) . support for the laboratory ' s artificial intelligence research is provided in part by the advanced research projects agency of the dept . of defense . the authors were supported by a grant from the mcdonnell - pew foundation , by a grant from atr human information processing research laboratories , by a grant from siemens corporation , by by grant iri - #NUM# from the national science foundation , by grant n #NUM# - #NUM# - j - #NUM# from the office of naval research , and by nsf grant ecs - #NUM# to support an initiative in intelligent control at mit . michael i . jordan is a nsf presidential young investigator .
an agent that must learn to act in the world by trial and error faces the reinforcement learning problem , which is quite different from standard concept learning . although good algorithms exist for this problem in the general case , they are often quite inefficient and do not exhibit generalization . one strategy is to find restricted classes of action policies that can be learned more efficiently . this paper pursues that strategy by developing algorithms that can efficiently learn action maps that are expressible in k - dnf . the algorithms are compared with existing methods in empirical trials and are shown to have very good performance .
the aim of this paper is to describe the adapter system , a diagnostic architecture combining case - based reasoning with abductive reasoning and exploiting the adaptation of the solution of old episodes , in order to focus the reasoning process . domain knowledge is represented via a logical model and basic mechanisms , based on abductive reasoning with consistency constraints , have been defined for solving complex diagnostic problems involving multiple faults . the model - based component has been supplemented with a case memory and adaptation mechanisms have been developed , in order to make the diagnostic system able to exploit past experience in solving new cases . a heuristic function is proposed , able to rank the solutions associated to retrieved cases with respect to the adaptation effort needed to transform such solutions into possible solutions for the current case . we will discuss some preliminary experiments showing the validity of the above heuristic and the convenience of solving a new case by adapting a retrieved solution rather than solving the new problem from scratch .
the most commonly used neural network models are not well suited to direct digital implementations because each node needs to perform a large number of operations between floating point values . fortunately , the ability to learn from examples and to generalize is not restricted to networks of this type . indeed , networks where each node implements a simple boolean function ( boolean networks ) can be designed in such a way as to exhibit similar properties . two algorithms that generate boolean networks from examples are presented . the results show that these algorithms generalize very well in a class of problems that accept compact boolean network descriptions . the techniques described are general and can be applied to tasks that are not known to have that characteristic . two examples of applications are presented : image reconstruction and hand - written character recognition .
this paper explores issues involved in implementing robot learning for a challenging dynamic task , using a case study from robot juggling . we use a memory - based local model - ing approach ( locally weighted regression ) to represent a learned model of the task to be performed . statistical tests are given to examine the uncertainty of a model , to optimize its pre diction quality , and to deal with noisy and corrupted data . we develop an exploration algorithm that explicitly deals with prediction accuracy requirements dur ing explo - ration . using all these ingredients in combination with methods from optimal control , our robot achieves fast real - time learning of the task within #NUM# to #NUM# trials .
genetic algorithms have been extensively used in different domains as a means of doing global optimization in a simple yet reliable manner . however , in some realistic engineering design optimization domains it was observed that a simple classical implementation of the ga based on binary encoding and bit mutation and crossover was sometimes inefficient and unable to reach the global optimum . using floating point representation alone does not eliminate the problem . in this paper we describe a way of augmenting the ga with new operators and strategies that take advantage of the structure and properties of such engineering design domains . empirical results ( initially in the domain of conceptual design of supersonic transport aircraft and the domain of high performance supersonic missile inlet design ) demonstrate that the newly formulated ga can be significantly better than the classical ga in terms of efficiency and reliability .
consider estimating the mean vector from data n n ( ; #NUM# i ) with l q norm loss , q #NUM# , when is known to lie in an n - dimensional l p ball , p #NUM# ( #NUM# ; #NUM# ) . for large n , the ratio of minimax linear risk to minimax risk can be arbitrarily large if p & lt ; q . obvious exceptions aside , the limiting ratio equals #NUM# only if p = q = #NUM# . our arguments are mostly indirect , involving a reduction to a univariate bayes minimax problem . when p & lt ; q , simple non - linear co - ordinatewise threshold rules are asymptotically minimax at small signal - to - noise ratios , and within a bounded factor of asymptotic minimaxity in general . our results are basic to a theory of estimation in besov spaces
searching for objects in scenes is a natural task for people and has been extensively studied by psychologists . in this paper we examine this task from a connectionist perspective . computational complexity arguments suggest that parallel feed - forward networks cannot perform this task ef ficiently . one difficulty is that , in order to distinguish the target from distractors , a combination of features must be associated with a single object . often called the binding problem , this requirement presents a serious hurdle for connectionist models of visual processing when multiple objects are present . psychophysical experiments suggest that people use covert visual attention to get around this problem . in this paper we describe a psychologically plausible system which uses a focus of attention mechanism to locate target objects . a strategy that combines top - down and bottom - up information is used to minimize search time . the behavior of the resulting system matches the reaction time behavior of people in several interesting tasks . #NUM# . this paper also appears in the proceedings of the #NUM# th annual conference of the cognitive science society , chicago , #NUM# .
we present an algorithm for inducing recursive clauses using inverse implication ( rather than inverse resolution ) as the underlying generalization method . our approach applies to a class of logic programs similar to the class of primitive recursive functions . induction is performed using a small number of positive examples that need not be along the same resolution path . our algorithm , implemented in a system named crustacean , locates matched lists of generating terms that determine the pattern of decomposition exhibited in the ( target ) recursive clause . our theoretical analysis defines the class of logic programs for which our approach is complete , described in terms characteristic of other ilp approaches . our current implementation is considerably faster than previously reported . we present evidence demonstrating that , given randomly selected inputs , increasing the number of positive examples increases accuracy and reduces the number of outputs . we relate our approach to similar recent work on inducing recursive clauses .
the pursuer - evader ( pe ) game is recognized as an important domain in which to study the coevolution of robust adaptive behavior and protean behavior ( miller and cliff , #NUM# ) . nevertheless , the potential of the game is largely unrealized due to methodological hurdles in coevolutionary simulation raised by pe ; versions of the game that have optimal solutions ( isaacs , #NUM# ) are closed - ended , while other formulations are opaque with respect to their solution space , for the lack of a rigorous metric of agent behavior . this inability to characterize behavior , in turn , obfuscates coevolutionary dynamics . we present a new formulation of pe that affords a rigorous measure of agent behavior and system dynamics . the game is moved from the two - dimensional plane to the one - dimensional bit - string ; at each time step , the evader generates a bit that the pursuer must simultaneously predict . because behavior is expressed as a time series , we can employ information theory to provide quantitative analysis of agent activity . further , this version of pe opens vistas onto the communicative component of pursuit and evasion behavior , providing an open - ended serial communications channel and an open world ( via coevolution ) . results show that subtle changes to our game determine whether it is open - ended , and profoundly affect the viability of arms - race dynamics .
in this paper we describe a self - adjusting algorithm for packet routing in which a reinforcement learning method is embedded into each node of a network . only local information is used at each node to keep accurate statistics on which routing policies lead to minimal routing times . in simple experiments involving a #NUM# - node irregularly - connected network , this learning approach proves superior to routing based on precomputed shortest paths .
we propose a new method for variable selection and estimation in cox ' s proportional hazards model . our proposal minimizes the log partial likelihood subject to the sum of the absolute values of the parameters being bounded by a constant . because of the nature of this constraint it tends to produce some coefficients that are exactly zero and hence gives interpretable models . the method is a variation of the " lasso " proposal of tibshirani ( #NUM# ) , designed for the linear regression context . simulations indicate that the lasso can be more accurate than stepwise selection in this setting .
many of the current artificial neural network systems have serious limitations , concerning accessibility , flexibility , scaling and reliability . in order to go some way to removing these we suggest a reflective neural network architecture . in such an architecture , the modular structure is the most important element . the building - block elements are called " minos ' modules . they perform self - observation and inform on the current level of development , or scope of expertise , within the module . a pandemonium system integrates such submodules so that they work together to handle mapping tasks . network complexity limitations are attacked in this way with the pandemonium problem decomposition paradigm , and both static and dynamic unreliability of the whole pandemonium system is effectively eliminated through the generation and interpretation of confidence and ambiguity measures at every moment during the development of the system . two problem domains are used to test and demonstrate various aspects of our architecture . reliability and quality measures are defined for systems that only answer part of the time . our system achieves better quality values than single networks of larger size for a handwritten digit problem . when both second and third best answers are accepted , our system is left with only #NUM# % error on the test set , #NUM# . #NUM# % better than the best single net . it is also shown how the system can elegantly learn to handle garbage patterns . with the parity problem it is demonstrated how complexity of problems may be decomposed automatically by the system , through solving it with networks of size smaller than a single net is required to be . even when the system does not find a solution to the parity problem , because networks of too small a size are used , the reliability remains around #NUM# - #NUM# % . our pandemonium architecture gives more power and flexibility to the higher levels of a large hybrid system than a single net system can , offering useful information for higher - level feedback loops , through which reliability of answers may be intelligently traded for less reliable but important " intuitional " answers . in providing weighted alternatives and possible generalizations , this architecture gives the best possible service to the larger system of which it will form part .
the demands of rapid response and the complexity of many environments make it difficult to decompose , tune and coordinate reactive behaviors while ensuring consistency . we hypothesize that complex behaviors should be decomposed into separate behaviors resident in separate networks , coordinated through a higher level controller . to explore these issues , we have implemented a neural network architecture as the reactive component of a two layer control system for a simulated race car . by varying the architecture , we tested whether decomposing reactivity into separate behaviors leads to superior overall performance and learning convergence . based on these results , we further modified the architecture to produce a race car that is competitive with publicly available solutions .
supervised classification problems have received considerable attention from the machine learning community . we propose a novel genetic algorithm based prototype learning system , please , for this class of problems . given a set of prototypes for each of the possible classes , the class of an input instance is determined by the prototype nearest to this instance . we assume ordinal attributes and prototypes are represented as sets of feature - value pairs . a genetic algorithm is used to evolve the number of prototypes per class and their positions on the input space as determined by corresponding feature - value pairs . comparisons with c #NUM# . #NUM# on a set of artificial problems of controlled complexity demonstrate the effectiveness of the pro posed system .
this paper compares two methods for refining uncertain knowledge bases using propositional certainty - factor rules . the first method , implemented in the rapture system , employs neural - network training to refine the certainties of existing rules but uses a symbolic technique to add new rules . the second method , based on the one used in the kbann system , initially adds a complete set of potential new rules with very low certainty and allows neural - network training to filter and adjust these rules . experimental results indicate that the former method results in significantly faster training and produces much simpler refined rule bases with slightly greater accuracy .
this paper discusses an approach of constructing new attributes based on decision trees and production rules . it can improve the concepts learned in the form of decision trees by simplifying them and improving their predictive accuracy . in addition , this approach can distinguish relevant primitive attributes from irrelevant primitive attributes .
performance of human subjects in a wide variety of early visual processing tasks improves with practice . hyperbf networks ( poggio and girosi , #NUM# ) constitute a mathematically well - founded framework for understanding such improvement in performance , or perceptual learning , in the class of tasks known as visual hyperacuity . the present article concentrates on two issues raised by the recent psychophysical and computational findings reported in ( poggio et al . , #NUM# b ; fahle and edelman , #NUM# ) . first , we develop a biologically plausible extension of the hyperbf model that takes into account basic features of the functional architecture of early vision . second , we explore various learning modes that can coexist within the hyperbf framework and focus on two unsupervised learning rules which may be involved in hyperacuity learning . finally , we report results of psychophysical experiments that are consistent with the hypothesis that activity - dependent presynaptic amplification may be involved in perceptual learning in hyperacuity .
we describe the results of performing data mining on a challenging medical diagnosis domain , acute abdominal pain . this domain is well known to be difficult , yielding little more than #NUM# % predictive accuracy for most human and machine diagnosticians . moreover , many researchers argue that one of the simplest approaches , the naive bayesian classifier , is optimal . by comparing the performance of the naive bayesian classifier to its more general cousin , the bayesian network classifier , and to selective bayesian classifiers with just #NUM# % of the total attributes , we show that the simplest models perform at least as well as the more complex models . we argue that simple models like the selective naive bayesian classifier will perform as well as more complicated models for similarly complex domains with relatively small data sets , thereby calling into question the extra expense necessary to induce more complex models .
this report describes statistical research and development work on hospital quality monitor data sets from the nationwide va hospital system . the project covers statistical analysis , exploration and modelling of data from several quality monitors , with the primary goals of : ( a ) understanding patterns of variability over time in hospital - level and monitor area specific quality monitor measures , and ( b ) understanding patterns of dependencies between sets of monitors . we present discussion of basic perspectives on data structure and preliminary data exploration for three monitors , followed by developments of several classes of formal models . we identify classes of hierarchical random effects time series models to be of relevance in mod - elling single or multiple monitor time series . we summarise basic model features and results of analyses of the three monitor data sets , in both single and multiple monitor frameworks , and present a variety of summary inferences in graphical displays . our discussion includes summary conclusions related to the two key goals , discussions of questions of comparisons across hospitals , and some recommendations about further potential substantive and statistical investigations .
adaptive ridge is a special form of ridge regression , balancing the quadratic penalization on each parameter of the model . this paper shows the equivalence between adaptive ridge and lasso ( least absolute shrinkage and selection operator ) . this equivalence states that both procedures produce the same estimate . least absolute shrinkage can thus be viewed as a particular quadratic penalization . from this observation , we derive an em algorithm to compute the lasso solution . we finally present a series of applications of this type of algorithm in regres sion problems : kernel regression , additive modeling and neural net training .
gaussian processes are a natural way of defining prior distributions over functions of one or more input variables . in a simple nonparametric regression problem , where such a function gives the mean of a gaussian distribution for an observed response , a gaussian process model can easily be implemented using matrix computations that are feasible for datasets of up to about a thousand cases . hyperparameters that define the covariance function of the gaussian process can be sampled using markov chain methods . regression models where the noise has a t distribution and logistic or probit models for classification applications can be implemented by sampling as well for latent values underlying the observations . software is now available that implements these methods using covariance functions with hierarchical parameterizations . models defined in this way can discover high - level properties of the data , such as which inputs are relevant to predicting the response .
simulated annealing | moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions | has traditionally been used as an inexact method of handling isolated modes in markov chain samplers . here , it is shown how one can use the markov chain transitions for such an annealing sequence to define an importance sampler . the markov chain aspect allows this method to perform acceptably even for high - dimensional problems , where finding good importance sampling distributions would otherwise be very difficult , while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases . this annealed importance sampling procedure resembles the second half of the previously - studied tempered transitions , and can be seen as a generalization of a recently - proposed variant of sequential importance sampling . it is also related to thermodynamic integration methods for estimating ratios of normalizing constants . annealed importance sampling is most attractive when isolated modes are present , or when estimates of normalizing constants are required , but it may also be more generally useful , since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in markov chain samplers .
inversion of multilayer synchronous networks is a method which tries to answer questions like " what kind of input will give a desired output ? " or " is it possible to get a desired output ( under special input / output constraints ) ? " . we will describe two methods of inverting a connectionist network . firstly , we extend inversion via backpropagation ( linden / kindermann [ #NUM# ] , williams [ #NUM# ] ) to recurrent ( el - man [ #NUM# ] , jordan [ #NUM# ] , mozer [ #NUM# ] , williams / zipser [ #NUM# ] ) , time - delayed ( waibel at al . [ #NUM# ] ) and discrete versions of continuous networks ( pineda [ #NUM# ] , pearlmutter [ #NUM# ] ) . the result of inversion is an input vector . the corresponding output vector is equal to the target vector except a small remainder . the knowledge of those attractors may help to understand the function and the generalization qualities of connectionist systems of this kind . secondly , we introduce a new inversion method for proving the non - existence of an input combination under special constraints , e . g . in a subspace of the input space . this method works by iterative exclusion of invalid activation values . it might be a helpful way to judge the properties of a trained network . we conclude with simulation results of three different tasks : xor , morse signal decoding and handwritten digit recognition .
in this paper we study learning algorithms for environments which are changing over time . unlike most previous work , we are interested in the case where the changes might be rapid but their " direction " is relatively constant . we model this type of change by assuming that the target distribution is changing continuously at a constant rate from one extreme distribution to another . we show in this case how to use a simple weighting scheme to estimate the error of an hypothesis , and using this estimate , to minimize the error of the prediction .
chaque parametre du modele est penalise individuellement . le reglage de ces penalisations se fait automatiquement a partir de la definition d ' un hyperparametre de regularisation globale . cet hyperparametre , qui controle la complexite du regresseur , peut ^ etre estime par des techniques de reechantillonnage . nous montrons experimentalement les performances et la stabilite de la penalisation multiple adaptative dans le cadre de la regression lineaire . nous avons choisi des problemes pour lesquels le probleme du controle de la complexite est particulierement crucial , comme dans le cadre plus general de l ' estimation fonctionnelle . les comparaisons avec les moindres carres regularises et la selection de variables nous permettent de deduire les conditions d ' application de chaque algorithme de penalisation . lors des simulations , nous testons egalement plusieurs techniques de reechantillonnage . ces techniques sont utilisees pour selectionner la complexite optimale des estimateurs de la fonction de regression . nous comparons les pertes occasionnees par chacune d ' entre elles lors de la selection de modeles sous - optimaux . nous regardons egalement si elles permettent de determiner l ' estimateur de la fonction de regression minimisant l ' erreur en generalisation parmi les differentes methodes de penalisation en competition .
the crossover operator is common to most implementations of genetic programming ( gp ) . another , usually unavoidable , factor is some form of restriction on the size of trees in the gp population . this paper concentrates on the interaction between the crossover operator and a restriction on tree depth demonstrated by the max problem , which involves returning the largest possible value for given function and terminal sets .
we propose a model of efficient on - line reinforcement learning based on the expected mistake bound framework introduced by haussler , littlestone and warmuth ( #NUM# ) . the measure of performance we use is the expected difference between the total reward received by the learning agent and that received by an agent behaving optimally from the start . we call this expected difference the cumulative mistake of the agent and we require that it " levels off " at a reasonably fast rate as the learning progresses . we show that this model is polynomially equivalent to the pac model of off - line reinforcement learning introduced in ( fiechter , #NUM# ) . in particular we show how an off - line pac reinforcement learning algorithm can be transformed into an efficient on - line algorithm in a simple and practical way . an immediate consequence of this result is that the pac algorithm for the general finite state - space reinforcement learning problem described in ( fiechter , #NUM# ) can be transformed into a polynomial on - line al gorithm with guaranteed performances .
this paper presents the system why , which learns and updates a diagnostic knowledge base using domain knowledge and a set of examples . the a - priori knowledge consists of a causal model of the domain , stating the relationships among basic phenomena , and a body of phenomenological theory , describing the links between abstract concepts and their possible manifestations in the world . the phenomenological knowledge is used deductively , the causal model is used abductively and the examples are used inductively . the problems of imperfection and intractability of the theory are handled by allowing the system to make assumptions during its reasoning . in this way , robust knowledge can be learned with limited complexity and limited number of examples . the system works in a first order logic environment and has been applied in a real domain .
this paper investigates the behaviour of the random walk metropolis algorithm in high dimensional problems . here we concentrate on the case where the components in the target density is a spatially homogeneous gibbs distribution with finite range . the performance of the algorithm is strongly linked to the presence or absence of phase transition for the gibbs distribution ; the convergence time being approximately linear in dimension for problems where phase transition is not present . related to this , there is an optimal way to scale the variance of the proposal distribution in order to maximise the speed of convergence of the algorithm . this turns out to involve scaling the variance of the proposal as the reciprocal of dimension ( at least in the phase transition free case ) . moreover the actual optimal scaling can be characterised in terms of the overall acceptance rate of the algorithm , the maximising value being #NUM# : #NUM# , the value as predicted by studies on simpler classes of target density . the results are proved in the framework of a weak convergence result , which shows that the algorithm actually behaves like an infinite dimensional diffusion process in high dimensions . #NUM# . introduction and discussion of results
this paper develops probabilistic bounds on out - of - sample error rates for several classifiers using a single set of in - sample data . the bounds are based on probabilities over partitions of the union of in - sample and out - of - sample data into in - sample and out - of - sample data sets . the bounds apply when in - sample and out - of - sample data are drawn from the same distribution . partition - based bounds are stronger than vc - type bounds , but they require more computation .
we present a framework for learning dfa from simple examples . we show that efficient pac learning of dfa is possible if the class of distributions is restricted to simple distributions where a teacher might choose examples based on the knowledge of the target concept . this answers an open research question posed in pitt ' s seminal paper : are dfa ' s pac - identifiable if examples are drawn from the uniform distribution , or some other known simple distribution ? our approach uses the rpni algorithm for learning dfa from labeled examples . in particular , we describe an efficient learning algorithm for exact learning of the target dfa with high probability when a bound on the number of states ( n ) of the target dfa is known in advance . when n is not known , we show how this algorithm can be used for efficient pac learning of dfas .
this paper deals with the combination of evolutionary algorithms and artificial neural networks ( ann ) . a new method is presented , to find good building - blocks for architectures of artificial neural networks . the method is based on cellular encoding , a representation scheme by f . gruau , and on genetic programming by j . koza . first it will be shown that a modified cellular encoding technique is able to find good architectures even for non - boolean networks . with the help of a graph - database and a new graph - rewriting method , it is secondly possible to build architectures from modular structures . the information about building - blocks for architectures is obtained by statistically analyzing the data in the graph - database . simulation results for two real world problems are given .
we have attempted to obtain a stronger correlation between the relationship between g #NUM# and g #NUM# and performance . this has included studying the variance in the fitnesses of the members of the population , as well as observing the rate of convergence of the gp with respect to g #NUM# when a population was evolved for g #NUM# . #NUM# unfortunately , we have not yet been able to obtain a significant correlation . in future work , we plan to to track the genetic diversity ( we have only considered phenotypic variance so far ) of populations in order to shed some light on the underlying mechanism for priming . one factor that has made this analysis difficult so far is our use of genetic programming , for which the space of genotypes is very large , ( i . e . , there are many redundant solutions ) , and for which the neighborhood structure is less easily intuited than that of a standard genetic algorithm . since there is every reason to believe that the underlying mechanism of incremental evolution is largely independent of the peculiarities of genetic programming , we are currently investigating the incremental evolution mechanism using genetic algorithms with fixed - length genotypes . this should enable a better understanding of the mechanism . ultimately , we will scale up this research effort to analyze incremental evolution with more than one transition between test cases . this will involve many open issues regarding the optimization of the transition schedule between test cases . #NUM# we performed the following experiment : let f it ( i ; g ) be the fitness value of a genetic program i according to the evaluation function g , and best of ( p op ; t ; g ) be the member i fl of population p op at time t with highest fitness according to g | in other words , i fl = best of ( p op ; t ; g ) maximizes f it ( i ; g ) over all i #NUM# p op . a population p op #NUM# was evolved in the usual manner using evaluation function g #NUM# for t = #NUM# generations . however , at each generation #NUM# i #NUM# we also evaluated the current population using evaluation function g #NUM# , and recorded the value of f it ( best of ( p op ; i ; g #NUM# ) ; g #NUM# ) . in other words , we evolved the population using g #NUM# as the evaluation function , but at every generation we also computed the fitness of the best individual in the population according to g #NUM# and saved this value . using the same random seed and control parameters , we then evolved a population p op #NUM# for t = #NUM# generations using g #NUM# as the evaluation function ( note that at generation #NUM# , p op #NUM# is identical to p op #NUM# ) . for all values of t , we compared f it ( best of ( p op #NUM# ; t ; g #NUM# ) ; g #NUM# ) with f it ( best of ( p op #NUM# ; t ; g #NUM# ) ; g #NUM# ) . in order to better formalize and exploit this notion of domain difficulty .
genetic programming is very computationally expensive . for most applications , the vast majority of time is spent evaluating candidate solutions , so it is desirable to make individual evaluation as efficient as possible . we describe a genome compiler which compiles s - expressions to machine code , resulting in significant speedup of individual evaluations over standard gp systems . based on performance results with symbolic regression , we show that the execution of the genome compiler system is comparable to the fastest alternative gp systems . we also demonstrate the utility of compilation on a real - world problem , lossless image compression . a somewhat surprising result is that in our test domains , the overhead of compilation is negligible .
the crossover operator is common to most implementations of genetic programming ( gp ) . another , usually unavoidable , factor is some form of restriction on the size of trees in the gp population . this paper concentrates on the interaction between the crossover operator and a restriction on tree depth demonstrated by the max problem , which involves returning the largest possible value for given function and terminal sets .
design rationale is a record of design activity : of alternatives available , choices made , the reasons for them , and explanations of how a proposed design is intended to work . we describe a representation called the functional representation ( fr ) that has been used to represent how a device ' s functions arise causally from the functions of its components and their interconnections . we propose that fr can provide the basis for capturing the causal aspects of the design rationale . we briefly discuss the use of fr for a number of tasks in which we would expect the design rationale to be useful : generation of diagnostic knowledge , design verification and redesign .
we present a neural network - based face detection system . a retinally connected neural network examines small windows of an image , and decides whether each window contains a face . the system arbitrates between multiple networks to improve performance over a single network . we use a bootstrap algorithm for training , which adds false detections into the training set as training progresses . this eliminates the difficult task of manually selecting non - face training examples , which must be chosen to span the entire space of non - face images . comparisons with another state - of - the - art face detection system are presented ; our system has better performance in terms of detection and false - positive rates .
